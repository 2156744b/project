-- number of partitions when shuffling data for aggregates and joins
--set spark.sql.shuffle.partitions=1024;

DROP TABLE q11_important_stock_par_spark;
DROP TABLE q11_part_tmp_par_spark;

-- create the target table
create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet;
create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet;

-- the query
insert into table q11_part_tmp_par_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_par_spark
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par_spark) sum_tmp
        join q11_part_tmp_par_spark
where part_value > total_value * 0.0001
order by value desc;

Spark assembly has been built with Hive, including Datanucleus jars on classpath
15/08/06 17:33:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/06 17:33:32 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/06 17:33:32 INFO metastore: Connected to metastore.
15/08/06 17:33:33 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/06 17:33:33 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/06 17:33:33 INFO SecurityManager: Changing view acls to: hive
15/08/06 17:33:33 INFO SecurityManager: Changing modify acls to: hive
15/08/06 17:33:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/06 17:33:33 INFO Slf4jLogger: Slf4jLogger started
15/08/06 17:33:33 INFO Remoting: Starting remoting
15/08/06 17:33:34 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@sandbox.hortonworks.com:57294]
15/08/06 17:33:34 INFO Utils: Successfully started service 'sparkDriver' on port 57294.
15/08/06 17:33:34 INFO SparkEnv: Registering MapOutputTracker
15/08/06 17:33:34 INFO SparkEnv: Registering BlockManagerMaster
15/08/06 17:33:34 INFO DiskBlockManager: Created local directory at /tmp/spark-d2e0dd6f-3e69-4fe6-8937-498059747ae0/spark-4e315e0b-d25e-4fc0-a2c3-868fe6ff1e25
15/08/06 17:33:34 INFO MemoryStore: MemoryStore started with capacity 3.1 GB
15/08/06 17:33:34 INFO HttpFileServer: HTTP File server directory is /tmp/spark-514a02a7-9c0a-4b24-9548-e81076c1a3a9/spark-43e0019a-97f1-47c4-8201-e54db0f30047
15/08/06 17:33:34 INFO HttpServer: Starting HTTP Server
15/08/06 17:33:34 INFO Utils: Successfully started service 'HTTP file server' on port 38816.
15/08/06 17:33:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/06 17:33:34 INFO SparkUI: Started SparkUI at http://sandbox.hortonworks.com:4040
15/08/06 17:33:34 INFO Executor: Starting executor ID <driver> on host localhost
15/08/06 17:33:35 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@sandbox.hortonworks.com:57294/user/HeartbeatReceiver
15/08/06 17:33:35 INFO NettyBlockTransferService: Server created on 42931
15/08/06 17:33:35 INFO BlockManagerMaster: Trying to register BlockManager
15/08/06 17:33:35 INFO BlockManagerMasterActor: Registering block manager localhost:42931 with 3.1 GB RAM, BlockManagerId(<driver>, localhost, 42931)
15/08/06 17:33:35 INFO BlockManagerMaster: Registered BlockManager
SET spark.sql.hive.version=0.13.1
15/08/06 17:33:36 INFO ParseDriver: Parsing command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:33:36 INFO ParseDriver: Parse Completed
15/08/06 17:33:37 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:33:37 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO ParseDriver: Parsing command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:33:37 INFO ParseDriver: Parse Completed
15/08/06 17:33:37 INFO PerfLogger: </PERFLOG method=parse start=1438882417729 end=1438882417730 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO Driver: Semantic Analysis Completed
15/08/06 17:33:37 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438882417730 end=1438882417906 duration=176 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:33:37 INFO PerfLogger: </PERFLOG method=compile start=1438882417692 end=1438882417922 duration=230 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO Driver: Starting command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:33:37 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438882417683 end=1438882417950 duration=267 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:37 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: </PERFLOG method=runTasks start=1438882417950 end=1438882418151 duration=201 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438882417923 end=1438882418152 duration=229 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:33:38 INFO Driver: OK
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438882418152 end=1438882418152 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: </PERFLOG method=Driver.run start=1438882417683 end=1438882418152 duration=469 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:33:38 INFO DAGScheduler: Got job 0 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:33:38 INFO DAGScheduler: Final stage: Stage 0(collect at SparkPlan.scala:84)
15/08/06 17:33:38 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:33:38 INFO DAGScheduler: Missing parents: List()
15/08/06 17:33:38 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[2] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:33:38 INFO MemoryStore: ensureFreeSpace(1896) called with curMem=0, maxMem=3333968363
15/08/06 17:33:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1896.0 B, free 3.1 GB)
15/08/06 17:33:38 INFO MemoryStore: ensureFreeSpace(1208) called with curMem=1896, maxMem=3333968363
15/08/06 17:33:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1208.0 B, free 3.1 GB)
15/08/06 17:33:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:42931 (size: 1208.0 B, free: 3.1 GB)
15/08/06 17:33:38 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/08/06 17:33:38 INFO DefaultExecutionContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:38 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[2] at map at SparkPlan.scala:84)
15/08/06 17:33:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/08/06 17:33:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:33:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/06 17:33:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 618 bytes result sent to driver
15/08/06 17:33:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 86 ms on localhost (1/1)
15/08/06 17:33:38 INFO DAGScheduler: Stage 0 (collect at SparkPlan.scala:84) finished in 0.106 s
15/08/06 17:33:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/06 17:33:38 INFO DAGScheduler: Job 0 finished: collect at SparkPlan.scala:84, took 0.545349 s
Time taken: 2.966 seconds
15/08/06 17:33:38 INFO CliDriver: Time taken: 2.966 seconds
15/08/06 17:33:38 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7458d255
15/08/06 17:33:38 INFO StatsReportListener: task runtime:(count: 1, mean: 86.000000, stdev: 0.000000, max: 86.000000, min: 86.000000)
15/08/06 17:33:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:38 INFO StatsReportListener: 	86.0 ms	86.0 ms	86.0 ms	86.0 ms	86.0 ms	86.0 ms	86.0 ms	86.0 ms	86.0 ms
15/08/06 17:33:38 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:33:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:38 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:33:38 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 20.930233, stdev: 0.000000, max: 20.930233, min: 20.930233)
15/08/06 17:33:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:38 INFO StatsReportListener: 	21 %	21 %	21 %	21 %	21 %	21 %	21 %	21 %	21 %
15/08/06 17:33:38 INFO StatsReportListener: other time pct: (count: 1, mean: 79.069767, stdev: 0.000000, max: 79.069767, min: 79.069767)
15/08/06 17:33:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:38 INFO StatsReportListener: 	79 %	79 %	79 %	79 %	79 %	79 %	79 %	79 %	79 %
15/08/06 17:33:38 INFO ParseDriver: Parsing command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:33:38 INFO ParseDriver: Parse Completed
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO ParseDriver: Parsing command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:33:38 INFO ParseDriver: Parse Completed
15/08/06 17:33:38 INFO PerfLogger: </PERFLOG method=parse start=1438882418953 end=1438882418953 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO Driver: Semantic Analysis Completed
15/08/06 17:33:38 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438882418953 end=1438882418978 duration=25 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:33:38 INFO PerfLogger: </PERFLOG method=compile start=1438882418952 end=1438882418979 duration=27 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO Driver: Starting command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:33:38 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438882418952 end=1438882418982 duration=30 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:38 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=runTasks start=1438882418983 end=1438882419066 duration=83 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438882418979 end=1438882419066 duration=87 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:33:39 INFO Driver: OK
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438882419066 end=1438882419066 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=Driver.run start=1438882418952 end=1438882419066 duration=114 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:33:39 INFO DAGScheduler: Got job 1 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:33:39 INFO DAGScheduler: Final stage: Stage 1(collect at SparkPlan.scala:84)
15/08/06 17:33:39 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:33:39 INFO DAGScheduler: Missing parents: List()
15/08/06 17:33:39 INFO DAGScheduler: Submitting Stage 1 (MappedRDD[5] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:33:39 INFO MemoryStore: ensureFreeSpace(1896) called with curMem=3104, maxMem=3333968363
15/08/06 17:33:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 1896.0 B, free 3.1 GB)
15/08/06 17:33:39 INFO MemoryStore: ensureFreeSpace(1207) called with curMem=5000, maxMem=3333968363
15/08/06 17:33:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1207.0 B, free 3.1 GB)
15/08/06 17:33:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:42931 (size: 1207.0 B, free: 3.1 GB)
15/08/06 17:33:39 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/08/06 17:33:39 INFO DefaultExecutionContext: Created broadcast 1 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:39 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1 (MappedRDD[5] at map at SparkPlan.scala:84)
15/08/06 17:33:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/08/06 17:33:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:33:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/08/06 17:33:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 618 bytes result sent to driver
15/08/06 17:33:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 25 ms on localhost (1/1)
15/08/06 17:33:39 INFO DAGScheduler: Stage 1 (collect at SparkPlan.scala:84) finished in 0.031 s
15/08/06 17:33:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/06 17:33:39 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@cd09251
15/08/06 17:33:39 INFO DAGScheduler: Job 1 finished: collect at SparkPlan.scala:84, took 0.066829 s
Time taken: 0.318 seconds
15/08/06 17:33:39 INFO CliDriver: Time taken: 0.318 seconds
15/08/06 17:33:39 INFO StatsReportListener: task runtime:(count: 1, mean: 25.000000, stdev: 0.000000, max: 25.000000, min: 25.000000)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	25.0 ms	25.0 ms	25.0 ms	25.0 ms	25.0 ms	25.0 ms	25.0 ms	25.0 ms	25.0 ms
15/08/06 17:33:39 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:33:39 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 8.000000, stdev: 0.000000, max: 8.000000, min: 8.000000)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	 8 %	 8 %	 8 %	 8 %	 8 %	 8 %	 8 %	 8 %	 8 %
15/08/06 17:33:39 INFO StatsReportListener: other time pct: (count: 1, mean: 92.000000, stdev: 0.000000, max: 92.000000, min: 92.000000)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	92 %	92 %	92 %	92 %	92 %	92 %	92 %	92 %	92 %
15/08/06 17:33:39 INFO ParseDriver: Parsing command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:33:39 INFO ParseDriver: Parse Completed
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO ParseDriver: Parsing command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:33:39 INFO ParseDriver: Parse Completed
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=parse start=1438882419356 end=1438882419357 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO SemanticAnalyzer: Starting Semantic Analysis
15/08/06 17:33:39 INFO SemanticAnalyzer: Creating table q11_important_stock_par_spark position=13
15/08/06 17:33:39 INFO Driver: Semantic Analysis Completed
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438882419358 end=1438882419404 duration=46 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=compile start=1438882419356 end=1438882419405 duration=49 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO Driver: Starting command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438882419355 end=1438882419407 duration=52 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=runTasks start=1438882419407 end=1438882419463 duration=56 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438882419405 end=1438882419464 duration=59 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:33:39 INFO Driver: OK
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438882419464 end=1438882419464 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=Driver.run start=1438882419355 end=1438882419464 duration=109 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:33:39 INFO DAGScheduler: Got job 2 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:33:39 INFO DAGScheduler: Final stage: Stage 2(collect at SparkPlan.scala:84)
15/08/06 17:33:39 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:33:39 INFO DAGScheduler: Missing parents: List()
15/08/06 17:33:39 INFO DAGScheduler: Submitting Stage 2 (MappedRDD[8] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:33:39 INFO MemoryStore: ensureFreeSpace(2560) called with curMem=6207, maxMem=3333968363
15/08/06 17:33:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 3.1 GB)
15/08/06 17:33:39 INFO MemoryStore: ensureFreeSpace(1562) called with curMem=8767, maxMem=3333968363
15/08/06 17:33:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1562.0 B, free 3.1 GB)
15/08/06 17:33:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:42931 (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:33:39 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/08/06 17:33:39 INFO DefaultExecutionContext: Created broadcast 2 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:39 INFO DAGScheduler: Submitting 1 missing tasks from Stage 2 (MappedRDD[8] at map at SparkPlan.scala:84)
15/08/06 17:33:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/06 17:33:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:33:39 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
15/08/06 17:33:39 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 618 bytes result sent to driver
15/08/06 17:33:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 34 ms on localhost (1/1)
15/08/06 17:33:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/06 17:33:39 INFO DAGScheduler: Stage 2 (collect at SparkPlan.scala:84) finished in 0.043 s
15/08/06 17:33:39 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3f04e6cb
15/08/06 17:33:39 INFO DAGScheduler: Job 2 finished: collect at SparkPlan.scala:84, took 0.069765 s
15/08/06 17:33:39 INFO StatsReportListener: task runtime:(count: 1, mean: 34.000000, stdev: 0.000000, max: 34.000000, min: 34.000000)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	34.0 ms	34.0 ms	34.0 ms	34.0 ms	34.0 ms	34.0 ms	34.0 ms	34.0 ms	34.0 ms
15/08/06 17:33:39 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:33:39 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 26.470588, stdev: 0.000000, max: 26.470588, min: 26.470588)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	26 %	26 %	26 %	26 %	26 %	26 %	26 %	26 %	26 %
Time taken: 0.415 seconds
15/08/06 17:33:39 INFO CliDriver: Time taken: 0.415 seconds
15/08/06 17:33:39 INFO StatsReportListener: other time pct: (count: 1, mean: 73.529412, stdev: 0.000000, max: 73.529412, min: 73.529412)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	74 %	74 %	74 %	74 %	74 %	74 %	74 %	74 %	74 %
15/08/06 17:33:39 INFO ParseDriver: Parsing command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:33:39 INFO ParseDriver: Parse Completed
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO ParseDriver: Parsing command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:33:39 INFO ParseDriver: Parse Completed
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=parse start=1438882419696 end=1438882419697 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO SemanticAnalyzer: Starting Semantic Analysis
15/08/06 17:33:39 INFO SemanticAnalyzer: Creating table q11_part_tmp_par_spark position=13
15/08/06 17:33:39 INFO Driver: Semantic Analysis Completed
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438882419697 end=1438882419706 duration=9 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=compile start=1438882419695 end=1438882419707 duration=12 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO Driver: Starting command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438882419695 end=1438882419708 duration=13 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=runTasks start=1438882419708 end=1438882419772 duration=64 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438882419707 end=1438882419773 duration=66 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:33:39 INFO Driver: OK
15/08/06 17:33:39 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438882419773 end=1438882419773 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO PerfLogger: </PERFLOG method=Driver.run start=1438882419695 end=1438882419773 duration=78 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:33:39 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:33:39 INFO DAGScheduler: Got job 3 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:33:39 INFO DAGScheduler: Final stage: Stage 3(collect at SparkPlan.scala:84)
15/08/06 17:33:39 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:33:39 INFO DAGScheduler: Missing parents: List()
15/08/06 17:33:39 INFO DAGScheduler: Submitting Stage 3 (MappedRDD[11] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:33:39 INFO MemoryStore: ensureFreeSpace(2560) called with curMem=10329, maxMem=3333968363
15/08/06 17:33:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 3.1 GB)
15/08/06 17:33:39 INFO MemoryStore: ensureFreeSpace(1562) called with curMem=12889, maxMem=3333968363
15/08/06 17:33:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1562.0 B, free 3.1 GB)
15/08/06 17:33:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:42931 (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:33:39 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/08/06 17:33:39 INFO DefaultExecutionContext: Created broadcast 3 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:39 INFO DAGScheduler: Submitting 1 missing tasks from Stage 3 (MappedRDD[11] at map at SparkPlan.scala:84)
15/08/06 17:33:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
15/08/06 17:33:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:33:39 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
15/08/06 17:33:39 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 618 bytes result sent to driver
15/08/06 17:33:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 27 ms on localhost (1/1)
15/08/06 17:33:39 INFO DAGScheduler: Stage 3 (collect at SparkPlan.scala:84) finished in 0.033 s
15/08/06 17:33:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/06 17:33:39 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@60be4640
15/08/06 17:33:39 INFO DAGScheduler: Job 3 finished: collect at SparkPlan.scala:84, took 0.080315 s
Time taken: 0.297 seconds
15/08/06 17:33:39 INFO CliDriver: Time taken: 0.297 seconds
15/08/06 17:33:39 INFO StatsReportListener: task runtime:(count: 1, mean: 27.000000, stdev: 0.000000, max: 27.000000, min: 27.000000)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms
15/08/06 17:33:39 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:33:39 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 11.111111, stdev: 0.000000, max: 11.111111, min: 11.111111)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	11 %	11 %	11 %	11 %	11 %	11 %	11 %	11 %	11 %
15/08/06 17:33:39 INFO StatsReportListener: other time pct: (count: 1, mean: 88.888889, stdev: 0.000000, max: 88.888889, min: 88.888889)
15/08/06 17:33:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:39 INFO StatsReportListener: 	89 %	89 %	89 %	89 %	89 %	89 %	89 %	89 %	89 %
15/08/06 17:33:40 INFO ParseDriver: Parsing command: insert into table q11_part_tmp_par_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey
15/08/06 17:33:40 INFO ParseDriver: Parse Completed
15/08/06 17:33:40 INFO BlockManager: Removing broadcast 3
15/08/06 17:33:40 INFO BlockManager: Removing block broadcast_3_piece0
15/08/06 17:33:40 INFO MemoryStore: Block broadcast_3_piece0 of size 1562 dropped from memory (free 3333955474)
15/08/06 17:33:40 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:42931 in memory (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:33:40 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/08/06 17:33:40 INFO BlockManager: Removing block broadcast_3
15/08/06 17:33:40 INFO MemoryStore: Block broadcast_3 of size 2560 dropped from memory (free 3333958034)
15/08/06 17:33:40 INFO ContextCleaner: Cleaned broadcast 3
15/08/06 17:33:40 INFO BlockManager: Removing broadcast 2
15/08/06 17:33:40 INFO BlockManager: Removing block broadcast_2
15/08/06 17:33:40 INFO MemoryStore: Block broadcast_2 of size 2560 dropped from memory (free 3333960594)
15/08/06 17:33:40 INFO BlockManager: Removing block broadcast_2_piece0
15/08/06 17:33:40 INFO MemoryStore: Block broadcast_2_piece0 of size 1562 dropped from memory (free 3333962156)
15/08/06 17:33:40 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:42931 in memory (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:33:40 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/08/06 17:33:40 INFO ContextCleaner: Cleaned broadcast 2
15/08/06 17:33:40 INFO BlockManager: Removing broadcast 1
15/08/06 17:33:40 INFO BlockManager: Removing block broadcast_1_piece0
15/08/06 17:33:40 INFO MemoryStore: Block broadcast_1_piece0 of size 1207 dropped from memory (free 3333963363)
15/08/06 17:33:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:42931 in memory (size: 1207.0 B, free: 3.1 GB)
15/08/06 17:33:40 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/08/06 17:33:40 INFO BlockManager: Removing block broadcast_1
15/08/06 17:33:40 INFO MemoryStore: Block broadcast_1 of size 1896 dropped from memory (free 3333965259)
15/08/06 17:33:40 INFO ContextCleaner: Cleaned broadcast 1
15/08/06 17:33:40 INFO BlockManager: Removing broadcast 0
15/08/06 17:33:40 INFO BlockManager: Removing block broadcast_0
15/08/06 17:33:40 INFO MemoryStore: Block broadcast_0 of size 1896 dropped from memory (free 3333967155)
15/08/06 17:33:40 INFO BlockManager: Removing block broadcast_0_piece0
15/08/06 17:33:40 INFO MemoryStore: Block broadcast_0_piece0 of size 1208 dropped from memory (free 3333968363)
15/08/06 17:33:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:42931 in memory (size: 1208.0 B, free: 3.1 GB)
15/08/06 17:33:40 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/08/06 17:33:40 INFO ContextCleaner: Cleaned broadcast 0
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/06 17:33:41 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(n_nationkey#31, n_name#32, n_regionkey#33, n_comment#34)
15/08/06 17:33:41 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(s_suppkey#35, s_name#36, s_address#37, s_nationkey#38, s_phone#39, s_acctbal#40, s_comment#41)
15/08/06 17:33:41 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#42, ps_suppkey#43, ps_availqty#44, ps_supplycost#45, ps_comment#46)
15/08/06 17:33:41 INFO MemoryStore: ensureFreeSpace(281594) called with curMem=0, maxMem=3333968363
15/08/06 17:33:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 275.0 KB, free 3.1 GB)
15/08/06 17:33:41 INFO MemoryStore: ensureFreeSpace(281634) called with curMem=281594, maxMem=3333968363
15/08/06 17:33:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 275.0 KB, free 3.1 GB)
15/08/06 17:33:41 INFO MemoryStore: ensureFreeSpace(31949) called with curMem=563228, maxMem=3333968363
15/08/06 17:33:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 31.2 KB, free 3.1 GB)
15/08/06 17:33:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:42931 (size: 31.2 KB, free: 3.1 GB)
15/08/06 17:33:41 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0
15/08/06 17:33:41 INFO DefaultExecutionContext: Created broadcast 5 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:33:41 INFO MemoryStore: ensureFreeSpace(31895) called with curMem=595177, maxMem=3333968363
15/08/06 17:33:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:33:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:42931 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:33:41 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/08/06 17:33:41 INFO DefaultExecutionContext: Created broadcast 4 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:33:41 INFO FileInputFormat: Total input paths to process : 1
15/08/06 17:33:41 INFO ParquetInputFormat: Total input paths to process : 1
15/08/06 17:33:41 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:33:41 INFO ParquetFileReader: reading another 1 footers
15/08/06 17:33:41 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:33:41 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0; isDirectory=false; length=1493206; replication=1; blocksize=134217728; modification_time=1438802778074; access_time=1438881494768; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 18 ms
15/08/06 17:33:41 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/06 17:33:41 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/06 17:33:41 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:33:41 INFO DefaultExecutionContext: Starting job: collect at BroadcastHashJoin.scala:53
15/08/06 17:33:41 INFO DAGScheduler: Got job 4 (collect at BroadcastHashJoin.scala:53) with 1 output partitions (allowLocal=false)
15/08/06 17:33:41 INFO DAGScheduler: Final stage: Stage 4(collect at BroadcastHashJoin.scala:53)
15/08/06 17:33:41 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:33:41 INFO DAGScheduler: Missing parents: List()
15/08/06 17:33:41 INFO DAGScheduler: Submitting Stage 4 (MappedRDD[29] at map at BroadcastHashJoin.scala:53), which has no missing parents
15/08/06 17:33:41 INFO MemoryStore: ensureFreeSpace(2488) called with curMem=627072, maxMem=3333968363
15/08/06 17:33:41 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.4 KB, free 3.1 GB)
15/08/06 17:33:41 INFO MemoryStore: ensureFreeSpace(1469) called with curMem=629560, maxMem=3333968363
15/08/06 17:33:41 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1469.0 B, free 3.1 GB)
15/08/06 17:33:41 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:42931 (size: 1469.0 B, free: 3.1 GB)
15/08/06 17:33:41 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/08/06 17:33:41 INFO DefaultExecutionContext: Created broadcast 6 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:41 INFO DAGScheduler: Submitting 1 missing tasks from Stage 4 (MappedRDD[29] at map at BroadcastHashJoin.scala:53)
15/08/06 17:33:41 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15/08/06 17:33:41 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, ANY, 1586 bytes)
15/08/06 17:33:41 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
15/08/06 17:33:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0 start: 0 end: 1493206 length: 1493206 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 10000 records.
15/08/06 17:33:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:41 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 10000
15/08/06 17:33:42 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 144335 bytes result sent to driver
15/08/06 17:33:42 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 470 ms on localhost (1/1)
15/08/06 17:33:42 INFO DAGScheduler: Stage 4 (collect at BroadcastHashJoin.scala:53) finished in 0.672 s
15/08/06 17:33:42 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/06 17:33:42 INFO DAGScheduler: Job 4 finished: collect at BroadcastHashJoin.scala:53, took 0.715846 s
15/08/06 17:33:42 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@cf76937
15/08/06 17:33:42 INFO StatsReportListener: task runtime:(count: 1, mean: 470.000000, stdev: 0.000000, max: 470.000000, min: 470.000000)
15/08/06 17:33:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:42 INFO StatsReportListener: 	470.0 ms	470.0 ms	470.0 ms	470.0 ms	470.0 ms	470.0 ms	470.0 ms	470.0 ms	470.0 ms
15/08/06 17:33:42 INFO StatsReportListener: task result size:(count: 1, mean: 144335.000000, stdev: 0.000000, max: 144335.000000, min: 144335.000000)
15/08/06 17:33:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:42 INFO StatsReportListener: 	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB
15/08/06 17:33:42 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 70.425532, stdev: 0.000000, max: 70.425532, min: 70.425532)
15/08/06 17:33:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:42 INFO StatsReportListener: 	70 %	70 %	70 %	70 %	70 %	70 %	70 %	70 %	70 %
15/08/06 17:33:42 INFO StatsReportListener: other time pct: (count: 1, mean: 29.574468, stdev: 0.000000, max: 29.574468, min: 29.574468)
15/08/06 17:33:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:42 INFO StatsReportListener: 	30 %	30 %	30 %	30 %	30 %	30 %	30 %	30 %	30 %
15/08/06 17:33:42 INFO MemoryStore: ensureFreeSpace(65648) called with curMem=631029, maxMem=3333968363
15/08/06 17:33:42 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 64.1 KB, free 3.1 GB)
15/08/06 17:33:42 INFO MemoryStore: ensureFreeSpace(55237) called with curMem=696677, maxMem=3333968363
15/08/06 17:33:42 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 53.9 KB, free 3.1 GB)
15/08/06 17:33:42 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:42931 (size: 53.9 KB, free: 3.1 GB)
15/08/06 17:33:42 INFO BlockManagerMaster: Updated info of block broadcast_7_piece0
15/08/06 17:33:42 INFO DefaultExecutionContext: Created broadcast 7 from broadcast at BroadcastHashJoin.scala:55
15/08/06 17:33:42 INFO MemoryStore: ensureFreeSpace(281194) called with curMem=751914, maxMem=3333968363
15/08/06 17:33:42 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 274.6 KB, free 3.1 GB)
15/08/06 17:33:42 INFO MemoryStore: ensureFreeSpace(31858) called with curMem=1033108, maxMem=3333968363
15/08/06 17:33:42 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:33:42 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:42931 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:33:42 INFO BlockManagerMaster: Updated info of block broadcast_8_piece0
15/08/06 17:33:42 INFO DefaultExecutionContext: Created broadcast 8 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:33:42 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/06 17:33:42 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/06 17:33:42 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/06 17:33:42 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/06 17:33:42 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/06 17:33:42 INFO DefaultExecutionContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/08/06 17:33:42 INFO FileInputFormat: Total input paths to process : 1
15/08/06 17:33:42 INFO ParquetInputFormat: Total input paths to process : 1
15/08/06 17:33:42 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:33:42 INFO ParquetFileReader: reading another 1 footers
15/08/06 17:33:42 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:33:42 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0; isDirectory=false; length=3216; replication=1; blocksize=134217728; modification_time=1438802774354; access_time=1438881494331; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 16 ms
15/08/06 17:33:42 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:33:42 INFO FileInputFormat: Total input paths to process : 10
15/08/06 17:33:42 INFO ParquetInputFormat: Total input paths to process : 10
15/08/06 17:33:42 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:33:42 INFO ParquetFileReader: reading another 10 footers
15/08/06 17:33:42 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:33:42 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0; isDirectory=false; length=11478369; replication=1; blocksize=134217728; modification_time=1438802779757; access_time=1438881494861; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0; isDirectory=false; length=11437027; replication=1; blocksize=134217728; modification_time=1438802780428; access_time=1438881496406; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0; isDirectory=false; length=11437121; replication=1; blocksize=134217728; modification_time=1438802779669; access_time=1438881496411; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0; isDirectory=false; length=11437895; replication=1; blocksize=134217728; modification_time=1438802779902; access_time=1438881496416; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0; isDirectory=false; length=11436112; replication=1; blocksize=134217728; modification_time=1438802779829; access_time=1438881496416; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0; isDirectory=false; length=11361987; replication=1; blocksize=134217728; modification_time=1438802780059; access_time=1438881496419; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0; isDirectory=false; length=11361789; replication=1; blocksize=134217728; modification_time=1438802780860; access_time=1438881496423; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0; isDirectory=false; length=11362156; replication=1; blocksize=134217728; modification_time=1438802781041; access_time=1438881496427; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0; isDirectory=false; length=11362379; replication=1; blocksize=134217728; modification_time=1438802780967; access_time=1438881496428; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0; isDirectory=false; length=11361401; replication=1; blocksize=134217728; modification_time=1438802781090; access_time=1438881496428; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 48 ms
15/08/06 17:33:42 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:33:42 INFO DAGScheduler: Registering RDD 30 (mapPartitions at Exchange.scala:64)
15/08/06 17:33:42 INFO DAGScheduler: Registering RDD 39 (mapPartitions at Exchange.scala:64)
15/08/06 17:33:42 INFO DAGScheduler: Registering RDD 45 (mapPartitions at Exchange.scala:64)
15/08/06 17:33:42 INFO DAGScheduler: Got job 5 (runJob at InsertIntoHiveTable.scala:93) with 200 output partitions (allowLocal=false)
15/08/06 17:33:42 INFO DAGScheduler: Final stage: Stage 8(runJob at InsertIntoHiveTable.scala:93)
15/08/06 17:33:42 INFO DAGScheduler: Parents of final stage: List(Stage 7)
15/08/06 17:33:43 INFO DAGScheduler: Missing parents: List(Stage 7)
15/08/06 17:33:43 INFO DAGScheduler: Submitting Stage 5 (MapPartitionsRDD[30] at mapPartitions at Exchange.scala:64), which has no missing parents
15/08/06 17:33:43 INFO MemoryStore: ensureFreeSpace(7264) called with curMem=1064966, maxMem=3333968363
15/08/06 17:33:43 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.1 KB, free 3.1 GB)
15/08/06 17:33:43 INFO MemoryStore: ensureFreeSpace(4178) called with curMem=1072230, maxMem=3333968363
15/08/06 17:33:43 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 3.1 GB)
15/08/06 17:33:43 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:42931 (size: 4.1 KB, free: 3.1 GB)
15/08/06 17:33:43 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/08/06 17:33:43 INFO DefaultExecutionContext: Created broadcast 9 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:43 INFO DAGScheduler: Submitting 10 missing tasks from Stage 5 (MapPartitionsRDD[30] at mapPartitions at Exchange.scala:64)
15/08/06 17:33:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 10 tasks
15/08/06 17:33:43 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, ANY, 1581 bytes)
15/08/06 17:33:43 INFO DAGScheduler: Submitting Stage 6 (MapPartitionsRDD[39] at mapPartitions at Exchange.scala:64), which has no missing parents
15/08/06 17:33:43 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, ANY, 1585 bytes)
15/08/06 17:33:43 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 7, localhost, ANY, 1584 bytes)
15/08/06 17:33:43 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 8, localhost, ANY, 1584 bytes)
15/08/06 17:33:43 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 9, localhost, ANY, 1584 bytes)
15/08/06 17:33:43 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 10, localhost, ANY, 1584 bytes)
15/08/06 17:33:43 INFO MemoryStore: ensureFreeSpace(10096) called with curMem=1076408, maxMem=3333968363
15/08/06 17:33:43 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.9 KB, free 3.1 GB)
15/08/06 17:33:43 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 11, localhost, ANY, 1583 bytes)
15/08/06 17:33:43 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 12, localhost, ANY, 1582 bytes)
15/08/06 17:33:43 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 13, localhost, ANY, 1583 bytes)
15/08/06 17:33:43 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 14, localhost, ANY, 1582 bytes)
15/08/06 17:33:43 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
15/08/06 17:33:43 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
15/08/06 17:33:43 INFO Executor: Running task 2.0 in stage 5.0 (TID 7)
15/08/06 17:33:43 INFO MemoryStore: ensureFreeSpace(5665) called with curMem=1086504, maxMem=3333968363
15/08/06 17:33:43 INFO Executor: Running task 3.0 in stage 5.0 (TID 8)
15/08/06 17:33:43 INFO Executor: Running task 4.0 in stage 5.0 (TID 9)
15/08/06 17:33:43 INFO Executor: Running task 5.0 in stage 5.0 (TID 10)
15/08/06 17:33:43 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.5 KB, free 3.1 GB)
15/08/06 17:33:43 INFO Executor: Running task 8.0 in stage 5.0 (TID 13)
15/08/06 17:33:43 INFO Executor: Running task 6.0 in stage 5.0 (TID 11)
15/08/06 17:33:43 INFO Executor: Running task 7.0 in stage 5.0 (TID 12)
15/08/06 17:33:43 INFO Executor: Running task 9.0 in stage 5.0 (TID 14)
15/08/06 17:33:43 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:42931 (size: 5.5 KB, free: 3.1 GB)
15/08/06 17:33:43 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/08/06 17:33:43 INFO DefaultExecutionContext: Created broadcast 10 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:43 INFO DAGScheduler: Submitting 1 missing tasks from Stage 6 (MapPartitionsRDD[39] at mapPartitions at Exchange.scala:64)
15/08/06 17:33:43 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 0 end: 11478369 length: 11478369 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 0 end: 11437121 length: 11437121 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 0 end: 11436112 length: 11436112 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 0 end: 11437027 length: 11437027 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 15, localhost, ANY, 1552 bytes)
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 0 end: 11437895 length: 11437895 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 0 end: 11362379 length: 11362379 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 INFO Executor: Running task 0.0 in stage 6.0 (TID 15)
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 0 end: 11362156 length: 11362156 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 0 end: 11361789 length: 11361789 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 0 end: 11361401 length: 11361401 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 0 end: 11361987 length: 11361987 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80260 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80615 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80206 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80265 records.
15/08/06 17:33:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0 start: 0 end: 3216 length: 3216 hosts: [] requestedSchema: message root {
  optional int32 n_nationkey;
  optional binary n_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}},{"name":"n_regionkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/06 17:33:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79667 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79671 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80195 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79706 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79826 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79589 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 25 records.
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 25
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 79826
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 80265
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 80260
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 79667
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 79671
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 80206
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 80195
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 80615
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 79706
15/08/06 17:33:43 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 79589
15/08/06 17:33:43 INFO Executor: Finished task 0.0 in stage 6.0 (TID 15). 2019 bytes result sent to driver
15/08/06 17:33:43 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 15) in 812 ms on localhost (1/1)
15/08/06 17:33:43 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/06 17:33:43 INFO DAGScheduler: Stage 6 (mapPartitions at Exchange.scala:64) finished in 0.818 s
15/08/06 17:33:43 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:33:43 INFO DAGScheduler: running: Set(Stage 5)
15/08/06 17:33:43 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@25d08402
15/08/06 17:33:43 INFO DAGScheduler: waiting: Set(Stage 7, Stage 8)
15/08/06 17:33:43 INFO DAGScheduler: failed: Set()
15/08/06 17:33:43 INFO StatsReportListener: task runtime:(count: 1, mean: 812.000000, stdev: 0.000000, max: 812.000000, min: 812.000000)
15/08/06 17:33:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:43 INFO StatsReportListener: 	812.0 ms	812.0 ms	812.0 ms	812.0 ms	812.0 ms	812.0 ms	812.0 ms	812.0 ms	812.0 ms
15/08/06 17:33:44 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 13231.000000, stdev: 0.000000, max: 13231.000000, min: 13231.000000)
15/08/06 17:33:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:44 INFO StatsReportListener: 	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB
15/08/06 17:33:44 INFO StatsReportListener: task result size:(count: 1, mean: 2019.000000, stdev: 0.000000, max: 2019.000000, min: 2019.000000)
15/08/06 17:33:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:44 INFO StatsReportListener: 	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B
15/08/06 17:33:44 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 96.921182, stdev: 0.000000, max: 96.921182, min: 96.921182)
15/08/06 17:33:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:44 INFO StatsReportListener: 	97 %	97 %	97 %	97 %	97 %	97 %	97 %	97 %	97 %
15/08/06 17:33:44 INFO StatsReportListener: other time pct: (count: 1, mean: 3.078818, stdev: 0.000000, max: 3.078818, min: 3.078818)
15/08/06 17:33:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:44 INFO StatsReportListener: 	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %
15/08/06 17:33:44 INFO DAGScheduler: Missing parents for Stage 7: List(Stage 5)
15/08/06 17:33:44 INFO DAGScheduler: Missing parents for Stage 8: List(Stage 7)
15/08/06 17:33:45 INFO Executor: Finished task 6.0 in stage 5.0 (TID 11). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 11) in 2070 ms on localhost (1/10)
15/08/06 17:33:45 INFO Executor: Finished task 4.0 in stage 5.0 (TID 9). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 9) in 2174 ms on localhost (2/10)
15/08/06 17:33:45 INFO Executor: Finished task 3.0 in stage 5.0 (TID 8). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO Executor: Finished task 5.0 in stage 5.0 (TID 10). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO Executor: Finished task 2.0 in stage 5.0 (TID 7). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 8) in 2208 ms on localhost (3/10)
15/08/06 17:33:45 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 10) in 2211 ms on localhost (4/10)
15/08/06 17:33:45 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 7) in 2216 ms on localhost (5/10)
15/08/06 17:33:45 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2225 ms on localhost (6/10)
15/08/06 17:33:45 INFO Executor: Finished task 8.0 in stage 5.0 (TID 13). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO Executor: Finished task 9.0 in stage 5.0 (TID 14). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 13) in 2223 ms on localhost (7/10)
15/08/06 17:33:45 INFO Executor: Finished task 7.0 in stage 5.0 (TID 12). 2019 bytes result sent to driver
15/08/06 17:33:45 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 2232 ms on localhost (8/10)
15/08/06 17:33:45 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 14) in 2226 ms on localhost (9/10)
15/08/06 17:33:45 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 12) in 2232 ms on localhost (10/10)
15/08/06 17:33:45 INFO DAGScheduler: Stage 5 (mapPartitions at Exchange.scala:64) finished in 2.241 s
15/08/06 17:33:45 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/06 17:33:45 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:33:45 INFO DAGScheduler: running: Set()
15/08/06 17:33:45 INFO DAGScheduler: waiting: Set(Stage 7, Stage 8)
15/08/06 17:33:45 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7b54fe52
15/08/06 17:33:45 INFO DAGScheduler: failed: Set()
15/08/06 17:33:45 INFO StatsReportListener: task runtime:(count: 10, mean: 2201.700000, stdev: 46.761202, max: 2232.000000, min: 2070.000000)
15/08/06 17:33:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:45 INFO StatsReportListener: 	2.1 s	2.1 s	2.2 s	2.2 s	2.2 s	2.2 s	2.2 s	2.2 s	2.2 s
15/08/06 17:33:45 INFO StatsReportListener: shuffle bytes written:(count: 10, mean: 7582991.200000, stdev: 26756.903116, max: 7616204.000000, min: 7549451.000000)
15/08/06 17:33:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:45 INFO StatsReportListener: 	7.2 MB	7.2 MB	7.2 MB	7.2 MB	7.3 MB	7.3 MB	7.3 MB	7.3 MB	7.3 MB
15/08/06 17:33:45 INFO StatsReportListener: task result size:(count: 10, mean: 2019.000000, stdev: 0.000000, max: 2019.000000, min: 2019.000000)
15/08/06 17:33:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:45 INFO StatsReportListener: 	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B
15/08/06 17:33:45 INFO StatsReportListener: executor (non-fetch) time pct: (count: 10, mean: 98.539560, stdev: 0.432495, max: 99.191011, min: 97.584541)
15/08/06 17:33:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:45 INFO StatsReportListener: 	98 %	98 %	98 %	98 %	98 %	99 %	99 %	99 %	99 %
15/08/06 17:33:45 INFO StatsReportListener: other time pct: (count: 10, mean: 1.460440, stdev: 0.432495, max: 2.415459, min: 0.808989)
15/08/06 17:33:45 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:45 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 2 %	 2 %	 2 %	 2 %	 2 %
15/08/06 17:33:45 INFO DAGScheduler: Missing parents for Stage 7: List()
15/08/06 17:33:45 INFO DAGScheduler: Missing parents for Stage 8: List(Stage 7)
15/08/06 17:33:45 INFO DAGScheduler: Submitting Stage 7 (MapPartitionsRDD[45] at mapPartitions at Exchange.scala:64), which is now runnable
15/08/06 17:33:45 INFO MemoryStore: ensureFreeSpace(13544) called with curMem=1092169, maxMem=3333968363
15/08/06 17:33:45 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.2 KB, free 3.1 GB)
15/08/06 17:33:45 INFO MemoryStore: ensureFreeSpace(7369) called with curMem=1105713, maxMem=3333968363
15/08/06 17:33:45 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.2 KB, free 3.1 GB)
15/08/06 17:33:45 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:42931 (size: 7.2 KB, free: 3.1 GB)
15/08/06 17:33:45 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/08/06 17:33:45 INFO DefaultExecutionContext: Created broadcast 11 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:45 INFO DAGScheduler: Submitting 200 missing tasks from Stage 7 (MapPartitionsRDD[45] at mapPartitions at Exchange.scala:64)
15/08/06 17:33:45 INFO TaskSchedulerImpl: Adding task set 7.0 with 200 tasks
15/08/06 17:33:45 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 16, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 17, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 18, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 19, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 20, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 21, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 22, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 23, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 24, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 25, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 26, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 27, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 28, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 29, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 30, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 31, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:45 INFO Executor: Running task 0.0 in stage 7.0 (TID 16)
15/08/06 17:33:45 INFO Executor: Running task 1.0 in stage 7.0 (TID 17)
15/08/06 17:33:45 INFO Executor: Running task 2.0 in stage 7.0 (TID 18)
15/08/06 17:33:45 INFO Executor: Running task 3.0 in stage 7.0 (TID 19)
15/08/06 17:33:45 INFO Executor: Running task 4.0 in stage 7.0 (TID 20)
15/08/06 17:33:45 INFO Executor: Running task 6.0 in stage 7.0 (TID 22)
15/08/06 17:33:45 INFO Executor: Running task 5.0 in stage 7.0 (TID 21)
15/08/06 17:33:45 INFO Executor: Running task 7.0 in stage 7.0 (TID 23)
15/08/06 17:33:45 INFO Executor: Running task 9.0 in stage 7.0 (TID 25)
15/08/06 17:33:45 INFO Executor: Running task 8.0 in stage 7.0 (TID 24)
15/08/06 17:33:45 INFO Executor: Running task 10.0 in stage 7.0 (TID 26)
15/08/06 17:33:45 INFO Executor: Running task 11.0 in stage 7.0 (TID 27)
15/08/06 17:33:45 INFO Executor: Running task 12.0 in stage 7.0 (TID 28)
15/08/06 17:33:45 INFO Executor: Running task 13.0 in stage 7.0 (TID 29)
15/08/06 17:33:45 INFO Executor: Running task 14.0 in stage 7.0 (TID 30)
15/08/06 17:33:45 INFO Executor: Running task 15.0 in stage 7.0 (TID 31)
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:46 INFO Executor: Finished task 7.0 in stage 7.0 (TID 23). 1124 bytes result sent to driver
15/08/06 17:33:46 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 32, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:46 INFO Executor: Finished task 15.0 in stage 7.0 (TID 31). 1124 bytes result sent to driver
15/08/06 17:33:46 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 23) in 755 ms on localhost (1/200)
15/08/06 17:33:46 INFO Executor: Running task 16.0 in stage 7.0 (TID 32)
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:46 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 33, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:46 INFO Executor: Running task 17.0 in stage 7.0 (TID 33)
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:46 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 31) in 771 ms on localhost (2/200)
15/08/06 17:33:46 INFO Executor: Finished task 12.0 in stage 7.0 (TID 28). 1124 bytes result sent to driver
15/08/06 17:33:46 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 34, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:46 INFO Executor: Running task 18.0 in stage 7.0 (TID 34)
15/08/06 17:33:46 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 28) in 812 ms on localhost (3/200)
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:46 INFO BlockManager: Removing broadcast 10
15/08/06 17:33:46 INFO BlockManager: Removing block broadcast_10
15/08/06 17:33:46 INFO MemoryStore: Block broadcast_10 of size 10096 dropped from memory (free 3332865377)
15/08/06 17:33:46 INFO BlockManager: Removing block broadcast_10_piece0
15/08/06 17:33:46 INFO MemoryStore: Block broadcast_10_piece0 of size 5665 dropped from memory (free 3332871042)
15/08/06 17:33:46 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:42931 in memory (size: 5.5 KB, free: 3.1 GB)
15/08/06 17:33:46 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/08/06 17:33:46 INFO ContextCleaner: Cleaned broadcast 10
15/08/06 17:33:46 INFO BlockManager: Removing broadcast 9
15/08/06 17:33:46 INFO BlockManager: Removing block broadcast_9
15/08/06 17:33:46 INFO MemoryStore: Block broadcast_9 of size 7264 dropped from memory (free 3332878306)
15/08/06 17:33:46 INFO BlockManager: Removing block broadcast_9_piece0
15/08/06 17:33:46 INFO MemoryStore: Block broadcast_9_piece0 of size 4178 dropped from memory (free 3332882484)
15/08/06 17:33:46 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:42931 in memory (size: 4.1 KB, free: 3.1 GB)
15/08/06 17:33:46 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/08/06 17:33:46 INFO ContextCleaner: Cleaned broadcast 9
15/08/06 17:33:46 INFO BlockManager: Removing broadcast 6
15/08/06 17:33:46 INFO BlockManager: Removing block broadcast_6_piece0
15/08/06 17:33:46 INFO MemoryStore: Block broadcast_6_piece0 of size 1469 dropped from memory (free 3332883953)
15/08/06 17:33:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:42931 in memory (size: 1469.0 B, free: 3.1 GB)
15/08/06 17:33:46 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/08/06 17:33:46 INFO BlockManager: Removing block broadcast_6
15/08/06 17:33:46 INFO MemoryStore: Block broadcast_6 of size 2488 dropped from memory (free 3332886441)
15/08/06 17:33:46 INFO ContextCleaner: Cleaned broadcast 6
15/08/06 17:33:47 INFO BlockManager: Removing broadcast 4
15/08/06 17:33:47 INFO BlockManager: Removing block broadcast_4
15/08/06 17:33:47 INFO MemoryStore: Block broadcast_4 of size 281594 dropped from memory (free 3333168035)
15/08/06 17:33:47 INFO BlockManager: Removing block broadcast_4_piece0
15/08/06 17:33:47 INFO MemoryStore: Block broadcast_4_piece0 of size 31895 dropped from memory (free 3333199930)
15/08/06 17:33:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:42931 in memory (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:33:47 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/08/06 17:33:47 INFO ContextCleaner: Cleaned broadcast 4
15/08/06 17:33:47 INFO Executor: Finished task 14.0 in stage 7.0 (TID 30). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 35, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 19.0 in stage 7.0 (TID 35)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 30) in 1681 ms on localhost (4/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 11.0 in stage 7.0 (TID 27). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 36, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 20.0 in stage 7.0 (TID 36)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 27) in 1822 ms on localhost (5/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO Executor: Finished task 5.0 in stage 7.0 (TID 21). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 37, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 21.0 in stage 7.0 (TID 37)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 21) in 1855 ms on localhost (6/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 8.0 in stage 7.0 (TID 24). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 38, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 22.0 in stage 7.0 (TID 38)
15/08/06 17:33:47 INFO Executor: Finished task 0.0 in stage 7.0 (TID 16). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 24) in 1931 ms on localhost (7/200)
15/08/06 17:33:47 INFO Executor: Finished task 9.0 in stage 7.0 (TID 25). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 39, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 23.0 in stage 7.0 (TID 39)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 16) in 1939 ms on localhost (8/200)
15/08/06 17:33:47 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 40, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 24.0 in stage 7.0 (TID 40)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 25) in 1936 ms on localhost (9/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 4.0 in stage 7.0 (TID 20). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO Executor: Finished task 21.0 in stage 7.0 (TID 37). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 41, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 25.0 in stage 7.0 (TID 41)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 20) in 1946 ms on localhost (10/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 42, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 26.0 in stage 7.0 (TID 42)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 37) in 95 ms on localhost (11/200)
15/08/06 17:33:47 INFO Executor: Finished task 13.0 in stage 7.0 (TID 29). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 43, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 27.0 in stage 7.0 (TID 43)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 29) in 1948 ms on localhost (12/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 2.0 in stage 7.0 (TID 18). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 44, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 18) in 1969 ms on localhost (13/200)
15/08/06 17:33:47 INFO Executor: Finished task 1.0 in stage 7.0 (TID 17). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO Executor: Finished task 3.0 in stage 7.0 (TID 19). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 45, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 29.0 in stage 7.0 (TID 45)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 17) in 1978 ms on localhost (14/200)
15/08/06 17:33:47 INFO Executor: Finished task 10.0 in stage 7.0 (TID 26). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 19) in 1979 ms on localhost (15/200)
15/08/06 17:33:47 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 46, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 47, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 31.0 in stage 7.0 (TID 47)
15/08/06 17:33:47 INFO Executor: Running task 30.0 in stage 7.0 (TID 46)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 26) in 1979 ms on localhost (16/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Running task 28.0 in stage 7.0 (TID 44)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 6.0 in stage 7.0 (TID 22). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 48, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 32.0 in stage 7.0 (TID 48)
15/08/06 17:33:47 INFO Executor: Finished task 17.0 in stage 7.0 (TID 33). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 22) in 2009 ms on localhost (17/200)
15/08/06 17:33:47 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 49, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 33.0 in stage 7.0 (TID 49)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 33) in 1253 ms on localhost (18/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 31.0 in stage 7.0 (TID 47). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 50, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 34.0 in stage 7.0 (TID 50)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 47) in 46 ms on localhost (19/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 16.0 in stage 7.0 (TID 32). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 51, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 35.0 in stage 7.0 (TID 51)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 32) in 1277 ms on localhost (20/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 18.0 in stage 7.0 (TID 34). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 52, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Finished task 19.0 in stage 7.0 (TID 35). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 34) in 1236 ms on localhost (21/200)
15/08/06 17:33:47 INFO Executor: Running task 36.0 in stage 7.0 (TID 52)
15/08/06 17:33:47 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 53, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 37.0 in stage 7.0 (TID 53)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 35) in 370 ms on localhost (22/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 32.0 in stage 7.0 (TID 48). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 54, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 38.0 in stage 7.0 (TID 54)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 48) in 63 ms on localhost (23/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 20.0 in stage 7.0 (TID 36). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 55, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 39.0 in stage 7.0 (TID 55)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 36) in 268 ms on localhost (24/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO Executor: Finished task 23.0 in stage 7.0 (TID 39). 1124 bytes result sent to driver
15/08/06 17:33:47 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 56, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:47 INFO Executor: Running task 40.0 in stage 7.0 (TID 56)
15/08/06 17:33:47 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 39) in 568 ms on localhost (25/200)
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 27.0 in stage 7.0 (TID 43). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 57, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 41.0 in stage 7.0 (TID 57)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 43) in 644 ms on localhost (26/200)
15/08/06 17:33:48 INFO Executor: Finished task 26.0 in stage 7.0 (TID 42). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 58, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 42) in 654 ms on localhost (27/200)
15/08/06 17:33:48 INFO Executor: Running task 42.0 in stage 7.0 (TID 58)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO Executor: Finished task 24.0 in stage 7.0 (TID 40). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO Executor: Finished task 30.0 in stage 7.0 (TID 46). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO Executor: Finished task 29.0 in stage 7.0 (TID 45). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 59, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 43.0 in stage 7.0 (TID 59)
15/08/06 17:33:48 INFO Executor: Finished task 28.0 in stage 7.0 (TID 44). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 40) in 694 ms on localhost (28/200)
15/08/06 17:33:48 INFO Executor: Finished task 25.0 in stage 7.0 (TID 41). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 60, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 44.0 in stage 7.0 (TID 60)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 46) in 654 ms on localhost (29/200)
15/08/06 17:33:48 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 61, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 45.0 in stage 7.0 (TID 61)
15/08/06 17:33:48 INFO Executor: Finished task 22.0 in stage 7.0 (TID 38). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 62, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 46.0 in stage 7.0 (TID 62)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 45) in 663 ms on localhost (30/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 41) in 696 ms on localhost (31/200)
15/08/06 17:33:48 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 63, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO Executor: Running task 47.0 in stage 7.0 (TID 63)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 44) in 675 ms on localhost (32/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 64, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 38) in 721 ms on localhost (33/200)
15/08/06 17:33:48 INFO Executor: Running task 48.0 in stage 7.0 (TID 64)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 34.0 in stage 7.0 (TID 50). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 65, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 49.0 in stage 7.0 (TID 65)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 50) in 630 ms on localhost (34/200)
15/08/06 17:33:48 INFO Executor: Finished task 37.0 in stage 7.0 (TID 53). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 66, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 53) in 606 ms on localhost (35/200)
15/08/06 17:33:48 INFO Executor: Running task 50.0 in stage 7.0 (TID 66)
15/08/06 17:33:48 INFO Executor: Finished task 36.0 in stage 7.0 (TID 52). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 67, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 51.0 in stage 7.0 (TID 67)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 52) in 622 ms on localhost (36/200)
15/08/06 17:33:48 INFO Executor: Finished task 38.0 in stage 7.0 (TID 54). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO Executor: Finished task 39.0 in stage 7.0 (TID 55). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO Executor: Finished task 35.0 in stage 7.0 (TID 51). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 68, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 52.0 in stage 7.0 (TID 68)
15/08/06 17:33:48 INFO Executor: Finished task 43.0 in stage 7.0 (TID 59). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 54) in 606 ms on localhost (37/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 69, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 53.0 in stage 7.0 (TID 69)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 55) in 588 ms on localhost (38/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 70, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Running task 54.0 in stage 7.0 (TID 70)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 51) in 650 ms on localhost (39/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 71, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 33.0 in stage 7.0 (TID 49). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO Executor: Running task 55.0 in stage 7.0 (TID 71)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 59) in 53 ms on localhost (40/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 72, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 56.0 in stage 7.0 (TID 72)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 49) in 675 ms on localhost (41/200)
15/08/06 17:33:48 INFO Executor: Finished task 46.0 in stage 7.0 (TID 62). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 73, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Running task 57.0 in stage 7.0 (TID 73)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 62) in 60 ms on localhost (42/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO Executor: Finished task 52.0 in stage 7.0 (TID 68). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO Executor: Finished task 50.0 in stage 7.0 (TID 66). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 74, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 58.0 in stage 7.0 (TID 74)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 68) in 53 ms on localhost (43/200)
15/08/06 17:33:48 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 75, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 59.0 in stage 7.0 (TID 75)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 66) in 69 ms on localhost (44/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO Executor: Finished task 56.0 in stage 7.0 (TID 72). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 76, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 60.0 in stage 7.0 (TID 76)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 72) in 71 ms on localhost (45/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO Executor: Finished task 59.0 in stage 7.0 (TID 75). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 77, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 61.0 in stage 7.0 (TID 77)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 75) in 63 ms on localhost (46/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 40.0 in stage 7.0 (TID 56). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 78, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 62.0 in stage 7.0 (TID 78)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 56) in 304 ms on localhost (47/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 41.0 in stage 7.0 (TID 57). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 79, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 63.0 in stage 7.0 (TID 79)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 57) in 331 ms on localhost (48/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO Executor: Finished task 42.0 in stage 7.0 (TID 58). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 80, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 64.0 in stage 7.0 (TID 80)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 58) in 348 ms on localhost (49/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 45.0 in stage 7.0 (TID 61). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 81, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 65.0 in stage 7.0 (TID 81)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 61) in 560 ms on localhost (50/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:48 INFO Executor: Finished task 49.0 in stage 7.0 (TID 65). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 65) in 581 ms on localhost (51/200)
15/08/06 17:33:48 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 82, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 66.0 in stage 7.0 (TID 82)
15/08/06 17:33:48 INFO Executor: Finished task 48.0 in stage 7.0 (TID 64). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 83, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 67.0 in stage 7.0 (TID 83)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 64) in 597 ms on localhost (52/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 44.0 in stage 7.0 (TID 60). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 84, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 68.0 in stage 7.0 (TID 84)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 60) in 667 ms on localhost (53/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 47.0 in stage 7.0 (TID 63). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 85, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 69.0 in stage 7.0 (TID 85)
15/08/06 17:33:48 INFO Executor: Finished task 54.0 in stage 7.0 (TID 70). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 63) in 701 ms on localhost (54/200)
15/08/06 17:33:48 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 86, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 70.0 in stage 7.0 (TID 86)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 70) in 664 ms on localhost (55/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 51.0 in stage 7.0 (TID 67). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 87, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 71.0 in stage 7.0 (TID 87)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 67) in 855 ms on localhost (56/200)
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:48 INFO Executor: Finished task 55.0 in stage 7.0 (TID 71). 1124 bytes result sent to driver
15/08/06 17:33:48 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 88, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:48 INFO Executor: Running task 72.0 in stage 7.0 (TID 88)
15/08/06 17:33:48 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 71) in 854 ms on localhost (57/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 70.0 in stage 7.0 (TID 86). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 89, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 86) in 201 ms on localhost (58/200)
15/08/06 17:33:49 INFO Executor: Running task 73.0 in stage 7.0 (TID 89)
15/08/06 17:33:49 INFO Executor: Finished task 69.0 in stage 7.0 (TID 85). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 90, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 74.0 in stage 7.0 (TID 90)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 85) in 212 ms on localhost (59/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 57.0 in stage 7.0 (TID 73). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 91, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 75.0 in stage 7.0 (TID 91)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 73) in 859 ms on localhost (60/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO Executor: Finished task 53.0 in stage 7.0 (TID 69). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 92, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 76.0 in stage 7.0 (TID 92)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 69) in 881 ms on localhost (61/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 58.0 in stage 7.0 (TID 74). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 93, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 77.0 in stage 7.0 (TID 93)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 74) in 867 ms on localhost (62/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO Executor: Finished task 60.0 in stage 7.0 (TID 76). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 94, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 78.0 in stage 7.0 (TID 94)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 76) in 856 ms on localhost (63/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 61.0 in stage 7.0 (TID 77). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 95, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 79.0 in stage 7.0 (TID 95)
15/08/06 17:33:49 INFO Executor: Finished task 62.0 in stage 7.0 (TID 78). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 77) in 837 ms on localhost (64/200)
15/08/06 17:33:49 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 96, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 80.0 in stage 7.0 (TID 96)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 78) in 825 ms on localhost (65/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 63.0 in stage 7.0 (TID 79). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 97, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 81.0 in stage 7.0 (TID 97)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 79) in 719 ms on localhost (66/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 64.0 in stage 7.0 (TID 80). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 98, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 82.0 in stage 7.0 (TID 98)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 80) in 712 ms on localhost (67/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 65.0 in stage 7.0 (TID 81). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 99, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 83.0 in stage 7.0 (TID 99)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 81) in 573 ms on localhost (68/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 66.0 in stage 7.0 (TID 82). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 100, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 84.0 in stage 7.0 (TID 100)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 82) in 564 ms on localhost (69/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO Executor: Finished task 67.0 in stage 7.0 (TID 83). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 101, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 85.0 in stage 7.0 (TID 101)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 83) in 619 ms on localhost (70/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 68.0 in stage 7.0 (TID 84). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 102, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 86.0 in stage 7.0 (TID 102)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 84) in 703 ms on localhost (71/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 72.0 in stage 7.0 (TID 88). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO Executor: Finished task 74.0 in stage 7.0 (TID 90). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 103, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 87.0 in stage 7.0 (TID 103)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 88) in 543 ms on localhost (72/200)
15/08/06 17:33:49 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 104, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 88.0 in stage 7.0 (TID 104)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 90) in 531 ms on localhost (73/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 71.0 in stage 7.0 (TID 87). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 105, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 89.0 in stage 7.0 (TID 105)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 87) in 572 ms on localhost (74/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 75.0 in stage 7.0 (TID 91). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 106, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 90.0 in stage 7.0 (TID 106)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 91) in 556 ms on localhost (75/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO Executor: Finished task 73.0 in stage 7.0 (TID 89). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 107, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 91.0 in stage 7.0 (TID 107)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 89) in 582 ms on localhost (76/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 76.0 in stage 7.0 (TID 92). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 108, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 92.0 in stage 7.0 (TID 108)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 92) in 608 ms on localhost (77/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO Executor: Finished task 77.0 in stage 7.0 (TID 93). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 109, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 93.0 in stage 7.0 (TID 109)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 93) in 602 ms on localhost (78/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 78.0 in stage 7.0 (TID 94). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 110, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 94.0 in stage 7.0 (TID 110)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 94) in 593 ms on localhost (79/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 79.0 in stage 7.0 (TID 95). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 111, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 95.0 in stage 7.0 (TID 111)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 95) in 600 ms on localhost (80/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 94.0 in stage 7.0 (TID 110). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 112, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 96.0 in stage 7.0 (TID 112)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 110) in 49 ms on localhost (81/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 80.0 in stage 7.0 (TID 96). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO Executor: Finished task 82.0 in stage 7.0 (TID 98). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 113, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 97.0 in stage 7.0 (TID 113)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 98) in 604 ms on localhost (82/200)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 96) in 634 ms on localhost (83/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 114, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 98.0 in stage 7.0 (TID 114)
15/08/06 17:33:49 INFO Executor: Finished task 81.0 in stage 7.0 (TID 97). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 115, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 99.0 in stage 7.0 (TID 115)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 97) in 635 ms on localhost (84/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 83.0 in stage 7.0 (TID 99). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 116, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 100.0 in stage 7.0 (TID 116)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 99) in 532 ms on localhost (85/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:33:49 INFO Executor: Finished task 97.0 in stage 7.0 (TID 113). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 117, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 101.0 in stage 7.0 (TID 117)
15/08/06 17:33:49 INFO Executor: Finished task 84.0 in stage 7.0 (TID 100). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 113) in 43 ms on localhost (86/200)
15/08/06 17:33:49 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 118, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 102.0 in stage 7.0 (TID 118)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 100) in 506 ms on localhost (87/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 85.0 in stage 7.0 (TID 101). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 119, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 103.0 in stage 7.0 (TID 119)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 101) in 497 ms on localhost (88/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 86.0 in stage 7.0 (TID 102). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 120, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 104.0 in stage 7.0 (TID 120)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 102) in 394 ms on localhost (89/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 87.0 in stage 7.0 (TID 103). 1124 bytes result sent to driver
15/08/06 17:33:49 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 121, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:49 INFO Executor: Running task 105.0 in stage 7.0 (TID 121)
15/08/06 17:33:49 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 103) in 413 ms on localhost (90/200)
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:49 INFO Executor: Finished task 88.0 in stage 7.0 (TID 104). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 122, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 106.0 in stage 7.0 (TID 122)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 104) in 460 ms on localhost (91/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:50 INFO Executor: Finished task 90.0 in stage 7.0 (TID 106). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 123, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 107.0 in stage 7.0 (TID 123)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 106) in 506 ms on localhost (92/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 89.0 in stage 7.0 (TID 105). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 124, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 108.0 in stage 7.0 (TID 124)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 105) in 538 ms on localhost (93/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 91.0 in stage 7.0 (TID 107). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 125, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 109.0 in stage 7.0 (TID 125)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 107) in 558 ms on localhost (94/200)
15/08/06 17:33:50 INFO Executor: Finished task 92.0 in stage 7.0 (TID 108). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 126, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 108) in 523 ms on localhost (95/200)
15/08/06 17:33:50 INFO Executor: Running task 110.0 in stage 7.0 (TID 126)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 93.0 in stage 7.0 (TID 109). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 127, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 111.0 in stage 7.0 (TID 127)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 109) in 529 ms on localhost (96/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 95.0 in stage 7.0 (TID 111). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 128, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 112.0 in stage 7.0 (TID 128)
15/08/06 17:33:50 INFO Executor: Finished task 96.0 in stage 7.0 (TID 112). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 111) in 605 ms on localhost (97/200)
15/08/06 17:33:50 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 129, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 113.0 in stage 7.0 (TID 129)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 112) in 587 ms on localhost (98/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 98.0 in stage 7.0 (TID 114). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 130, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 114.0 in stage 7.0 (TID 130)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 114) in 605 ms on localhost (99/200)
15/08/06 17:33:50 INFO Executor: Finished task 99.0 in stage 7.0 (TID 115). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 131, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 115.0 in stage 7.0 (TID 131)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 115) in 602 ms on localhost (100/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:50 INFO Executor: Finished task 100.0 in stage 7.0 (TID 116). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 132, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 116.0 in stage 7.0 (TID 132)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 116) in 603 ms on localhost (101/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 101.0 in stage 7.0 (TID 117). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 133, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 117.0 in stage 7.0 (TID 133)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 117) in 606 ms on localhost (102/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 102.0 in stage 7.0 (TID 118). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 134, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 118.0 in stage 7.0 (TID 134)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 118) in 611 ms on localhost (103/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 103.0 in stage 7.0 (TID 119). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 135, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 119.0 in stage 7.0 (TID 135)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 119) in 590 ms on localhost (104/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 104.0 in stage 7.0 (TID 120). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 136, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 120) in 599 ms on localhost (105/200)
15/08/06 17:33:50 INFO Executor: Running task 120.0 in stage 7.0 (TID 136)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 105.0 in stage 7.0 (TID 121). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 137, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 121.0 in stage 7.0 (TID 137)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 121) in 744 ms on localhost (106/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 106.0 in stage 7.0 (TID 122). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 138, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 122.0 in stage 7.0 (TID 138)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 122) in 713 ms on localhost (107/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:50 INFO Executor: Finished task 107.0 in stage 7.0 (TID 123). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 139, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 123.0 in stage 7.0 (TID 139)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 123) in 675 ms on localhost (108/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 108.0 in stage 7.0 (TID 124). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 140, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 124.0 in stage 7.0 (TID 140)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 124) in 687 ms on localhost (109/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 109.0 in stage 7.0 (TID 125). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 141, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 125.0 in stage 7.0 (TID 141)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 125) in 683 ms on localhost (110/200)
15/08/06 17:33:50 INFO Executor: Finished task 110.0 in stage 7.0 (TID 126). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 142, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 126.0 in stage 7.0 (TID 142)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 126) in 680 ms on localhost (111/200)
15/08/06 17:33:50 INFO Executor: Finished task 111.0 in stage 7.0 (TID 127). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 143, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 127.0 in stage 7.0 (TID 143)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 127) in 657 ms on localhost (112/200)
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:50 INFO Executor: Finished task 113.0 in stage 7.0 (TID 129). 1124 bytes result sent to driver
15/08/06 17:33:50 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 144, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:50 INFO Executor: Running task 128.0 in stage 7.0 (TID 144)
15/08/06 17:33:50 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 129) in 702 ms on localhost (113/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 112.0 in stage 7.0 (TID 128). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 145, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 129.0 in stage 7.0 (TID 145)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 128) in 728 ms on localhost (114/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 114.0 in stage 7.0 (TID 130). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 146, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 130.0 in stage 7.0 (TID 146)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 130) in 778 ms on localhost (115/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 117.0 in stage 7.0 (TID 133). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 147, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 131.0 in stage 7.0 (TID 147)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 133) in 756 ms on localhost (116/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 115.0 in stage 7.0 (TID 131). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 148, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 132.0 in stage 7.0 (TID 148)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 131) in 801 ms on localhost (117/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 116.0 in stage 7.0 (TID 132). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 149, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 133.0 in stage 7.0 (TID 149)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 132) in 805 ms on localhost (118/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 118.0 in stage 7.0 (TID 134). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 150, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 134.0 in stage 7.0 (TID 150)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 134) in 820 ms on localhost (119/200)
15/08/06 17:33:51 INFO Executor: Finished task 119.0 in stage 7.0 (TID 135). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 151, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 135.0 in stage 7.0 (TID 151)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 135) in 800 ms on localhost (120/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO Executor: Finished task 133.0 in stage 7.0 (TID 149). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 152, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 149) in 58 ms on localhost (121/200)
15/08/06 17:33:51 INFO Executor: Running task 136.0 in stage 7.0 (TID 152)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO Executor: Finished task 120.0 in stage 7.0 (TID 136). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 153, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 137.0 in stage 7.0 (TID 153)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 136) in 781 ms on localhost (122/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO Executor: Finished task 121.0 in stage 7.0 (TID 137). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 154, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 138.0 in stage 7.0 (TID 154)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 137) in 581 ms on localhost (123/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 122.0 in stage 7.0 (TID 138). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 155, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 139.0 in stage 7.0 (TID 155)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 138) in 586 ms on localhost (124/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 123.0 in stage 7.0 (TID 139). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 156, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 140.0 in stage 7.0 (TID 156)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 139) in 572 ms on localhost (125/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 124.0 in stage 7.0 (TID 140). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 157, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 141.0 in stage 7.0 (TID 157)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 140) in 559 ms on localhost (126/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 126.0 in stage 7.0 (TID 142). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 158, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 142.0 in stage 7.0 (TID 158)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 142) in 534 ms on localhost (127/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO Executor: Finished task 127.0 in stage 7.0 (TID 143). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 159, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Finished task 125.0 in stage 7.0 (TID 141). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO Executor: Running task 143.0 in stage 7.0 (TID 159)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 143) in 557 ms on localhost (128/200)
15/08/06 17:33:51 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 160, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 144.0 in stage 7.0 (TID 160)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 141) in 568 ms on localhost (129/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 144.0 in stage 7.0 (TID 160). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 161, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 145.0 in stage 7.0 (TID 161)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 160) in 46 ms on localhost (130/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 128.0 in stage 7.0 (TID 144). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 144) in 490 ms on localhost (131/200)
15/08/06 17:33:51 INFO Executor: Finished task 129.0 in stage 7.0 (TID 145). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 162, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 146.0 in stage 7.0 (TID 162)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 145) in 475 ms on localhost (132/200)
15/08/06 17:33:51 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 163, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 147.0 in stage 7.0 (TID 163)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO Executor: Finished task 130.0 in stage 7.0 (TID 146). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 164, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 148.0 in stage 7.0 (TID 164)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 146) in 439 ms on localhost (133/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 132.0 in stage 7.0 (TID 148). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 165, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 149.0 in stage 7.0 (TID 165)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 148) in 421 ms on localhost (134/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 131.0 in stage 7.0 (TID 147). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 166, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 150.0 in stage 7.0 (TID 166)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 147) in 453 ms on localhost (135/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 150.0 in stage 7.0 (TID 166). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 167, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 151.0 in stage 7.0 (TID 167)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 166) in 52 ms on localhost (136/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 134.0 in stage 7.0 (TID 150). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 168, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 152.0 in stage 7.0 (TID 168)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 150) in 445 ms on localhost (137/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 135.0 in stage 7.0 (TID 151). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 169, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 153.0 in stage 7.0 (TID 169)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 151) in 484 ms on localhost (138/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 137.0 in stage 7.0 (TID 153). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 170, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 154.0 in stage 7.0 (TID 170)
15/08/06 17:33:51 INFO Executor: Finished task 136.0 in stage 7.0 (TID 152). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 153) in 465 ms on localhost (139/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 171, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO Executor: Running task 155.0 in stage 7.0 (TID 171)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 152) in 490 ms on localhost (140/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 138.0 in stage 7.0 (TID 154). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 172, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 156.0 in stage 7.0 (TID 172)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 154) in 454 ms on localhost (141/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO Executor: Finished task 153.0 in stage 7.0 (TID 169). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 173, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 157.0 in stage 7.0 (TID 173)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 169) in 62 ms on localhost (142/200)
15/08/06 17:33:51 INFO Executor: Finished task 140.0 in stage 7.0 (TID 156). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 174, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 158.0 in stage 7.0 (TID 174)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 156) in 433 ms on localhost (143/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 141.0 in stage 7.0 (TID 157). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 175, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 159.0 in stage 7.0 (TID 175)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 157) in 447 ms on localhost (144/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 139.0 in stage 7.0 (TID 155). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO Executor: Finished task 142.0 in stage 7.0 (TID 158). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 176, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 160.0 in stage 7.0 (TID 176)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 155) in 498 ms on localhost (145/200)
15/08/06 17:33:51 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 177, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 161.0 in stage 7.0 (TID 177)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 158) in 437 ms on localhost (146/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 143.0 in stage 7.0 (TID 159). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 178, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 162.0 in stage 7.0 (TID 178)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 159) in 445 ms on localhost (147/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 145.0 in stage 7.0 (TID 161). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 179, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 163.0 in stage 7.0 (TID 179)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 161) in 409 ms on localhost (148/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 162.0 in stage 7.0 (TID 178). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 180, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 164.0 in stage 7.0 (TID 180)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 178) in 56 ms on localhost (149/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 146.0 in stage 7.0 (TID 162). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 181, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 165.0 in stage 7.0 (TID 181)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 162) in 414 ms on localhost (150/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 164.0 in stage 7.0 (TID 180). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 182, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 166.0 in stage 7.0 (TID 182)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 180) in 45 ms on localhost (151/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 147.0 in stage 7.0 (TID 163). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 183, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 167.0 in stage 7.0 (TID 183)
15/08/06 17:33:51 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 163) in 461 ms on localhost (152/200)
15/08/06 17:33:51 INFO Executor: Finished task 148.0 in stage 7.0 (TID 164). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO Executor: Finished task 149.0 in stage 7.0 (TID 165). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 184, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:51 INFO Executor: Running task 168.0 in stage 7.0 (TID 184)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 164) in 422 ms on localhost (153/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 185, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 169.0 in stage 7.0 (TID 185)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 165) in 411 ms on localhost (154/200)
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:51 INFO Executor: Finished task 167.0 in stage 7.0 (TID 183). 1124 bytes result sent to driver
15/08/06 17:33:51 INFO TaskSetManager: Starting task 170.0 in stage 7.0 (TID 186, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:51 INFO Executor: Running task 170.0 in stage 7.0 (TID 186)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 183) in 46 ms on localhost (155/200)
15/08/06 17:33:52 INFO Executor: Finished task 169.0 in stage 7.0 (TID 185). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 171.0 in stage 7.0 (TID 187, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 171.0 in stage 7.0 (TID 187)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 185) in 39 ms on localhost (156/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 151.0 in stage 7.0 (TID 167). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 172.0 in stage 7.0 (TID 188, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 172.0 in stage 7.0 (TID 188)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 167) in 409 ms on localhost (157/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 152.0 in stage 7.0 (TID 168). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 173.0 in stage 7.0 (TID 189, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 173.0 in stage 7.0 (TID 189)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 168) in 420 ms on localhost (158/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 154.0 in stage 7.0 (TID 170). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 174.0 in stage 7.0 (TID 190, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 174.0 in stage 7.0 (TID 190)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 170) in 517 ms on localhost (159/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO Executor: Finished task 156.0 in stage 7.0 (TID 172). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 175.0 in stage 7.0 (TID 191, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 175.0 in stage 7.0 (TID 191)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 172) in 520 ms on localhost (160/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 173.0 in stage 7.0 (TID 189). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 176.0 in stage 7.0 (TID 192, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 176.0 in stage 7.0 (TID 192)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 173.0 in stage 7.0 (TID 189) in 190 ms on localhost (161/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 155.0 in stage 7.0 (TID 171). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 177.0 in stage 7.0 (TID 193, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 177.0 in stage 7.0 (TID 193)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 171) in 555 ms on localhost (162/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 157.0 in stage 7.0 (TID 173). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 178.0 in stage 7.0 (TID 194, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 178.0 in stage 7.0 (TID 194)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 173) in 540 ms on localhost (163/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 158.0 in stage 7.0 (TID 174). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 179.0 in stage 7.0 (TID 195, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 179.0 in stage 7.0 (TID 195)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 174) in 560 ms on localhost (164/200)
15/08/06 17:33:52 INFO Executor: Finished task 160.0 in stage 7.0 (TID 176). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 180.0 in stage 7.0 (TID 196, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 180.0 in stage 7.0 (TID 196)
15/08/06 17:33:52 INFO Executor: Finished task 159.0 in stage 7.0 (TID 175). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 176) in 521 ms on localhost (165/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO TaskSetManager: Starting task 181.0 in stage 7.0 (TID 197, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 181.0 in stage 7.0 (TID 197)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 175) in 537 ms on localhost (166/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO Executor: Finished task 180.0 in stage 7.0 (TID 196). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 182.0 in stage 7.0 (TID 198, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 182.0 in stage 7.0 (TID 198)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 180.0 in stage 7.0 (TID 196) in 48 ms on localhost (167/200)
15/08/06 17:33:52 INFO Executor: Finished task 161.0 in stage 7.0 (TID 177). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 183.0 in stage 7.0 (TID 199, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Finished task 163.0 in stage 7.0 (TID 179). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO Executor: Running task 183.0 in stage 7.0 (TID 199)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO TaskSetManager: Starting task 184.0 in stage 7.0 (TID 200, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Running task 184.0 in stage 7.0 (TID 200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 177) in 572 ms on localhost (168/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 179) in 524 ms on localhost (169/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 165.0 in stage 7.0 (TID 181). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 185.0 in stage 7.0 (TID 201, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 185.0 in stage 7.0 (TID 201)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 181) in 504 ms on localhost (170/200)
15/08/06 17:33:52 INFO Executor: Finished task 184.0 in stage 7.0 (TID 200). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 186.0 in stage 7.0 (TID 202, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 186.0 in stage 7.0 (TID 202)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 184.0 in stage 7.0 (TID 200) in 44 ms on localhost (171/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 168.0 in stage 7.0 (TID 184). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 187.0 in stage 7.0 (TID 203, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 187.0 in stage 7.0 (TID 203)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 184) in 485 ms on localhost (172/200)
15/08/06 17:33:52 INFO Executor: Finished task 186.0 in stage 7.0 (TID 202). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO TaskSetManager: Starting task 188.0 in stage 7.0 (TID 204, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO Executor: Running task 188.0 in stage 7.0 (TID 204)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 186.0 in stage 7.0 (TID 202) in 47 ms on localhost (173/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 166.0 in stage 7.0 (TID 182). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 189.0 in stage 7.0 (TID 205, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 189.0 in stage 7.0 (TID 205)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 182) in 541 ms on localhost (174/200)
15/08/06 17:33:52 INFO Executor: Finished task 171.0 in stage 7.0 (TID 187). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 190.0 in stage 7.0 (TID 206, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO Executor: Running task 190.0 in stage 7.0 (TID 206)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO TaskSetManager: Finished task 171.0 in stage 7.0 (TID 187) in 471 ms on localhost (175/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 170.0 in stage 7.0 (TID 186). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 191.0 in stage 7.0 (TID 207, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 191.0 in stage 7.0 (TID 207)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 170.0 in stage 7.0 (TID 186) in 484 ms on localhost (176/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 172.0 in stage 7.0 (TID 188). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 192.0 in stage 7.0 (TID 208, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 192.0 in stage 7.0 (TID 208)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 172.0 in stage 7.0 (TID 188) in 480 ms on localhost (177/200)
15/08/06 17:33:52 INFO Executor: Finished task 190.0 in stage 7.0 (TID 206). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 193.0 in stage 7.0 (TID 209, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 193.0 in stage 7.0 (TID 209)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 190.0 in stage 7.0 (TID 206) in 42 ms on localhost (178/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 174.0 in stage 7.0 (TID 190). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 194.0 in stage 7.0 (TID 210, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 194.0 in stage 7.0 (TID 210)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 174.0 in stage 7.0 (TID 190) in 356 ms on localhost (179/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 175.0 in stage 7.0 (TID 191). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 195.0 in stage 7.0 (TID 211, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 195.0 in stage 7.0 (TID 211)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 175.0 in stage 7.0 (TID 191) in 383 ms on localhost (180/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO Executor: Finished task 176.0 in stage 7.0 (TID 192). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 196.0 in stage 7.0 (TID 212, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 196.0 in stage 7.0 (TID 212)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 176.0 in stage 7.0 (TID 192) in 401 ms on localhost (181/200)
15/08/06 17:33:52 INFO Executor: Finished task 177.0 in stage 7.0 (TID 193). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 197.0 in stage 7.0 (TID 213, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 197.0 in stage 7.0 (TID 213)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 177.0 in stage 7.0 (TID 193) in 395 ms on localhost (182/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:52 INFO Executor: Finished task 195.0 in stage 7.0 (TID 211). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 198.0 in stage 7.0 (TID 214, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 198.0 in stage 7.0 (TID 214)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 195.0 in stage 7.0 (TID 211) in 47 ms on localhost (183/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 178.0 in stage 7.0 (TID 194). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Starting task 199.0 in stage 7.0 (TID 215, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:33:52 INFO Executor: Running task 199.0 in stage 7.0 (TID 215)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 178.0 in stage 7.0 (TID 194) in 402 ms on localhost (184/200)
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:33:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:52 INFO Executor: Finished task 197.0 in stage 7.0 (TID 213). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 197.0 in stage 7.0 (TID 213) in 43 ms on localhost (185/200)
15/08/06 17:33:52 INFO Executor: Finished task 179.0 in stage 7.0 (TID 195). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 179.0 in stage 7.0 (TID 195) in 418 ms on localhost (186/200)
15/08/06 17:33:52 INFO Executor: Finished task 181.0 in stage 7.0 (TID 197). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 181.0 in stage 7.0 (TID 197) in 411 ms on localhost (187/200)
15/08/06 17:33:52 INFO Executor: Finished task 182.0 in stage 7.0 (TID 198). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO Executor: Finished task 183.0 in stage 7.0 (TID 199). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 182.0 in stage 7.0 (TID 198) in 426 ms on localhost (188/200)
15/08/06 17:33:52 INFO TaskSetManager: Finished task 183.0 in stage 7.0 (TID 199) in 423 ms on localhost (189/200)
15/08/06 17:33:52 INFO Executor: Finished task 185.0 in stage 7.0 (TID 201). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 185.0 in stage 7.0 (TID 201) in 418 ms on localhost (190/200)
15/08/06 17:33:52 INFO Executor: Finished task 188.0 in stage 7.0 (TID 204). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 188.0 in stage 7.0 (TID 204) in 392 ms on localhost (191/200)
15/08/06 17:33:52 INFO Executor: Finished task 187.0 in stage 7.0 (TID 203). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 187.0 in stage 7.0 (TID 203) in 408 ms on localhost (192/200)
15/08/06 17:33:52 INFO Executor: Finished task 189.0 in stage 7.0 (TID 205). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 189.0 in stage 7.0 (TID 205) in 393 ms on localhost (193/200)
15/08/06 17:33:52 INFO Executor: Finished task 191.0 in stage 7.0 (TID 207). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 191.0 in stage 7.0 (TID 207) in 385 ms on localhost (194/200)
15/08/06 17:33:52 INFO Executor: Finished task 193.0 in stage 7.0 (TID 209). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 193.0 in stage 7.0 (TID 209) in 370 ms on localhost (195/200)
15/08/06 17:33:52 INFO Executor: Finished task 192.0 in stage 7.0 (TID 208). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 192.0 in stage 7.0 (TID 208) in 376 ms on localhost (196/200)
15/08/06 17:33:52 INFO Executor: Finished task 194.0 in stage 7.0 (TID 210). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 194.0 in stage 7.0 (TID 210) in 328 ms on localhost (197/200)
15/08/06 17:33:52 INFO Executor: Finished task 196.0 in stage 7.0 (TID 212). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 196.0 in stage 7.0 (TID 212) in 276 ms on localhost (198/200)
15/08/06 17:33:52 INFO Executor: Finished task 198.0 in stage 7.0 (TID 214). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 198.0 in stage 7.0 (TID 214) in 259 ms on localhost (199/200)
15/08/06 17:33:52 INFO Executor: Finished task 199.0 in stage 7.0 (TID 215). 1124 bytes result sent to driver
15/08/06 17:33:52 INFO TaskSetManager: Finished task 199.0 in stage 7.0 (TID 215) in 246 ms on localhost (200/200)
15/08/06 17:33:52 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/06 17:33:52 INFO DAGScheduler: Stage 7 (mapPartitions at Exchange.scala:64) finished in 7.463 s
15/08/06 17:33:52 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:33:52 INFO DAGScheduler: running: Set()
15/08/06 17:33:52 INFO DAGScheduler: waiting: Set(Stage 8)
15/08/06 17:33:52 INFO DAGScheduler: failed: Set()
15/08/06 17:33:52 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@11660226
15/08/06 17:33:52 INFO StatsReportListener: task runtime:(count: 200, mean: 591.350000, stdev: 420.528878, max: 2009.000000, min: 39.000000)
15/08/06 17:33:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:52 INFO StatsReportListener: 	39.0 ms	47.0 ms	63.0 ms	409.0 ms	555.0 ms	675.0 ms	856.0 ms	1.9 s	2.0 s
15/08/06 17:33:52 INFO DAGScheduler: Missing parents for Stage 8: List()
15/08/06 17:33:52 INFO DAGScheduler: Submitting Stage 8 (MapPartitionsRDD[48] at mapPartitions at Aggregate.scala:151), which is now runnable
15/08/06 17:33:52 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 7580.820000, stdev: 3876.470167, max: 17153.000000, min: 0.000000)
15/08/06 17:33:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:52 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	6.3 KB	8.1 KB	9.9 KB	11.7 KB	13.4 KB	16.8 KB
15/08/06 17:33:52 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.150000, stdev: 0.829156, max: 11.000000, min: 0.000000)
15/08/06 17:33:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:52 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	11.0 ms
15/08/06 17:33:52 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:33:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:52 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:33:52 INFO StatsReportListener: task result size:(count: 200, mean: 1124.000000, stdev: 0.000000, max: 1124.000000, min: 1124.000000)
15/08/06 17:33:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:52 INFO StatsReportListener: 	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B
15/08/06 17:33:52 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.282942, stdev: 4.022629, max: 99.595469, min: 77.358491)
15/08/06 17:33:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:52 INFO StatsReportListener: 	77 %	88 %	92 %	98 %	99 %	99 %	99 %	99 %	100 %
15/08/06 17:33:52 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.050574, stdev: 0.258793, max: 2.127660, min: 0.000000)
15/08/06 17:33:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:52 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 2 %
15/08/06 17:33:52 INFO StatsReportListener: other time pct: (count: 200, mean: 2.666484, stdev: 3.950552, max: 22.641509, min: 0.363636)
15/08/06 17:33:52 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:52 INFO StatsReportListener: 	 0 %	 1 %	 1 %	 1 %	 1 %	 2 %	 9 %	12 %	23 %
15/08/06 17:33:53 INFO MemoryStore: ensureFreeSpace(149960) called with curMem=768433, maxMem=3333968363
15/08/06 17:33:53 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 146.4 KB, free 3.1 GB)
15/08/06 17:33:53 INFO MemoryStore: ensureFreeSpace(65197) called with curMem=918393, maxMem=3333968363
15/08/06 17:33:53 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 63.7 KB, free 3.1 GB)
15/08/06 17:33:53 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:42931 (size: 63.7 KB, free: 3.1 GB)
15/08/06 17:33:53 INFO BlockManagerMaster: Updated info of block broadcast_12_piece0
15/08/06 17:33:53 INFO DefaultExecutionContext: Created broadcast 12 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:53 INFO DAGScheduler: Submitting 200 missing tasks from Stage 8 (MapPartitionsRDD[48] at mapPartitions at Aggregate.scala:151)
15/08/06 17:33:53 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/06 17:33:53 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 216, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 217, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 218, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 219, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 220, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 221, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 222, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 223, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 224, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 225, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 226, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 227, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 228, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 229, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 230, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 231, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:53 INFO Executor: Running task 0.0 in stage 8.0 (TID 216)
15/08/06 17:33:53 INFO Executor: Running task 6.0 in stage 8.0 (TID 222)
15/08/06 17:33:53 INFO Executor: Running task 9.0 in stage 8.0 (TID 225)
15/08/06 17:33:53 INFO Executor: Running task 15.0 in stage 8.0 (TID 231)
15/08/06 17:33:53 INFO Executor: Running task 2.0 in stage 8.0 (TID 218)
15/08/06 17:33:53 INFO Executor: Running task 1.0 in stage 8.0 (TID 217)
15/08/06 17:33:53 INFO Executor: Running task 13.0 in stage 8.0 (TID 229)
15/08/06 17:33:53 INFO Executor: Running task 10.0 in stage 8.0 (TID 226)
15/08/06 17:33:53 INFO Executor: Running task 12.0 in stage 8.0 (TID 228)
15/08/06 17:33:53 INFO Executor: Running task 14.0 in stage 8.0 (TID 230)
15/08/06 17:33:53 INFO Executor: Running task 7.0 in stage 8.0 (TID 223)
15/08/06 17:33:53 INFO Executor: Running task 5.0 in stage 8.0 (TID 221)
15/08/06 17:33:53 INFO Executor: Running task 11.0 in stage 8.0 (TID 227)
15/08/06 17:33:53 INFO Executor: Running task 4.0 in stage 8.0 (TID 220)
15/08/06 17:33:53 INFO Executor: Running task 8.0 in stage 8.0 (TID 224)
15/08/06 17:33:53 INFO Executor: Running task 3.0 in stage 8.0 (TID 219)
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@403131d3
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5dbe3491
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6583023d
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7b2f337a
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57dccdc4
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19d095d7
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d1d637e
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000012_228/part-00012
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000006_222/part-00006
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000010_226/part-00010
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000007_223/part-00007
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000000_216/part-00000
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000014_230/part-00014
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000003_219/part-00003
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d2614f3
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000004_220/part-00004
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6870191a
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000015_231/part-00015
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5862af29
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@60b0541e
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000013_229/part-00013
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000008_224/part-00008
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c371682
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000009_225/part-00009
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f19866
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000005_221/part-00005
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23c8bc5d
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000002_218/part-00002
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a489ed4
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000011_227/part-00011
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@77eaa446
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000001_217/part-00001
15/08/06 17:33:53 INFO CodecConfig: Compression set to false
15/08/06 17:33:53 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:53 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7a9db421
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f7d779c
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e7a7575
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d02df7
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@fe2fdb1
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64f1161d
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5cfcfbb4
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79d216b
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2679b61c
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@69b56c29
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e40de68
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6ff6d92
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f640148
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e903e7b
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6ff61d53
15/08/06 17:33:53 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@637e428
15/08/06 17:33:53 INFO BlockManager: Removing broadcast 11
15/08/06 17:33:53 INFO BlockManager: Removing block broadcast_11
15/08/06 17:33:53 INFO MemoryStore: Block broadcast_11 of size 13544 dropped from memory (free 3332998317)
15/08/06 17:33:53 INFO BlockManager: Removing block broadcast_11_piece0
15/08/06 17:33:53 INFO MemoryStore: Block broadcast_11_piece0 of size 7369 dropped from memory (free 3333005686)
15/08/06 17:33:53 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:42931 in memory (size: 7.2 KB, free: 3.1 GB)
15/08/06 17:33:53 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/08/06 17:33:53 INFO ContextCleaner: Cleaned broadcast 11
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,836
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,136
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,396
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,616
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,356
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,076
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,816
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,136
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:33:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,156
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 711B for [ps_partkey] INT32: 167 values, 675B raw, 675B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 615B for [ps_partkey] INT32: 143 values, 579B raw, 579B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 751B for [ps_partkey] INT32: 177 values, 715B raw, 715B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,387B for [part_value] DOUBLE: 167 values, 1,343B raw, 1,343B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,195B for [part_value] DOUBLE: 143 values, 1,151B raw, 1,151B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 815B for [ps_partkey] INT32: 193 values, 779B raw, 779B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,467B for [part_value] DOUBLE: 177 values, 1,423B raw, 1,423B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,595B for [part_value] DOUBLE: 193 values, 1,551B raw, 1,551B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 659B for [ps_partkey] INT32: 154 values, 623B raw, 623B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 555B for [ps_partkey] INT32: 128 values, 519B raw, 519B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 603B for [ps_partkey] INT32: 140 values, 567B raw, 567B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 619B for [ps_partkey] INT32: 144 values, 583B raw, 583B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,171B for [part_value] DOUBLE: 140 values, 1,127B raw, 1,127B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 667B for [ps_partkey] INT32: 156 values, 631B raw, 631B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,203B for [part_value] DOUBLE: 144 values, 1,159B raw, 1,159B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,075B for [part_value] DOUBLE: 128 values, 1,031B raw, 1,031B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,283B for [part_value] DOUBLE: 154 values, 1,239B raw, 1,239B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:53 INFO ColumnChunkPageWriteStore: written 1,299B for [part_value] DOUBLE: 156 values, 1,255B raw, 1,255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000007_223' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000007
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000000_216' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000000
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000010_226' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000010
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000010_226: Committed
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000007_223: Committed
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000000_216: Committed
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000005_221' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000005
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000005_221: Committed
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000001_217' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000001
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000004_220' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000004
15/08/06 17:33:54 INFO Executor: Finished task 5.0 in stage 8.0 (TID 221). 781 bytes result sent to driver
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000004_220: Committed
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000001_217: Committed
15/08/06 17:33:54 INFO Executor: Finished task 0.0 in stage 8.0 (TID 216). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 7.0 in stage 8.0 (TID 223). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 10.0 in stage 8.0 (TID 226). 781 bytes result sent to driver
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000013_229' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000013
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000013_229: Committed
15/08/06 17:33:54 INFO Executor: Finished task 1.0 in stage 8.0 (TID 217). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 4.0 in stage 8.0 (TID 220). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 232, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 16.0 in stage 8.0 (TID 232)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000003_219' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000003
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000014_230' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000014
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000014_230: Committed
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000003_219: Committed
15/08/06 17:33:54 INFO Executor: Finished task 13.0 in stage 8.0 (TID 229). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 221) in 1031 ms on localhost (1/200)
15/08/06 17:33:54 INFO Executor: Finished task 3.0 in stage 8.0 (TID 219). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 14.0 in stage 8.0 (TID 230). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 233, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 17.0 in stage 8.0 (TID 233)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000009_225' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000009
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000009_225: Committed
15/08/06 17:33:54 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 216) in 1035 ms on localhost (2/200)
15/08/06 17:33:54 INFO Executor: Finished task 9.0 in stage 8.0 (TID 225). 781 bytes result sent to driver
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000008_224' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000008
15/08/06 17:33:54 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 234, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000012_228' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000012
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000012_228: Committed
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000002_218' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000002
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000006_222' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000006
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000002_218: Committed
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000006_222: Committed
15/08/06 17:33:54 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 223) in 1032 ms on localhost (3/200)
15/08/06 17:33:54 INFO Executor: Finished task 12.0 in stage 8.0 (TID 228). 781 bytes result sent to driver
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000008_224: Committed
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000015_231' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000015
15/08/06 17:33:54 INFO Executor: Finished task 2.0 in stage 8.0 (TID 218). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Running task 18.0 in stage 8.0 (TID 234)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 235, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 19.0 in stage 8.0 (TID 235)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 236, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Finished task 8.0 in stage 8.0 (TID 224). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Running task 20.0 in stage 8.0 (TID 236)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 237, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 21.0 in stage 8.0 (TID 237)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 238, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000011_227' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000011
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000011_227: Committed
15/08/06 17:33:54 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 229) in 1033 ms on localhost (4/200)
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000015_231: Committed
15/08/06 17:33:54 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 220) in 1039 ms on localhost (5/200)
15/08/06 17:33:54 INFO Executor: Finished task 11.0 in stage 8.0 (TID 227). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Running task 22.0 in stage 8.0 (TID 238)
15/08/06 17:33:54 INFO Executor: Finished task 6.0 in stage 8.0 (TID 222). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 15.0 in stage 8.0 (TID 231). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 217) in 1041 ms on localhost (6/200)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 226) in 1038 ms on localhost (7/200)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 219) in 1042 ms on localhost (8/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 239, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 23.0 in stage 8.0 (TID 239)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 240, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 24.0 in stage 8.0 (TID 240)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 230) in 1040 ms on localhost (9/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 241, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 25.0 in stage 8.0 (TID 241)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 225) in 1044 ms on localhost (10/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 242, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 26.0 in stage 8.0 (TID 242)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 228) in 1045 ms on localhost (11/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 243, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 218) in 1052 ms on localhost (12/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 244, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 28.0 in stage 8.0 (TID 244)
15/08/06 17:33:54 INFO Executor: Running task 27.0 in stage 8.0 (TID 243)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 224) in 1051 ms on localhost (13/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 245, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 29.0 in stage 8.0 (TID 245)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 222) in 1054 ms on localhost (14/200)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 231) in 1051 ms on localhost (15/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 246, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 30.0 in stage 8.0 (TID 246)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 247, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 31.0 in stage 8.0 (TID 247)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 227) in 1055 ms on localhost (16/200)
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4e305688
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6f152daa
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@478122d2
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000018_234/part-00018
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000021_237/part-00021
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000016_232/part-00016
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57692cdf
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000029_245/part-00029
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d1e1bf3
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000022_238/part-00022
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e51e032
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1156022b
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000024_240/part-00024
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000025_241/part-00025
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@50af8c87
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35331c73
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000023_239/part-00023
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@dd80dbb
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@636d7716
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7fa77fba
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000026_242/part-00026
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d370a50
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2df96371
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,276
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1fb939ff
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@43aee4e
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,356
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000020_236/part-00020
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 659B for [ps_partkey] INT32: 154 values, 623B raw, 623B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,283B for [part_value] DOUBLE: 154 values, 1,239B raw, 1,239B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b3a75b0
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b04accf
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 643B for [ps_partkey] INT32: 150 values, 607B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000030_246/part-00030
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,251B for [part_value] DOUBLE: 150 values, 1,207B raw, 1,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d75b8d5
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000019_235/part-00019
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4365e5db
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,856
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@533bf759
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@50d6d249
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 759B for [ps_partkey] INT32: 179 values, 723B raw, 723B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000017_233/part-00017
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,483B for [part_value] DOUBLE: 179 values, 1,439B raw, 1,439B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@36aeb523
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000027_243/part-00027
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31f5cfdc
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000028_244/part-00028
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4545889d
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f5ce775
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3be1ff89
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7984cef
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000031_247/part-00031
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1fe2e2ce
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@13c359ab
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000025_241' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000025
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000025_241: Committed
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000029_245' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000029
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000018_234' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000018
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000029_245: Committed
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000022_238' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000022
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000018_234: Committed
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000022_238: Committed
15/08/06 17:33:54 INFO Executor: Finished task 29.0 in stage 8.0 (TID 245). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 25.0 in stage 8.0 (TID 241). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 22.0 in stage 8.0 (TID 238). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 18.0 in stage 8.0 (TID 234). 781 bytes result sent to driver
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15ee474f
15/08/06 17:33:54 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 248, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 32.0 in stage 8.0 (TID 248)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000024_240' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000024
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000016_232' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000016
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000024_240: Committed
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000016_232: Committed
15/08/06 17:33:54 INFO Executor: Finished task 16.0 in stage 8.0 (TID 232). 781 bytes result sent to driver
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,296
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO Executor: Finished task 24.0 in stage 8.0 (TID 240). 781 bytes result sent to driver
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 647B for [ps_partkey] INT32: 151 values, 611B raw, 611B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 245) in 232 ms on localhost (17/200)
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,259B for [part_value] DOUBLE: 151 values, 1,215B raw, 1,215B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 249, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 33.0 in stage 8.0 (TID 249)
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@52dc9c9c
15/08/06 17:33:54 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 241) in 351 ms on localhost (18/200)
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,036
15/08/06 17:33:54 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 250, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 34.0 in stage 8.0 (TID 250)
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 395B for [ps_partkey] INT32: 88 values, 359B raw, 359B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 238) in 366 ms on localhost (19/200)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000019_235' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000019
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000019_235: Committed
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 755B for [part_value] DOUBLE: 88 values, 711B raw, 711B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 251, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 35.0 in stage 8.0 (TID 251)
15/08/06 17:33:54 INFO Executor: Finished task 19.0 in stage 8.0 (TID 235). 781 bytes result sent to driver
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000026_242' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000026
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000020_236' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000020
15/08/06 17:33:54 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 234) in 371 ms on localhost (20/200)
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000020_236: Committed
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000026_242: Committed
15/08/06 17:33:54 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 252, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 36.0 in stage 8.0 (TID 252)
15/08/06 17:33:54 INFO Executor: Finished task 20.0 in stage 8.0 (TID 236). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 232) in 376 ms on localhost (21/200)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000023_239' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000023
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000023_239: Committed
15/08/06 17:33:54 INFO Executor: Finished task 26.0 in stage 8.0 (TID 242). 781 bytes result sent to driver
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000030_246' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000030
15/08/06 17:33:54 INFO Executor: Finished task 23.0 in stage 8.0 (TID 239). 781 bytes result sent to driver
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000030_246: Committed
15/08/06 17:33:54 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 253, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 37.0 in stage 8.0 (TID 253)
15/08/06 17:33:54 INFO Executor: Finished task 30.0 in stage 8.0 (TID 246). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 240) in 365 ms on localhost (22/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 254, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 38.0 in stage 8.0 (TID 254)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 235) in 379 ms on localhost (23/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 255, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 39.0 in stage 8.0 (TID 255)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 236) in 380 ms on localhost (24/200)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000027_243' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000027
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000027_243: Committed
15/08/06 17:33:54 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 256, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Finished task 27.0 in stage 8.0 (TID 243). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 242) in 369 ms on localhost (25/200)
15/08/06 17:33:54 INFO Executor: Running task 40.0 in stage 8.0 (TID 256)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 257, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 41.0 in stage 8.0 (TID 257)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 239) in 377 ms on localhost (26/200)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000017_233' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000017
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000017_233: Committed
15/08/06 17:33:54 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 258, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Finished task 17.0 in stage 8.0 (TID 233). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Running task 42.0 in stage 8.0 (TID 258)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 246) in 365 ms on localhost (27/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 259, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 43.0 in stage 8.0 (TID 259)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 243) in 374 ms on localhost (28/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 260, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 44.0 in stage 8.0 (TID 260)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 233) in 397 ms on localhost (29/200)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000031_247' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000031
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000031_247: Committed
15/08/06 17:33:54 INFO Executor: Finished task 31.0 in stage 8.0 (TID 247). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 261, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 45.0 in stage 8.0 (TID 261)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 247) in 374 ms on localhost (30/200)
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@41d563a4
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000032_248/part-00032
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2080c5c0
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000035_251/part-00035
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e0dc449
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@79d1b472
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000036_252/part-00036
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000038_254/part-00038
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@369de6d9
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@389ecc92
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000034_250/part-00034
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000037_253/part-00037
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@71564694
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5733d8ad
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000039_255/part-00039
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@cd67295
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000040_256/part-00040
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@402efcb
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5cccfa7b
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000042_258/part-00042
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,576
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7621100a
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000043_259/part-00043
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@74a43d1c
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4aa83517
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 703B for [ps_partkey] INT32: 165 values, 667B raw, 667B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,371B for [part_value] DOUBLE: 165 values, 1,327B raw, 1,327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6381db12
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6255adc7
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000033_249/part-00033
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67041640
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2363627
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@80567ce
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,116
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000045_261/part-00045
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23e2f02a
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ae2097f
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 811B for [ps_partkey] INT32: 192 values, 775B raw, 775B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,587B for [part_value] DOUBLE: 192 values, 1,543B raw, 1,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,676
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000044_260/part-00044
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 523B for [ps_partkey] INT32: 120 values, 487B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,011B for [part_value] DOUBLE: 120 values, 967B raw, 967B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6ddd7ea2
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@68a4f607
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@26147b3d
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000041_257/part-00041
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000035_251' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000035
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000035_251: Committed
15/08/06 17:33:54 INFO Executor: Finished task 35.0 in stage 8.0 (TID 251). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 262, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000037_253' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000037
15/08/06 17:33:54 INFO Executor: Running task 46.0 in stage 8.0 (TID 262)
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12dfb6a6
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d5055f8
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@53c1bd1
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000037_253: Committed
15/08/06 17:33:54 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 251) in 239 ms on localhost (31/200)
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,636
15/08/06 17:33:54 INFO Executor: Finished task 37.0 in stage 8.0 (TID 253). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 263, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,816
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO Executor: Running task 47.0 in stage 8.0 (TID 263)
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 253) in 243 ms on localhost (32/200)
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 515B for [ps_partkey] INT32: 118 values, 479B raw, 479B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 551B for [ps_partkey] INT32: 127 values, 515B raw, 515B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 995B for [part_value] DOUBLE: 118 values, 951B raw, 951B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,067B for [part_value] DOUBLE: 127 values, 1,023B raw, 1,023B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bde8e6
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000039_255' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000039
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000039_255: Committed
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000036_252' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000036
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000036_252: Committed
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000038_254' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000038
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000038_254: Committed
15/08/06 17:33:54 INFO Executor: Finished task 39.0 in stage 8.0 (TID 255). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 36.0 in stage 8.0 (TID 252). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Finished task 38.0 in stage 8.0 (TID 254). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 264, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 48.0 in stage 8.0 (TID 264)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 255) in 249 ms on localhost (33/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 265, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 49.0 in stage 8.0 (TID 265)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 252) in 260 ms on localhost (34/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 266, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 50.0 in stage 8.0 (TID 266)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 254) in 254 ms on localhost (35/200)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000032_248' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000032
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000032_248: Committed
15/08/06 17:33:54 INFO Executor: Finished task 32.0 in stage 8.0 (TID 248). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 267, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 51.0 in stage 8.0 (TID 267)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 248) in 390 ms on localhost (36/200)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000044_260' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000044
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000044_260: Committed
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000021_237' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000021
15/08/06 17:33:54 INFO Executor: Finished task 44.0 in stage 8.0 (TID 260). 781 bytes result sent to driver
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000021_237: Committed
15/08/06 17:33:54 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 268, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Finished task 21.0 in stage 8.0 (TID 237). 781 bytes result sent to driver
15/08/06 17:33:54 INFO Executor: Running task 52.0 in stage 8.0 (TID 268)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000045_261' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000045
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000045_261: Committed
15/08/06 17:33:54 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 260) in 254 ms on localhost (37/200)
15/08/06 17:33:54 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 269, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO Executor: Running task 53.0 in stage 8.0 (TID 269)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 237) in 647 ms on localhost (38/200)
15/08/06 17:33:54 INFO Executor: Finished task 45.0 in stage 8.0 (TID 261). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 270, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 54.0 in stage 8.0 (TID 270)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 261) in 252 ms on localhost (39/200)
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000033_249' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000033
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000033_249: Committed
15/08/06 17:33:54 INFO Executor: Finished task 33.0 in stage 8.0 (TID 249). 781 bytes result sent to driver
15/08/06 17:33:54 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 271, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 55.0 in stage 8.0 (TID 271)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 249) in 294 ms on localhost (40/200)
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53cced39
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000047_263/part-00047
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@32ea642a
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000049_265/part-00049
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a561b83
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000050_266/part-00050
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6e2c173c
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@10a99ca6
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,176
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 423B for [ps_partkey] INT32: 95 values, 387B raw, 387B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 811B for [part_value] DOUBLE: 95 values, 767B raw, 767B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51c2ca55
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b71bc62
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000051_267/part-00051
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000028_244' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000028
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000028_244: Committed
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO Executor: Finished task 28.0 in stage 8.0 (TID 244). 781 bytes result sent to driver
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62bce5ca
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000054_270/part-00054
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 272, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:54 INFO Executor: Running task 56.0 in stage 8.0 (TID 272)
15/08/06 17:33:54 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 244) in 832 ms on localhost (41/200)
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4839b078
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000052_268/part-00052
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@364d4c60
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@610adcd2
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000053_269/part-00053
15/08/06 17:33:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:54 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62d316c6
15/08/06 17:33:54 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000055_271/part-00055
15/08/06 17:33:54 INFO CodecConfig: Compression set to false
15/08/06 17:33:54 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:54 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:54 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@9c82161
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30bac624
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1939975c
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000046_262/part-00046
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000047_263' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000047
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000047_263: Committed
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5888891e
15/08/06 17:33:55 INFO Executor: Finished task 47.0 in stage 8.0 (TID 263). 781 bytes result sent to driver
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,636
15/08/06 17:33:55 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 273, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 57.0 in stage 8.0 (TID 273)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000049_265' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000049
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000049_265: Committed
15/08/06 17:33:55 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 263) in 262 ms on localhost (42/200)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 515B for [ps_partkey] INT32: 118 values, 479B raw, 479B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO Executor: Finished task 49.0 in stage 8.0 (TID 265). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 274, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 58.0 in stage 8.0 (TID 274)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 265) in 250 ms on localhost (43/200)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e5b55bb
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,336
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 455B for [ps_partkey] INT32: 103 values, 419B raw, 419B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 875B for [part_value] DOUBLE: 103 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 995B for [part_value] DOUBLE: 118 values, 951B raw, 951B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@726609ad
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000050_266' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000050
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000050_266: Committed
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@154c04a4
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000048_264/part-00048
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO Executor: Finished task 50.0 in stage 8.0 (TID 266). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 275, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000051_267' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000051
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000051_267: Committed
15/08/06 17:33:55 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 266) in 260 ms on localhost (44/200)
15/08/06 17:33:55 INFO Executor: Running task 59.0 in stage 8.0 (TID 275)
15/08/06 17:33:55 INFO Executor: Finished task 51.0 in stage 8.0 (TID 267). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 276, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 60.0 in stage 8.0 (TID 276)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 267) in 257 ms on localhost (45/200)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000052_268' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000052
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000054_270' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000054
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000054_270: Committed
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000052_268: Committed
15/08/06 17:33:55 INFO Executor: Finished task 54.0 in stage 8.0 (TID 270). 781 bytes result sent to driver
15/08/06 17:33:55 INFO Executor: Finished task 52.0 in stage 8.0 (TID 268). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 277, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 61.0 in stage 8.0 (TID 277)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@514e19b6
15/08/06 17:33:55 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 270) in 253 ms on localhost (46/200)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 278, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 62.0 in stage 8.0 (TID 278)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 268) in 259 ms on localhost (47/200)
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000055_271' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000055
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000055_271: Committed
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO Executor: Finished task 55.0 in stage 8.0 (TID 271). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 279, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 63.0 in stage 8.0 (TID 279)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 271) in 257 ms on localhost (48/200)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000046_262' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000046
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000046_262: Committed
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO Executor: Finished task 46.0 in stage 8.0 (TID 262). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 280, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 64.0 in stage 8.0 (TID 280)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 262) in 313 ms on localhost (49/200)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000048_264' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000048
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000048_264: Committed
15/08/06 17:33:55 INFO Executor: Finished task 48.0 in stage 8.0 (TID 264). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 281, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 65.0 in stage 8.0 (TID 281)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 264) in 308 ms on localhost (50/200)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000034_250' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000034
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000034_250: Committed
15/08/06 17:33:55 INFO Executor: Finished task 34.0 in stage 8.0 (TID 250). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 282, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 66.0 in stage 8.0 (TID 282)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 250) in 654 ms on localhost (51/200)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000040_256' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000040
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000040_256: Committed
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000043_259' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000043
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000043_259: Committed
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000042_258' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000042
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000042_258: Committed
15/08/06 17:33:55 INFO Executor: Finished task 40.0 in stage 8.0 (TID 256). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 283, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 67.0 in stage 8.0 (TID 283)
15/08/06 17:33:55 INFO Executor: Finished task 43.0 in stage 8.0 (TID 259). 781 bytes result sent to driver
15/08/06 17:33:55 INFO Executor: Finished task 42.0 in stage 8.0 (TID 258). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 256) in 650 ms on localhost (52/200)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 284, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 68.0 in stage 8.0 (TID 284)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 258) in 648 ms on localhost (53/200)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 285, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 69.0 in stage 8.0 (TID 285)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 259) in 647 ms on localhost (54/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d847267
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000058_274/part-00058
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000041_257' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000041
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000041_257: Committed
15/08/06 17:33:55 INFO Executor: Finished task 41.0 in stage 8.0 (TID 257). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 286, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 70.0 in stage 8.0 (TID 286)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 257) in 663 ms on localhost (55/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7c8c976e
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000059_275/part-00059
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@75fd89c1
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ca74ae0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000064_280/part-00064
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,516
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 491B for [ps_partkey] INT32: 112 values, 455B raw, 455B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 947B for [part_value] DOUBLE: 112 values, 903B raw, 903B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4df463ef
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000060_276/part-00060
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@120d7136
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000057_273/part-00057
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10b33b47
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000056_272/part-00056
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@55478652
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a77f2a4
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000062_278/part-00062
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000063_279/part-00063
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ac39d3b
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@743e77f8
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6e1ed59
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,336
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1518d8a4
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 855B for [ps_partkey] INT32: 203 values, 819B raw, 819B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,675B for [part_value] DOUBLE: 203 values, 1,631B raw, 1,631B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c1ca421
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51913dba
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@166ba82c
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,056
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000058_274' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000058
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000058_274: Committed
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 599B for [ps_partkey] INT32: 139 values, 563B raw, 563B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,163B for [part_value] DOUBLE: 139 values, 1,119B raw, 1,119B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO Executor: Finished task 58.0 in stage 8.0 (TID 274). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 287, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 71.0 in stage 8.0 (TID 287)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 274) in 210 ms on localhost (56/200)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000059_275' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000059
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000059_275: Committed
15/08/06 17:33:55 INFO Executor: Finished task 59.0 in stage 8.0 (TID 275). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 288, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 72.0 in stage 8.0 (TID 288)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 275) in 216 ms on localhost (57/200)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000060_276' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000060
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000062_278' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000062
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000060_276: Committed
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000062_278: Committed
15/08/06 17:33:55 INFO Executor: Finished task 60.0 in stage 8.0 (TID 276). 781 bytes result sent to driver
15/08/06 17:33:55 INFO Executor: Finished task 62.0 in stage 8.0 (TID 278). 781 bytes result sent to driver
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000057_273' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000057
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000057_273: Committed
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000056_272' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000056
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000056_272: Committed
15/08/06 17:33:55 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 289, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 73.0 in stage 8.0 (TID 289)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 290, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Finished task 57.0 in stage 8.0 (TID 273). 781 bytes result sent to driver
15/08/06 17:33:55 INFO Executor: Finished task 56.0 in stage 8.0 (TID 272). 781 bytes result sent to driver
15/08/06 17:33:55 INFO Executor: Running task 74.0 in stage 8.0 (TID 290)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 276) in 217 ms on localhost (58/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2b4ed974
15/08/06 17:33:55 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 291, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000061_277/part-00061
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO Executor: Running task 75.0 in stage 8.0 (TID 291)
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 278) in 210 ms on localhost (59/200)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000063_279' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000063
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000063_279: Committed
15/08/06 17:33:55 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 273) in 239 ms on localhost (60/200)
15/08/06 17:33:55 INFO Executor: Finished task 63.0 in stage 8.0 (TID 279). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 272) in 268 ms on localhost (61/200)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 292, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 76.0 in stage 8.0 (TID 292)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 293, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 77.0 in stage 8.0 (TID 293)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 279) in 208 ms on localhost (62/200)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@736c6a99
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@52ca9e02
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000065_281/part-00065
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1a23c1a1
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e0c92d9
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000066_282/part-00066
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000061_277' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000061
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000061_277: Committed
15/08/06 17:33:55 INFO Executor: Finished task 61.0 in stage 8.0 (TID 277). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 294, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 78.0 in stage 8.0 (TID 294)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5fd07764
15/08/06 17:33:55 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 277) in 322 ms on localhost (63/200)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000068_284/part-00068
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4a651360
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,816
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 751B for [ps_partkey] INT32: 177 values, 715B raw, 715B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,467B for [part_value] DOUBLE: 177 values, 1,423B raw, 1,423B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5dabf27f
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000070_286/part-00070
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000065_281' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000065
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000065_281: Committed
15/08/06 17:33:55 INFO Executor: Finished task 65.0 in stage 8.0 (TID 281). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 295, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 79.0 in stage 8.0 (TID 295)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@27f97070
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000067_283/part-00067
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 281) in 320 ms on localhost (64/200)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7fd1ce11
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@272fffa6
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e06dd85
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000071_287/part-00071
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,716
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 531B for [ps_partkey] INT32: 122 values, 495B raw, 495B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,027B for [part_value] DOUBLE: 122 values, 983B raw, 983B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000066_282' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000066
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000066_282: Committed
15/08/06 17:33:55 INFO Executor: Finished task 66.0 in stage 8.0 (TID 282). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5e9a9ffb
15/08/06 17:33:55 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 296, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 80.0 in stage 8.0 (TID 296)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 282) in 246 ms on localhost (65/200)
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@16be1c6c
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3ac6c26
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000069_285/part-00069
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000068_284' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000068
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000068_284: Committed
15/08/06 17:33:55 INFO Executor: Finished task 68.0 in stage 8.0 (TID 284). 781 bytes result sent to driver
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000070_286' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000070
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000070_286: Committed
15/08/06 17:33:55 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 297, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 81.0 in stage 8.0 (TID 297)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 284) in 257 ms on localhost (66/200)
15/08/06 17:33:55 INFO Executor: Finished task 70.0 in stage 8.0 (TID 286). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 298, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 82.0 in stage 8.0 (TID 298)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 286) in 247 ms on localhost (67/200)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d5d3111
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5ecb0150
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000072_288/part-00072
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e8d0139
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000074_290/part-00074
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000053_269' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000053
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000053_269: Committed
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO Executor: Finished task 53.0 in stage 8.0 (TID 269). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000071_287' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000071
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000071_287: Committed
15/08/06 17:33:55 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 299, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 83.0 in stage 8.0 (TID 299)
15/08/06 17:33:55 INFO Executor: Finished task 71.0 in stage 8.0 (TID 287). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 269) in 665 ms on localhost (68/200)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 300, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 84.0 in stage 8.0 (TID 300)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 287) in 226 ms on localhost (69/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c2ade30
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000069_285' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000069
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000069_285: Committed
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000077_293/part-00077
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO Executor: Finished task 69.0 in stage 8.0 (TID 285). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51030b61
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 301, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@54147ee1
15/08/06 17:33:55 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 285) in 285 ms on localhost (70/200)
15/08/06 17:33:55 INFO Executor: Running task 85.0 in stage 8.0 (TID 301)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000073_289/part-00073
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3859adee
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@11e76a39
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000076_292/part-00076
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6feecd5d
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3255c339
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,236
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 635B for [ps_partkey] INT32: 148 values, 599B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,235B for [part_value] DOUBLE: 148 values, 1,191B raw, 1,191B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000072_288' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000072
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000072_288: Committed
15/08/06 17:33:55 INFO Executor: Finished task 72.0 in stage 8.0 (TID 288). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 302, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 86.0 in stage 8.0 (TID 302)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 288) in 251 ms on localhost (71/200)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000077_293' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000077
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000077_293: Committed
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f31ea9
15/08/06 17:33:55 INFO Executor: Finished task 77.0 in stage 8.0 (TID 293). 781 bytes result sent to driver
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:33:55 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 303, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 87.0 in stage 8.0 (TID 303)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 293) in 242 ms on localhost (72/200)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000074_290' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000074
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000074_290: Committed
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@415a109f
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000079_295/part-00079
15/08/06 17:33:55 INFO Executor: Finished task 74.0 in stage 8.0 (TID 290). 781 bytes result sent to driver
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 304, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@f136e5
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000075_291/part-00075
15/08/06 17:33:55 INFO Executor: Running task 88.0 in stage 8.0 (TID 304)
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 290) in 261 ms on localhost (73/200)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2f480cd6
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000076_292' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000076
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000076_292: Committed
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO Executor: Finished task 76.0 in stage 8.0 (TID 292). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 305, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7cec9cb9
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 292) in 269 ms on localhost (74/200)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000080_296/part-00080
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:33:55 INFO Executor: Running task 89.0 in stage 8.0 (TID 305)
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a270de1
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000078_294/part-00078
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7abdf0ff
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1db21ac4
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@727b914d
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,096
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7858f0b9
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000081_297/part-00081
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 807B for [ps_partkey] INT32: 191 values, 771B raw, 771B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,579B for [part_value] DOUBLE: 191 values, 1,535B raw, 1,535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000079_295' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000079
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000079_295: Committed
15/08/06 17:33:55 INFO Executor: Finished task 79.0 in stage 8.0 (TID 295). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 306, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 90.0 in stage 8.0 (TID 306)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 295) in 167 ms on localhost (75/200)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@421ea1ff
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000075_291' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000075
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000075_291: Committed
15/08/06 17:33:55 INFO Executor: Finished task 75.0 in stage 8.0 (TID 291). 781 bytes result sent to driver
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000078_294' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000078
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000078_294: Committed
15/08/06 17:33:55 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 307, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 91.0 in stage 8.0 (TID 307)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 291) in 312 ms on localhost (76/200)
15/08/06 17:33:55 INFO Executor: Finished task 78.0 in stage 8.0 (TID 294). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 308, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000080_296' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000080
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000080_296: Committed
15/08/06 17:33:55 INFO Executor: Running task 92.0 in stage 8.0 (TID 308)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@511f7433
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000084_300/part-00084
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO Executor: Finished task 80.0 in stage 8.0 (TID 296). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 294) in 212 ms on localhost (77/200)
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 309, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 93.0 in stage 8.0 (TID 309)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 296) in 173 ms on localhost (78/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1263bebe
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000082_298/part-00082
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@33440121
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000081_297' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000081
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000081_297: Committed
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@113fb296
15/08/06 17:33:55 INFO Executor: Finished task 81.0 in stage 8.0 (TID 297). 781 bytes result sent to driver
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000064_280' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000064
15/08/06 17:33:55 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 310, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000064_280: Committed
15/08/06 17:33:55 INFO Executor: Running task 94.0 in stage 8.0 (TID 310)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 297) in 262 ms on localhost (79/200)
15/08/06 17:33:55 INFO Executor: Finished task 64.0 in stage 8.0 (TID 280). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 311, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 95.0 in stage 8.0 (TID 311)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 280) in 635 ms on localhost (80/200)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000084_300' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000084
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000084_300: Committed
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO Executor: Finished task 84.0 in stage 8.0 (TID 300). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 312, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 96.0 in stage 8.0 (TID 312)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 300) in 255 ms on localhost (81/200)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000082_298' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000082
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000082_298: Committed
15/08/06 17:33:55 INFO Executor: Finished task 82.0 in stage 8.0 (TID 298). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 313, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 97.0 in stage 8.0 (TID 313)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 298) in 280 ms on localhost (82/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@12277387
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000087_303/part-00087
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c25995f
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@68783a68
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000086_302/part-00086
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000085_301/part-00085
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e3f285c
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,316
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 651B for [ps_partkey] INT32: 152 values, 615B raw, 615B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,267B for [part_value] DOUBLE: 152 values, 1,223B raw, 1,223B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@642bc6a3
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000089_305/part-00089
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@233a7853
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7612f42a
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@755ad544
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e679f8d
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000088_304/part-00088
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000083_299/part-00083
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@69cd5363
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000087_303' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000087
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000087_303: Committed
15/08/06 17:33:55 INFO Executor: Finished task 87.0 in stage 8.0 (TID 303). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a30076e
15/08/06 17:33:55 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 314, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 98.0 in stage 8.0 (TID 314)
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,276
15/08/06 17:33:55 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 303) in 261 ms on localhost (83/200)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 843B for [ps_partkey] INT32: 200 values, 807B raw, 807B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,651B for [part_value] DOUBLE: 200 values, 1,607B raw, 1,607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000085_301' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000085
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000085_301: Committed
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e8da8df
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:33:55 INFO Executor: Finished task 85.0 in stage 8.0 (TID 301). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 315, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO Executor: Running task 99.0 in stage 8.0 (TID 315)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 301) in 311 ms on localhost (84/200)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000089_305' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000089
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000089_305: Committed
15/08/06 17:33:55 INFO Executor: Finished task 89.0 in stage 8.0 (TID 305). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 316, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 305) in 256 ms on localhost (85/200)
15/08/06 17:33:55 INFO Executor: Running task 100.0 in stage 8.0 (TID 316)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000083_299' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000083
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000083_299: Committed
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b0b19c8
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000091_307/part-00091
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO Executor: Finished task 83.0 in stage 8.0 (TID 299). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 317, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 101.0 in stage 8.0 (TID 317)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 299) in 342 ms on localhost (86/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@46ddf1ca
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d87d293
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000093_309/part-00093
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000095_311/part-00095
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@700a658d
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000092_308/part-00092
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@242686ff
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@585b0ec
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,656
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22f2fa58
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 519B for [ps_partkey] INT32: 119 values, 483B raw, 483B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000090_306/part-00090
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,003B for [part_value] DOUBLE: 119 values, 959B raw, 959B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fd8bac8
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000096_312/part-00096
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@317db90b
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34ff9a8f
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000094_310/part-00094
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6cc6d822
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@57e311c5
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,736
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 735B for [ps_partkey] INT32: 173 values, 699B raw, 699B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,435B for [part_value] DOUBLE: 173 values, 1,391B raw, 1,391B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42ed24a9
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,556
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000067_283' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000067
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000067_283: Committed
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 499B for [ps_partkey] INT32: 114 values, 463B raw, 463B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 963B for [part_value] DOUBLE: 114 values, 919B raw, 919B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO Executor: Finished task 67.0 in stage 8.0 (TID 283). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 318, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@538ad233
15/08/06 17:33:55 INFO Executor: Running task 102.0 in stage 8.0 (TID 318)
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:33:55 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 283) in 677 ms on localhost (87/200)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000091_307' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000091
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000091_307: Committed
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000093_309' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000093
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000093_309: Committed
15/08/06 17:33:55 INFO Executor: Finished task 91.0 in stage 8.0 (TID 307). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 319, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Finished task 93.0 in stage 8.0 (TID 309). 781 bytes result sent to driver
15/08/06 17:33:55 INFO Executor: Running task 103.0 in stage 8.0 (TID 319)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 320, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 104.0 in stage 8.0 (TID 320)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 307) in 287 ms on localhost (88/200)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 309) in 276 ms on localhost (89/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@469f3058
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000097_313/part-00097
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000095_311' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000095
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000095_311: Committed
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000092_308' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000092
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000092_308: Committed
15/08/06 17:33:55 INFO Executor: Finished task 95.0 in stage 8.0 (TID 311). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 321, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Finished task 92.0 in stage 8.0 (TID 308). 781 bytes result sent to driver
15/08/06 17:33:55 INFO Executor: Running task 105.0 in stage 8.0 (TID 321)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 311) in 167 ms on localhost (90/200)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000090_306' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000090
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000090_306: Committed
15/08/06 17:33:55 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 322, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 106.0 in stage 8.0 (TID 322)
15/08/06 17:33:55 INFO Executor: Finished task 90.0 in stage 8.0 (TID 306). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 308) in 289 ms on localhost (91/200)
15/08/06 17:33:55 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 323, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 107.0 in stage 8.0 (TID 323)
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000096_312' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000096
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000096_312: Committed
15/08/06 17:33:55 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 306) in 311 ms on localhost (92/200)
15/08/06 17:33:55 INFO Executor: Finished task 96.0 in stage 8.0 (TID 312). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 324, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 108.0 in stage 8.0 (TID 324)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 312) in 159 ms on localhost (93/200)
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f970193
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,396
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 667B for [ps_partkey] INT32: 156 values, 631B raw, 631B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,299B for [part_value] DOUBLE: 156 values, 1,255B raw, 1,255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000094_310' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000094
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000094_310: Committed
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO Executor: Finished task 94.0 in stage 8.0 (TID 310). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 325, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 109.0 in stage 8.0 (TID 325)
15/08/06 17:33:55 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 310) in 199 ms on localhost (94/200)
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@d046745
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000098_314/part-00098
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@854ca44
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000100_316/part-00100
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000097_313' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000097
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000097_313: Committed
15/08/06 17:33:55 INFO Executor: Finished task 97.0 in stage 8.0 (TID 313). 781 bytes result sent to driver
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000073_289' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000073
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000073_289: Committed
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ffd1dae
15/08/06 17:33:55 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 326, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 110.0 in stage 8.0 (TID 326)
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:33:55 INFO Executor: Finished task 73.0 in stage 8.0 (TID 289). 781 bytes result sent to driver
15/08/06 17:33:55 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 313) in 200 ms on localhost (95/200)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 327, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO Executor: Running task 111.0 in stage 8.0 (TID 327)
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 289) in 663 ms on localhost (96/200)
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@60f69406
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000101_317/part-00101
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6dd5cee4
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2cdfa489
15/08/06 17:33:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000098_314' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000098
15/08/06 17:33:55 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000098_314: Committed
15/08/06 17:33:55 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6bf5a3dd
15/08/06 17:33:55 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000099_315/part-00099
15/08/06 17:33:55 INFO CodecConfig: Compression set to false
15/08/06 17:33:55 INFO Executor: Finished task 98.0 in stage 8.0 (TID 314). 781 bytes result sent to driver
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:55 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:55 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:55 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 328, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,756
15/08/06 17:33:56 INFO Executor: Running task 112.0 in stage 8.0 (TID 328)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 739B for [ps_partkey] INT32: 174 values, 703B raw, 703B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 314) in 247 ms on localhost (97/200)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,443B for [part_value] DOUBLE: 174 values, 1,399B raw, 1,399B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000100_316' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000100
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000100_316: Committed
15/08/06 17:33:56 INFO Executor: Finished task 100.0 in stage 8.0 (TID 316). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 329, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 113.0 in stage 8.0 (TID 329)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77e5a42e
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,516
15/08/06 17:33:56 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 316) in 236 ms on localhost (98/200)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 691B for [ps_partkey] INT32: 162 values, 655B raw, 655B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,347B for [part_value] DOUBLE: 162 values, 1,303B raw, 1,303B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000101_317' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000101
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000101_317: Committed
15/08/06 17:33:56 INFO Executor: Finished task 101.0 in stage 8.0 (TID 317). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 330, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 114.0 in stage 8.0 (TID 330)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 317) in 238 ms on localhost (99/200)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000099_315' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000099
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000099_315: Committed
15/08/06 17:33:56 INFO Executor: Finished task 99.0 in stage 8.0 (TID 315). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 331, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 115.0 in stage 8.0 (TID 331)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 315) in 270 ms on localhost (100/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e81250f
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000108_324/part-00108
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@75c15b30
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000102_318/part-00102
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6949e8b5
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000106_322/part-00106
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@774c13a8
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@23edc803
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1965d3e9
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000107_323/part-00107
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3152214a
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000105_321/part-00105
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7079a92
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000103_319/part-00103
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@630ba13e
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5bab918
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000109_325/part-00109
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d86303d
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,616
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 711B for [ps_partkey] INT32: 167 values, 675B raw, 675B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,387B for [part_value] DOUBLE: 167 values, 1,343B raw, 1,343B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78d58b93
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@9ee76d0
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000108_324' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000108
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000108_324: Committed
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000102_318' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000102
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000102_318: Committed
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO Executor: Finished task 108.0 in stage 8.0 (TID 324). 781 bytes result sent to driver
15/08/06 17:33:56 INFO Executor: Finished task 102.0 in stage 8.0 (TID 318). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 332, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 116.0 in stage 8.0 (TID 332)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 324) in 249 ms on localhost (101/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 333, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 117.0 in stage 8.0 (TID 333)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 318) in 276 ms on localhost (102/200)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@626a479f
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2f844a33
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000104_320/part-00104
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7759902d
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000110_326/part-00110
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000106_322' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000106
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000106_322: Committed
15/08/06 17:33:56 INFO Executor: Finished task 106.0 in stage 8.0 (TID 322). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 334, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 118.0 in stage 8.0 (TID 334)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 322) in 266 ms on localhost (103/200)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000107_323' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000107
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000107_323: Committed
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3837437d
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000112_328/part-00112
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO Executor: Finished task 107.0 in stage 8.0 (TID 323). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000109_325' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000109
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000109_325: Committed
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 335, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO Executor: Running task 119.0 in stage 8.0 (TID 335)
15/08/06 17:33:56 INFO Executor: Finished task 109.0 in stage 8.0 (TID 325). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 323) in 270 ms on localhost (104/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 336, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 120.0 in stage 8.0 (TID 336)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 325) in 246 ms on localhost (105/200)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@325dd496
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@38632866
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000113_329/part-00113
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,796
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 747B for [ps_partkey] INT32: 176 values, 711B raw, 711B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,459B for [part_value] DOUBLE: 176 values, 1,415B raw, 1,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000103_319' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000103
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000103_319: Committed
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30aeb4a7
15/08/06 17:33:56 INFO Executor: Finished task 103.0 in stage 8.0 (TID 319). 781 bytes result sent to driver
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:33:56 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 337, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 121.0 in stage 8.0 (TID 337)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 319) in 297 ms on localhost (106/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a0f4351
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000111_327/part-00111
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000105_321' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000105
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000105_321: Committed
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO Executor: Finished task 105.0 in stage 8.0 (TID 321). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 338, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO Executor: Running task 122.0 in stage 8.0 (TID 338)
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 321) in 294 ms on localhost (107/200)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4a82b839
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@36c1115e
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000114_330/part-00114
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@21b6db44
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7273db5a
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14462c6a
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000104_320' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000104
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000104_320: Committed
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO Executor: Finished task 104.0 in stage 8.0 (TID 320). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 339, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 123.0 in stage 8.0 (TID 339)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000086_302' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000086
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000086_302: Committed
15/08/06 17:33:56 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 320) in 326 ms on localhost (108/200)
15/08/06 17:33:56 INFO Executor: Finished task 86.0 in stage 8.0 (TID 302). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 340, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 124.0 in stage 8.0 (TID 340)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 302) in 684 ms on localhost (109/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000110_326' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000110
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000110_326: Committed
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO Executor: Finished task 110.0 in stage 8.0 (TID 326). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 341, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 125.0 in stage 8.0 (TID 341)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000113_329' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000113
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000113_329: Committed
15/08/06 17:33:56 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 326) in 274 ms on localhost (110/200)
15/08/06 17:33:56 INFO Executor: Finished task 113.0 in stage 8.0 (TID 329). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 342, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 126.0 in stage 8.0 (TID 342)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 329) in 169 ms on localhost (111/200)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000112_328' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000112
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000112_328: Committed
15/08/06 17:33:56 INFO Executor: Finished task 112.0 in stage 8.0 (TID 328). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 343, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 127.0 in stage 8.0 (TID 343)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000088_304' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000088
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000088_304: Committed
15/08/06 17:33:56 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 328) in 187 ms on localhost (112/200)
15/08/06 17:33:56 INFO Executor: Finished task 88.0 in stage 8.0 (TID 304). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 344, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 128.0 in stage 8.0 (TID 344)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 304) in 686 ms on localhost (113/200)
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6954bd84
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000115_331/part-00115
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000111_327' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000111
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000111_327: Committed
15/08/06 17:33:56 INFO Executor: Finished task 111.0 in stage 8.0 (TID 327). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 345, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 129.0 in stage 8.0 (TID 345)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 327) in 290 ms on localhost (114/200)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000114_330' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000114
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000114_330: Committed
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO Executor: Finished task 114.0 in stage 8.0 (TID 330). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6157d809
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,836
15/08/06 17:33:56 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 346, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 330) in 186 ms on localhost (115/200)
15/08/06 17:33:56 INFO Executor: Running task 130.0 in stage 8.0 (TID 346)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 555B for [ps_partkey] INT32: 128 values, 519B raw, 519B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,075B for [part_value] DOUBLE: 128 values, 1,031B raw, 1,031B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7bed2dec
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000117_333/part-00117
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2db425a6
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000119_335/part-00119
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61843cc8
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000116_332/part-00116
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@28ad329d
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000115_331' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000115
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000115_331: Committed
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,056
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 799B for [ps_partkey] INT32: 189 values, 763B raw, 763B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO Executor: Finished task 115.0 in stage 8.0 (TID 331). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,563B for [part_value] DOUBLE: 189 values, 1,519B raw, 1,519B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 347, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 131.0 in stage 8.0 (TID 347)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 331) in 327 ms on localhost (116/200)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@20383b6f
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4c4a5d91
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@591a0429
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000121_337/part-00121
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000117_333' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000117
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000117_333: Committed
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5e3b8639
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:33:56 INFO Executor: Finished task 117.0 in stage 8.0 (TID 333). 781 bytes result sent to driver
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@dbcc996
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 348, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000118_334/part-00118
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO Executor: Running task 132.0 in stage 8.0 (TID 348)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 333) in 281 ms on localhost (117/200)
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c531e20
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000122_338/part-00122
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000119_335' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000119
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000119_335: Committed
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000116_332' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000116
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000116_332: Committed
15/08/06 17:33:56 INFO Executor: Finished task 119.0 in stage 8.0 (TID 335). 781 bytes result sent to driver
15/08/06 17:33:56 INFO Executor: Finished task 116.0 in stage 8.0 (TID 332). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 349, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 133.0 in stage 8.0 (TID 349)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 335) in 279 ms on localhost (118/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 350, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 134.0 in stage 8.0 (TID 350)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 332) in 298 ms on localhost (119/200)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@959a5c1
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d839f98
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,776
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25c1f539
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000126_342/part-00126
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 743B for [ps_partkey] INT32: 175 values, 707B raw, 707B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,451B for [part_value] DOUBLE: 175 values, 1,407B raw, 1,407B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1dfa59e7
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000124_340/part-00124
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19a84e4b
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000121_337' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000121
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000121_337: Committed
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000123_339/part-00123
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO Executor: Finished task 121.0 in stage 8.0 (TID 337). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 351, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 135.0 in stage 8.0 (TID 351)
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@140071cf
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000120_336/part-00120
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@701616de
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000128_344/part-00128
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 337) in 282 ms on localhost (120/200)
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1f792009
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,656
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15a010f3
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 719B for [ps_partkey] INT32: 169 values, 683B raw, 683B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,236
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,403B for [part_value] DOUBLE: 169 values, 1,359B raw, 1,359B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 635B for [ps_partkey] INT32: 148 values, 599B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,235B for [part_value] DOUBLE: 148 values, 1,191B raw, 1,191B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000118_334' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000118
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000118_334: Committed
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d96e5c1
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@528bf60d
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@62c6f9d1
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000127_343/part-00127
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,876
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO Executor: Finished task 118.0 in stage 8.0 (TID 334). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,116
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7137a5e7
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 763B for [ps_partkey] INT32: 180 values, 727B raw, 727B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 352, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,491B for [part_value] DOUBLE: 180 values, 1,447B raw, 1,447B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 334) in 318 ms on localhost (121/200)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 811B for [ps_partkey] INT32: 192 values, 775B raw, 775B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO Executor: Running task 136.0 in stage 8.0 (TID 352)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,587B for [part_value] DOUBLE: 192 values, 1,543B raw, 1,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a33515f
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000125_341/part-00125
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@475dbf8d
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f1cddfb
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,736
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000124_340' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000124
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000124_340: Committed
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 535B for [ps_partkey] INT32: 123 values, 499B raw, 499B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,035B for [part_value] DOUBLE: 123 values, 991B raw, 991B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO Executor: Finished task 124.0 in stage 8.0 (TID 340). 781 bytes result sent to driver
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000123_339' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000123
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000123_339: Committed
15/08/06 17:33:56 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 353, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 137.0 in stage 8.0 (TID 353)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000128_344' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000128
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000128_344: Committed
15/08/06 17:33:56 INFO Executor: Finished task 123.0 in stage 8.0 (TID 339). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 340) in 296 ms on localhost (122/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 354, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Finished task 128.0 in stage 8.0 (TID 344). 781 bytes result sent to driver
15/08/06 17:33:56 INFO Executor: Running task 138.0 in stage 8.0 (TID 354)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 339) in 300 ms on localhost (123/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 355, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 139.0 in stage 8.0 (TID 355)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 344) in 283 ms on localhost (124/200)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000120_336' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000120
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000120_336: Committed
15/08/06 17:33:56 INFO Executor: Finished task 120.0 in stage 8.0 (TID 336). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 356, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 140.0 in stage 8.0 (TID 356)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 336) in 349 ms on localhost (125/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000127_343' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000127
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2aa971c3
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000127_343: Committed
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000129_345/part-00129
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO Executor: Finished task 127.0 in stage 8.0 (TID 343). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 357, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 141.0 in stage 8.0 (TID 357)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 343) in 296 ms on localhost (126/200)
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@13bf5b6a
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000130_346/part-00130
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@11f92218
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1790abbf
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1afb94af
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000132_348/part-00132
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000129_345' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000129
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000129_345: Committed
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c69577b
15/08/06 17:33:56 INFO Executor: Finished task 129.0 in stage 8.0 (TID 345). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000133_349/part-00133
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@327fd20
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000134_350/part-00134
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 358, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO Executor: Running task 142.0 in stage 8.0 (TID 358)
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 345) in 328 ms on localhost (127/200)
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f399566
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48be3a5e
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000135_351/part-00135
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@222e3add
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,056
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 599B for [ps_partkey] INT32: 139 values, 563B raw, 563B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,163B for [part_value] DOUBLE: 139 values, 1,119B raw, 1,119B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000130_346' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000130
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000130_346: Committed
15/08/06 17:33:56 INFO Executor: Finished task 130.0 in stage 8.0 (TID 346). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 359, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d7b34fb
15/08/06 17:33:56 INFO Executor: Running task 143.0 in stage 8.0 (TID 359)
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@73d36210
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 346) in 337 ms on localhost (128/200)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000132_348' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000132
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000132_348: Committed
15/08/06 17:33:56 INFO Executor: Finished task 132.0 in stage 8.0 (TID 348). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 360, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b51da2
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000136_352/part-00136
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 348) in 170 ms on localhost (129/200)
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO Executor: Running task 144.0 in stage 8.0 (TID 360)
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000134_350' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000134
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000134_350: Committed
15/08/06 17:33:56 INFO Executor: Finished task 134.0 in stage 8.0 (TID 350). 781 bytes result sent to driver
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000135_351' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000135
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000135_351: Committed
15/08/06 17:33:56 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 361, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 145.0 in stage 8.0 (TID 361)
15/08/06 17:33:56 INFO Executor: Finished task 135.0 in stage 8.0 (TID 351). 781 bytes result sent to driver
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000133_349' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000133
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000133_349: Committed
15/08/06 17:33:56 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 350) in 166 ms on localhost (130/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO Executor: Finished task 133.0 in stage 8.0 (TID 349). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 362, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 146.0 in stage 8.0 (TID 362)
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d10a4fe
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000131_347/part-00131
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 351) in 155 ms on localhost (131/200)
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 363, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@197eb418
15/08/06 17:33:56 INFO Executor: Running task 147.0 in stage 8.0 (TID 363)
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:33:56 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 349) in 172 ms on localhost (132/200)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4a6828ce
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000136_352' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000136
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000136_352: Committed
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@568bd786
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000138_354/part-00138
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO Executor: Finished task 136.0 in stage 8.0 (TID 352). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 364, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 148.0 in stage 8.0 (TID 364)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 352) in 165 ms on localhost (133/200)
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2e53ef13
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000141_357/part-00141
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@795d780c
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,536
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 495B for [ps_partkey] INT32: 113 values, 459B raw, 459B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 955B for [part_value] DOUBLE: 113 values, 911B raw, 911B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@71157f42
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000139_355/part-00139
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ac42579
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000140_356/part-00140
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6455e6b0
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000138_354' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000138
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000138_354: Committed
15/08/06 17:33:56 INFO Executor: Finished task 138.0 in stage 8.0 (TID 354). 781 bytes result sent to driver
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1aebaeff
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:33:56 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 365, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7a3a085c
15/08/06 17:33:56 INFO Executor: Running task 149.0 in stage 8.0 (TID 365)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 354) in 229 ms on localhost (134/200)
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000141_357' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000141
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000141_357: Committed
15/08/06 17:33:56 INFO Executor: Finished task 141.0 in stage 8.0 (TID 357). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 366, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 150.0 in stage 8.0 (TID 366)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 357) in 230 ms on localhost (135/200)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000139_355' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000139
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000139_355: Committed
15/08/06 17:33:56 INFO Executor: Finished task 139.0 in stage 8.0 (TID 355). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 367, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 151.0 in stage 8.0 (TID 367)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 355) in 255 ms on localhost (136/200)
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1bc4eb31
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000143_359/part-00143
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6555121a
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000137_353/part-00137
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7299c7a1
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,336
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 455B for [ps_partkey] INT32: 103 values, 419B raw, 419B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 875B for [part_value] DOUBLE: 103 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f00d689
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3004ed19
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000145_361/part-00145
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1b91da85
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000144_360/part-00144
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@796b3bfc
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@136a1cf0
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000147_363/part-00147
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,736
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 535B for [ps_partkey] INT32: 123 values, 499B raw, 499B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000142_358/part-00142
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,035B for [part_value] DOUBLE: 123 values, 991B raw, 991B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7af67fd6
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d42f9ab
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@57b976bd
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51229f49
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,656
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 719B for [ps_partkey] INT32: 169 values, 683B raw, 683B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,403B for [part_value] DOUBLE: 169 values, 1,359B raw, 1,359B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3cf900d2
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000146_362/part-00146
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000143_359' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000143
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000143_359: Committed
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000137_353' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000137
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000137_353: Committed
15/08/06 17:33:56 INFO Executor: Finished task 143.0 in stage 8.0 (TID 359). 781 bytes result sent to driver
15/08/06 17:33:56 INFO Executor: Finished task 137.0 in stage 8.0 (TID 353). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 368, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 152.0 in stage 8.0 (TID 368)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 369, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 153.0 in stage 8.0 (TID 369)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 359) in 241 ms on localhost (137/200)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 353) in 317 ms on localhost (138/200)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1c7d5add
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000144_360' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000144
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000144_360: Committed
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000145_361' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000145
15/08/06 17:33:56 INFO Executor: Finished task 144.0 in stage 8.0 (TID 360). 781 bytes result sent to driver
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000145_361: Committed
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000147_363' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000147
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000147_363: Committed
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000142_358' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000142
15/08/06 17:33:56 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 370, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000142_358: Committed
15/08/06 17:33:56 INFO Executor: Running task 154.0 in stage 8.0 (TID 370)
15/08/06 17:33:56 INFO Executor: Finished task 145.0 in stage 8.0 (TID 361). 781 bytes result sent to driver
15/08/06 17:33:56 INFO Executor: Finished task 147.0 in stage 8.0 (TID 363). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 360) in 238 ms on localhost (139/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 371, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 155.0 in stage 8.0 (TID 371)
15/08/06 17:33:56 INFO Executor: Finished task 142.0 in stage 8.0 (TID 358). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 361) in 229 ms on localhost (140/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 372, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 156.0 in stage 8.0 (TID 372)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 363) in 227 ms on localhost (141/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 373, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 157.0 in stage 8.0 (TID 373)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 358) in 282 ms on localhost (142/200)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000146_362' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000146
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b2c1233
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000146_362: Committed
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000149_365/part-00149
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO Executor: Finished task 146.0 in stage 8.0 (TID 362). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 374, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 158.0 in stage 8.0 (TID 374)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 362) in 251 ms on localhost (143/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@66d87834
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000148_364/part-00148
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@41a88ae2
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000122_338' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000122
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000122_338: Committed
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO Executor: Finished task 122.0 in stage 8.0 (TID 338). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 375, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 159.0 in stage 8.0 (TID 375)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 338) in 695 ms on localhost (144/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@54d27cd6
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4edc7750
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000151_367/part-00151
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4f5cfbe3
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000150_366/part-00150
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000126_342' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000126
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000126_342: Committed
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO Executor: Finished task 126.0 in stage 8.0 (TID 342). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 376, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000149_365' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000149
15/08/06 17:33:56 INFO Executor: Running task 160.0 in stage 8.0 (TID 376)
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000149_365: Committed
15/08/06 17:33:56 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 342) in 680 ms on localhost (145/200)
15/08/06 17:33:56 INFO Executor: Finished task 149.0 in stage 8.0 (TID 365). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 377, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 161.0 in stage 8.0 (TID 377)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 365) in 163 ms on localhost (146/200)
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@205baee8
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@633811f0
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,076
15/08/06 17:33:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 803B for [ps_partkey] INT32: 190 values, 767B raw, 767B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,571B for [part_value] DOUBLE: 190 values, 1,527B raw, 1,527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000148_364' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000148
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000148_364: Committed
15/08/06 17:33:56 INFO Executor: Finished task 148.0 in stage 8.0 (TID 364). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 378, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 364) in 284 ms on localhost (147/200)
15/08/06 17:33:56 INFO Executor: Running task 162.0 in stage 8.0 (TID 378)
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000125_341' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000125
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000125_341: Committed
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO Executor: Finished task 125.0 in stage 8.0 (TID 341). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 379, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 163.0 in stage 8.0 (TID 379)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 341) in 715 ms on localhost (148/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000151_367' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000151
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000151_367: Committed
15/08/06 17:33:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000150_366' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000150
15/08/06 17:33:56 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000150_366: Committed
15/08/06 17:33:56 INFO Executor: Finished task 151.0 in stage 8.0 (TID 367). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 380, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 164.0 in stage 8.0 (TID 380)
15/08/06 17:33:56 INFO Executor: Finished task 150.0 in stage 8.0 (TID 366). 781 bytes result sent to driver
15/08/06 17:33:56 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 367) in 171 ms on localhost (149/200)
15/08/06 17:33:56 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 381, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:56 INFO Executor: Running task 165.0 in stage 8.0 (TID 381)
15/08/06 17:33:56 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 366) in 188 ms on localhost (150/200)
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@68681d94
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000153_369/part-00153
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22f7ae69
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000154_370/part-00154
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@690e76d7
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000155_371/part-00155
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@349c5323
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000157_373/part-00157
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@282dc9ec
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000152_368/part-00152
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b43c4c8
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000156_372/part-00156
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48232b37
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000158_374/part-00158
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO CodecConfig: Compression set to false
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@453c5385
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b8f6445
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@246de682
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b38737d
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72629b6
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,536
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,056
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 495B for [ps_partkey] INT32: 113 values, 459B raw, 459B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 799B for [ps_partkey] INT32: 189 values, 763B raw, 763B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 955B for [part_value] DOUBLE: 113 values, 911B raw, 911B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,563B for [part_value] DOUBLE: 189 values, 1,519B raw, 1,519B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,816
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 551B for [ps_partkey] INT32: 127 values, 515B raw, 515B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@332cc767
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,067B for [part_value] DOUBLE: 127 values, 1,023B raw, 1,023B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,636
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@40d75570
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 715B for [ps_partkey] INT32: 168 values, 679B raw, 679B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,395B for [part_value] DOUBLE: 168 values, 1,351B raw, 1,351B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000131_347' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000131
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000131_347: Committed
15/08/06 17:33:57 INFO Executor: Finished task 131.0 in stage 8.0 (TID 347). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 382, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 166.0 in stage 8.0 (TID 382)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 347) in 756 ms on localhost (151/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000155_371' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000155
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000155_371: Committed
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@630ae774
15/08/06 17:33:57 INFO Executor: Finished task 155.0 in stage 8.0 (TID 371). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000161_377/part-00161
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d1470bd
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000160_376/part-00160
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 383, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO Executor: Running task 167.0 in stage 8.0 (TID 383)
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 371) in 325 ms on localhost (152/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000154_370' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000154
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000154_370: Committed
15/08/06 17:33:57 INFO Executor: Finished task 154.0 in stage 8.0 (TID 370). 781 bytes result sent to driver
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000157_373' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000157
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000157_373: Committed
15/08/06 17:33:57 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 384, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 168.0 in stage 8.0 (TID 384)
15/08/06 17:33:57 INFO Executor: Finished task 157.0 in stage 8.0 (TID 373). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 370) in 336 ms on localhost (153/200)
15/08/06 17:33:57 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 385, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000140_356' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000140
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000140_356: Committed
15/08/06 17:33:57 INFO Executor: Running task 169.0 in stage 8.0 (TID 385)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000153_369' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000153
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000153_369: Committed
15/08/06 17:33:57 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 373) in 332 ms on localhost (154/200)
15/08/06 17:33:57 INFO Executor: Finished task 140.0 in stage 8.0 (TID 356). 781 bytes result sent to driver
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000156_372' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000156
15/08/06 17:33:57 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 386, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000156_372: Committed
15/08/06 17:33:57 INFO Executor: Running task 170.0 in stage 8.0 (TID 386)
15/08/06 17:33:57 INFO Executor: Finished task 156.0 in stage 8.0 (TID 372). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 356) in 662 ms on localhost (155/200)
15/08/06 17:33:57 INFO Executor: Finished task 153.0 in stage 8.0 (TID 369). 781 bytes result sent to driver
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000158_374' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000158
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000158_374: Committed
15/08/06 17:33:57 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 387, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Finished task 158.0 in stage 8.0 (TID 374). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 372) in 338 ms on localhost (156/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000152_368' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000152
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000152_368: Committed
15/08/06 17:33:57 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 388, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 172.0 in stage 8.0 (TID 388)
15/08/06 17:33:57 INFO Executor: Finished task 152.0 in stage 8.0 (TID 368). 781 bytes result sent to driver
15/08/06 17:33:57 INFO Executor: Running task 171.0 in stage 8.0 (TID 387)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 369) in 359 ms on localhost (157/200)
15/08/06 17:33:57 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 389, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 173.0 in stage 8.0 (TID 389)
15/08/06 17:33:57 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 390, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 174.0 in stage 8.0 (TID 390)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@48e0c99c
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:33:57 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 374) in 321 ms on localhost (158/200)
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 368) in 363 ms on localhost (159/200)
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3302e49a
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,596
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 507B for [ps_partkey] INT32: 116 values, 471B raw, 471B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 979B for [part_value] DOUBLE: 116 values, 935B raw, 935B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@75208eec
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000165_381/part-00165
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@54753ccd
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000159_375/part-00159
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1291bbd
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000162_378/part-00162
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d682a5f
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000160_376' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000160
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000160_376: Committed
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO Executor: Finished task 160.0 in stage 8.0 (TID 376). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 391, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 175.0 in stage 8.0 (TID 391)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@191e1db4
15/08/06 17:33:57 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 376) in 318 ms on localhost (160/200)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@33956879
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,996
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 587B for [ps_partkey] INT32: 136 values, 551B raw, 551B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,139B for [part_value] DOUBLE: 136 values, 1,095B raw, 1,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@514fee2f
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000163_379/part-00163
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@11b552ac
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000164_380/part-00164
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000165_381' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000165
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000165_381: Committed
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a7542e6
15/08/06 17:33:57 INFO Executor: Finished task 165.0 in stage 8.0 (TID 381). 781 bytes result sent to driver
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,476
15/08/06 17:33:57 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 392, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 176.0 in stage 8.0 (TID 392)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 381) in 301 ms on localhost (161/200)
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 483B for [ps_partkey] INT32: 110 values, 447B raw, 447B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 931B for [part_value] DOUBLE: 110 values, 887B raw, 887B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000159_375' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000159
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000159_375: Committed
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000162_378' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000162
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000162_378: Committed
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f74fb77
15/08/06 17:33:57 INFO Executor: Finished task 159.0 in stage 8.0 (TID 375). 781 bytes result sent to driver
15/08/06 17:33:57 INFO Executor: Finished task 162.0 in stage 8.0 (TID 378). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 393, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 177.0 in stage 8.0 (TID 393)
15/08/06 17:33:57 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 394, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 375) in 371 ms on localhost (162/200)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 378) in 326 ms on localhost (163/200)
15/08/06 17:33:57 INFO Executor: Running task 178.0 in stage 8.0 (TID 394)
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@733c3c3c
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000166_382/part-00166
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48863cbc
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000167_383/part-00167
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000163_379' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000163
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000163_379: Committed
15/08/06 17:33:57 INFO Executor: Finished task 163.0 in stage 8.0 (TID 379). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 395, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 179.0 in stage 8.0 (TID 395)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 379) in 349 ms on localhost (164/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000164_380' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000164
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000164_380: Committed
15/08/06 17:33:57 INFO Executor: Finished task 164.0 in stage 8.0 (TID 380). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 396, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 180.0 in stage 8.0 (TID 396)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@26edc824
15/08/06 17:33:57 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 380) in 352 ms on localhost (165/200)
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,576
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 903B for [ps_partkey] INT32: 215 values, 867B raw, 867B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,771B for [part_value] DOUBLE: 215 values, 1,727B raw, 1,727B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@48aa0589
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,556
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 499B for [ps_partkey] INT32: 114 values, 463B raw, 463B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 963B for [part_value] DOUBLE: 114 values, 919B raw, 919B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fda244d
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000169_385/part-00169
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3237488b
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000173_389/part-00173
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@408eccdc
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000172_388/part-00172
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@754e15b9
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000174_390/part-00174
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000166_382' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000166
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000166_382: Committed
15/08/06 17:33:57 INFO Executor: Finished task 166.0 in stage 8.0 (TID 382). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 397, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 181.0 in stage 8.0 (TID 397)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@470d5c0c
15/08/06 17:33:57 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 382) in 175 ms on localhost (166/200)
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,596
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e163c45
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000171_387/part-00171
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@25cef5db
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35943752
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,756
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 939B for [ps_partkey] INT32: 224 values, 903B raw, 903B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@240d4295
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,843B for [part_value] DOUBLE: 224 values, 1,799B raw, 1,799B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5871bb36
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 707B for [ps_partkey] INT32: 166 values, 671B raw, 671B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000168_384/part-00168
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,379B for [part_value] DOUBLE: 166 values, 1,335B raw, 1,335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b53ebda
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78a073d5
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000175_391/part-00175
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000172_388' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000172
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000172_388: Committed
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e77a408
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:33:57 INFO Executor: Finished task 172.0 in stage 8.0 (TID 388). 781 bytes result sent to driver
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000174_390' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000174
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000174_390: Committed
15/08/06 17:33:57 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 398, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 182.0 in stage 8.0 (TID 398)
15/08/06 17:33:57 INFO Executor: Finished task 174.0 in stage 8.0 (TID 390). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@72d1b1fc
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000176_392/part-00176
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 388) in 183 ms on localhost (167/200)
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 399, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 183.0 in stage 8.0 (TID 399)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000173_389' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000173
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000173_389: Committed
15/08/06 17:33:57 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 390) in 188 ms on localhost (168/200)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d03597b
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO Executor: Finished task 173.0 in stage 8.0 (TID 389). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 400, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@56c7557a
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000170_386/part-00170
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO Executor: Running task 184.0 in stage 8.0 (TID 400)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@182d72b1
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000169_385' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000169
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000169_385: Committed
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,216
15/08/06 17:33:57 INFO Executor: Finished task 169.0 in stage 8.0 (TID 385). 781 bytes result sent to driver
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a437754
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000177_393/part-00177
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000171_387' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000171
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000171_387: Committed
15/08/06 17:33:57 INFO Executor: Finished task 171.0 in stage 8.0 (TID 387). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 831B for [ps_partkey] INT32: 197 values, 795B raw, 795B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,627B for [part_value] DOUBLE: 197 values, 1,583B raw, 1,583B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 389) in 196 ms on localhost (169/200)
15/08/06 17:33:57 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 401, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 185.0 in stage 8.0 (TID 401)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 385) in 342 ms on localhost (170/200)
15/08/06 17:33:57 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 402, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 186.0 in stage 8.0 (TID 402)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 387) in 339 ms on localhost (171/200)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@68b40d50
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,996
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 787B for [ps_partkey] INT32: 186 values, 751B raw, 751B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@650b44bb
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,539B for [part_value] DOUBLE: 186 values, 1,495B raw, 1,495B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,996
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 787B for [ps_partkey] INT32: 186 values, 751B raw, 751B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,539B for [part_value] DOUBLE: 186 values, 1,495B raw, 1,495B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000168_384' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000168
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000168_384: Committed
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000175_391' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000175
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000175_391: Committed
15/08/06 17:33:57 INFO Executor: Finished task 168.0 in stage 8.0 (TID 384). 781 bytes result sent to driver
15/08/06 17:33:57 INFO Executor: Finished task 175.0 in stage 8.0 (TID 391). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 403, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 187.0 in stage 8.0 (TID 403)
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2fba19ac
15/08/06 17:33:57 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 384) in 358 ms on localhost (172/200)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000179_395/part-00179
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 404, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 188.0 in stage 8.0 (TID 404)
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 391) in 320 ms on localhost (173/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000176_392' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000176
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000176_392: Committed
15/08/06 17:33:57 INFO Executor: Finished task 176.0 in stage 8.0 (TID 392). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 405, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO Executor: Running task 189.0 in stage 8.0 (TID 405)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 392) in 300 ms on localhost (174/200)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000177_393' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000177
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000170_386' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000170
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4040db52
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000177_393: Committed
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000170_386: Committed
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62a46719
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000178_394/part-00178
15/08/06 17:33:57 INFO Executor: Finished task 177.0 in stage 8.0 (TID 393). 781 bytes result sent to driver
15/08/06 17:33:57 INFO Executor: Finished task 170.0 in stage 8.0 (TID 386). 781 bytes result sent to driver
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 406, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO Executor: Running task 190.0 in stage 8.0 (TID 406)
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 393) in 306 ms on localhost (175/200)
15/08/06 17:33:57 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 407, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 191.0 in stage 8.0 (TID 407)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 386) in 380 ms on localhost (176/200)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3db1a33
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,462,196
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,027B for [ps_partkey] INT32: 246 values, 991B raw, 991B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 2,019B for [part_value] DOUBLE: 246 values, 1,975B raw, 1,975B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f616dad
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000180_396/part-00180
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000179_395' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000179
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000179_395: Committed
15/08/06 17:33:57 INFO Executor: Finished task 179.0 in stage 8.0 (TID 395). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 408, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 192.0 in stage 8.0 (TID 408)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 395) in 302 ms on localhost (177/200)
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@58e5e25
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@41b2e
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000181_397/part-00181
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000178_394' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000178
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000178_394: Committed
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO Executor: Finished task 178.0 in stage 8.0 (TID 394). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 409, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO Executor: Running task 193.0 in stage 8.0 (TID 409)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 394) in 346 ms on localhost (178/200)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1fa1a776
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,796
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 747B for [ps_partkey] INT32: 176 values, 711B raw, 711B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,459B for [part_value] DOUBLE: 176 values, 1,415B raw, 1,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000161_377' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000161
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000161_377: Committed
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000180_396' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000180
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000180_396: Committed
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO Executor: Finished task 161.0 in stage 8.0 (TID 377). 781 bytes result sent to driver
15/08/06 17:33:57 INFO Executor: Finished task 180.0 in stage 8.0 (TID 396). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 410, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 194.0 in stage 8.0 (TID 410)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 377) in 719 ms on localhost (179/200)
15/08/06 17:33:57 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 411, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 195.0 in stage 8.0 (TID 411)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 396) in 333 ms on localhost (180/200)
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1aa58489
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000182_398/part-00182
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7f9d865d
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000184_400/part-00184
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000181_397' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000181
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000181_397: Committed
15/08/06 17:33:57 INFO Executor: Finished task 181.0 in stage 8.0 (TID 397). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 412, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c312729
15/08/06 17:33:57 INFO Executor: Running task 196.0 in stage 8.0 (TID 412)
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1f022408
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000186_402/part-00186
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 397) in 313 ms on localhost (181/200)
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4ff1e5e4
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000185_401/part-00185
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@122c04bd
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@dd8668e
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000188_404/part-00188
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d99fdc8
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@20dc5a31
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000184_400' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000184
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000184_400: Committed
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7249d3dd
15/08/06 17:33:57 INFO Executor: Finished task 184.0 in stage 8.0 (TID 400). 781 bytes result sent to driver
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3880c4d3
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000189_405/part-00189
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 413, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO Executor: Running task 197.0 in stage 8.0 (TID 413)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 400) in 289 ms on localhost (182/200)
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25195818
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000187_403/part-00187
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000185_401' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000185
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000185_401: Committed
15/08/06 17:33:57 INFO Executor: Finished task 185.0 in stage 8.0 (TID 401). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 414, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 198.0 in stage 8.0 (TID 414)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 401) in 167 ms on localhost (183/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000186_402' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000186
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000186_402: Committed
15/08/06 17:33:57 INFO Executor: Finished task 186.0 in stage 8.0 (TID 402). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 415, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:33:57 INFO Executor: Running task 199.0 in stage 8.0 (TID 415)
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6413f918
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@16d29890
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,156
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 402) in 170 ms on localhost (184/200)
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 819B for [ps_partkey] INT32: 194 values, 783B raw, 783B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,603B for [part_value] DOUBLE: 194 values, 1,559B raw, 1,559B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000188_404' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000188
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000188_404: Committed
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69ba80a2
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@535341e8
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000192_408/part-00192
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000190_406/part-00190
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO Executor: Finished task 188.0 in stage 8.0 (TID 404). 781 bytes result sent to driver
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2a2b4b2b
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000183_399/part-00183
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 404) in 171 ms on localhost (185/200)
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3ad7686d
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d21be23
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,136
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000167_383' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000167
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000167_383: Committed
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c9238f1
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000193_409/part-00193
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 615B for [ps_partkey] INT32: 143 values, 579B raw, 579B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO Executor: Finished task 167.0 in stage 8.0 (TID 383). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,195B for [part_value] DOUBLE: 143 values, 1,151B raw, 1,151B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 383) in 557 ms on localhost (186/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000189_405' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000189
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000189_405: Committed
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ea1784a
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:33:57 INFO Executor: Finished task 189.0 in stage 8.0 (TID 405). 781 bytes result sent to driver
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 405) in 188 ms on localhost (187/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000187_403' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000187
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000187_403: Committed
15/08/06 17:33:57 INFO Executor: Finished task 187.0 in stage 8.0 (TID 403). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 403) in 199 ms on localhost (188/200)
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2937bd67
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000191_407/part-00191
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4213d40
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1f363a61
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000194_410/part-00194
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@43d09deb
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000195_411/part-00195
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6be25f8d
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@27752974
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,896
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 567B for [ps_partkey] INT32: 131 values, 531B raw, 531B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,099B for [part_value] DOUBLE: 131 values, 1,055B raw, 1,055B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@422fd434
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,276
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000193_409' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000193
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 643B for [ps_partkey] INT32: 150 values, 607B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000193_409: Committed
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,251B for [part_value] DOUBLE: 150 values, 1,207B raw, 1,207B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO Executor: Finished task 193.0 in stage 8.0 (TID 409). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 409) in 275 ms on localhost (189/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000191_407' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000191
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000191_407: Committed
15/08/06 17:33:57 INFO Executor: Finished task 191.0 in stage 8.0 (TID 407). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 407) in 323 ms on localhost (190/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000194_410' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000194
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000194_410: Committed
15/08/06 17:33:57 INFO Executor: Finished task 194.0 in stage 8.0 (TID 410). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 410) in 263 ms on localhost (191/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000195_411' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000195
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000195_411: Committed
15/08/06 17:33:57 INFO Executor: Finished task 195.0 in stage 8.0 (TID 411). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 411) in 267 ms on localhost (192/200)
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6cbeb7b8
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000197_413/part-00197
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@77cc7045
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@edeff75
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000198_414/part-00198
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000196_412/part-00196
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6391c2bf
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b1a79b2
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/_temporary/attempt_201508061733_0008_m_000199_415/part-00199
15/08/06 17:33:57 INFO CodecConfig: Compression set to false
15/08/06 17:33:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:33:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:33:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:33:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ad72351
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@11607616
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@489cc4a3
15/08/06 17:33:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,776
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 743B for [ps_partkey] INT32: 175 values, 707B raw, 707B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO ColumnChunkPageWriteStore: written 1,451B for [part_value] DOUBLE: 175 values, 1,407B raw, 1,407B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000197_413' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000197
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000197_413: Committed
15/08/06 17:33:57 INFO Executor: Finished task 197.0 in stage 8.0 (TID 413). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 413) in 286 ms on localhost (193/200)
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000196_412' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000196
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000196_412: Committed
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000199_415' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000199
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000199_415: Committed
15/08/06 17:33:57 INFO Executor: Finished task 196.0 in stage 8.0 (TID 412). 781 bytes result sent to driver
15/08/06 17:33:57 INFO Executor: Finished task 199.0 in stage 8.0 (TID 415). 781 bytes result sent to driver
15/08/06 17:33:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000198_414' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000198
15/08/06 17:33:57 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000198_414: Committed
15/08/06 17:33:57 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 412) in 323 ms on localhost (194/200)
15/08/06 17:33:57 INFO Executor: Finished task 198.0 in stage 8.0 (TID 414). 781 bytes result sent to driver
15/08/06 17:33:57 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 415) in 275 ms on localhost (195/200)
15/08/06 17:33:57 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 414) in 280 ms on localhost (196/200)
15/08/06 17:33:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000182_398' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000182
15/08/06 17:33:58 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000182_398: Committed
15/08/06 17:33:58 INFO Executor: Finished task 182.0 in stage 8.0 (TID 398). 781 bytes result sent to driver
15/08/06 17:33:58 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 398) in 705 ms on localhost (197/200)
15/08/06 17:33:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000192_408' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000192
15/08/06 17:33:58 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000192_408: Committed
15/08/06 17:33:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000183_399' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000183
15/08/06 17:33:58 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000183_399: Committed
15/08/06 17:33:58 INFO Executor: Finished task 192.0 in stage 8.0 (TID 408). 781 bytes result sent to driver
15/08/06 17:33:58 INFO Executor: Finished task 183.0 in stage 8.0 (TID 399). 781 bytes result sent to driver
15/08/06 17:33:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061733_0008_m_000190_406' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_temporary/0/task_201508061733_0008_m_000190
15/08/06 17:33:58 INFO SparkHiveWriterContainer: attempt_201508061733_0008_m_000190_406: Committed
15/08/06 17:33:58 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 408) in 572 ms on localhost (198/200)
15/08/06 17:33:58 INFO Executor: Finished task 190.0 in stage 8.0 (TID 406). 781 bytes result sent to driver
15/08/06 17:33:58 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 399) in 786 ms on localhost (199/200)
15/08/06 17:33:58 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 406) in 600 ms on localhost (200/200)
15/08/06 17:33:58 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/06 17:33:58 INFO DAGScheduler: Stage 8 (runJob at InsertIntoHiveTable.scala:93) finished in 5.021 s
15/08/06 17:33:58 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1f514b44
15/08/06 17:33:58 INFO DAGScheduler: Job 5 finished: runJob at InsertIntoHiveTable.scala:93, took 15.260914 s
15/08/06 17:33:58 INFO StatsReportListener: task runtime:(count: 200, mean: 383.500000, stdev: 240.768582, max: 1055.000000, min: 155.000000)
15/08/06 17:33:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:58 INFO StatsReportListener: 	155.0 ms	170.0 ms	188.0 ms	249.0 ms	296.0 ms	371.0 ms	719.0 ms	1.0 s	1.1 s
15/08/06 17:33:58 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.475000, stdev: 1.029259, max: 9.000000, min: 0.000000)
15/08/06 17:33:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:58 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	9.0 ms
15/08/06 17:33:58 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:33:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:58 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:33:58 INFO StatsReportListener: task result size:(count: 200, mean: 781.000000, stdev: 0.000000, max: 781.000000, min: 781.000000)
15/08/06 17:33:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:58 INFO StatsReportListener: 	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B
15/08/06 17:33:58 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 96.841437, stdev: 6.253808, max: 99.716312, min: 57.439446)
15/08/06 17:33:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:58 INFO StatsReportListener: 	57 %	94 %	96 %	98 %	98 %	99 %	99 %	99 %	100 %
15/08/06 17:33:58 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.130131, stdev: 0.308204, max: 2.611940, min: 0.000000)
15/08/06 17:33:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:58 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 3 %
15/08/06 17:33:58 INFO StatsReportListener: other time pct: (count: 200, mean: 3.028432, stdev: 6.239625, max: 42.214533, min: 0.279720)
15/08/06 17:33:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:58 INFO StatsReportListener: 	 0 %	 1 %	 1 %	 1 %	 2 %	 2 %	 4 %	 6 %	42 %
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/_SUCCESS;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/_SUCCESS;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00000;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00001;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00002;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00003;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00004;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00005;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00006;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00007;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00008;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00009;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00010;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00011;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00012;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00013;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00014;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00015;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00016;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00017;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00018;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00019;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00020;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00021;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00022;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00023;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00024;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00025;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00026;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00027;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00028;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00029;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00030;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00031;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00032;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00033;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00034;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00035;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00036;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00037;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00038;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00039;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00040;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00041;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00042;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00043;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00044;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00045;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00046;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00047;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00048;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00049;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00050;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00051;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00052;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00053;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00054;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00055;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00056;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00057;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00058;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00059;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00060;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00061;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00062;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00063;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00064;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00065;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00066;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00067;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00068;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00069;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00070;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00071;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00072;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00073;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00074;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00075;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00076;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00077;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00078;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00079;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00080;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00081;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00082;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00083;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00084;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00085;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00086;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00087;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00088;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00089;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00090;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00091;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00092;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00093;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00094;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00095;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00096;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00097;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00098;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00099;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00100;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00101;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00102;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00103;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00104;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00105;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00106;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00107;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00108;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00109;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00110;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00111;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00112;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00113;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00114;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00115;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00116;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00117;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00118;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00119;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00120;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00121;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00122;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00123;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00124;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00125;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00126;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00127;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00128;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00129;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00130;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00131;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00132;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00133;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00134;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00135;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00136;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00137;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00138;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00139;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00140;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00141;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00142;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00143;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00144;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00145;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00146;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00147;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00148;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00149;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00150;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00151;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00152;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00153;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00154;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00155;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00156;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00157;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00158;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00159;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00160;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00161;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00162;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00163;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00164;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00165;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00166;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00167;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00168;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00169;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00170;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00171;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00172;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00173;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00174;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00175;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00176;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00177;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00178;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00179;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00180;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00181;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00182;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00183;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00184;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00185;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00186;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00187;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00188;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00189;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00190;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00191;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00192;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00193;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00194;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00195;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00196;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00197;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00198;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198;Status:true
15/08/06 17:33:59 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-33-41_434_4795727510949762773-1/-ext-10000/part-00199;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199;Status:true
15/08/06 17:33:59 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:33:59 INFO DAGScheduler: Got job 6 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:33:59 INFO DAGScheduler: Final stage: Stage 9(collect at SparkPlan.scala:84)
15/08/06 17:33:59 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:33:59 INFO DAGScheduler: Missing parents: List()
15/08/06 17:33:59 INFO DAGScheduler: Submitting Stage 9 (MappedRDD[50] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:33:59 INFO MemoryStore: ensureFreeSpace(3240) called with curMem=962677, maxMem=3333968363
15/08/06 17:33:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 3.2 KB, free 3.1 GB)
15/08/06 17:33:59 INFO MemoryStore: ensureFreeSpace(1941) called with curMem=965917, maxMem=3333968363
15/08/06 17:33:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1941.0 B, free 3.1 GB)
15/08/06 17:33:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:42931 (size: 1941.0 B, free: 3.1 GB)
15/08/06 17:33:59 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/08/06 17:33:59 INFO DefaultExecutionContext: Created broadcast 13 from broadcast at DAGScheduler.scala:838
15/08/06 17:33:59 INFO DAGScheduler: Submitting 1 missing tasks from Stage 9 (MappedRDD[50] at map at SparkPlan.scala:84)
15/08/06 17:33:59 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/08/06 17:33:59 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 416, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:33:59 INFO Executor: Running task 0.0 in stage 9.0 (TID 416)
15/08/06 17:33:59 INFO Executor: Finished task 0.0 in stage 9.0 (TID 416). 618 bytes result sent to driver
15/08/06 17:33:59 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 416) in 6 ms on localhost (1/1)
15/08/06 17:33:59 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/06 17:33:59 INFO DAGScheduler: Stage 9 (collect at SparkPlan.scala:84) finished in 0.007 s
15/08/06 17:33:59 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@62ad3757
15/08/06 17:33:59 INFO DAGScheduler: Job 6 finished: collect at SparkPlan.scala:84, took 0.025664 s
Time taken: 20.111 seconds
15/08/06 17:33:59 INFO CliDriver: Time taken: 20.111 seconds
15/08/06 17:33:59 INFO StatsReportListener: task runtime:(count: 1, mean: 6.000000, stdev: 0.000000, max: 6.000000, min: 6.000000)
15/08/06 17:33:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:59 INFO StatsReportListener: 	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms
15/08/06 17:33:59 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:33:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:59 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:33:59 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 16.666667, stdev: 0.000000, max: 16.666667, min: 16.666667)
15/08/06 17:33:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:59 INFO StatsReportListener: 	17 %	17 %	17 %	17 %	17 %	17 %	17 %	17 %	17 %
15/08/06 17:33:59 INFO StatsReportListener: other time pct: (count: 1, mean: 83.333333, stdev: 0.000000, max: 83.333333, min: 83.333333)
15/08/06 17:33:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:33:59 INFO StatsReportListener: 	83 %	83 %	83 %	83 %	83 %	83 %	83 %	83 %	83 %
15/08/06 17:34:00 INFO ParseDriver: Parsing command: insert into table q11_important_stock_par_spark
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par_spark) sum_tmp
        join q11_part_tmp_par_spark
where part_value > total_value * 0.0001
order by value desc
15/08/06 17:34:00 INFO ParseDriver: Parse Completed
15/08/06 17:34:00 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#114, part_value#115)
15/08/06 17:34:00 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#118, part_value#119)
15/08/06 17:34:00 INFO MemoryStore: ensureFreeSpace(280818) called with curMem=967858, maxMem=3333968363
15/08/06 17:34:00 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 274.2 KB, free 3.1 GB)
15/08/06 17:34:00 INFO MemoryStore: ensureFreeSpace(31760) called with curMem=1248676, maxMem=3333968363
15/08/06 17:34:00 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 31.0 KB, free 3.1 GB)
15/08/06 17:34:00 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:42931 (size: 31.0 KB, free: 3.1 GB)
15/08/06 17:34:00 INFO BlockManagerMaster: Updated info of block broadcast_14_piece0
15/08/06 17:34:00 INFO DefaultExecutionContext: Created broadcast 14 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:34:00 INFO MemoryStore: ensureFreeSpace(280962) called with curMem=1280436, maxMem=3333968363
15/08/06 17:34:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 274.4 KB, free 3.1 GB)
15/08/06 17:34:00 INFO MemoryStore: ensureFreeSpace(31842) called with curMem=1561398, maxMem=3333968363
15/08/06 17:34:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:34:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:42931 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:34:00 INFO BlockManagerMaster: Updated info of block broadcast_15_piece0
15/08/06 17:34:00 INFO DefaultExecutionContext: Created broadcast 15 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:34:00 INFO FileInputFormat: Total input paths to process : 200
15/08/06 17:34:00 INFO ParquetInputFormat: Total input paths to process : 200
15/08/06 17:34:00 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:34:00 INFO ParquetFileReader: reading another 200 footers
15/08/06 17:34:00 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:34:00 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000; isDirectory=false; length=2638; replication=1; blocksize=134217728; modification_time=1438882434105; access_time=1438882433533; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001; isDirectory=false; length=2326; replication=1; blocksize=134217728; modification_time=1438882434101; access_time=1438882433530; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438882434104; access_time=1438882433531; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438882434105; access_time=1438882433536; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004; isDirectory=false; length=2194; replication=1; blocksize=134217728; modification_time=1438882434102; access_time=1438882433538; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005; isDirectory=false; length=2446; replication=1; blocksize=134217728; modification_time=1438882434105; access_time=1438882433538; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006; isDirectory=false; length=1858; replication=1; blocksize=134217728; modification_time=1438882434104; access_time=1438882433539; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438882434105; access_time=1438882433537; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438882434102; access_time=1438882433540; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009; isDirectory=false; length=2050; replication=1; blocksize=134217728; modification_time=1438882434104; access_time=1438882433539; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010; isDirectory=false; length=2002; replication=1; blocksize=134217728; modification_time=1438882434099; access_time=1438882433533; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438882434100; access_time=1438882433541; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012; isDirectory=false; length=2038; replication=1; blocksize=134217728; modification_time=1438882434100; access_time=1438882433534; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013; isDirectory=false; length=2170; replication=1; blocksize=134217728; modification_time=1438882434101; access_time=1438882433536; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438882434103; access_time=1438882433537; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438882434104; access_time=1438882433535; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438882434365; access_time=1438882434282; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438882434509; access_time=1438882434338; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438882434355; access_time=1438882434282; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438882434383; access_time=1438882434332; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438882434382; access_time=1438882434324; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021; isDirectory=false; length=2122; replication=1; blocksize=134217728; modification_time=1438882434760; access_time=1438882434285; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022; isDirectory=false; length=2170; replication=1; blocksize=134217728; modification_time=1438882434355; access_time=1438882434286; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438882434383; access_time=1438882434311; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024; isDirectory=false; length=2470; replication=1; blocksize=134217728; modification_time=1438882434364; access_time=1438882434310; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438882434356; access_time=1438882434302; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438882434380; access_time=1438882434318; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438882434506; access_time=1438882434339; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028; isDirectory=false; length=2134; replication=1; blocksize=134217728; modification_time=1438882434960; access_time=1438882434354; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438882434355; access_time=1438882434286; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438882434493; access_time=1438882434329; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031; isDirectory=false; length=1378; replication=1; blocksize=134217728; modification_time=1438882434518; access_time=1438882434379; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032; isDirectory=false; length=2302; replication=1; blocksize=134217728; modification_time=1438882434748; access_time=1438882434621; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438882434773; access_time=1438882434680; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034; isDirectory=false; length=2626; replication=1; blocksize=134217728; modification_time=1438882435139; access_time=1438882434632; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438882434668; access_time=1438882434620; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438882434748; access_time=1438882434633; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438882434729; access_time=1438882434633; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438882434748; access_time=1438882434632; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438882434747; access_time=1438882434633; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438882435146; access_time=1438882434634; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041; isDirectory=false; length=1738; replication=1; blocksize=134217728; modification_time=1438882435167; access_time=1438882434701; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042; isDirectory=false; length=1762; replication=1; blocksize=134217728; modification_time=1438882435148; access_time=1438882434644; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438882435148; access_time=1438882434650; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044; isDirectory=false; length=1846; replication=1; blocksize=134217728; modification_time=1438882434762; access_time=1438882434681; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438882434760; access_time=1438882434679; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438882435041; access_time=1438882435009; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438882434995; access_time=1438882434866; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438882435050; access_time=1438882435025; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049; isDirectory=false; length=1462; replication=1; blocksize=134217728; modification_time=1438882434995; access_time=1438882434867; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438882434998; access_time=1438882434867; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438882435006; access_time=1438882434979; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438882435019; access_time=1438882434987; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053; isDirectory=false; length=1738; replication=1; blocksize=134217728; modification_time=1438882435432; access_time=1438882434997; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438882435021; access_time=1438882434983; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055; isDirectory=false; length=1558; replication=1; blocksize=134217728; modification_time=1438882435032; access_time=1438882435001; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438882435231; access_time=1438882435200; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438882435232; access_time=1438882435201; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058; isDirectory=false; length=1666; replication=1; blocksize=134217728; modification_time=1438882435207; access_time=1438882435174; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438882435224; access_time=1438882435186; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060; isDirectory=false; length=2758; replication=1; blocksize=134217728; modification_time=1438882435232; access_time=1438882435195; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438882435343; access_time=1438882435250; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438882435231; access_time=1438882435201; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063; isDirectory=false; length=1990; replication=1; blocksize=134217728; modification_time=1438882435239; access_time=1438882435204; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438882435667; access_time=1438882435188; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438882435365; access_time=1438882435266; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066; isDirectory=false; length=2446; replication=1; blocksize=134217728; modification_time=1438882435380; access_time=1438882435348; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438882435821; access_time=1438882435384; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068; isDirectory=false; length=1786; replication=1; blocksize=134217728; modification_time=1438882435408; access_time=1438882435366; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438882435441; access_time=1438882435417; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438882435410; access_time=1438882435370; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438882435438; access_time=1438882435393; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438882435478; access_time=1438882435436; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438882435891; access_time=1438882435466; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074; isDirectory=false; length=2098; replication=1; blocksize=134217728; modification_time=1438882435489; access_time=1438882435454; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438882435542; access_time=1438882435511; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438882435505; access_time=1438882435479; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438882435482; access_time=1438882435451; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078; isDirectory=false; length=2614; replication=1; blocksize=134217728; modification_time=1438882435550; access_time=1438882435524; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438882435534; access_time=1438882435506; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438882435550; access_time=1438882435523; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438882435569; access_time=1438882435538; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438882435693; access_time=1438882435574; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438882435775; access_time=1438882435746; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438882435688; access_time=1438882435573; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438882435750; access_time=1438882435725; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438882436156; access_time=1438882435724; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087; isDirectory=false; length=2146; replication=1; blocksize=134217728; modification_time=1438882435739; access_time=1438882435714; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088; isDirectory=false; length=2722; replication=1; blocksize=134217728; modification_time=1438882436176; access_time=1438882435744; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438882435760; access_time=1438882435733; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090; isDirectory=false; length=2398; replication=1; blocksize=134217728; modification_time=1438882435843; access_time=1438882435813; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091; isDirectory=false; length=1750; replication=1; blocksize=134217728; modification_time=1438882435828; access_time=1438882435788; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438882435841; access_time=1438882435802; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438882435828; access_time=1438882435795; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438882435853; access_time=1438882435825; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438882435840; access_time=1438882435795; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096; isDirectory=false; length=1690; replication=1; blocksize=134217728; modification_time=1438882435844; access_time=1438882435816; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097; isDirectory=false; length=2194; replication=1; blocksize=134217728; modification_time=1438882435888; access_time=1438882435850; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438882435916; access_time=1438882435889; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099; isDirectory=false; length=2266; replication=1; blocksize=134217728; modification_time=1438882436021; access_time=1438882436001; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438882436000; access_time=1438882435901; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101; isDirectory=false; length=2410; replication=1; blocksize=134217728; modification_time=1438882436013; access_time=1438882435914; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438882436087; access_time=1438882436059; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438882436121; access_time=1438882436080; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104; isDirectory=false; length=2434; replication=1; blocksize=134217728; modification_time=1438882436149; access_time=1438882436117; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438882436132; access_time=1438882436080; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438882436106; access_time=1438882436062; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107; isDirectory=false; length=2326; replication=1; blocksize=134217728; modification_time=1438882436112; access_time=1438882436075; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438882436085; access_time=1438882436051; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438882436116; access_time=1438882436088; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438882436160; access_time=1438882436120; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438882436180; access_time=1438882436150; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438882436175; access_time=1438882436136; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438882436167; access_time=1438882436137; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438882436185; access_time=1438882436157; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115; isDirectory=false; length=1858; replication=1; blocksize=134217728; modification_time=1438882436240; access_time=1438882436194; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438882436385; access_time=1438882436247; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117; isDirectory=false; length=2590; replication=1; blocksize=134217728; modification_time=1438882436380; access_time=1438882436230; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438882436424; access_time=1438882436399; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438882436388; access_time=1438882436243; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120; isDirectory=false; length=2626; replication=1; blocksize=134217728; modification_time=1438882436461; access_time=1438882436425; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438882436410; access_time=1438882436379; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122; isDirectory=false; length=2422; replication=1; blocksize=134217728; modification_time=1438882436826; access_time=1438882436398; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123; isDirectory=false; length=2482; replication=1; blocksize=134217728; modification_time=1438882436454; access_time=1438882436422; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124; isDirectory=false; length=2098; replication=1; blocksize=134217728; modification_time=1438882436446; access_time=1438882436417; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125; isDirectory=false; length=1798; replication=1; blocksize=134217728; modification_time=1438882436882; access_time=1438882436445; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126; isDirectory=false; length=2350; replication=1; blocksize=134217728; modification_time=1438882436847; access_time=1438882436415; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438882436472; access_time=1438882436439; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438882436459; access_time=1438882436427; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438882436511; access_time=1438882436484; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438882436526; access_time=1438882436493; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438882437093; access_time=1438882436577; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438882436546; access_time=1438882436516; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438882436554; access_time=1438882436525; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134; isDirectory=false; length=1990; replication=1; blocksize=134217728; modification_time=1438882436552; access_time=1438882436527; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438882436562; access_time=1438882436535; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438882436587; access_time=1438882436566; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137; isDirectory=false; length=1798; replication=1; blocksize=134217728; modification_time=1438882436763; access_time=1438882436735; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138; isDirectory=false; length=1678; replication=1; blocksize=134217728; modification_time=1438882436686; access_time=1438882436603; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438882436715; access_time=1438882436686; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438882437116; access_time=1438882436687; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438882436697; access_time=1438882436674; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142; isDirectory=false; length=2350; replication=1; blocksize=134217728; modification_time=1438882436788; access_time=1438882436753; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143; isDirectory=false; length=1558; replication=1; blocksize=134217728; modification_time=1438882436759; access_time=1438882436732; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438882436785; access_time=1438882436749; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438882436785; access_time=1438882436747; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438882436813; access_time=1438882436777; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438882436787; access_time=1438882436750; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438882436867; access_time=1438882436838; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438882436847; access_time=1438882436823; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438882436886; access_time=1438882436858; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151; isDirectory=false; length=2602; replication=1; blocksize=134217728; modification_time=1438882436886; access_time=1438882436855; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152; isDirectory=false; length=1678; replication=1; blocksize=134217728; modification_time=1438882437120; access_time=1438882436933; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153; isDirectory=false; length=1846; replication=1; blocksize=134217728; modification_time=1438882437118; access_time=1438882436920; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154; isDirectory=false; length=2590; replication=1; blocksize=134217728; modification_time=1438882437114; access_time=1438882436927; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438882437110; access_time=1438882436930; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438882437116; access_time=1438882436940; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438882437113; access_time=1438882436931; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158; isDirectory=false; length=2338; replication=1; blocksize=134217728; modification_time=1438882437121; access_time=1438882436939; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438882437194; access_time=1438882437164; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438882437165; access_time=1438882437132; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161; isDirectory=false; length=1714; replication=1; blocksize=134217728; modification_time=1438882437567; access_time=1438882437134; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162; isDirectory=false; length=1954; replication=1; blocksize=134217728; modification_time=1438882437201; access_time=1438882437170; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163; isDirectory=false; length=1642; replication=1; blocksize=134217728; modification_time=1438882437226; access_time=1438882437187; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438882437239; access_time=1438882437193; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438882437188; access_time=1438882437163; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166; isDirectory=false; length=1690; replication=1; blocksize=134217728; modification_time=1438882437273; access_time=1438882437236; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167; isDirectory=false; length=2902; replication=1; blocksize=134217728; modification_time=1438882437662; access_time=1438882437237; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438882437473; access_time=1438882437303; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438882437323; access_time=1438882437265; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170; isDirectory=false; length=2554; replication=1; blocksize=134217728; modification_time=1438882437494; access_time=1438882437450; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438882437338; access_time=1438882437295; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438882437311; access_time=1438882437276; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173; isDirectory=false; length=2314; replication=1; blocksize=134217728; modification_time=1438882437318; access_time=1438882437276; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174; isDirectory=false; length=3010; replication=1; blocksize=134217728; modification_time=1438882437313; access_time=1438882437279; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438882437475; access_time=1438882437321; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176; isDirectory=false; length=2686; replication=1; blocksize=134217728; modification_time=1438882437487; access_time=1438882437334; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177; isDirectory=false; length=2554; replication=1; blocksize=134217728; modification_time=1438882437492; access_time=1438882437461; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178; isDirectory=false; length=3274; replication=1; blocksize=134217728; modification_time=1438882437542; access_time=1438882437517; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438882437531; access_time=1438882437502; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438882437569; access_time=1438882437539; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181; isDirectory=false; length=2434; replication=1; blocksize=134217728; modification_time=1438882437583; access_time=1438882437554; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438882438020; access_time=1438882437587; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438882438097; access_time=1438882437669; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438882437617; access_time=1438882437590; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438882437628; access_time=1438882437605; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438882437631; access_time=1438882437603; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187; isDirectory=false; length=2650; replication=1; blocksize=134217728; modification_time=1438882437673; access_time=1438882437640; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438882437641; access_time=1438882437614; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438882437670; access_time=1438882437630; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438882438097; access_time=1438882437666; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438882437828; access_time=1438882437696; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192; isDirectory=false; length=2038; replication=1; blocksize=134217728; modification_time=1438882438096; access_time=1438882437667; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438882437710; access_time=1438882437684; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194; isDirectory=false; length=1894; replication=1; blocksize=134217728; modification_time=1438882437831; access_time=1438882437699; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195; isDirectory=false; length=2122; replication=1; blocksize=134217728; modification_time=1438882437838; access_time=1438882437707; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438882437910; access_time=1438882437871; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438882437895; access_time=1438882437861; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198; isDirectory=false; length=2422; replication=1; blocksize=134217728; modification_time=1438882437915; access_time=1438882437872; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438882437912; access_time=1438882437885; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 225 ms
15/08/06 17:34:00 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:34:00 INFO DefaultExecutionContext: Starting job: RangePartitioner at Exchange.scala:88
15/08/06 17:34:01 INFO FileInputFormat: Total input paths to process : 200
15/08/06 17:34:01 INFO ParquetInputFormat: Total input paths to process : 200
15/08/06 17:34:01 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:34:01 INFO DAGScheduler: Registering RDD 63 (mapPartitions at Exchange.scala:100)
15/08/06 17:34:01 INFO DAGScheduler: Got job 7 (RangePartitioner at Exchange.scala:88) with 200 output partitions (allowLocal=false)
15/08/06 17:34:01 INFO DAGScheduler: Final stage: Stage 11(RangePartitioner at Exchange.scala:88)
15/08/06 17:34:01 INFO DAGScheduler: Parents of final stage: List(Stage 10)
15/08/06 17:34:01 INFO DAGScheduler: Missing parents: List(Stage 10)
15/08/06 17:34:01 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[63] at mapPartitions at Exchange.scala:100), which has no missing parents
15/08/06 17:34:01 INFO MemoryStore: ensureFreeSpace(8200) called with curMem=1593240, maxMem=3333968363
15/08/06 17:34:01 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 8.0 KB, free 3.1 GB)
15/08/06 17:34:01 INFO MemoryStore: ensureFreeSpace(4450) called with curMem=1601440, maxMem=3333968363
15/08/06 17:34:01 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.1 GB)
15/08/06 17:34:01 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:42931 (size: 4.3 KB, free: 3.1 GB)
15/08/06 17:34:01 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/08/06 17:34:01 INFO DefaultExecutionContext: Created broadcast 16 from broadcast at DAGScheduler.scala:838
15/08/06 17:34:01 INFO DAGScheduler: Submitting 200 missing tasks from Stage 10 (MapPartitionsRDD[63] at mapPartitions at Exchange.scala:100)
15/08/06 17:34:01 INFO TaskSchedulerImpl: Adding task set 10.0 with 200 tasks
15/08/06 17:34:01 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 417, localhost, ANY, 1525 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 418, localhost, ANY, 1526 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 419, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 420, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 421, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 422, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 423, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 424, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 425, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 426, localhost, ANY, 1524 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 427, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 428, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 429, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 430, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 431, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 15.0 in stage 10.0 (TID 432, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 2.0 in stage 10.0 (TID 419)
15/08/06 17:34:01 INFO Executor: Running task 0.0 in stage 10.0 (TID 417)
15/08/06 17:34:01 INFO Executor: Running task 1.0 in stage 10.0 (TID 418)
15/08/06 17:34:01 INFO Executor: Running task 3.0 in stage 10.0 (TID 420)
15/08/06 17:34:01 INFO Executor: Running task 7.0 in stage 10.0 (TID 424)
15/08/06 17:34:01 INFO Executor: Running task 8.0 in stage 10.0 (TID 425)
15/08/06 17:34:01 INFO Executor: Running task 12.0 in stage 10.0 (TID 429)
15/08/06 17:34:01 INFO Executor: Running task 5.0 in stage 10.0 (TID 422)
15/08/06 17:34:01 INFO Executor: Running task 10.0 in stage 10.0 (TID 427)
15/08/06 17:34:01 INFO Executor: Running task 14.0 in stage 10.0 (TID 431)
15/08/06 17:34:01 INFO Executor: Running task 15.0 in stage 10.0 (TID 432)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 4.0 in stage 10.0 (TID 421)
15/08/06 17:34:01 INFO Executor: Running task 9.0 in stage 10.0 (TID 426)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 13.0 in stage 10.0 (TID 430)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 11.0 in stage 10.0 (TID 428)
15/08/06 17:34:01 INFO Executor: Running task 6.0 in stage 10.0 (TID 423)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 193
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 177
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 125
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 126
15/08/06 17:34:01 INFO Executor: Finished task 0.0 in stage 10.0 (TID 417). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 140
15/08/06 17:34:01 INFO Executor: Finished task 2.0 in stage 10.0 (TID 419). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 129
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 167
15/08/06 17:34:01 INFO TaskSetManager: Starting task 16.0 in stage 10.0 (TID 433, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:01 INFO Executor: Running task 16.0 in stage 10.0 (TID 433)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 11.0 in stage 10.0 (TID 428). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 13.0 in stage 10.0 (TID 430). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 417) in 54 ms on localhost (1/200)
15/08/06 17:34:01 INFO Executor: Finished task 14.0 in stage 10.0 (TID 431). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 144
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 138
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 17.0 in stage 10.0 (TID 434, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 1.0 in stage 10.0 (TID 418). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 17.0 in stage 10.0 (TID 434)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 15.0 in stage 10.0 (TID 432). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 128
15/08/06 17:34:01 INFO Executor: Finished task 10.0 in stage 10.0 (TID 427). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 156
15/08/06 17:34:01 INFO Executor: Finished task 5.0 in stage 10.0 (TID 422). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 143
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 419) in 58 ms on localhost (2/200)
15/08/06 17:34:01 INFO Executor: Finished task 7.0 in stage 10.0 (TID 424). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 3.0 in stage 10.0 (TID 420). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 18.0 in stage 10.0 (TID 435, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 8.0 in stage 10.0 (TID 425). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 18.0 in stage 10.0 (TID 435)
15/08/06 17:34:01 INFO Executor: Finished task 9.0 in stage 10.0 (TID 426). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 6.0 in stage 10.0 (TID 423). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 428) in 61 ms on localhost (3/200)
15/08/06 17:34:01 INFO Executor: Finished task 4.0 in stage 10.0 (TID 421). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 12.0 in stage 10.0 (TID 429). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 430) in 61 ms on localhost (4/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 19.0 in stage 10.0 (TID 436, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 19.0 in stage 10.0 (TID 436)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 20.0 in stage 10.0 (TID 437, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 20.0 in stage 10.0 (TID 437)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 431) in 64 ms on localhost (5/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 21.0 in stage 10.0 (TID 438, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 21.0 in stage 10.0 (TID 438)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 418) in 73 ms on localhost (6/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 137
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:01 INFO TaskSetManager: Starting task 22.0 in stage 10.0 (TID 439, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 22.0 in stage 10.0 (TID 439)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 15.0 in stage 10.0 (TID 432) in 69 ms on localhost (7/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 23.0 in stage 10.0 (TID 440, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 23.0 in stage 10.0 (TID 440)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 24.0 in stage 10.0 (TID 441, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:01 INFO Executor: Running task 24.0 in stage 10.0 (TID 441)
15/08/06 17:34:01 INFO Executor: Finished task 17.0 in stage 10.0 (TID 434). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 16.0 in stage 10.0 (TID 433). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 427) in 75 ms on localhost (8/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 25.0 in stage 10.0 (TID 442, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 25.0 in stage 10.0 (TID 442)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 422) in 82 ms on localhost (9/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 171
15/08/06 17:34:01 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 424) in 85 ms on localhost (10/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 121
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 26.0 in stage 10.0 (TID 443, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 18.0 in stage 10.0 (TID 435). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 26.0 in stage 10.0 (TID 443)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 27.0 in stage 10.0 (TID 444, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 27.0 in stage 10.0 (TID 444)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 150
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 420) in 90 ms on localhost (11/200)
15/08/06 17:34:01 INFO Executor: Finished task 19.0 in stage 10.0 (TID 436). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:01 INFO Executor: Finished task 20.0 in stage 10.0 (TID 437). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 425) in 90 ms on localhost (12/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 161
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 179
15/08/06 17:34:01 INFO TaskSetManager: Starting task 28.0 in stage 10.0 (TID 445, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:01 INFO Executor: Running task 28.0 in stage 10.0 (TID 445)
15/08/06 17:34:01 INFO Executor: Finished task 21.0 in stage 10.0 (TID 438). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:34:01 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 426) in 93 ms on localhost (13/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 23.0 in stage 10.0 (TID 440). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 24.0 in stage 10.0 (TID 441). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 29.0 in stage 10.0 (TID 446, localhost, ANY, 1526 bytes)
15/08/06 17:34:01 INFO Executor: Running task 29.0 in stage 10.0 (TID 446)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 153
15/08/06 17:34:01 INFO Executor: Finished task 25.0 in stage 10.0 (TID 442). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 423) in 97 ms on localhost (14/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 30.0 in stage 10.0 (TID 447, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 22.0 in stage 10.0 (TID 439). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 30.0 in stage 10.0 (TID 447)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:01 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 421) in 100 ms on localhost (15/200)
15/08/06 17:34:01 INFO Executor: Finished task 26.0 in stage 10.0 (TID 443). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 31.0 in stage 10.0 (TID 448, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 31.0 in stage 10.0 (TID 448)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 429) in 99 ms on localhost (16/200)
15/08/06 17:34:01 INFO Executor: Finished task 27.0 in stage 10.0 (TID 444). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 32.0 in stage 10.0 (TID 449, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 32.0 in stage 10.0 (TID 449)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 33.0 in stage 10.0 (TID 450, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 33.0 in stage 10.0 (TID 450)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 17.0 in stage 10.0 (TID 434) in 53 ms on localhost (17/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 151
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 16.0 in stage 10.0 (TID 433) in 56 ms on localhost (18/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 34.0 in stage 10.0 (TID 451, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 34.0 in stage 10.0 (TID 451)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 196
15/08/06 17:34:01 INFO Executor: Finished task 28.0 in stage 10.0 (TID 445). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 18.0 in stage 10.0 (TID 435) in 46 ms on localhost (19/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 35.0 in stage 10.0 (TID 452, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 35.0 in stage 10.0 (TID 452)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 88
15/08/06 17:34:01 INFO TaskSetManager: Finished task 19.0 in stage 10.0 (TID 436) in 46 ms on localhost (20/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 165
15/08/06 17:34:01 INFO Executor: Finished task 30.0 in stage 10.0 (TID 447). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 36.0 in stage 10.0 (TID 453, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 29.0 in stage 10.0 (TID 446). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 36.0 in stage 10.0 (TID 453)
15/08/06 17:34:01 INFO Executor: Finished task 31.0 in stage 10.0 (TID 448). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 20.0 in stage 10.0 (TID 437) in 47 ms on localhost (21/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 37.0 in stage 10.0 (TID 454, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 32.0 in stage 10.0 (TID 449). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 37.0 in stage 10.0 (TID 454)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:01 INFO TaskSetManager: Finished task 21.0 in stage 10.0 (TID 438) in 48 ms on localhost (22/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:34:01 INFO TaskSetManager: Starting task 38.0 in stage 10.0 (TID 455, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 38.0 in stage 10.0 (TID 455)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 23.0 in stage 10.0 (TID 440) in 45 ms on localhost (23/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:34:01 INFO Executor: Finished task 33.0 in stage 10.0 (TID 450). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 34.0 in stage 10.0 (TID 451). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 39.0 in stage 10.0 (TID 456, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 39.0 in stage 10.0 (TID 456)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:34:01 INFO TaskSetManager: Finished task 24.0 in stage 10.0 (TID 441) in 46 ms on localhost (24/200)
15/08/06 17:34:01 INFO Executor: Finished task 35.0 in stage 10.0 (TID 452). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 40.0 in stage 10.0 (TID 457, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 40.0 in stage 10.0 (TID 457)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 36.0 in stage 10.0 (TID 453). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 25.0 in stage 10.0 (TID 442) in 47 ms on localhost (25/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 22.0 in stage 10.0 (TID 439) in 54 ms on localhost (26/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 41.0 in stage 10.0 (TID 458, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Running task 41.0 in stage 10.0 (TID 458)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 42.0 in stage 10.0 (TID 459, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Running task 42.0 in stage 10.0 (TID 459)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 26.0 in stage 10.0 (TID 443) in 44 ms on localhost (27/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 43.0 in stage 10.0 (TID 460, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:01 INFO Executor: Running task 43.0 in stage 10.0 (TID 460)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 27.0 in stage 10.0 (TID 444) in 44 ms on localhost (28/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 44.0 in stage 10.0 (TID 461, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:01 INFO Executor: Finished task 37.0 in stage 10.0 (TID 454). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 44.0 in stage 10.0 (TID 461)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 28.0 in stage 10.0 (TID 445) in 41 ms on localhost (29/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:34:01 INFO TaskSetManager: Starting task 45.0 in stage 10.0 (TID 462, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 38.0 in stage 10.0 (TID 455). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 45.0 in stage 10.0 (TID 462)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 30.0 in stage 10.0 (TID 447) in 39 ms on localhost (30/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 46.0 in stage 10.0 (TID 463, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:34:01 INFO Executor: Running task 46.0 in stage 10.0 (TID 463)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 120
15/08/06 17:34:01 INFO TaskSetManager: Finished task 29.0 in stage 10.0 (TID 446) in 43 ms on localhost (31/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 39.0 in stage 10.0 (TID 456). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 41.0 in stage 10.0 (TID 458). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 31.0 in stage 10.0 (TID 448) in 40 ms on localhost (32/200)
15/08/06 17:34:01 INFO Executor: Finished task 42.0 in stage 10.0 (TID 459). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 111
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 47.0 in stage 10.0 (TID 464, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 40.0 in stage 10.0 (TID 457). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:34:01 INFO Executor: Running task 47.0 in stage 10.0 (TID 464)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 48.0 in stage 10.0 (TID 465, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 48.0 in stage 10.0 (TID 465)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 32.0 in stage 10.0 (TID 449) in 52 ms on localhost (33/200)
15/08/06 17:34:01 INFO Executor: Finished task 43.0 in stage 10.0 (TID 460). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 49.0 in stage 10.0 (TID 466, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Running task 49.0 in stage 10.0 (TID 466)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 34.0 in stage 10.0 (TID 451) in 49 ms on localhost (34/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 50.0 in stage 10.0 (TID 467, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 44.0 in stage 10.0 (TID 461). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 50.0 in stage 10.0 (TID 467)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 141
15/08/06 17:34:01 INFO Executor: Finished task 45.0 in stage 10.0 (TID 462). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 33.0 in stage 10.0 (TID 450) in 56 ms on localhost (35/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 51.0 in stage 10.0 (TID 468, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Running task 51.0 in stage 10.0 (TID 468)
15/08/06 17:34:01 INFO Executor: Finished task 46.0 in stage 10.0 (TID 463). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 35.0 in stage 10.0 (TID 452) in 53 ms on localhost (36/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 52.0 in stage 10.0 (TID 469, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 52.0 in stage 10.0 (TID 469)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:34:01 INFO TaskSetManager: Finished task 36.0 in stage 10.0 (TID 453) in 54 ms on localhost (37/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 95
15/08/06 17:34:01 INFO TaskSetManager: Starting task 53.0 in stage 10.0 (TID 470, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:01 INFO Executor: Running task 53.0 in stage 10.0 (TID 470)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:01 INFO TaskSetManager: Finished task 37.0 in stage 10.0 (TID 454) in 53 ms on localhost (38/200)
15/08/06 17:34:01 INFO Executor: Finished task 48.0 in stage 10.0 (TID 465). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 49.0 in stage 10.0 (TID 466). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 50.0 in stage 10.0 (TID 467). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 47.0 in stage 10.0 (TID 464). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 38.0 in stage 10.0 (TID 455) in 53 ms on localhost (39/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 54.0 in stage 10.0 (TID 471, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 54.0 in stage 10.0 (TID 471)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:01 INFO TaskSetManager: Starting task 55.0 in stage 10.0 (TID 472, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 55.0 in stage 10.0 (TID 472)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:01 INFO TaskSetManager: Starting task 56.0 in stage 10.0 (TID 473, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 56.0 in stage 10.0 (TID 473)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 39.0 in stage 10.0 (TID 456) in 55 ms on localhost (40/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 57.0 in stage 10.0 (TID 474, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:34:01 INFO Executor: Running task 57.0 in stage 10.0 (TID 474)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 52.0 in stage 10.0 (TID 469). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 51.0 in stage 10.0 (TID 468). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 41.0 in stage 10.0 (TID 458) in 50 ms on localhost (41/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 118
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 58.0 in stage 10.0 (TID 475, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 58.0 in stage 10.0 (TID 475)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 42.0 in stage 10.0 (TID 459) in 51 ms on localhost (42/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 40.0 in stage 10.0 (TID 457) in 58 ms on localhost (43/200)
15/08/06 17:34:01 INFO Executor: Finished task 53.0 in stage 10.0 (TID 470). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:34:01 INFO TaskSetManager: Starting task 59.0 in stage 10.0 (TID 476, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 59.0 in stage 10.0 (TID 476)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:01 INFO TaskSetManager: Finished task 43.0 in stage 10.0 (TID 460) in 53 ms on localhost (44/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 60.0 in stage 10.0 (TID 477, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 60.0 in stage 10.0 (TID 477)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 54.0 in stage 10.0 (TID 471). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 133
15/08/06 17:34:01 INFO Executor: Finished task 55.0 in stage 10.0 (TID 472). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 44.0 in stage 10.0 (TID 461) in 52 ms on localhost (45/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 61.0 in stage 10.0 (TID 478, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 61.0 in stage 10.0 (TID 478)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 45.0 in stage 10.0 (TID 462) in 52 ms on localhost (46/200)
15/08/06 17:34:01 INFO Executor: Finished task 56.0 in stage 10.0 (TID 473). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 62.0 in stage 10.0 (TID 479, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 62.0 in stage 10.0 (TID 479)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 57.0 in stage 10.0 (TID 474). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 46.0 in stage 10.0 (TID 463) in 52 ms on localhost (47/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 112
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:01 INFO TaskSetManager: Starting task 63.0 in stage 10.0 (TID 480, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Running task 63.0 in stage 10.0 (TID 480)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 48.0 in stage 10.0 (TID 465) in 39 ms on localhost (48/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 64.0 in stage 10.0 (TID 481, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 64.0 in stage 10.0 (TID 481)
15/08/06 17:34:01 INFO Executor: Finished task 59.0 in stage 10.0 (TID 476). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 58.0 in stage 10.0 (TID 475). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 49.0 in stage 10.0 (TID 466) in 39 ms on localhost (49/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 203
15/08/06 17:34:01 INFO TaskSetManager: Starting task 65.0 in stage 10.0 (TID 482, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:34:01 INFO Executor: Running task 65.0 in stage 10.0 (TID 482)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 50.0 in stage 10.0 (TID 467) in 38 ms on localhost (50/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 66.0 in stage 10.0 (TID 483, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 66.0 in stage 10.0 (TID 483)
15/08/06 17:34:01 INFO Executor: Finished task 60.0 in stage 10.0 (TID 477). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 67.0 in stage 10.0 (TID 484, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Running task 67.0 in stage 10.0 (TID 484)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 178
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 47.0 in stage 10.0 (TID 464) in 47 ms on localhost (51/200)
15/08/06 17:34:01 INFO Executor: Finished task 61.0 in stage 10.0 (TID 478). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 68.0 in stage 10.0 (TID 485, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 68.0 in stage 10.0 (TID 485)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 52.0 in stage 10.0 (TID 469) in 37 ms on localhost (52/200)
15/08/06 17:34:01 INFO Executor: Finished task 62.0 in stage 10.0 (TID 479). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 51.0 in stage 10.0 (TID 468) in 41 ms on localhost (53/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 69.0 in stage 10.0 (TID 486, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Running task 69.0 in stage 10.0 (TID 486)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 53.0 in stage 10.0 (TID 470) in 36 ms on localhost (54/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 70.0 in stage 10.0 (TID 487, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 70.0 in stage 10.0 (TID 487)
15/08/06 17:34:01 INFO Executor: Finished task 63.0 in stage 10.0 (TID 480). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 71.0 in stage 10.0 (TID 488, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 71.0 in stage 10.0 (TID 488)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 72.0 in stage 10.0 (TID 489, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 72.0 in stage 10.0 (TID 489)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 54.0 in stage 10.0 (TID 471) in 36 ms on localhost (55/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 73.0 in stage 10.0 (TID 490, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:01 INFO Executor: Running task 73.0 in stage 10.0 (TID 490)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 122
15/08/06 17:34:01 INFO TaskSetManager: Starting task 74.0 in stage 10.0 (TID 491, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 64.0 in stage 10.0 (TID 481). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 74.0 in stage 10.0 (TID 491)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 65.0 in stage 10.0 (TID 482). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 66.0 in stage 10.0 (TID 483). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 56.0 in stage 10.0 (TID 473) in 39 ms on localhost (56/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 67.0 in stage 10.0 (TID 484). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 68.0 in stage 10.0 (TID 485). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:34:01 INFO TaskSetManager: Finished task 55.0 in stage 10.0 (TID 472) in 42 ms on localhost (57/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 75.0 in stage 10.0 (TID 492, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 75.0 in stage 10.0 (TID 492)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 134
15/08/06 17:34:01 INFO TaskSetManager: Finished task 59.0 in stage 10.0 (TID 476) in 36 ms on localhost (58/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 160
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 71.0 in stage 10.0 (TID 488). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 57.0 in stage 10.0 (TID 474) in 43 ms on localhost (59/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:01 INFO TaskSetManager: Finished task 58.0 in stage 10.0 (TID 475) in 41 ms on localhost (60/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 70.0 in stage 10.0 (TID 487). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 76.0 in stage 10.0 (TID 493, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Running task 76.0 in stage 10.0 (TID 493)
15/08/06 17:34:01 INFO Executor: Finished task 69.0 in stage 10.0 (TID 486). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 137
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 72.0 in stage 10.0 (TID 489). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 60.0 in stage 10.0 (TID 477) in 38 ms on localhost (61/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 61.0 in stage 10.0 (TID 478) in 39 ms on localhost (62/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 77.0 in stage 10.0 (TID 494, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 73.0 in stage 10.0 (TID 490). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 74.0 in stage 10.0 (TID 491). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 77.0 in stage 10.0 (TID 494)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:34:01 INFO TaskSetManager: Starting task 78.0 in stage 10.0 (TID 495, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 78.0 in stage 10.0 (TID 495)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 62.0 in stage 10.0 (TID 479) in 40 ms on localhost (63/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 63.0 in stage 10.0 (TID 480) in 38 ms on localhost (64/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 79.0 in stage 10.0 (TID 496, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:01 INFO Executor: Running task 79.0 in stage 10.0 (TID 496)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 80.0 in stage 10.0 (TID 497, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 75.0 in stage 10.0 (TID 492). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 80.0 in stage 10.0 (TID 497)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:34:01 INFO TaskSetManager: Finished task 64.0 in stage 10.0 (TID 481) in 39 ms on localhost (65/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 81.0 in stage 10.0 (TID 498, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 81.0 in stage 10.0 (TID 498)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 65.0 in stage 10.0 (TID 482) in 39 ms on localhost (66/200)
15/08/06 17:34:01 INFO Executor: Finished task 76.0 in stage 10.0 (TID 493). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 82.0 in stage 10.0 (TID 499, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 82.0 in stage 10.0 (TID 499)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 66.0 in stage 10.0 (TID 483) in 39 ms on localhost (67/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 191
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 83.0 in stage 10.0 (TID 500, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 83.0 in stage 10.0 (TID 500)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:01 INFO TaskSetManager: Finished task 67.0 in stage 10.0 (TID 484) in 41 ms on localhost (68/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 84.0 in stage 10.0 (TID 501, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 78.0 in stage 10.0 (TID 495). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 84.0 in stage 10.0 (TID 501)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 85.0 in stage 10.0 (TID 502, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 85.0 in stage 10.0 (TID 502)
15/08/06 17:34:01 INFO Executor: Finished task 77.0 in stage 10.0 (TID 494). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:34:01 INFO TaskSetManager: Starting task 86.0 in stage 10.0 (TID 503, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 86.0 in stage 10.0 (TID 503)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 68.0 in stage 10.0 (TID 485) in 43 ms on localhost (69/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 70.0 in stage 10.0 (TID 487) in 39 ms on localhost (70/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 80.0 in stage 10.0 (TID 497). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 71.0 in stage 10.0 (TID 488) in 39 ms on localhost (71/200)
15/08/06 17:34:01 INFO Executor: Finished task 79.0 in stage 10.0 (TID 496). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:34:01 INFO TaskSetManager: Finished task 69.0 in stage 10.0 (TID 486) in 44 ms on localhost (72/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 87.0 in stage 10.0 (TID 504, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 87.0 in stage 10.0 (TID 504)
15/08/06 17:34:01 INFO Executor: Finished task 82.0 in stage 10.0 (TID 499). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 88.0 in stage 10.0 (TID 505, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 88.0 in stage 10.0 (TID 505)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 72.0 in stage 10.0 (TID 489) in 51 ms on localhost (73/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 81.0 in stage 10.0 (TID 498). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 89.0 in stage 10.0 (TID 506, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:01 INFO Executor: Finished task 83.0 in stage 10.0 (TID 500). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 89.0 in stage 10.0 (TID 506)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:34:01 INFO TaskSetManager: Finished task 73.0 in stage 10.0 (TID 490) in 50 ms on localhost (74/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 90.0 in stage 10.0 (TID 507, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 84.0 in stage 10.0 (TID 501). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 85.0 in stage 10.0 (TID 502). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 90.0 in stage 10.0 (TID 507)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 74.0 in stage 10.0 (TID 491) in 52 ms on localhost (75/200)
15/08/06 17:34:01 INFO Executor: Finished task 86.0 in stage 10.0 (TID 503). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 91.0 in stage 10.0 (TID 508, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 91.0 in stage 10.0 (TID 508)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 75.0 in stage 10.0 (TID 492) in 49 ms on localhost (76/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 200
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 92.0 in stage 10.0 (TID 509, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 92.0 in stage 10.0 (TID 509)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 76.0 in stage 10.0 (TID 493) in 47 ms on localhost (77/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 152
15/08/06 17:34:01 INFO TaskSetManager: Starting task 93.0 in stage 10.0 (TID 510, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 93.0 in stage 10.0 (TID 510)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 88.0 in stage 10.0 (TID 505). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 78.0 in stage 10.0 (TID 495) in 44 ms on localhost (78/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 94.0 in stage 10.0 (TID 511, localhost, ANY, 1526 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 87.0 in stage 10.0 (TID 504). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 94.0 in stage 10.0 (TID 511)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 77.0 in stage 10.0 (TID 494) in 47 ms on localhost (79/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 173
15/08/06 17:34:01 INFO TaskSetManager: Starting task 95.0 in stage 10.0 (TID 512, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 95.0 in stage 10.0 (TID 512)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/06 17:34:01 INFO TaskSetManager: Finished task 80.0 in stage 10.0 (TID 497) in 43 ms on localhost (80/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 90.0 in stage 10.0 (TID 507). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 96.0 in stage 10.0 (TID 513, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 96.0 in stage 10.0 (TID 513)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 79.0 in stage 10.0 (TID 496) in 49 ms on localhost (81/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 133
15/08/06 17:34:01 INFO TaskSetManager: Starting task 97.0 in stage 10.0 (TID 514, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 97.0 in stage 10.0 (TID 514)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 119
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:01 INFO TaskSetManager: Finished task 82.0 in stage 10.0 (TID 499) in 45 ms on localhost (82/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 89.0 in stage 10.0 (TID 506). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 98.0 in stage 10.0 (TID 515, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 98.0 in stage 10.0 (TID 515)
15/08/06 17:34:01 INFO Executor: Finished task 93.0 in stage 10.0 (TID 510). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:34:01 INFO TaskSetManager: Finished task 81.0 in stage 10.0 (TID 498) in 49 ms on localhost (83/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 99.0 in stage 10.0 (TID 516, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 92.0 in stage 10.0 (TID 509). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 99.0 in stage 10.0 (TID 516)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 91.0 in stage 10.0 (TID 508). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 94.0 in stage 10.0 (TID 511). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 83.0 in stage 10.0 (TID 500) in 48 ms on localhost (84/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 100.0 in stage 10.0 (TID 517, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 100.0 in stage 10.0 (TID 517)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/06 17:34:01 INFO Executor: Finished task 95.0 in stage 10.0 (TID 512). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 84.0 in stage 10.0 (TID 501) in 51 ms on localhost (85/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 101.0 in stage 10.0 (TID 518, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Running task 101.0 in stage 10.0 (TID 518)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 156
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 96.0 in stage 10.0 (TID 513). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 85.0 in stage 10.0 (TID 502) in 54 ms on localhost (86/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 102.0 in stage 10.0 (TID 519, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 102.0 in stage 10.0 (TID 519)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 86.0 in stage 10.0 (TID 503) in 56 ms on localhost (87/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 103.0 in stage 10.0 (TID 520, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 103.0 in stage 10.0 (TID 520)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 97.0 in stage 10.0 (TID 514). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 162
15/08/06 17:34:01 INFO TaskSetManager: Finished task 88.0 in stage 10.0 (TID 505) in 44 ms on localhost (88/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:34:01 INFO TaskSetManager: Finished task 87.0 in stage 10.0 (TID 504) in 53 ms on localhost (89/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 104.0 in stage 10.0 (TID 521, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 104.0 in stage 10.0 (TID 521)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 90.0 in stage 10.0 (TID 507) in 43 ms on localhost (90/200)
15/08/06 17:34:01 INFO Executor: Finished task 99.0 in stage 10.0 (TID 516). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 105.0 in stage 10.0 (TID 522, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 98.0 in stage 10.0 (TID 515). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 105.0 in stage 10.0 (TID 522)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 174
15/08/06 17:34:01 INFO TaskSetManager: Starting task 106.0 in stage 10.0 (TID 523, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 106.0 in stage 10.0 (TID 523)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 89.0 in stage 10.0 (TID 506) in 48 ms on localhost (91/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:01 INFO Executor: Finished task 100.0 in stage 10.0 (TID 517). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 107.0 in stage 10.0 (TID 524, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 107.0 in stage 10.0 (TID 524)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 93.0 in stage 10.0 (TID 510) in 40 ms on localhost (92/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 101.0 in stage 10.0 (TID 518). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 103.0 in stage 10.0 (TID 520). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 108.0 in stage 10.0 (TID 525, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 108.0 in stage 10.0 (TID 525)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 92.0 in stage 10.0 (TID 509) in 46 ms on localhost (93/200)
15/08/06 17:34:01 INFO Executor: Finished task 104.0 in stage 10.0 (TID 521). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 102.0 in stage 10.0 (TID 519). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 109.0 in stage 10.0 (TID 526, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 109.0 in stage 10.0 (TID 526)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 91.0 in stage 10.0 (TID 508) in 54 ms on localhost (94/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 105.0 in stage 10.0 (TID 522). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 94.0 in stage 10.0 (TID 511) in 48 ms on localhost (95/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:34:01 INFO Executor: Finished task 106.0 in stage 10.0 (TID 523). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 110.0 in stage 10.0 (TID 527, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 110.0 in stage 10.0 (TID 527)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 111.0 in stage 10.0 (TID 528, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 111.0 in stage 10.0 (TID 528)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 107.0 in stage 10.0 (TID 524). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 95.0 in stage 10.0 (TID 512) in 51 ms on localhost (96/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 112.0 in stage 10.0 (TID 529, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 112.0 in stage 10.0 (TID 529)
15/08/06 17:34:01 INFO Executor: Finished task 108.0 in stage 10.0 (TID 525). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 96.0 in stage 10.0 (TID 513) in 51 ms on localhost (97/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 113.0 in stage 10.0 (TID 530, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 113.0 in stage 10.0 (TID 530)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 97.0 in stage 10.0 (TID 514) in 51 ms on localhost (98/200)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 99.0 in stage 10.0 (TID 516) in 48 ms on localhost (99/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 178
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 114.0 in stage 10.0 (TID 531, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 114.0 in stage 10.0 (TID 531)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:01 INFO TaskSetManager: Starting task 115.0 in stage 10.0 (TID 532, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 115.0 in stage 10.0 (TID 532)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 109.0 in stage 10.0 (TID 526). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 111.0 in stage 10.0 (TID 528). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 110.0 in stage 10.0 (TID 527). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 98.0 in stage 10.0 (TID 515) in 55 ms on localhost (100/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 116.0 in stage 10.0 (TID 533, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 116.0 in stage 10.0 (TID 533)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 112.0 in stage 10.0 (TID 529). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 100.0 in stage 10.0 (TID 517) in 52 ms on localhost (101/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 117.0 in stage 10.0 (TID 534, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Running task 117.0 in stage 10.0 (TID 534)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 101.0 in stage 10.0 (TID 518) in 50 ms on localhost (102/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:01 INFO Executor: Finished task 113.0 in stage 10.0 (TID 530). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 118.0 in stage 10.0 (TID 535, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 206
15/08/06 17:34:01 INFO TaskSetManager: Finished task 103.0 in stage 10.0 (TID 520) in 51 ms on localhost (103/200)
15/08/06 17:34:01 INFO Executor: Running task 118.0 in stage 10.0 (TID 535)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:34:01 INFO TaskSetManager: Starting task 119.0 in stage 10.0 (TID 536, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 119.0 in stage 10.0 (TID 536)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 104.0 in stage 10.0 (TID 521) in 51 ms on localhost (104/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:34:01 INFO TaskSetManager: Starting task 120.0 in stage 10.0 (TID 537, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 115.0 in stage 10.0 (TID 532). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 120.0 in stage 10.0 (TID 537)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 114.0 in stage 10.0 (TID 531). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 116.0 in stage 10.0 (TID 533). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 102.0 in stage 10.0 (TID 519) in 59 ms on localhost (105/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 121.0 in stage 10.0 (TID 538, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Running task 121.0 in stage 10.0 (TID 538)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 105.0 in stage 10.0 (TID 522) in 55 ms on localhost (106/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 122.0 in stage 10.0 (TID 539, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Running task 122.0 in stage 10.0 (TID 539)
15/08/06 17:34:01 INFO Executor: Finished task 117.0 in stage 10.0 (TID 534). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:01 INFO TaskSetManager: Finished task 106.0 in stage 10.0 (TID 523) in 55 ms on localhost (107/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 123.0 in stage 10.0 (TID 540, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 107.0 in stage 10.0 (TID 524) in 55 ms on localhost (108/200)
15/08/06 17:34:01 INFO Executor: Running task 123.0 in stage 10.0 (TID 540)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 124.0 in stage 10.0 (TID 541, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 124.0 in stage 10.0 (TID 541)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:34:01 INFO Executor: Finished task 118.0 in stage 10.0 (TID 535). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 108.0 in stage 10.0 (TID 525) in 55 ms on localhost (109/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 119.0 in stage 10.0 (TID 536). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 153
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 192
15/08/06 17:34:01 INFO TaskSetManager: Finished task 109.0 in stage 10.0 (TID 526) in 56 ms on localhost (110/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 125.0 in stage 10.0 (TID 542, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 125.0 in stage 10.0 (TID 542)
15/08/06 17:34:01 INFO Executor: Finished task 121.0 in stage 10.0 (TID 538). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 120.0 in stage 10.0 (TID 537). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 180
15/08/06 17:34:01 INFO Executor: Finished task 122.0 in stage 10.0 (TID 539). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 111.0 in stage 10.0 (TID 528) in 54 ms on localhost (111/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 126.0 in stage 10.0 (TID 543, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 126.0 in stage 10.0 (TID 543)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 127.0 in stage 10.0 (TID 544, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 127.0 in stage 10.0 (TID 544)
15/08/06 17:34:01 INFO Executor: Finished task 123.0 in stage 10.0 (TID 540). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 110.0 in stage 10.0 (TID 527) in 60 ms on localhost (112/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 112.0 in stage 10.0 (TID 529) in 55 ms on localhost (113/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 128.0 in stage 10.0 (TID 545, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 128.0 in stage 10.0 (TID 545)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 129.0 in stage 10.0 (TID 546, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 129.0 in stage 10.0 (TID 546)
15/08/06 17:34:01 INFO Executor: Finished task 124.0 in stage 10.0 (TID 541). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 113.0 in stage 10.0 (TID 530) in 56 ms on localhost (114/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 130.0 in stage 10.0 (TID 547, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 130.0 in stage 10.0 (TID 547)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:01 INFO TaskSetManager: Finished task 115.0 in stage 10.0 (TID 532) in 54 ms on localhost (115/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 131.0 in stage 10.0 (TID 548, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:01 INFO Executor: Running task 131.0 in stage 10.0 (TID 548)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 125.0 in stage 10.0 (TID 542). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 169
15/08/06 17:34:01 INFO TaskSetManager: Finished task 114.0 in stage 10.0 (TID 531) in 57 ms on localhost (116/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 127.0 in stage 10.0 (TID 544). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 116.0 in stage 10.0 (TID 533) in 55 ms on localhost (117/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:34:01 INFO TaskSetManager: Starting task 132.0 in stage 10.0 (TID 549, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 132.0 in stage 10.0 (TID 549)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 133.0 in stage 10.0 (TID 550, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 133.0 in stage 10.0 (TID 550)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:34:01 INFO Executor: Finished task 126.0 in stage 10.0 (TID 543). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 117.0 in stage 10.0 (TID 534) in 55 ms on localhost (118/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 129.0 in stage 10.0 (TID 546). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 134.0 in stage 10.0 (TID 551, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 134.0 in stage 10.0 (TID 551)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 118.0 in stage 10.0 (TID 535) in 56 ms on localhost (119/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 128.0 in stage 10.0 (TID 545). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 135.0 in stage 10.0 (TID 552, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 135.0 in stage 10.0 (TID 552)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 142
15/08/06 17:34:01 INFO TaskSetManager: Finished task 119.0 in stage 10.0 (TID 536) in 49 ms on localhost (120/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:01 INFO TaskSetManager: Finished task 121.0 in stage 10.0 (TID 538) in 45 ms on localhost (121/200)
15/08/06 17:34:01 INFO Executor: Finished task 131.0 in stage 10.0 (TID 548). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 130.0 in stage 10.0 (TID 547). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 136.0 in stage 10.0 (TID 553, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 136.0 in stage 10.0 (TID 553)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 137.0 in stage 10.0 (TID 554, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:34:01 INFO Executor: Running task 137.0 in stage 10.0 (TID 554)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:01 INFO Executor: Finished task 132.0 in stage 10.0 (TID 549). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 120.0 in stage 10.0 (TID 537) in 56 ms on localhost (122/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:34:01 INFO TaskSetManager: Starting task 138.0 in stage 10.0 (TID 555, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 138.0 in stage 10.0 (TID 555)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 122.0 in stage 10.0 (TID 539) in 53 ms on localhost (123/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:01 INFO Executor: Finished task 133.0 in stage 10.0 (TID 550). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 139.0 in stage 10.0 (TID 556, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 134.0 in stage 10.0 (TID 551). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 139.0 in stage 10.0 (TID 556)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 135.0 in stage 10.0 (TID 552). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 123.0 in stage 10.0 (TID 540) in 55 ms on localhost (124/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 140.0 in stage 10.0 (TID 557, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 124.0 in stage 10.0 (TID 541) in 55 ms on localhost (125/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 130
15/08/06 17:34:01 INFO Executor: Running task 140.0 in stage 10.0 (TID 557)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 141.0 in stage 10.0 (TID 558, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 141.0 in stage 10.0 (TID 558)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:34:01 INFO TaskSetManager: Finished task 125.0 in stage 10.0 (TID 542) in 51 ms on localhost (126/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 136.0 in stage 10.0 (TID 553). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 127.0 in stage 10.0 (TID 544) in 45 ms on localhost (127/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 123
15/08/06 17:34:01 INFO TaskSetManager: Starting task 142.0 in stage 10.0 (TID 559, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 142.0 in stage 10.0 (TID 559)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 143.0 in stage 10.0 (TID 560, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 143.0 in stage 10.0 (TID 560)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:34:01 INFO TaskSetManager: Finished task 126.0 in stage 10.0 (TID 543) in 49 ms on localhost (128/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:34:01 INFO Executor: Finished task 137.0 in stage 10.0 (TID 554). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 144.0 in stage 10.0 (TID 561, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 144.0 in stage 10.0 (TID 561)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 129.0 in stage 10.0 (TID 546) in 46 ms on localhost (129/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 145.0 in stage 10.0 (TID 562, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 138.0 in stage 10.0 (TID 555). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 145.0 in stage 10.0 (TID 562)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:01 INFO Executor: Finished task 139.0 in stage 10.0 (TID 556). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 128.0 in stage 10.0 (TID 545) in 49 ms on localhost (130/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 146.0 in stage 10.0 (TID 563, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:34:01 INFO Executor: Running task 146.0 in stage 10.0 (TID 563)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 131.0 in stage 10.0 (TID 548) in 45 ms on localhost (131/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 140.0 in stage 10.0 (TID 557). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 147.0 in stage 10.0 (TID 564, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 141.0 in stage 10.0 (TID 558). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 147.0 in stage 10.0 (TID 564)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 130.0 in stage 10.0 (TID 547) in 52 ms on localhost (132/200)
15/08/06 17:34:01 INFO Executor: Finished task 143.0 in stage 10.0 (TID 560). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:34:01 INFO TaskSetManager: Starting task 148.0 in stage 10.0 (TID 565, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 132.0 in stage 10.0 (TID 549) in 45 ms on localhost (133/200)
15/08/06 17:34:01 INFO Executor: Running task 148.0 in stage 10.0 (TID 565)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 142.0 in stage 10.0 (TID 559). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 149.0 in stage 10.0 (TID 566, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 149.0 in stage 10.0 (TID 566)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 130
15/08/06 17:34:01 INFO TaskSetManager: Finished task 133.0 in stage 10.0 (TID 550) in 48 ms on localhost (134/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 155
15/08/06 17:34:01 INFO TaskSetManager: Starting task 150.0 in stage 10.0 (TID 567, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 150.0 in stage 10.0 (TID 567)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 134.0 in stage 10.0 (TID 551) in 55 ms on localhost (135/200)
15/08/06 17:34:01 INFO Executor: Finished task 146.0 in stage 10.0 (TID 563). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 151.0 in stage 10.0 (TID 568, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 144.0 in stage 10.0 (TID 561). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 151.0 in stage 10.0 (TID 568)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 152.0 in stage 10.0 (TID 569, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 145.0 in stage 10.0 (TID 562). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 152.0 in stage 10.0 (TID 569)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 153.0 in stage 10.0 (TID 570, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 153.0 in stage 10.0 (TID 570)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 135.0 in stage 10.0 (TID 552) in 58 ms on localhost (136/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 137.0 in stage 10.0 (TID 554) in 52 ms on localhost (137/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:01 INFO TaskSetManager: Finished task 136.0 in stage 10.0 (TID 553) in 56 ms on localhost (138/200)
15/08/06 17:34:01 INFO Executor: Finished task 147.0 in stage 10.0 (TID 564). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 154.0 in stage 10.0 (TID 571, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:01 INFO Executor: Running task 154.0 in stage 10.0 (TID 571)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 138.0 in stage 10.0 (TID 555) in 52 ms on localhost (139/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 155.0 in stage 10.0 (TID 572, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 148.0 in stage 10.0 (TID 565). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 139.0 in stage 10.0 (TID 556) in 52 ms on localhost (140/200)
15/08/06 17:34:01 INFO Executor: Running task 155.0 in stage 10.0 (TID 572)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 156.0 in stage 10.0 (TID 573, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 156.0 in stage 10.0 (TID 573)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 157.0 in stage 10.0 (TID 574, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 149.0 in stage 10.0 (TID 566). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:34:01 INFO Executor: Running task 157.0 in stage 10.0 (TID 574)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 190
15/08/06 17:34:01 INFO TaskSetManager: Finished task 140.0 in stage 10.0 (TID 557) in 51 ms on localhost (141/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 158.0 in stage 10.0 (TID 575, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 158.0 in stage 10.0 (TID 575)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 150.0 in stage 10.0 (TID 567). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 141.0 in stage 10.0 (TID 558) in 50 ms on localhost (142/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 127
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 151.0 in stage 10.0 (TID 568). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 152.0 in stage 10.0 (TID 569). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 143.0 in stage 10.0 (TID 560) in 48 ms on localhost (143/200)
15/08/06 17:34:01 INFO Executor: Finished task 153.0 in stage 10.0 (TID 570). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 159.0 in stage 10.0 (TID 576, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 159.0 in stage 10.0 (TID 576)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 142.0 in stage 10.0 (TID 559) in 51 ms on localhost (144/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 160.0 in stage 10.0 (TID 577, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/06 17:34:01 INFO Executor: Running task 160.0 in stage 10.0 (TID 577)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 146.0 in stage 10.0 (TID 563) in 46 ms on localhost (145/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 149
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 144.0 in stage 10.0 (TID 561) in 52 ms on localhost (146/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 161.0 in stage 10.0 (TID 578, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 154.0 in stage 10.0 (TID 571). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 161.0 in stage 10.0 (TID 578)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 168
15/08/06 17:34:01 INFO Executor: Finished task 155.0 in stage 10.0 (TID 572). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 162.0 in stage 10.0 (TID 579, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 162.0 in stage 10.0 (TID 579)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 163.0 in stage 10.0 (TID 580, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 163.0 in stage 10.0 (TID 580)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 157.0 in stage 10.0 (TID 574). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:34:01 INFO Executor: Finished task 156.0 in stage 10.0 (TID 573). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 145.0 in stage 10.0 (TID 562) in 55 ms on localhost (147/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 158.0 in stage 10.0 (TID 575). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 147.0 in stage 10.0 (TID 564) in 52 ms on localhost (148/200)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 148.0 in stage 10.0 (TID 565) in 51 ms on localhost (149/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 164.0 in stage 10.0 (TID 581, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 164.0 in stage 10.0 (TID 581)
15/08/06 17:34:01 INFO Executor: Finished task 160.0 in stage 10.0 (TID 577). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 159.0 in stage 10.0 (TID 576). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 165.0 in stage 10.0 (TID 582, localhost, ANY, 1531 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 165.0 in stage 10.0 (TID 582)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 149.0 in stage 10.0 (TID 566) in 52 ms on localhost (150/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 116
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 166.0 in stage 10.0 (TID 583, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:34:01 INFO Executor: Running task 166.0 in stage 10.0 (TID 583)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 136
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 150.0 in stage 10.0 (TID 567) in 44 ms on localhost (151/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 167.0 in stage 10.0 (TID 584, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 167.0 in stage 10.0 (TID 584)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 110
15/08/06 17:34:01 INFO Executor: Finished task 161.0 in stage 10.0 (TID 578). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 151.0 in stage 10.0 (TID 568) in 42 ms on localhost (152/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 168.0 in stage 10.0 (TID 585, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 168.0 in stage 10.0 (TID 585)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 152.0 in stage 10.0 (TID 569) in 43 ms on localhost (153/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 169.0 in stage 10.0 (TID 586, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 163.0 in stage 10.0 (TID 580). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 162.0 in stage 10.0 (TID 579). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 169.0 in stage 10.0 (TID 586)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 153.0 in stage 10.0 (TID 570) in 44 ms on localhost (154/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 170.0 in stage 10.0 (TID 587, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 170.0 in stage 10.0 (TID 587)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 154.0 in stage 10.0 (TID 571) in 40 ms on localhost (155/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:01 INFO TaskSetManager: Starting task 171.0 in stage 10.0 (TID 588, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Running task 171.0 in stage 10.0 (TID 588)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 155.0 in stage 10.0 (TID 572) in 40 ms on localhost (156/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 172.0 in stage 10.0 (TID 589, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 164.0 in stage 10.0 (TID 581). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 157.0 in stage 10.0 (TID 574) in 40 ms on localhost (157/200)
15/08/06 17:34:01 INFO Executor: Running task 172.0 in stage 10.0 (TID 589)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 114
15/08/06 17:34:01 INFO TaskSetManager: Starting task 173.0 in stage 10.0 (TID 590, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Running task 173.0 in stage 10.0 (TID 590)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 156.0 in stage 10.0 (TID 573) in 43 ms on localhost (158/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 174.0 in stage 10.0 (TID 591, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 174.0 in stage 10.0 (TID 591)
15/08/06 17:34:01 INFO Executor: Finished task 165.0 in stage 10.0 (TID 582). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 158.0 in stage 10.0 (TID 575) in 42 ms on localhost (159/200)
15/08/06 17:34:01 INFO Executor: Finished task 167.0 in stage 10.0 (TID 584). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 175.0 in stage 10.0 (TID 592, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 168.0 in stage 10.0 (TID 585). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:01 INFO Executor: Running task 175.0 in stage 10.0 (TID 592)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 166.0 in stage 10.0 (TID 583). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 160.0 in stage 10.0 (TID 577) in 38 ms on localhost (160/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 155
15/08/06 17:34:01 INFO TaskSetManager: Starting task 176.0 in stage 10.0 (TID 593, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO Executor: Running task 176.0 in stage 10.0 (TID 593)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 186
15/08/06 17:34:01 INFO TaskSetManager: Finished task 159.0 in stage 10.0 (TID 576) in 44 ms on localhost (161/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 177.0 in stage 10.0 (TID 594, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 177.0 in stage 10.0 (TID 594)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 169.0 in stage 10.0 (TID 586). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 171.0 in stage 10.0 (TID 588). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 161.0 in stage 10.0 (TID 578) in 40 ms on localhost (162/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 178.0 in stage 10.0 (TID 595, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 170.0 in stage 10.0 (TID 587). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 178.0 in stage 10.0 (TID 595)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 163.0 in stage 10.0 (TID 580) in 40 ms on localhost (163/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 179.0 in stage 10.0 (TID 596, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 179.0 in stage 10.0 (TID 596)
15/08/06 17:34:01 INFO Executor: Finished task 173.0 in stage 10.0 (TID 590). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 224
15/08/06 17:34:01 INFO Executor: Finished task 172.0 in stage 10.0 (TID 589). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 162.0 in stage 10.0 (TID 579) in 43 ms on localhost (164/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 180.0 in stage 10.0 (TID 597, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 180.0 in stage 10.0 (TID 597)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 164.0 in stage 10.0 (TID 581) in 37 ms on localhost (165/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 181.0 in stage 10.0 (TID 598, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 197
15/08/06 17:34:01 INFO Executor: Finished task 174.0 in stage 10.0 (TID 591). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 175.0 in stage 10.0 (TID 592). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 181.0 in stage 10.0 (TID 598)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 165.0 in stage 10.0 (TID 582) in 38 ms on localhost (166/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 182.0 in stage 10.0 (TID 599, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 182.0 in stage 10.0 (TID 599)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 167.0 in stage 10.0 (TID 584) in 35 ms on localhost (167/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 176.0 in stage 10.0 (TID 593). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 183.0 in stage 10.0 (TID 600, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:34:01 INFO Executor: Finished task 177.0 in stage 10.0 (TID 594). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 183.0 in stage 10.0 (TID 600)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 168.0 in stage 10.0 (TID 585) in 36 ms on localhost (168/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 184.0 in stage 10.0 (TID 601, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 246
15/08/06 17:34:01 INFO Executor: Running task 184.0 in stage 10.0 (TID 601)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 185.0 in stage 10.0 (TID 602, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 185.0 in stage 10.0 (TID 602)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 186.0 in stage 10.0 (TID 603, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 187.0 in stage 10.0 (TID 604, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 187.0 in stage 10.0 (TID 604)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 188.0 in stage 10.0 (TID 605, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 178.0 in stage 10.0 (TID 595). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 186.0 in stage 10.0 (TID 603)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:01 INFO TaskSetManager: Starting task 189.0 in stage 10.0 (TID 606, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO Executor: Running task 188.0 in stage 10.0 (TID 605)
15/08/06 17:34:01 INFO Executor: Running task 189.0 in stage 10.0 (TID 606)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 166.0 in stage 10.0 (TID 583) in 46 ms on localhost (169/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Finished task 180.0 in stage 10.0 (TID 597). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 173.0 in stage 10.0 (TID 590) in 33 ms on localhost (170/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 170.0 in stage 10.0 (TID 587) in 41 ms on localhost (171/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Finished task 179.0 in stage 10.0 (TID 596). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 171.0 in stage 10.0 (TID 588) in 40 ms on localhost (172/200)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 169.0 in stage 10.0 (TID 586) in 50 ms on localhost (173/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:34:01 INFO TaskSetManager: Finished task 172.0 in stage 10.0 (TID 589) in 45 ms on localhost (174/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:34:01 INFO TaskSetManager: Starting task 190.0 in stage 10.0 (TID 607, localhost, ANY, 1528 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/06 17:34:01 INFO Executor: Running task 190.0 in stage 10.0 (TID 607)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 184
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 174.0 in stage 10.0 (TID 591) in 42 ms on localhost (175/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:34:01 INFO TaskSetManager: Finished task 175.0 in stage 10.0 (TID 592) in 42 ms on localhost (176/200)
15/08/06 17:34:01 INFO Executor: Finished task 183.0 in stage 10.0 (TID 600). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 182.0 in stage 10.0 (TID 599). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 184.0 in stage 10.0 (TID 601). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 191.0 in stage 10.0 (TID 608, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 191.0 in stage 10.0 (TID 608)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:01 INFO Executor: Finished task 181.0 in stage 10.0 (TID 598). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 192.0 in stage 10.0 (TID 609, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Finished task 188.0 in stage 10.0 (TID 605). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 194
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 193.0 in stage 10.0 (TID 610, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 192.0 in stage 10.0 (TID 609)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 134
15/08/06 17:34:01 INFO TaskSetManager: Finished task 176.0 in stage 10.0 (TID 593) in 43 ms on localhost (177/200)
15/08/06 17:34:01 INFO Executor: Finished task 185.0 in stage 10.0 (TID 602). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Running task 193.0 in stage 10.0 (TID 610)
15/08/06 17:34:01 INFO Executor: Finished task 186.0 in stage 10.0 (TID 603). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 187.0 in stage 10.0 (TID 604). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 177.0 in stage 10.0 (TID 594) in 42 ms on localhost (178/200)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 194.0 in stage 10.0 (TID 611, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 194.0 in stage 10.0 (TID 611)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 178.0 in stage 10.0 (TID 595) in 45 ms on localhost (179/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:01 INFO TaskSetManager: Starting task 195.0 in stage 10.0 (TID 612, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:01 INFO Executor: Running task 195.0 in stage 10.0 (TID 612)
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 189.0 in stage 10.0 (TID 606). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Starting task 196.0 in stage 10.0 (TID 613, localhost, ANY, 1530 bytes)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO Executor: Running task 196.0 in stage 10.0 (TID 613)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 133
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 158
15/08/06 17:34:01 INFO TaskSetManager: Finished task 179.0 in stage 10.0 (TID 596) in 48 ms on localhost (180/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 143
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO TaskSetManager: Finished task 180.0 in stage 10.0 (TID 597) in 49 ms on localhost (181/200)
15/08/06 17:34:01 INFO Executor: Finished task 191.0 in stage 10.0 (TID 608). 1819 bytes result sent to driver
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Starting task 197.0 in stage 10.0 (TID 614, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO Executor: Running task 197.0 in stage 10.0 (TID 614)
15/08/06 17:34:01 INFO Executor: Finished task 190.0 in stage 10.0 (TID 607). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO Executor: Finished task 193.0 in stage 10.0 (TID 610). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 183.0 in stage 10.0 (TID 600) in 46 ms on localhost (182/200)
15/08/06 17:34:01 INFO Executor: Finished task 192.0 in stage 10.0 (TID 609). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Starting task 198.0 in stage 10.0 (TID 615, localhost, ANY, 1527 bytes)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO Executor: Running task 198.0 in stage 10.0 (TID 615)
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 182.0 in stage 10.0 (TID 599) in 52 ms on localhost (183/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Starting task 199.0 in stage 10.0 (TID 616, localhost, ANY, 1529 bytes)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/06 17:34:01 INFO Executor: Running task 199.0 in stage 10.0 (TID 616)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 184.0 in stage 10.0 (TID 601) in 50 ms on localhost (184/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 150
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:01 INFO TaskSetManager: Finished task 188.0 in stage 10.0 (TID 605) in 48 ms on localhost (185/200)
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:01 INFO TaskSetManager: Finished task 181.0 in stage 10.0 (TID 598) in 60 ms on localhost (186/200)
15/08/06 17:34:01 INFO Executor: Finished task 194.0 in stage 10.0 (TID 611). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 186.0 in stage 10.0 (TID 603) in 53 ms on localhost (187/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO Executor: Finished task 195.0 in stage 10.0 (TID 612). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 185.0 in stage 10.0 (TID 602) in 55 ms on localhost (188/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:01 INFO TaskSetManager: Finished task 187.0 in stage 10.0 (TID 604) in 55 ms on localhost (189/200)
15/08/06 17:34:01 INFO Executor: Finished task 196.0 in stage 10.0 (TID 613). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 189.0 in stage 10.0 (TID 606) in 57 ms on localhost (190/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:01 INFO TaskSetManager: Finished task 190.0 in stage 10.0 (TID 607) in 45 ms on localhost (191/200)
15/08/06 17:34:01 INFO Executor: Finished task 197.0 in stage 10.0 (TID 614). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:34:01 INFO TaskSetManager: Finished task 193.0 in stage 10.0 (TID 610) in 39 ms on localhost (192/200)
15/08/06 17:34:01 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 161
15/08/06 17:34:01 INFO TaskSetManager: Finished task 191.0 in stage 10.0 (TID 608) in 42 ms on localhost (193/200)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 195.0 in stage 10.0 (TID 612) in 32 ms on localhost (194/200)
15/08/06 17:34:01 INFO Executor: Finished task 198.0 in stage 10.0 (TID 615). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 192.0 in stage 10.0 (TID 609) in 44 ms on localhost (195/200)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 194.0 in stage 10.0 (TID 611) in 37 ms on localhost (196/200)
15/08/06 17:34:01 INFO Executor: Finished task 199.0 in stage 10.0 (TID 616). 1819 bytes result sent to driver
15/08/06 17:34:01 INFO TaskSetManager: Finished task 196.0 in stage 10.0 (TID 613) in 34 ms on localhost (197/200)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 197.0 in stage 10.0 (TID 614) in 29 ms on localhost (198/200)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 198.0 in stage 10.0 (TID 615) in 26 ms on localhost (199/200)
15/08/06 17:34:01 INFO TaskSetManager: Finished task 199.0 in stage 10.0 (TID 616) in 25 ms on localhost (200/200)
15/08/06 17:34:01 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/06 17:34:01 INFO DAGScheduler: Stage 10 (mapPartitions at Exchange.scala:100) finished in 0.616 s
15/08/06 17:34:01 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:34:01 INFO DAGScheduler: running: Set()
15/08/06 17:34:01 INFO DAGScheduler: waiting: Set(Stage 11)
15/08/06 17:34:01 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@112492aa
15/08/06 17:34:01 INFO DAGScheduler: failed: Set()
15/08/06 17:34:01 INFO StatsReportListener: task runtime:(count: 200, mean: 49.415000, stdev: 11.579412, max: 100.000000, min: 25.000000)
15/08/06 17:34:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:01 INFO StatsReportListener: 	25.0 ms	36.0 ms	39.0 ms	42.0 ms	49.0 ms	54.0 ms	58.0 ms	73.0 ms	100.0 ms
15/08/06 17:34:01 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 56.000000, stdev: 0.000000, max: 56.000000, min: 56.000000)
15/08/06 17:34:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:01 INFO StatsReportListener: 	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B
15/08/06 17:34:01 INFO StatsReportListener: task result size:(count: 200, mean: 1819.000000, stdev: 0.000000, max: 1819.000000, min: 1819.000000)
15/08/06 17:34:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:01 INFO StatsReportListener: 	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B
15/08/06 17:34:01 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 36.278445, stdev: 11.344120, max: 75.925926, min: 19.607843)
15/08/06 17:34:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:01 INFO StatsReportListener: 	20 %	21 %	24 %	28 %	34 %	42 %	53 %	62 %	76 %
15/08/06 17:34:01 INFO StatsReportListener: other time pct: (count: 200, mean: 63.721555, stdev: 11.344120, max: 80.392157, min: 24.074074)
15/08/06 17:34:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:01 INFO StatsReportListener: 	24 %	40 %	48 %	58 %	66 %	72 %	76 %	79 %	80 %
15/08/06 17:34:01 INFO DAGScheduler: Missing parents for Stage 11: List()
15/08/06 17:34:01 INFO DAGScheduler: Submitting Stage 11 (MapPartitionsRDD[77] at RangePartitioner at Exchange.scala:88), which is now runnable
15/08/06 17:34:01 INFO MemoryStore: ensureFreeSpace(12256) called with curMem=1605890, maxMem=3333968363
15/08/06 17:34:01 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 12.0 KB, free 3.1 GB)
15/08/06 17:34:01 INFO MemoryStore: ensureFreeSpace(6466) called with curMem=1618146, maxMem=3333968363
15/08/06 17:34:01 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.3 KB, free 3.1 GB)
15/08/06 17:34:01 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:42931 (size: 6.3 KB, free: 3.1 GB)
15/08/06 17:34:01 INFO BlockManagerMaster: Updated info of block broadcast_17_piece0
15/08/06 17:34:01 INFO DefaultExecutionContext: Created broadcast 17 from broadcast at DAGScheduler.scala:838
15/08/06 17:34:01 INFO DAGScheduler: Submitting 200 missing tasks from Stage 11 (MapPartitionsRDD[77] at RangePartitioner at Exchange.scala:88)
15/08/06 17:34:01 INFO TaskSchedulerImpl: Adding task set 11.0 with 200 tasks
15/08/06 17:34:01 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 617, localhost, ANY, 1821 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 618, localhost, ANY, 1823 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 619, localhost, ANY, 1824 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 620, localhost, ANY, 1823 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 621, localhost, ANY, 1825 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 622, localhost, ANY, 1824 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 623, localhost, ANY, 1824 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 624, localhost, ANY, 1825 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 625, localhost, ANY, 1824 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 626, localhost, ANY, 1821 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 627, localhost, ANY, 1826 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 11.0 in stage 11.0 (TID 628, localhost, ANY, 1825 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 12.0 in stage 11.0 (TID 629, localhost, ANY, 1825 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 13.0 in stage 11.0 (TID 630, localhost, ANY, 1824 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 14.0 in stage 11.0 (TID 631, localhost, ANY, 1825 bytes)
15/08/06 17:34:01 INFO TaskSetManager: Starting task 15.0 in stage 11.0 (TID 632, localhost, ANY, 1825 bytes)
15/08/06 17:34:01 INFO Executor: Running task 0.0 in stage 11.0 (TID 617)
15/08/06 17:34:01 INFO Executor: Running task 4.0 in stage 11.0 (TID 621)
15/08/06 17:34:01 INFO Executor: Running task 12.0 in stage 11.0 (TID 629)
15/08/06 17:34:01 INFO Executor: Running task 14.0 in stage 11.0 (TID 631)
15/08/06 17:34:01 INFO Executor: Running task 5.0 in stage 11.0 (TID 622)
15/08/06 17:34:01 INFO Executor: Running task 3.0 in stage 11.0 (TID 620)
15/08/06 17:34:01 INFO Executor: Running task 2.0 in stage 11.0 (TID 619)
15/08/06 17:34:01 INFO Executor: Running task 11.0 in stage 11.0 (TID 628)
15/08/06 17:34:01 INFO Executor: Running task 13.0 in stage 11.0 (TID 630)
15/08/06 17:34:01 INFO Executor: Running task 15.0 in stage 11.0 (TID 632)
15/08/06 17:34:01 INFO Executor: Running task 1.0 in stage 11.0 (TID 618)
15/08/06 17:34:01 INFO Executor: Running task 10.0 in stage 11.0 (TID 627)
15/08/06 17:34:01 INFO Executor: Running task 9.0 in stage 11.0 (TID 626)
15/08/06 17:34:01 INFO Executor: Running task 8.0 in stage 11.0 (TID 625)
15/08/06 17:34:01 INFO Executor: Running task 6.0 in stage 11.0 (TID 623)
15/08/06 17:34:01 INFO Executor: Running task 7.0 in stage 11.0 (TID 624)
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 140
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 128
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO Executor: Finished task 10.0 in stage 11.0 (TID 627). 2054 bytes result sent to driver
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Starting task 16.0 in stage 11.0 (TID 633, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Finished task 7.0 in stage 11.0 (TID 624). 2018 bytes result sent to driver
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO Executor: Running task 16.0 in stage 11.0 (TID 633)
15/08/06 17:34:02 INFO Executor: Finished task 6.0 in stage 11.0 (TID 623). 2019 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:02 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 627) in 231 ms on localhost (1/200)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:34:02 INFO TaskSetManager: Starting task 17.0 in stage 11.0 (TID 634, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Running task 17.0 in stage 11.0 (TID 634)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Finished task 14.0 in stage 11.0 (TID 631). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 624) in 241 ms on localhost (2/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 144
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:02 INFO Executor: Finished task 5.0 in stage 11.0 (TID 622). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:34:02 INFO TaskSetManager: Starting task 18.0 in stage 11.0 (TID 635, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 18.0 in stage 11.0 (TID 635)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 623) in 246 ms on localhost (3/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Starting task 19.0 in stage 11.0 (TID 636, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Finished task 8.0 in stage 11.0 (TID 625). 2055 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO Executor: Finished task 1.0 in stage 11.0 (TID 618). 2073 bytes result sent to driver
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO Executor: Finished task 15.0 in stage 11.0 (TID 632). 2112 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 14.0 in stage 11.0 (TID 631) in 244 ms on localhost (4/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/06 17:34:02 INFO Executor: Running task 19.0 in stage 11.0 (TID 636)
15/08/06 17:34:02 INFO Executor: Finished task 9.0 in stage 11.0 (TID 626). 2055 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 20.0 in stage 11.0 (TID 637, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:34:02 INFO Executor: Finished task 13.0 in stage 11.0 (TID 630). 2237 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 622) in 250 ms on localhost (5/200)
15/08/06 17:34:02 INFO Executor: Running task 20.0 in stage 11.0 (TID 637)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 21.0 in stage 11.0 (TID 638, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 21.0 in stage 11.0 (TID 638)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Finished task 2.0 in stage 11.0 (TID 619). 2073 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 625) in 251 ms on localhost (6/200)
15/08/06 17:34:02 INFO Executor: Finished task 4.0 in stage 11.0 (TID 621). 2094 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Starting task 22.0 in stage 11.0 (TID 639, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 22.0 in stage 11.0 (TID 639)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:02 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 618) in 255 ms on localhost (7/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 23.0 in stage 11.0 (TID 640, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 23.0 in stage 11.0 (TID 640)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 15.0 in stage 11.0 (TID 632) in 250 ms on localhost (8/200)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Finished task 11.0 in stage 11.0 (TID 628). 2055 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 24.0 in stage 11.0 (TID 641, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 193
15/08/06 17:34:02 INFO Executor: Running task 24.0 in stage 11.0 (TID 641)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 626) in 255 ms on localhost (9/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 25.0 in stage 11.0 (TID 642, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO Executor: Running task 25.0 in stage 11.0 (TID 642)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 13.0 in stage 11.0 (TID 630) in 256 ms on localhost (10/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 26.0 in stage 11.0 (TID 643, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 26.0 in stage 11.0 (TID 643)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 619) in 263 ms on localhost (11/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 27.0 in stage 11.0 (TID 644, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 27.0 in stage 11.0 (TID 644)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 621) in 263 ms on localhost (12/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Finished task 0.0 in stage 11.0 (TID 617). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO TaskSetManager: Starting task 28.0 in stage 11.0 (TID 645, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 28.0 in stage 11.0 (TID 645)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Finished task 11.0 in stage 11.0 (TID 628) in 263 ms on localhost (13/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 29.0 in stage 11.0 (TID 646, localhost, ANY, 1822 bytes)
15/08/06 17:34:02 INFO Executor: Running task 29.0 in stage 11.0 (TID 646)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 143
15/08/06 17:34:02 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 617) in 271 ms on localhost (14/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Finished task 12.0 in stage 11.0 (TID 629). 2072 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 30.0 in stage 11.0 (TID 647, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 30.0 in stage 11.0 (TID 647)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 12.0 in stage 11.0 (TID 629) in 270 ms on localhost (15/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:34:02 INFO Executor: Finished task 3.0 in stage 11.0 (TID 620). 2111 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 31.0 in stage 11.0 (TID 648, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 31.0 in stage 11.0 (TID 648)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 620) in 281 ms on localhost (16/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO BlockManager: Removing broadcast 16
15/08/06 17:34:02 INFO BlockManager: Removing block broadcast_16_piece0
15/08/06 17:34:02 INFO MemoryStore: Block broadcast_16_piece0 of size 4450 dropped from memory (free 3332348201)
15/08/06 17:34:02 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:42931 in memory (size: 4.3 KB, free: 3.1 GB)
15/08/06 17:34:02 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/08/06 17:34:02 INFO BlockManager: Removing block broadcast_16
15/08/06 17:34:02 INFO MemoryStore: Block broadcast_16 of size 8200 dropped from memory (free 3332356401)
15/08/06 17:34:02 INFO ContextCleaner: Cleaned broadcast 16
15/08/06 17:34:02 INFO BlockManager: Removing broadcast 13
15/08/06 17:34:02 INFO BlockManager: Removing block broadcast_13_piece0
15/08/06 17:34:02 INFO MemoryStore: Block broadcast_13_piece0 of size 1941 dropped from memory (free 3332358342)
15/08/06 17:34:02 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:42931 in memory (size: 1941.0 B, free: 3.1 GB)
15/08/06 17:34:02 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/08/06 17:34:02 INFO BlockManager: Removing block broadcast_13
15/08/06 17:34:02 INFO MemoryStore: Block broadcast_13 of size 3240 dropped from memory (free 3332361582)
15/08/06 17:34:02 INFO ContextCleaner: Cleaned broadcast 13
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 151
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 179
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:02 INFO Executor: Finished task 22.0 in stage 11.0 (TID 639). 2055 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Starting task 32.0 in stage 11.0 (TID 649, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO Executor: Running task 32.0 in stage 11.0 (TID 649)
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 16.0 in stage 11.0 (TID 633). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 22.0 in stage 11.0 (TID 639) in 242 ms on localhost (17/200)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 150
15/08/06 17:34:02 INFO Executor: Finished task 24.0 in stage 11.0 (TID 641). 2001 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO Executor: Finished task 28.0 in stage 11.0 (TID 645). 2073 bytes result sent to driver
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:02 INFO TaskSetManager: Starting task 33.0 in stage 11.0 (TID 650, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 33.0 in stage 11.0 (TID 650)
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 21.0 in stage 11.0 (TID 638). 2019 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 18.0 in stage 11.0 (TID 635). 2073 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 16.0 in stage 11.0 (TID 633) in 267 ms on localhost (18/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Starting task 34.0 in stage 11.0 (TID 651, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:02 INFO Executor: Running task 34.0 in stage 11.0 (TID 651)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 24.0 in stage 11.0 (TID 641) in 244 ms on localhost (19/200)
15/08/06 17:34:02 INFO Executor: Finished task 23.0 in stage 11.0 (TID 640). 2130 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 35.0 in stage 11.0 (TID 652, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Finished task 29.0 in stage 11.0 (TID 646). 2166 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 30.0 in stage 11.0 (TID 647). 2055 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 19.0 in stage 11.0 (TID 636). 2146 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 88
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 31.0 in stage 11.0 (TID 648). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 27.0 in stage 11.0 (TID 644). 2019 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Running task 35.0 in stage 11.0 (TID 652)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 28.0 in stage 11.0 (TID 645) in 237 ms on localhost (20/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 121
15/08/06 17:34:02 INFO TaskSetManager: Finished task 21.0 in stage 11.0 (TID 638) in 260 ms on localhost (21/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:02 INFO TaskSetManager: Starting task 36.0 in stage 11.0 (TID 653, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 37.0 in stage 11.0 (TID 654, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Finished task 20.0 in stage 11.0 (TID 637). 2055 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Running task 36.0 in stage 11.0 (TID 653)
15/08/06 17:34:02 INFO Executor: Running task 37.0 in stage 11.0 (TID 654)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Finished task 17.0 in stage 11.0 (TID 634). 2093 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 18.0 in stage 11.0 (TID 635) in 268 ms on localhost (22/200)
15/08/06 17:34:02 INFO Executor: Finished task 25.0 in stage 11.0 (TID 642). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 26.0 in stage 11.0 (TID 643). 2036 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 38.0 in stage 11.0 (TID 655, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 38.0 in stage 11.0 (TID 655)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 23.0 in stage 11.0 (TID 640) in 261 ms on localhost (23/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 39.0 in stage 11.0 (TID 656, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 39.0 in stage 11.0 (TID 656)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 29.0 in stage 11.0 (TID 646) in 250 ms on localhost (24/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 40.0 in stage 11.0 (TID 657, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 40.0 in stage 11.0 (TID 657)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 30.0 in stage 11.0 (TID 647) in 247 ms on localhost (25/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 41.0 in stage 11.0 (TID 658, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 41.0 in stage 11.0 (TID 658)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 19.0 in stage 11.0 (TID 636) in 273 ms on localhost (26/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 42.0 in stage 11.0 (TID 659, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO Executor: Running task 42.0 in stage 11.0 (TID 659)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 31.0 in stage 11.0 (TID 648) in 245 ms on localhost (27/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 43.0 in stage 11.0 (TID 660, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 43.0 in stage 11.0 (TID 660)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 27.0 in stage 11.0 (TID 644) in 263 ms on localhost (28/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 44.0 in stage 11.0 (TID 661, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 44.0 in stage 11.0 (TID 661)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 20.0 in stage 11.0 (TID 637) in 281 ms on localhost (29/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 45.0 in stage 11.0 (TID 662, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 45.0 in stage 11.0 (TID 662)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 17.0 in stage 11.0 (TID 634) in 298 ms on localhost (30/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 46.0 in stage 11.0 (TID 663, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO Executor: Running task 46.0 in stage 11.0 (TID 663)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 47.0 in stage 11.0 (TID 664, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO Executor: Running task 47.0 in stage 11.0 (TID 664)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 25.0 in stage 11.0 (TID 642) in 280 ms on localhost (31/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 26.0 in stage 11.0 (TID 643) in 279 ms on localhost (32/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 165
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 192
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 32.0 in stage 11.0 (TID 649). 2094 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:34:02 INFO Executor: Finished task 34.0 in stage 11.0 (TID 651). 2093 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:02 INFO Executor: Finished task 37.0 in stage 11.0 (TID 654). 2073 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 48.0 in stage 11.0 (TID 665, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 48.0 in stage 11.0 (TID 665)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Finished task 33.0 in stage 11.0 (TID 650). 2073 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 40.0 in stage 11.0 (TID 657). 2093 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 36.0 in stage 11.0 (TID 653). 2112 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 32.0 in stage 11.0 (TID 649) in 186 ms on localhost (33/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 49.0 in stage 11.0 (TID 666, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO Executor: Running task 49.0 in stage 11.0 (TID 666)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 34.0 in stage 11.0 (TID 651) in 183 ms on localhost (34/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO TaskSetManager: Starting task 50.0 in stage 11.0 (TID 667, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO Executor: Running task 50.0 in stage 11.0 (TID 667)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO TaskSetManager: Finished task 37.0 in stage 11.0 (TID 654) in 172 ms on localhost (35/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 51.0 in stage 11.0 (TID 668, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Finished task 33.0 in stage 11.0 (TID 650) in 191 ms on localhost (36/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Running task 51.0 in stage 11.0 (TID 668)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 52.0 in stage 11.0 (TID 669, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 52.0 in stage 11.0 (TID 669)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 40.0 in stage 11.0 (TID 657) in 170 ms on localhost (37/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 134
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO TaskSetManager: Starting task 53.0 in stage 11.0 (TID 670, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Finished task 36.0 in stage 11.0 (TID 653) in 179 ms on localhost (38/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 39.0 in stage 11.0 (TID 656). 2019 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Running task 53.0 in stage 11.0 (TID 670)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO TaskSetManager: Starting task 54.0 in stage 11.0 (TID 671, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 54.0 in stage 11.0 (TID 671)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO Executor: Finished task 44.0 in stage 11.0 (TID 661). 2148 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 39.0 in stage 11.0 (TID 656) in 177 ms on localhost (39/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO TaskSetManager: Starting task 55.0 in stage 11.0 (TID 672, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Finished task 45.0 in stage 11.0 (TID 662). 2001 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Running task 55.0 in stage 11.0 (TID 672)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Finished task 44.0 in stage 11.0 (TID 661) in 168 ms on localhost (40/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 43.0 in stage 11.0 (TID 660). 2037 bytes result sent to driver
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO TaskSetManager: Starting task 56.0 in stage 11.0 (TID 673, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:34:02 INFO Executor: Running task 56.0 in stage 11.0 (TID 673)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 45.0 in stage 11.0 (TID 662) in 168 ms on localhost (41/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 57.0 in stage 11.0 (TID 674, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 57.0 in stage 11.0 (TID 674)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO Executor: Finished task 41.0 in stage 11.0 (TID 658). 2094 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 43.0 in stage 11.0 (TID 660) in 177 ms on localhost (42/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Starting task 58.0 in stage 11.0 (TID 675, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 58.0 in stage 11.0 (TID 675)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Finished task 41.0 in stage 11.0 (TID 658) in 183 ms on localhost (43/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:02 INFO Executor: Finished task 38.0 in stage 11.0 (TID 655). 2073 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 35.0 in stage 11.0 (TID 652). 2001 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 59.0 in stage 11.0 (TID 676, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Finished task 47.0 in stage 11.0 (TID 664). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 120
15/08/06 17:34:02 INFO TaskSetManager: Finished task 38.0 in stage 11.0 (TID 655) in 193 ms on localhost (44/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 60.0 in stage 11.0 (TID 677, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 60.0 in stage 11.0 (TID 677)
15/08/06 17:34:02 INFO Executor: Finished task 42.0 in stage 11.0 (TID 659). 2073 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Running task 59.0 in stage 11.0 (TID 676)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 35.0 in stage 11.0 (TID 652) in 208 ms on localhost (45/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 61.0 in stage 11.0 (TID 678, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Running task 61.0 in stage 11.0 (TID 678)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO TaskSetManager: Finished task 47.0 in stage 11.0 (TID 664) in 175 ms on localhost (46/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 141
15/08/06 17:34:02 INFO TaskSetManager: Starting task 62.0 in stage 11.0 (TID 679, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 62.0 in stage 11.0 (TID 679)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 42.0 in stage 11.0 (TID 659) in 192 ms on localhost (47/200)
15/08/06 17:34:02 INFO Executor: Finished task 46.0 in stage 11.0 (TID 663). 2055 bytes result sent to driver
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 46.0 in stage 11.0 (TID 663) in 182 ms on localhost (48/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 63.0 in stage 11.0 (TID 680, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 63.0 in stage 11.0 (TID 680)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Finished task 48.0 in stage 11.0 (TID 665). 2147 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 50.0 in stage 11.0 (TID 667). 2055 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 64.0 in stage 11.0 (TID 681, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO Executor: Running task 64.0 in stage 11.0 (TID 681)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 48.0 in stage 11.0 (TID 665) in 127 ms on localhost (49/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Starting task 65.0 in stage 11.0 (TID 682, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 65.0 in stage 11.0 (TID 682)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 95
15/08/06 17:34:02 INFO TaskSetManager: Finished task 50.0 in stage 11.0 (TID 667) in 124 ms on localhost (50/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Finished task 49.0 in stage 11.0 (TID 666). 2019 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:02 INFO TaskSetManager: Starting task 66.0 in stage 11.0 (TID 683, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 66.0 in stage 11.0 (TID 683)
15/08/06 17:34:02 INFO Executor: Finished task 52.0 in stage 11.0 (TID 669). 2019 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 49.0 in stage 11.0 (TID 666) in 130 ms on localhost (51/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO TaskSetManager: Starting task 67.0 in stage 11.0 (TID 684, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO Executor: Running task 67.0 in stage 11.0 (TID 684)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO TaskSetManager: Finished task 52.0 in stage 11.0 (TID 669) in 127 ms on localhost (52/200)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:34:02 INFO Executor: Finished task 55.0 in stage 11.0 (TID 672). 2001 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Starting task 68.0 in stage 11.0 (TID 685, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 68.0 in stage 11.0 (TID 685)
15/08/06 17:34:02 INFO Executor: Finished task 54.0 in stage 11.0 (TID 671). 2093 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 55.0 in stage 11.0 (TID 672) in 133 ms on localhost (53/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 69.0 in stage 11.0 (TID 686, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 133
15/08/06 17:34:02 INFO TaskSetManager: Finished task 54.0 in stage 11.0 (TID 671) in 139 ms on localhost (54/200)
15/08/06 17:34:02 INFO Executor: Running task 69.0 in stage 11.0 (TID 686)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Finished task 57.0 in stage 11.0 (TID 674). 2112 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 56.0 in stage 11.0 (TID 673). 2019 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 70.0 in stage 11.0 (TID 687, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO Executor: Running task 70.0 in stage 11.0 (TID 687)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO TaskSetManager: Finished task 57.0 in stage 11.0 (TID 674) in 136 ms on localhost (55/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO TaskSetManager: Starting task 71.0 in stage 11.0 (TID 688, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Running task 71.0 in stage 11.0 (TID 688)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Finished task 56.0 in stage 11.0 (TID 673) in 141 ms on localhost (56/200)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 59.0 in stage 11.0 (TID 676). 2073 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:34:02 INFO Executor: Finished task 51.0 in stage 11.0 (TID 668). 2072 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 145
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 203
15/08/06 17:34:02 INFO TaskSetManager: Starting task 72.0 in stage 11.0 (TID 689, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:34:02 INFO Executor: Running task 72.0 in stage 11.0 (TID 689)
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Finished task 59.0 in stage 11.0 (TID 676) in 140 ms on localhost (57/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 112
15/08/06 17:34:02 INFO TaskSetManager: Starting task 73.0 in stage 11.0 (TID 690, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 73.0 in stage 11.0 (TID 690)
15/08/06 17:34:02 INFO Executor: Finished task 61.0 in stage 11.0 (TID 678). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Finished task 62.0 in stage 11.0 (TID 679). 2111 bytes result sent to driver
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 51.0 in stage 11.0 (TID 668) in 163 ms on localhost (58/200)
15/08/06 17:34:02 INFO Executor: Finished task 58.0 in stage 11.0 (TID 675). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 74.0 in stage 11.0 (TID 691, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 74.0 in stage 11.0 (TID 691)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 61.0 in stage 11.0 (TID 678) in 139 ms on localhost (59/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 118
15/08/06 17:34:02 INFO TaskSetManager: Starting task 75.0 in stage 11.0 (TID 692, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 75.0 in stage 11.0 (TID 692)
15/08/06 17:34:02 INFO Executor: Finished task 60.0 in stage 11.0 (TID 677). 2111 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 76.0 in stage 11.0 (TID 693, localhost, ANY, 1827 bytes)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Running task 76.0 in stage 11.0 (TID 693)
15/08/06 17:34:02 INFO Executor: Finished task 53.0 in stage 11.0 (TID 670). 2037 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 77.0 in stage 11.0 (TID 694, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO Executor: Running task 77.0 in stage 11.0 (TID 694)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 78.0 in stage 11.0 (TID 695, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 53.0 in stage 11.0 (TID 670) in 167 ms on localhost (60/200)
15/08/06 17:34:02 INFO Executor: Running task 78.0 in stage 11.0 (TID 695)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 60.0 in stage 11.0 (TID 677) in 149 ms on localhost (61/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO TaskSetManager: Finished task 58.0 in stage 11.0 (TID 675) in 156 ms on localhost (62/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 62.0 in stage 11.0 (TID 679) in 147 ms on localhost (63/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 139
15/08/06 17:34:02 INFO Executor: Finished task 63.0 in stage 11.0 (TID 680). 2019 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 79.0 in stage 11.0 (TID 696, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 79.0 in stage 11.0 (TID 696)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 63.0 in stage 11.0 (TID 680) in 184 ms on localhost (64/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 177
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Finished task 66.0 in stage 11.0 (TID 683). 2220 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 64.0 in stage 11.0 (TID 681). 2056 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 80.0 in stage 11.0 (TID 697, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Finished task 65.0 in stage 11.0 (TID 682). 2074 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Running task 80.0 in stage 11.0 (TID 697)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 66.0 in stage 11.0 (TID 683) in 161 ms on localhost (65/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 81.0 in stage 11.0 (TID 698, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:02 INFO TaskSetManager: Starting task 82.0 in stage 11.0 (TID 699, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO Executor: Running task 82.0 in stage 11.0 (TID 699)
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 122
15/08/06 17:34:02 INFO TaskSetManager: Finished task 64.0 in stage 11.0 (TID 681) in 171 ms on localhost (66/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO Executor: Running task 81.0 in stage 11.0 (TID 698)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO Executor: Finished task 68.0 in stage 11.0 (TID 685). 2038 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 65.0 in stage 11.0 (TID 682) in 169 ms on localhost (67/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 83.0 in stage 11.0 (TID 700, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 83.0 in stage 11.0 (TID 700)
15/08/06 17:34:02 INFO Executor: Finished task 67.0 in stage 11.0 (TID 684). 2038 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Finished task 68.0 in stage 11.0 (TID 685) in 152 ms on localhost (68/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Starting task 84.0 in stage 11.0 (TID 701, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO Executor: Running task 84.0 in stage 11.0 (TID 701)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 67.0 in stage 11.0 (TID 684) in 167 ms on localhost (69/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO Executor: Finished task 70.0 in stage 11.0 (TID 687). 2020 bytes result sent to driver
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO TaskSetManager: Starting task 85.0 in stage 11.0 (TID 702, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 85.0 in stage 11.0 (TID 702)
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO TaskSetManager: Finished task 70.0 in stage 11.0 (TID 687) in 160 ms on localhost (70/200)
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:02 INFO Executor: Finished task 69.0 in stage 11.0 (TID 686). 2056 bytes result sent to driver
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:34:02 INFO TaskSetManager: Starting task 86.0 in stage 11.0 (TID 703, localhost, ANY, 1826 bytes)
15/08/06 17:34:02 INFO Executor: Running task 86.0 in stage 11.0 (TID 703)
15/08/06 17:34:02 INFO TaskSetManager: Finished task 69.0 in stage 11.0 (TID 686) in 175 ms on localhost (71/200)
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO Executor: Finished task 76.0 in stage 11.0 (TID 693). 2038 bytes result sent to driver
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO TaskSetManager: Starting task 87.0 in stage 11.0 (TID 704, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 87.0 in stage 11.0 (TID 704)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 76.0 in stage 11.0 (TID 693) in 155 ms on localhost (72/200)
15/08/06 17:34:02 INFO Executor: Finished task 71.0 in stage 11.0 (TID 688). 2074 bytes result sent to driver
15/08/06 17:34:02 INFO Executor: Finished task 72.0 in stage 11.0 (TID 689). 2038 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 88.0 in stage 11.0 (TID 705, localhost, ANY, 1824 bytes)
15/08/06 17:34:02 INFO Executor: Running task 88.0 in stage 11.0 (TID 705)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO TaskSetManager: Finished task 71.0 in stage 11.0 (TID 688) in 171 ms on localhost (73/200)
15/08/06 17:34:02 INFO TaskSetManager: Starting task 89.0 in stage 11.0 (TID 706, localhost, ANY, 1825 bytes)
15/08/06 17:34:02 INFO Executor: Running task 89.0 in stage 11.0 (TID 706)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:02 INFO TaskSetManager: Finished task 72.0 in stage 11.0 (TID 689) in 167 ms on localhost (74/200)
15/08/06 17:34:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:02 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:34:02 INFO Executor: Finished task 75.0 in stage 11.0 (TID 692). 2131 bytes result sent to driver
15/08/06 17:34:02 INFO TaskSetManager: Starting task 90.0 in stage 11.0 (TID 707, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO Executor: Running task 90.0 in stage 11.0 (TID 707)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 75.0 in stage 11.0 (TID 692) in 167 ms on localhost (75/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:34:03 INFO Executor: Finished task 74.0 in stage 11.0 (TID 691). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 91.0 in stage 11.0 (TID 708, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 91.0 in stage 11.0 (TID 708)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 191
15/08/06 17:34:03 INFO TaskSetManager: Finished task 74.0 in stage 11.0 (TID 691) in 182 ms on localhost (76/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:03 INFO Executor: Finished task 78.0 in stage 11.0 (TID 695). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Finished task 77.0 in stage 11.0 (TID 694). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 92.0 in stage 11.0 (TID 709, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 92.0 in stage 11.0 (TID 709)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 78.0 in stage 11.0 (TID 695) in 183 ms on localhost (77/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 93.0 in stage 11.0 (TID 710, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 93.0 in stage 11.0 (TID 710)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 77.0 in stage 11.0 (TID 694) in 186 ms on localhost (78/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:34:03 INFO Executor: Finished task 73.0 in stage 11.0 (TID 690). 2055 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 94.0 in stage 11.0 (TID 711, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO Executor: Running task 94.0 in stage 11.0 (TID 711)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 73.0 in stage 11.0 (TID 690) in 202 ms on localhost (79/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:34:03 INFO Executor: Finished task 79.0 in stage 11.0 (TID 696). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 95.0 in stage 11.0 (TID 712, localhost, ANY, 1827 bytes)
15/08/06 17:34:03 INFO Executor: Running task 95.0 in stage 11.0 (TID 712)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 79.0 in stage 11.0 (TID 696) in 166 ms on localhost (80/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 206
15/08/06 17:34:03 INFO Executor: Finished task 82.0 in stage 11.0 (TID 699). 2130 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 96.0 in stage 11.0 (TID 713, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 96.0 in stage 11.0 (TID 713)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Finished task 82.0 in stage 11.0 (TID 699) in 222 ms on localhost (81/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:34:03 INFO Executor: Finished task 84.0 in stage 11.0 (TID 701). 2002 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 97.0 in stage 11.0 (TID 714, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 97.0 in stage 11.0 (TID 714)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:03 INFO TaskSetManager: Finished task 84.0 in stage 11.0 (TID 701) in 229 ms on localhost (82/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Finished task 81.0 in stage 11.0 (TID 698). 2055 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 98.0 in stage 11.0 (TID 715, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 98.0 in stage 11.0 (TID 715)
15/08/06 17:34:03 INFO Executor: Finished task 80.0 in stage 11.0 (TID 697). 2073 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 81.0 in stage 11.0 (TID 698) in 243 ms on localhost (83/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO TaskSetManager: Starting task 99.0 in stage 11.0 (TID 716, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO Executor: Running task 99.0 in stage 11.0 (TID 716)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 80.0 in stage 11.0 (TID 697) in 249 ms on localhost (84/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 163
15/08/06 17:34:03 INFO Executor: Finished task 83.0 in stage 11.0 (TID 700). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 100.0 in stage 11.0 (TID 717, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO Executor: Running task 100.0 in stage 11.0 (TID 717)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Finished task 83.0 in stage 11.0 (TID 700) in 249 ms on localhost (85/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO Executor: Finished task 87.0 in stage 11.0 (TID 704). 2113 bytes result sent to driver
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 200
15/08/06 17:34:03 INFO TaskSetManager: Starting task 101.0 in stage 11.0 (TID 718, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO Executor: Running task 101.0 in stage 11.0 (TID 718)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO Executor: Finished task 88.0 in stage 11.0 (TID 705). 2095 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 87.0 in stage 11.0 (TID 704) in 228 ms on localhost (86/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 102.0 in stage 11.0 (TID 719, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Running task 102.0 in stage 11.0 (TID 719)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Finished task 88.0 in stage 11.0 (TID 705) in 227 ms on localhost (87/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Finished task 85.0 in stage 11.0 (TID 702). 2113 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Starting task 103.0 in stage 11.0 (TID 720, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO Executor: Running task 103.0 in stage 11.0 (TID 720)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Finished task 85.0 in stage 11.0 (TID 702) in 246 ms on localhost (88/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 173
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:34:03 INFO Executor: Finished task 86.0 in stage 11.0 (TID 703). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Starting task 104.0 in stage 11.0 (TID 721, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 104.0 in stage 11.0 (TID 721)
15/08/06 17:34:03 INFO Executor: Finished task 89.0 in stage 11.0 (TID 706). 2095 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 86.0 in stage 11.0 (TID 703) in 240 ms on localhost (89/200)
15/08/06 17:34:03 INFO Executor: Finished task 90.0 in stage 11.0 (TID 707). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 105.0 in stage 11.0 (TID 722, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO Executor: Running task 105.0 in stage 11.0 (TID 722)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 89.0 in stage 11.0 (TID 706) in 235 ms on localhost (90/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO TaskSetManager: Starting task 106.0 in stage 11.0 (TID 723, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 106.0 in stage 11.0 (TID 723)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 90.0 in stage 11.0 (TID 707) in 229 ms on localhost (91/200)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 119
15/08/06 17:34:03 INFO Executor: Finished task 91.0 in stage 11.0 (TID 708). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 107.0 in stage 11.0 (TID 724, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 107.0 in stage 11.0 (TID 724)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 91.0 in stage 11.0 (TID 708) in 225 ms on localhost (92/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 196
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:03 INFO Executor: Finished task 94.0 in stage 11.0 (TID 711). 2095 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Finished task 92.0 in stage 11.0 (TID 709). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 108.0 in stage 11.0 (TID 725, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 108.0 in stage 11.0 (TID 725)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 94.0 in stage 11.0 (TID 711) in 219 ms on localhost (93/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 109.0 in stage 11.0 (TID 726, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 109.0 in stage 11.0 (TID 726)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:03 INFO TaskSetManager: Finished task 92.0 in stage 11.0 (TID 709) in 233 ms on localhost (94/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO Executor: Finished task 93.0 in stage 11.0 (TID 710). 2055 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 110.0 in stage 11.0 (TID 727, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO Executor: Running task 110.0 in stage 11.0 (TID 727)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 93.0 in stage 11.0 (TID 710) in 239 ms on localhost (95/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:03 INFO Executor: Finished task 95.0 in stage 11.0 (TID 712). 2131 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 111.0 in stage 11.0 (TID 728, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 111.0 in stage 11.0 (TID 728)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 95.0 in stage 11.0 (TID 712) in 226 ms on localhost (96/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 114
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO Executor: Finished task 96.0 in stage 11.0 (TID 713). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 112.0 in stage 11.0 (TID 729, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Finished task 96.0 in stage 11.0 (TID 713) in 162 ms on localhost (97/200)
15/08/06 17:34:03 INFO Executor: Running task 112.0 in stage 11.0 (TID 729)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 162
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:34:03 INFO Executor: Finished task 97.0 in stage 11.0 (TID 714). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Finished task 99.0 in stage 11.0 (TID 716). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 113.0 in stage 11.0 (TID 730, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO Executor: Running task 113.0 in stage 11.0 (TID 730)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 97.0 in stage 11.0 (TID 714) in 153 ms on localhost (98/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 114.0 in stage 11.0 (TID 731, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO Executor: Finished task 98.0 in stage 11.0 (TID 715). 2095 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:03 INFO TaskSetManager: Finished task 99.0 in stage 11.0 (TID 716) in 146 ms on localhost (99/200)
15/08/06 17:34:03 INFO Executor: Running task 114.0 in stage 11.0 (TID 731)
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Starting task 115.0 in stage 11.0 (TID 732, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 115.0 in stage 11.0 (TID 732)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 98.0 in stage 11.0 (TID 715) in 150 ms on localhost (100/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 155
15/08/06 17:34:03 INFO Executor: Finished task 100.0 in stage 11.0 (TID 717). 2147 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 116.0 in stage 11.0 (TID 733, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 116.0 in stage 11.0 (TID 733)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO TaskSetManager: Finished task 100.0 in stage 11.0 (TID 717) in 151 ms on localhost (101/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 174
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Finished task 106.0 in stage 11.0 (TID 723). 2167 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 117.0 in stage 11.0 (TID 734, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO Executor: Running task 117.0 in stage 11.0 (TID 734)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:03 INFO Executor: Finished task 101.0 in stage 11.0 (TID 718). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 106.0 in stage 11.0 (TID 723) in 150 ms on localhost (102/200)
15/08/06 17:34:03 INFO Executor: Finished task 102.0 in stage 11.0 (TID 719). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Starting task 118.0 in stage 11.0 (TID 735, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 118.0 in stage 11.0 (TID 735)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:03 INFO TaskSetManager: Finished task 101.0 in stage 11.0 (TID 718) in 166 ms on localhost (103/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 119.0 in stage 11.0 (TID 736, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO Executor: Running task 119.0 in stage 11.0 (TID 736)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Finished task 103.0 in stage 11.0 (TID 720). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO TaskSetManager: Finished task 102.0 in stage 11.0 (TID 719) in 166 ms on localhost (104/200)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Starting task 120.0 in stage 11.0 (TID 737, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO Executor: Running task 120.0 in stage 11.0 (TID 737)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 103.0 in stage 11.0 (TID 720) in 164 ms on localhost (105/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:03 INFO Executor: Finished task 105.0 in stage 11.0 (TID 722). 2094 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 121.0 in stage 11.0 (TID 738, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Running task 121.0 in stage 11.0 (TID 738)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 105.0 in stage 11.0 (TID 722) in 166 ms on localhost (106/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Finished task 107.0 in stage 11.0 (TID 724). 2113 bytes result sent to driver
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:34:03 INFO TaskSetManager: Starting task 122.0 in stage 11.0 (TID 739, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO Executor: Running task 122.0 in stage 11.0 (TID 739)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 107.0 in stage 11.0 (TID 724) in 161 ms on localhost (107/200)
15/08/06 17:34:03 INFO Executor: Finished task 104.0 in stage 11.0 (TID 721). 2073 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 123.0 in stage 11.0 (TID 740, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Running task 123.0 in stage 11.0 (TID 740)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 104.0 in stage 11.0 (TID 721) in 174 ms on localhost (108/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:03 INFO Executor: Finished task 109.0 in stage 11.0 (TID 726). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 124.0 in stage 11.0 (TID 741, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO Executor: Finished task 108.0 in stage 11.0 (TID 725). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Running task 124.0 in stage 11.0 (TID 741)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 109.0 in stage 11.0 (TID 726) in 180 ms on localhost (109/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 125.0 in stage 11.0 (TID 742, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 125.0 in stage 11.0 (TID 742)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 108.0 in stage 11.0 (TID 725) in 184 ms on localhost (110/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:03 INFO Executor: Finished task 110.0 in stage 11.0 (TID 727). 2149 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 126.0 in stage 11.0 (TID 743, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO Executor: Running task 126.0 in stage 11.0 (TID 743)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 110.0 in stage 11.0 (TID 727) in 187 ms on localhost (111/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 125
15/08/06 17:34:03 INFO Executor: Finished task 111.0 in stage 11.0 (TID 728). 2002 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 127.0 in stage 11.0 (TID 744, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 127.0 in stage 11.0 (TID 744)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 111.0 in stage 11.0 (TID 728) in 178 ms on localhost (112/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:34:03 INFO Executor: Finished task 115.0 in stage 11.0 (TID 732). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Finished task 112.0 in stage 11.0 (TID 729). 2055 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Finished task 114.0 in stage 11.0 (TID 731). 2095 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Finished task 113.0 in stage 11.0 (TID 730). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 128.0 in stage 11.0 (TID 745, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 128.0 in stage 11.0 (TID 745)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 129.0 in stage 11.0 (TID 746, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 129.0 in stage 11.0 (TID 746)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 130.0 in stage 11.0 (TID 747, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 130.0 in stage 11.0 (TID 747)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 131.0 in stage 11.0 (TID 748, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 131.0 in stage 11.0 (TID 748)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 113.0 in stage 11.0 (TID 730) in 160 ms on localhost (113/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 114.0 in stage 11.0 (TID 731) in 159 ms on localhost (114/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 112.0 in stage 11.0 (TID 729) in 168 ms on localhost (115/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 115.0 in stage 11.0 (TID 732) in 160 ms on localhost (116/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:34:03 INFO Executor: Finished task 116.0 in stage 11.0 (TID 733). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 132.0 in stage 11.0 (TID 749, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 132.0 in stage 11.0 (TID 749)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 116.0 in stage 11.0 (TID 733) in 158 ms on localhost (117/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 138
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 192
15/08/06 17:34:03 INFO Executor: Finished task 121.0 in stage 11.0 (TID 738). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Finished task 119.0 in stage 11.0 (TID 736). 2095 bytes result sent to driver
15/08/06 17:34:03 INFO Executor: Finished task 120.0 in stage 11.0 (TID 737). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 133.0 in stage 11.0 (TID 750, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO Executor: Running task 133.0 in stage 11.0 (TID 750)
15/08/06 17:34:03 INFO Executor: Finished task 118.0 in stage 11.0 (TID 735). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Finished task 122.0 in stage 11.0 (TID 739). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Finished task 121.0 in stage 11.0 (TID 738) in 159 ms on localhost (118/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 180
15/08/06 17:34:03 INFO Executor: Finished task 117.0 in stage 11.0 (TID 734). 2113 bytes result sent to driver
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Finished task 123.0 in stage 11.0 (TID 740). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 134.0 in stage 11.0 (TID 751, localhost, ANY, 1827 bytes)
15/08/06 17:34:03 INFO Executor: Running task 134.0 in stage 11.0 (TID 751)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 119.0 in stage 11.0 (TID 736) in 172 ms on localhost (119/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 135.0 in stage 11.0 (TID 752, localhost, ANY, 1827 bytes)
15/08/06 17:34:03 INFO Executor: Running task 135.0 in stage 11.0 (TID 752)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 120.0 in stage 11.0 (TID 737) in 171 ms on localhost (120/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO TaskSetManager: Starting task 136.0 in stage 11.0 (TID 753, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO Executor: Running task 136.0 in stage 11.0 (TID 753)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 123
15/08/06 17:34:03 INFO TaskSetManager: Finished task 118.0 in stage 11.0 (TID 735) in 180 ms on localhost (121/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 137.0 in stage 11.0 (TID 754, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 137.0 in stage 11.0 (TID 754)
15/08/06 17:34:03 INFO Executor: Finished task 125.0 in stage 11.0 (TID 742). 2002 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 122.0 in stage 11.0 (TID 739) in 171 ms on localhost (122/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Starting task 138.0 in stage 11.0 (TID 755, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Running task 138.0 in stage 11.0 (TID 755)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 117.0 in stage 11.0 (TID 734) in 190 ms on localhost (123/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 123.0 in stage 11.0 (TID 740) in 173 ms on localhost (124/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 139.0 in stage 11.0 (TID 756, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 139.0 in stage 11.0 (TID 756)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Starting task 140.0 in stage 11.0 (TID 757, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 140.0 in stage 11.0 (TID 757)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 125.0 in stage 11.0 (TID 742) in 143 ms on localhost (125/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO Executor: Finished task 124.0 in stage 11.0 (TID 741). 2130 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 141.0 in stage 11.0 (TID 758, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO Executor: Running task 141.0 in stage 11.0 (TID 758)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 124.0 in stage 11.0 (TID 741) in 152 ms on localhost (126/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:34:03 INFO Executor: Finished task 126.0 in stage 11.0 (TID 743). 2094 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 142.0 in stage 11.0 (TID 759, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO Executor: Running task 142.0 in stage 11.0 (TID 759)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Finished task 126.0 in stage 11.0 (TID 743) in 148 ms on localhost (127/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Finished task 127.0 in stage 11.0 (TID 744). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 143.0 in stage 11.0 (TID 760, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 127.0 in stage 11.0 (TID 744) in 146 ms on localhost (128/200)
15/08/06 17:34:03 INFO Executor: Running task 143.0 in stage 11.0 (TID 760)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:03 INFO Executor: Finished task 131.0 in stage 11.0 (TID 748). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 144.0 in stage 11.0 (TID 761, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 144.0 in stage 11.0 (TID 761)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO TaskSetManager: Finished task 131.0 in stage 11.0 (TID 748) in 145 ms on localhost (129/200)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Finished task 128.0 in stage 11.0 (TID 745). 2149 bytes result sent to driver
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Starting task 145.0 in stage 11.0 (TID 762, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 163
15/08/06 17:34:03 INFO Executor: Running task 145.0 in stage 11.0 (TID 762)
15/08/06 17:34:03 INFO Executor: Finished task 129.0 in stage 11.0 (TID 746). 2095 bytes result sent to driver
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Finished task 128.0 in stage 11.0 (TID 745) in 172 ms on localhost (130/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 146.0 in stage 11.0 (TID 763, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 146.0 in stage 11.0 (TID 763)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 129.0 in stage 11.0 (TID 746) in 173 ms on localhost (131/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Finished task 132.0 in stage 11.0 (TID 749). 2002 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 147.0 in stage 11.0 (TID 764, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 147.0 in stage 11.0 (TID 764)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 158
15/08/06 17:34:03 INFO Executor: Finished task 130.0 in stage 11.0 (TID 747). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO TaskSetManager: Finished task 132.0 in stage 11.0 (TID 749) in 165 ms on localhost (132/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO TaskSetManager: Starting task 148.0 in stage 11.0 (TID 765, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 148.0 in stage 11.0 (TID 765)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 130.0 in stage 11.0 (TID 747) in 185 ms on localhost (133/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:03 INFO Executor: Finished task 135.0 in stage 11.0 (TID 752). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 149.0 in stage 11.0 (TID 766, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 149.0 in stage 11.0 (TID 766)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Finished task 135.0 in stage 11.0 (TID 752) in 152 ms on localhost (134/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Finished task 134.0 in stage 11.0 (TID 751). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Starting task 150.0 in stage 11.0 (TID 767, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 150.0 in stage 11.0 (TID 767)
15/08/06 17:34:03 INFO Executor: Finished task 133.0 in stage 11.0 (TID 750). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 134.0 in stage 11.0 (TID 751) in 165 ms on localhost (135/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO TaskSetManager: Starting task 151.0 in stage 11.0 (TID 768, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO Executor: Running task 151.0 in stage 11.0 (TID 768)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Finished task 136.0 in stage 11.0 (TID 753). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO TaskSetManager: Finished task 133.0 in stage 11.0 (TID 750) in 171 ms on localhost (136/200)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO TaskSetManager: Starting task 152.0 in stage 11.0 (TID 769, localhost, ANY, 1827 bytes)
15/08/06 17:34:03 INFO Executor: Running task 152.0 in stage 11.0 (TID 769)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 136.0 in stage 11.0 (TID 753) in 164 ms on localhost (137/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 113
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO Executor: Finished task 138.0 in stage 11.0 (TID 755). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 153.0 in stage 11.0 (TID 770, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 153.0 in stage 11.0 (TID 770)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Finished task 138.0 in stage 11.0 (TID 755) in 167 ms on localhost (138/200)
15/08/06 17:34:03 INFO Executor: Finished task 140.0 in stage 11.0 (TID 757). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO TaskSetManager: Starting task 154.0 in stage 11.0 (TID 771, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 123
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Running task 154.0 in stage 11.0 (TID 771)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 140.0 in stage 11.0 (TID 757) in 164 ms on localhost (139/200)
15/08/06 17:34:03 INFO Executor: Finished task 137.0 in stage 11.0 (TID 754). 2113 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 155.0 in stage 11.0 (TID 772, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO Executor: Running task 155.0 in stage 11.0 (TID 772)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 137.0 in stage 11.0 (TID 754) in 174 ms on localhost (140/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 141
15/08/06 17:34:03 INFO Executor: Finished task 139.0 in stage 11.0 (TID 756). 2002 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 156.0 in stage 11.0 (TID 773, localhost, ANY, 1827 bytes)
15/08/06 17:34:03 INFO Executor: Running task 156.0 in stage 11.0 (TID 773)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 139.0 in stage 11.0 (TID 756) in 174 ms on localhost (141/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:34:03 INFO Executor: Finished task 141.0 in stage 11.0 (TID 758). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Starting task 157.0 in stage 11.0 (TID 774, localhost, ANY, 1827 bytes)
15/08/06 17:34:03 INFO Executor: Running task 157.0 in stage 11.0 (TID 774)
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Finished task 141.0 in stage 11.0 (TID 758) in 242 ms on localhost (142/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 169
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 103
15/08/06 17:34:03 INFO Executor: Finished task 143.0 in stage 11.0 (TID 760). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Starting task 158.0 in stage 11.0 (TID 775, localhost, ANY, 1824 bytes)
15/08/06 17:34:03 INFO Executor: Running task 158.0 in stage 11.0 (TID 775)
15/08/06 17:34:03 INFO Executor: Finished task 142.0 in stage 11.0 (TID 759). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 143.0 in stage 11.0 (TID 760) in 232 ms on localhost (143/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 159.0 in stage 11.0 (TID 776, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 159.0 in stage 11.0 (TID 776)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 142.0 in stage 11.0 (TID 759) in 240 ms on localhost (144/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:03 INFO Executor: Finished task 144.0 in stage 11.0 (TID 761). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 160.0 in stage 11.0 (TID 777, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 160.0 in stage 11.0 (TID 777)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 144.0 in stage 11.0 (TID 761) in 212 ms on localhost (145/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO Executor: Finished task 145.0 in stage 11.0 (TID 762). 2095 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Starting task 161.0 in stage 11.0 (TID 778, localhost, ANY, 1827 bytes)
15/08/06 17:34:03 INFO Executor: Running task 161.0 in stage 11.0 (TID 778)
15/08/06 17:34:03 INFO Executor: Finished task 147.0 in stage 11.0 (TID 764). 2020 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 145.0 in stage 11.0 (TID 762) in 225 ms on localhost (146/200)
15/08/06 17:34:03 INFO TaskSetManager: Starting task 162.0 in stage 11.0 (TID 779, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 162.0 in stage 11.0 (TID 779)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 147.0 in stage 11.0 (TID 764) in 215 ms on localhost (147/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO Executor: Finished task 148.0 in stage 11.0 (TID 765). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 163.0 in stage 11.0 (TID 780, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 163.0 in stage 11.0 (TID 780)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 148.0 in stage 11.0 (TID 765) in 218 ms on localhost (148/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Finished task 146.0 in stage 11.0 (TID 763). 2113 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 164.0 in stage 11.0 (TID 781, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 164.0 in stage 11.0 (TID 781)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 146.0 in stage 11.0 (TID 763) in 234 ms on localhost (149/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:34:03 INFO Executor: Finished task 152.0 in stage 11.0 (TID 769). 1913 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Starting task 165.0 in stage 11.0 (TID 782, localhost, ANY, 1827 bytes)
15/08/06 17:34:03 INFO Executor: Running task 165.0 in stage 11.0 (TID 782)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 190
15/08/06 17:34:03 INFO TaskSetManager: Finished task 152.0 in stage 11.0 (TID 769) in 194 ms on localhost (150/200)
15/08/06 17:34:03 INFO Executor: Finished task 151.0 in stage 11.0 (TID 768). 2148 bytes result sent to driver
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO TaskSetManager: Starting task 166.0 in stage 11.0 (TID 783, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO Executor: Running task 166.0 in stage 11.0 (TID 783)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 151.0 in stage 11.0 (TID 768) in 202 ms on localhost (151/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO Executor: Finished task 150.0 in stage 11.0 (TID 767). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:03 INFO TaskSetManager: Starting task 167.0 in stage 11.0 (TID 784, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 167.0 in stage 11.0 (TID 784)
15/08/06 17:34:03 INFO Executor: Finished task 149.0 in stage 11.0 (TID 766). 2038 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Finished task 150.0 in stage 11.0 (TID 767) in 221 ms on localhost (152/200)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO TaskSetManager: Starting task 168.0 in stage 11.0 (TID 785, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 168.0 in stage 11.0 (TID 785)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO TaskSetManager: Finished task 149.0 in stage 11.0 (TID 766) in 232 ms on localhost (153/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:03 INFO Executor: Finished task 153.0 in stage 11.0 (TID 770). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 169.0 in stage 11.0 (TID 786, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 169.0 in stage 11.0 (TID 786)
15/08/06 17:34:03 INFO Executor: Finished task 155.0 in stage 11.0 (TID 772). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO TaskSetManager: Finished task 153.0 in stage 11.0 (TID 770) in 219 ms on localhost (154/200)
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Starting task 170.0 in stage 11.0 (TID 787, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO Executor: Running task 170.0 in stage 11.0 (TID 787)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 155.0 in stage 11.0 (TID 772) in 218 ms on localhost (155/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO Executor: Finished task 154.0 in stage 11.0 (TID 771). 2130 bytes result sent to driver
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Starting task 171.0 in stage 11.0 (TID 788, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 171.0 in stage 11.0 (TID 788)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:03 INFO TaskSetManager: Finished task 154.0 in stage 11.0 (TID 771) in 233 ms on localhost (156/200)
15/08/06 17:34:03 INFO Executor: Finished task 156.0 in stage 11.0 (TID 773). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO TaskSetManager: Starting task 172.0 in stage 11.0 (TID 789, localhost, ANY, 1825 bytes)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 INFO Executor: Running task 172.0 in stage 11.0 (TID 789)
15/08/06 17:34:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO TaskSetManager: Finished task 156.0 in stage 11.0 (TID 773) in 228 ms on localhost (157/200)
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 168
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO Executor: Finished task 158.0 in stage 11.0 (TID 775). 2074 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 173.0 in stage 11.0 (TID 790, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO Executor: Running task 173.0 in stage 11.0 (TID 790)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 158.0 in stage 11.0 (TID 775) in 151 ms on localhost (158/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:34:03 INFO Executor: Finished task 157.0 in stage 11.0 (TID 774). 2113 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 174.0 in stage 11.0 (TID 791, localhost, ANY, 1826 bytes)
15/08/06 17:34:03 INFO Executor: Running task 174.0 in stage 11.0 (TID 791)
15/08/06 17:34:03 INFO TaskSetManager: Finished task 157.0 in stage 11.0 (TID 774) in 173 ms on localhost (159/200)
15/08/06 17:34:03 INFO Executor: Finished task 159.0 in stage 11.0 (TID 776). 2056 bytes result sent to driver
15/08/06 17:34:03 INFO TaskSetManager: Starting task 175.0 in stage 11.0 (TID 792, localhost, ANY, 1823 bytes)
15/08/06 17:34:03 INFO Executor: Running task 175.0 in stage 11.0 (TID 792)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:03 INFO TaskSetManager: Finished task 159.0 in stage 11.0 (TID 776) in 169 ms on localhost (160/200)
15/08/06 17:34:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/06 17:34:04 INFO Executor: Finished task 160.0 in stage 11.0 (TID 777). 2020 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 176.0 in stage 11.0 (TID 793, localhost, ANY, 1824 bytes)
15/08/06 17:34:04 INFO Executor: Running task 176.0 in stage 11.0 (TID 793)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 160.0 in stage 11.0 (TID 777) in 173 ms on localhost (161/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 116
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO Executor: Finished task 161.0 in stage 11.0 (TID 778). 2038 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 177.0 in stage 11.0 (TID 794, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO Executor: Running task 177.0 in stage 11.0 (TID 794)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 161.0 in stage 11.0 (TID 778) in 166 ms on localhost (162/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 136
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 110
15/08/06 17:34:04 INFO Executor: Finished task 163.0 in stage 11.0 (TID 780). 1913 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 178.0 in stage 11.0 (TID 795, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 178.0 in stage 11.0 (TID 795)
15/08/06 17:34:04 INFO Executor: Finished task 164.0 in stage 11.0 (TID 781). 2111 bytes result sent to driver
15/08/06 17:34:04 INFO Executor: Finished task 162.0 in stage 11.0 (TID 779). 2094 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 163.0 in stage 11.0 (TID 780) in 165 ms on localhost (163/200)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 179.0 in stage 11.0 (TID 796, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO Executor: Running task 179.0 in stage 11.0 (TID 796)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO TaskSetManager: Finished task 164.0 in stage 11.0 (TID 781) in 162 ms on localhost (164/200)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 180.0 in stage 11.0 (TID 797, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO Executor: Running task 180.0 in stage 11.0 (TID 797)
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Finished task 162.0 in stage 11.0 (TID 779) in 176 ms on localhost (165/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:04 INFO Executor: Finished task 165.0 in stage 11.0 (TID 782). 2094 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 181.0 in stage 11.0 (TID 798, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO Executor: Running task 181.0 in stage 11.0 (TID 798)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 165.0 in stage 11.0 (TID 782) in 170 ms on localhost (166/200)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 114
15/08/06 17:34:04 INFO Executor: Finished task 166.0 in stage 11.0 (TID 783). 2073 bytes result sent to driver
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Starting task 182.0 in stage 11.0 (TID 799, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:34:04 INFO Executor: Running task 182.0 in stage 11.0 (TID 799)
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/06 17:34:04 INFO Executor: Finished task 167.0 in stage 11.0 (TID 784). 2094 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 166.0 in stage 11.0 (TID 783) in 182 ms on localhost (167/200)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 183.0 in stage 11.0 (TID 800, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Finished task 167.0 in stage 11.0 (TID 784) in 170 ms on localhost (168/200)
15/08/06 17:34:04 INFO Executor: Running task 183.0 in stage 11.0 (TID 800)
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO Executor: Finished task 169.0 in stage 11.0 (TID 786). 2020 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 184.0 in stage 11.0 (TID 801, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 184.0 in stage 11.0 (TID 801)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO TaskSetManager: Finished task 169.0 in stage 11.0 (TID 786) in 161 ms on localhost (169/200)
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO Executor: Finished task 168.0 in stage 11.0 (TID 785). 2002 bytes result sent to driver
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:34:04 INFO TaskSetManager: Starting task 185.0 in stage 11.0 (TID 802, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO Executor: Running task 185.0 in stage 11.0 (TID 802)
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO TaskSetManager: Finished task 168.0 in stage 11.0 (TID 785) in 183 ms on localhost (170/200)
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:34:04 INFO Executor: Finished task 172.0 in stage 11.0 (TID 789). 2095 bytes result sent to driver
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO TaskSetManager: Starting task 186.0 in stage 11.0 (TID 803, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO Executor: Finished task 171.0 in stage 11.0 (TID 788). 2131 bytes result sent to driver
15/08/06 17:34:04 INFO Executor: Running task 186.0 in stage 11.0 (TID 803)
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Finished task 172.0 in stage 11.0 (TID 789) in 155 ms on localhost (171/200)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 187.0 in stage 11.0 (TID 804, localhost, ANY, 1823 bytes)
15/08/06 17:34:04 INFO Executor: Running task 187.0 in stage 11.0 (TID 804)
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO TaskSetManager: Finished task 171.0 in stage 11.0 (TID 788) in 160 ms on localhost (172/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 186
15/08/06 17:34:04 INFO Executor: Finished task 170.0 in stage 11.0 (TID 787). 2131 bytes result sent to driver
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO TaskSetManager: Starting task 188.0 in stage 11.0 (TID 805, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO Executor: Running task 188.0 in stage 11.0 (TID 805)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 170.0 in stage 11.0 (TID 787) in 179 ms on localhost (173/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:34:04 INFO Executor: Finished task 174.0 in stage 11.0 (TID 791). 2130 bytes result sent to driver
15/08/06 17:34:04 INFO Executor: Finished task 173.0 in stage 11.0 (TID 790). 1913 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 189.0 in stage 11.0 (TID 806, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 189.0 in stage 11.0 (TID 806)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 174.0 in stage 11.0 (TID 791) in 158 ms on localhost (174/200)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 190.0 in stage 11.0 (TID 807, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO TaskSetManager: Finished task 173.0 in stage 11.0 (TID 790) in 178 ms on localhost (175/200)
15/08/06 17:34:04 INFO Executor: Running task 190.0 in stage 11.0 (TID 807)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:34:04 INFO Executor: Finished task 175.0 in stage 11.0 (TID 792). 2113 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 191.0 in stage 11.0 (TID 808, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO Executor: Running task 191.0 in stage 11.0 (TID 808)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO TaskSetManager: Finished task 175.0 in stage 11.0 (TID 792) in 176 ms on localhost (176/200)
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 197
15/08/06 17:34:04 INFO Executor: Finished task 176.0 in stage 11.0 (TID 793). 2095 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 192.0 in stage 11.0 (TID 809, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 192.0 in stage 11.0 (TID 809)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 176.0 in stage 11.0 (TID 793) in 168 ms on localhost (177/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO Executor: Finished task 177.0 in stage 11.0 (TID 794). 2038 bytes result sent to driver
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:04 INFO TaskSetManager: Starting task 193.0 in stage 11.0 (TID 810, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 193.0 in stage 11.0 (TID 810)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO Executor: Finished task 180.0 in stage 11.0 (TID 797). 2056 bytes result sent to driver
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Finished task 177.0 in stage 11.0 (TID 794) in 164 ms on localhost (178/200)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 194.0 in stage 11.0 (TID 811, localhost, ANY, 1825 bytes)
15/08/06 17:34:04 INFO Executor: Running task 194.0 in stage 11.0 (TID 811)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 180.0 in stage 11.0 (TID 797) in 152 ms on localhost (179/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 246
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:34:04 INFO Executor: Finished task 178.0 in stage 11.0 (TID 795). 2185 bytes result sent to driver
15/08/06 17:34:04 INFO Executor: Finished task 179.0 in stage 11.0 (TID 796). 2020 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 195.0 in stage 11.0 (TID 812, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 195.0 in stage 11.0 (TID 812)
15/08/06 17:34:04 INFO Executor: Finished task 181.0 in stage 11.0 (TID 798). 2038 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 178.0 in stage 11.0 (TID 795) in 172 ms on localhost (180/200)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 196.0 in stage 11.0 (TID 813, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 196.0 in stage 11.0 (TID 813)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 179.0 in stage 11.0 (TID 796) in 173 ms on localhost (181/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO TaskSetManager: Starting task 197.0 in stage 11.0 (TID 814, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 197.0 in stage 11.0 (TID 814)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 181.0 in stage 11.0 (TID 798) in 156 ms on localhost (182/200)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO Executor: Finished task 185.0 in stage 11.0 (TID 802). 2038 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Starting task 198.0 in stage 11.0 (TID 815, localhost, ANY, 1824 bytes)
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:34:04 INFO Executor: Running task 198.0 in stage 11.0 (TID 815)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO Executor: Finished task 182.0 in stage 11.0 (TID 799). 2056 bytes result sent to driver
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Finished task 185.0 in stage 11.0 (TID 802) in 151 ms on localhost (183/200)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO TaskSetManager: Starting task 199.0 in stage 11.0 (TID 816, localhost, ANY, 1826 bytes)
15/08/06 17:34:04 INFO Executor: Running task 199.0 in stage 11.0 (TID 816)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 182.0 in stage 11.0 (TID 799) in 176 ms on localhost (184/200)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO Executor: Finished task 183.0 in stage 11.0 (TID 800). 2113 bytes result sent to driver
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO TaskSetManager: Finished task 183.0 in stage 11.0 (TID 800) in 178 ms on localhost (185/200)
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:34:04 INFO Executor: Finished task 184.0 in stage 11.0 (TID 801). 2020 bytes result sent to driver
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 194
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO TaskSetManager: Finished task 184.0 in stage 11.0 (TID 801) in 176 ms on localhost (186/200)
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:04 INFO Executor: Finished task 187.0 in stage 11.0 (TID 804). 2074 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 187.0 in stage 11.0 (TID 804) in 163 ms on localhost (187/200)
15/08/06 17:34:04 INFO Executor: Finished task 186.0 in stage 11.0 (TID 803). 2074 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 186.0 in stage 11.0 (TID 803) in 168 ms on localhost (188/200)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 133
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:34:04 INFO Executor: Finished task 190.0 in stage 11.0 (TID 807). 2038 bytes result sent to driver
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Finished task 190.0 in stage 11.0 (TID 807) in 157 ms on localhost (189/200)
15/08/06 17:34:04 INFO Executor: Finished task 188.0 in stage 11.0 (TID 805). 2002 bytes result sent to driver
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Finished task 188.0 in stage 11.0 (TID 805) in 178 ms on localhost (190/200)
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:34:04 INFO Executor: Finished task 191.0 in stage 11.0 (TID 808). 2112 bytes result sent to driver
15/08/06 17:34:04 INFO Executor: Finished task 189.0 in stage 11.0 (TID 806). 2055 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 191.0 in stage 11.0 (TID 808) in 148 ms on localhost (191/200)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 189.0 in stage 11.0 (TID 806) in 173 ms on localhost (192/200)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:34:04 INFO Executor: Finished task 192.0 in stage 11.0 (TID 809). 2020 bytes result sent to driver
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO TaskSetManager: Finished task 192.0 in stage 11.0 (TID 809) in 154 ms on localhost (193/200)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:34:04 INFO Executor: Finished task 194.0 in stage 11.0 (TID 811). 2054 bytes result sent to driver
15/08/06 17:34:04 INFO Executor: Finished task 193.0 in stage 11.0 (TID 810). 2037 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 194.0 in stage 11.0 (TID 811) in 140 ms on localhost (194/200)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 193.0 in stage 11.0 (TID 810) in 144 ms on localhost (195/200)
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO Executor: Finished task 196.0 in stage 11.0 (TID 813). 2113 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 196.0 in stage 11.0 (TID 813) in 130 ms on localhost (196/200)
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:34:04 INFO Executor: Finished task 195.0 in stage 11.0 (TID 812). 2074 bytes result sent to driver
15/08/06 17:34:04 INFO Executor: Finished task 197.0 in stage 11.0 (TID 814). 2056 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 195.0 in stage 11.0 (TID 812) in 146 ms on localhost (197/200)
15/08/06 17:34:04 INFO TaskSetManager: Finished task 197.0 in stage 11.0 (TID 814) in 143 ms on localhost (198/200)
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:34:04 INFO Executor: Finished task 198.0 in stage 11.0 (TID 815). 2095 bytes result sent to driver
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO TaskSetManager: Finished task 198.0 in stage 11.0 (TID 815) in 131 ms on localhost (199/200)
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:34:04 INFO Executor: Finished task 199.0 in stage 11.0 (TID 816). 2131 bytes result sent to driver
15/08/06 17:34:04 INFO TaskSetManager: Finished task 199.0 in stage 11.0 (TID 816) in 130 ms on localhost (200/200)
15/08/06 17:34:04 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/08/06 17:34:04 INFO DAGScheduler: Stage 11 (RangePartitioner at Exchange.scala:88) finished in 2.433 s
15/08/06 17:34:04 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5ac258d
15/08/06 17:34:04 INFO DAGScheduler: Job 7 finished: RangePartitioner at Exchange.scala:88, took 3.421294 s
15/08/06 17:34:04 INFO StatsReportListener: task runtime:(count: 200, mean: 189.630000, stdev: 41.024421, max: 298.000000, min: 124.000000)
15/08/06 17:34:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:04 INFO StatsReportListener: 	124.0 ms	139.0 ms	146.0 ms	161.0 ms	174.0 ms	228.0 ms	251.0 ms	267.0 ms	298.0 ms
15/08/06 17:34:04 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.270000, stdev: 0.497092, max: 3.000000, min: 0.000000)
15/08/06 17:34:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:04 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	1.0 ms	3.0 ms
15/08/06 17:34:04 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:34:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:04 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:34:04 INFO StatsReportListener: task result size:(count: 200, mean: 2066.055000, stdev: 46.469151, max: 2237.000000, min: 1913.000000)
15/08/06 17:34:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:04 INFO StatsReportListener: 	1913.0 B	2002.0 B	2019.0 B	2037.0 B	2.0 KB	2.0 KB	2.1 KB	2.1 KB	2.2 KB
15/08/06 17:34:04 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 95.689958, stdev: 2.733763, max: 98.445596, min: 71.074380)
15/08/06 17:34:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:04 INFO StatsReportListener: 	71 %	91 %	93 %	95 %	96 %	97 %	98 %	98 %	98 %
15/08/06 17:34:04 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.152798, stdev: 0.304203, max: 2.307692, min: 0.000000)
15/08/06 17:34:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:04 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 2 %
15/08/06 17:34:04 INFO StatsReportListener: other time pct: (count: 200, mean: 4.157244, stdev: 2.729402, max: 28.925620, min: 1.554404)
15/08/06 17:34:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:04 INFO StatsReportListener: 	 2 %	 2 %	 2 %	 2 %	 3 %	 5 %	 7 %	 9 %	29 %
15/08/06 17:34:04 INFO DefaultExecutionContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/08/06 17:34:04 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 204 bytes
15/08/06 17:34:04 INFO DAGScheduler: Registering RDD 75 (mapPartitions at Exchange.scala:77)
15/08/06 17:34:04 INFO DAGScheduler: Got job 8 (runJob at InsertIntoHiveTable.scala:93) with 200 output partitions (allowLocal=false)
15/08/06 17:34:04 INFO DAGScheduler: Final stage: Stage 14(runJob at InsertIntoHiveTable.scala:93)
15/08/06 17:34:04 INFO DAGScheduler: Parents of final stage: List(Stage 13)
15/08/06 17:34:04 INFO DAGScheduler: Missing parents: List(Stage 13)
15/08/06 17:34:04 INFO DAGScheduler: Submitting Stage 13 (MapPartitionsRDD[75] at mapPartitions at Exchange.scala:77), which has no missing parents
15/08/06 17:34:04 INFO MemoryStore: ensureFreeSpace(16352) called with curMem=1606781, maxMem=3333968363
15/08/06 17:34:04 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 16.0 KB, free 3.1 GB)
15/08/06 17:34:04 INFO MemoryStore: ensureFreeSpace(9186) called with curMem=1623133, maxMem=3333968363
15/08/06 17:34:04 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 9.0 KB, free 3.1 GB)
15/08/06 17:34:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:42931 (size: 9.0 KB, free: 3.1 GB)
15/08/06 17:34:04 INFO BlockManagerMaster: Updated info of block broadcast_18_piece0
15/08/06 17:34:04 INFO DefaultExecutionContext: Created broadcast 18 from broadcast at DAGScheduler.scala:838
15/08/06 17:34:04 INFO DAGScheduler: Submitting 200 missing tasks from Stage 13 (MapPartitionsRDD[75] at mapPartitions at Exchange.scala:77)
15/08/06 17:34:04 INFO TaskSchedulerImpl: Adding task set 13.0 with 200 tasks
15/08/06 17:34:04 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 817, localhost, ANY, 1810 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 818, localhost, ANY, 1812 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 819, localhost, ANY, 1813 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 820, localhost, ANY, 1812 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 821, localhost, ANY, 1814 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 822, localhost, ANY, 1813 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 823, localhost, ANY, 1813 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 824, localhost, ANY, 1814 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 825, localhost, ANY, 1813 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 826, localhost, ANY, 1810 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 827, localhost, ANY, 1815 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 828, localhost, ANY, 1814 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 829, localhost, ANY, 1814 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 830, localhost, ANY, 1813 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 14.0 in stage 13.0 (TID 831, localhost, ANY, 1814 bytes)
15/08/06 17:34:04 INFO TaskSetManager: Starting task 15.0 in stage 13.0 (TID 832, localhost, ANY, 1814 bytes)
15/08/06 17:34:04 INFO Executor: Running task 3.0 in stage 13.0 (TID 820)
15/08/06 17:34:04 INFO Executor: Running task 0.0 in stage 13.0 (TID 817)
15/08/06 17:34:04 INFO Executor: Running task 1.0 in stage 13.0 (TID 818)
15/08/06 17:34:04 INFO Executor: Running task 2.0 in stage 13.0 (TID 819)
15/08/06 17:34:04 INFO Executor: Running task 4.0 in stage 13.0 (TID 821)
15/08/06 17:34:04 INFO Executor: Running task 6.0 in stage 13.0 (TID 823)
15/08/06 17:34:04 INFO Executor: Running task 7.0 in stage 13.0 (TID 824)
15/08/06 17:34:04 INFO Executor: Running task 5.0 in stage 13.0 (TID 822)
15/08/06 17:34:04 INFO Executor: Running task 8.0 in stage 13.0 (TID 825)
15/08/06 17:34:04 INFO Executor: Running task 10.0 in stage 13.0 (TID 827)
15/08/06 17:34:04 INFO Executor: Running task 14.0 in stage 13.0 (TID 831)
15/08/06 17:34:04 INFO Executor: Running task 13.0 in stage 13.0 (TID 830)
15/08/06 17:34:04 INFO Executor: Running task 9.0 in stage 13.0 (TID 826)
15/08/06 17:34:04 INFO Executor: Running task 15.0 in stage 13.0 (TID 832)
15/08/06 17:34:04 INFO Executor: Running task 12.0 in stage 13.0 (TID 829)
15/08/06 17:34:04 INFO Executor: Running task 11.0 in stage 13.0 (TID 828)
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 142
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 193
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 177
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 154
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 129
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 140
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 126
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 138
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:34:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:34:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 144
15/08/06 17:34:05 INFO Executor: Finished task 8.0 in stage 13.0 (TID 825). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO Executor: Finished task 0.0 in stage 13.0 (TID 817). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 16.0 in stage 13.0 (TID 833, localhost, ANY, 1815 bytes)
15/08/06 17:34:05 INFO Executor: Running task 16.0 in stage 13.0 (TID 833)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 825) in 707 ms on localhost (1/200)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 17.0 in stage 13.0 (TID 834, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 17.0 in stage 13.0 (TID 834)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 817) in 715 ms on localhost (2/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO Executor: Finished task 3.0 in stage 13.0 (TID 820). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 18.0 in stage 13.0 (TID 835, localhost, ANY, 1815 bytes)
15/08/06 17:34:05 INFO Executor: Running task 18.0 in stage 13.0 (TID 835)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 820) in 769 ms on localhost (3/200)
15/08/06 17:34:05 INFO Executor: Finished task 11.0 in stage 13.0 (TID 828). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 19.0 in stage 13.0 (TID 836, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 19.0 in stage 13.0 (TID 836)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 828) in 768 ms on localhost (4/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO Executor: Finished task 5.0 in stage 13.0 (TID 822). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 20.0 in stage 13.0 (TID 837, localhost, ANY, 1814 bytes)
15/08/06 17:34:05 INFO Executor: Running task 20.0 in stage 13.0 (TID 837)
15/08/06 17:34:05 INFO Executor: Finished task 6.0 in stage 13.0 (TID 823). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 822) in 791 ms on localhost (5/200)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 21.0 in stage 13.0 (TID 838, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 21.0 in stage 13.0 (TID 838)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 823) in 806 ms on localhost (6/200)
15/08/06 17:34:05 INFO Executor: Finished task 15.0 in stage 13.0 (TID 832). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO Executor: Finished task 13.0 in stage 13.0 (TID 830). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 22.0 in stage 13.0 (TID 839, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 22.0 in stage 13.0 (TID 839)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 15.0 in stage 13.0 (TID 832) in 805 ms on localhost (7/200)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 23.0 in stage 13.0 (TID 840, localhost, ANY, 1814 bytes)
15/08/06 17:34:05 INFO Executor: Running task 23.0 in stage 13.0 (TID 840)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 830) in 807 ms on localhost (8/200)
15/08/06 17:34:05 INFO Executor: Finished task 10.0 in stage 13.0 (TID 827). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO Executor: Finished task 4.0 in stage 13.0 (TID 821). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO Executor: Finished task 14.0 in stage 13.0 (TID 831). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO TaskSetManager: Starting task 24.0 in stage 13.0 (TID 841, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 24.0 in stage 13.0 (TID 841)
15/08/06 17:34:05 INFO Executor: Finished task 12.0 in stage 13.0 (TID 829). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 827) in 811 ms on localhost (9/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO TaskSetManager: Starting task 25.0 in stage 13.0 (TID 842, localhost, ANY, 1816 bytes)
15/08/06 17:34:05 INFO Executor: Running task 25.0 in stage 13.0 (TID 842)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 26.0 in stage 13.0 (TID 843, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 26.0 in stage 13.0 (TID 843)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 14.0 in stage 13.0 (TID 831) in 811 ms on localhost (10/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO Executor: Finished task 7.0 in stage 13.0 (TID 824). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO Executor: Finished task 2.0 in stage 13.0 (TID 819). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 821) in 817 ms on localhost (11/200)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 27.0 in stage 13.0 (TID 844, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 27.0 in stage 13.0 (TID 844)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:05 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 829) in 815 ms on localhost (12/200)
15/08/06 17:34:05 INFO Executor: Finished task 1.0 in stage 13.0 (TID 818). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO TaskSetManager: Starting task 28.0 in stage 13.0 (TID 845, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 28.0 in stage 13.0 (TID 845)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 824) in 820 ms on localhost (13/200)
15/08/06 17:34:05 INFO Executor: Finished task 9.0 in stage 13.0 (TID 826). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 29.0 in stage 13.0 (TID 846, localhost, ANY, 1811 bytes)
15/08/06 17:34:05 INFO Executor: Running task 29.0 in stage 13.0 (TID 846)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 819) in 824 ms on localhost (14/200)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 30.0 in stage 13.0 (TID 847, localhost, ANY, 1814 bytes)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 31.0 in stage 13.0 (TID 848, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 31.0 in stage 13.0 (TID 848)
15/08/06 17:34:05 INFO Executor: Running task 30.0 in stage 13.0 (TID 847)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 826) in 823 ms on localhost (15/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 818) in 828 ms on localhost (16/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 171
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 179
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 88
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO Executor: Finished task 16.0 in stage 13.0 (TID 833). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 32.0 in stage 13.0 (TID 849, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 32.0 in stage 13.0 (TID 849)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 16.0 in stage 13.0 (TID 833) in 309 ms on localhost (17/200)
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 151
15/08/06 17:34:05 INFO Executor: Finished task 19.0 in stage 13.0 (TID 836). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 33.0 in stage 13.0 (TID 850, localhost, ANY, 1814 bytes)
15/08/06 17:34:05 INFO Executor: Running task 33.0 in stage 13.0 (TID 850)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 19.0 in stage 13.0 (TID 836) in 380 ms on localhost (18/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 165
15/08/06 17:34:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 149
15/08/06 17:34:05 INFO Executor: Finished task 18.0 in stage 13.0 (TID 835). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 34.0 in stage 13.0 (TID 851, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 34.0 in stage 13.0 (TID 851)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 18.0 in stage 13.0 (TID 835) in 578 ms on localhost (19/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO Executor: Finished task 17.0 in stage 13.0 (TID 834). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 35.0 in stage 13.0 (TID 852, localhost, ANY, 1814 bytes)
15/08/06 17:34:05 INFO Executor: Running task 35.0 in stage 13.0 (TID 852)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 17.0 in stage 13.0 (TID 834) in 635 ms on localhost (20/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO Executor: Finished task 22.0 in stage 13.0 (TID 839). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 36.0 in stage 13.0 (TID 853, localhost, ANY, 1815 bytes)
15/08/06 17:34:05 INFO Executor: Running task 36.0 in stage 13.0 (TID 853)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 22.0 in stage 13.0 (TID 839) in 559 ms on localhost (21/200)
15/08/06 17:34:05 INFO Executor: Finished task 24.0 in stage 13.0 (TID 841). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO TaskSetManager: Starting task 37.0 in stage 13.0 (TID 854, localhost, ANY, 1814 bytes)
15/08/06 17:34:05 INFO Executor: Running task 37.0 in stage 13.0 (TID 854)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 24.0 in stage 13.0 (TID 841) in 568 ms on localhost (22/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO Executor: Finished task 23.0 in stage 13.0 (TID 840). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 38.0 in stage 13.0 (TID 855, localhost, ANY, 1814 bytes)
15/08/06 17:34:05 INFO Executor: Running task 38.0 in stage 13.0 (TID 855)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 23.0 in stage 13.0 (TID 840) in 584 ms on localhost (23/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:05 INFO Executor: Finished task 30.0 in stage 13.0 (TID 847). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO Executor: Finished task 20.0 in stage 13.0 (TID 837). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 39.0 in stage 13.0 (TID 856, localhost, ANY, 1815 bytes)
15/08/06 17:34:05 INFO Executor: Running task 39.0 in stage 13.0 (TID 856)
15/08/06 17:34:05 INFO Executor: Finished task 29.0 in stage 13.0 (TID 846). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Finished task 30.0 in stage 13.0 (TID 847) in 595 ms on localhost (24/200)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 40.0 in stage 13.0 (TID 857, localhost, ANY, 1815 bytes)
15/08/06 17:34:05 INFO Executor: Running task 40.0 in stage 13.0 (TID 857)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 20.0 in stage 13.0 (TID 837) in 632 ms on localhost (25/200)
15/08/06 17:34:05 INFO Executor: Finished task 26.0 in stage 13.0 (TID 843). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 41.0 in stage 13.0 (TID 858, localhost, ANY, 1815 bytes)
15/08/06 17:34:05 INFO Executor: Running task 41.0 in stage 13.0 (TID 858)
15/08/06 17:34:05 INFO Executor: Finished task 31.0 in stage 13.0 (TID 848). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Finished task 29.0 in stage 13.0 (TID 846) in 600 ms on localhost (26/200)
15/08/06 17:34:05 INFO TaskSetManager: Starting task 42.0 in stage 13.0 (TID 859, localhost, ANY, 1816 bytes)
15/08/06 17:34:05 INFO Executor: Running task 42.0 in stage 13.0 (TID 859)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 26.0 in stage 13.0 (TID 843) in 610 ms on localhost (27/200)
15/08/06 17:34:05 INFO Executor: Finished task 25.0 in stage 13.0 (TID 842). 2182 bytes result sent to driver
15/08/06 17:34:05 INFO TaskSetManager: Starting task 43.0 in stage 13.0 (TID 860, localhost, ANY, 1813 bytes)
15/08/06 17:34:05 INFO Executor: Running task 43.0 in stage 13.0 (TID 860)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:05 INFO TaskSetManager: Starting task 44.0 in stage 13.0 (TID 861, localhost, ANY, 1814 bytes)
15/08/06 17:34:05 INFO Executor: Running task 44.0 in stage 13.0 (TID 861)
15/08/06 17:34:05 INFO TaskSetManager: Finished task 31.0 in stage 13.0 (TID 848) in 604 ms on localhost (28/200)
15/08/06 17:34:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:06 INFO TaskSetManager: Finished task 25.0 in stage 13.0 (TID 842) in 615 ms on localhost (29/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 192
15/08/06 17:34:06 INFO Executor: Finished task 27.0 in stage 13.0 (TID 844). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 45.0 in stage 13.0 (TID 862, localhost, ANY, 1814 bytes)
15/08/06 17:34:06 INFO Executor: Running task 45.0 in stage 13.0 (TID 862)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 27.0 in stage 13.0 (TID 844) in 640 ms on localhost (30/200)
15/08/06 17:34:06 INFO Executor: Finished task 28.0 in stage 13.0 (TID 845). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 46.0 in stage 13.0 (TID 863, localhost, ANY, 1816 bytes)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 28.0 in stage 13.0 (TID 845) in 642 ms on localhost (31/200)
15/08/06 17:34:06 INFO Executor: Running task 46.0 in stage 13.0 (TID 863)
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO Executor: Finished task 21.0 in stage 13.0 (TID 838). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO TaskSetManager: Starting task 47.0 in stage 13.0 (TID 864, localhost, ANY, 1816 bytes)
15/08/06 17:34:06 INFO Executor: Running task 47.0 in stage 13.0 (TID 864)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 21.0 in stage 13.0 (TID 838) in 666 ms on localhost (32/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:34:06 INFO Executor: Finished task 33.0 in stage 13.0 (TID 850). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 48.0 in stage 13.0 (TID 865, localhost, ANY, 1814 bytes)
15/08/06 17:34:06 INFO Executor: Running task 48.0 in stage 13.0 (TID 865)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 33.0 in stage 13.0 (TID 850) in 348 ms on localhost (33/200)
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO Executor: Finished task 32.0 in stage 13.0 (TID 849). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO TaskSetManager: Starting task 49.0 in stage 13.0 (TID 866, localhost, ANY, 1816 bytes)
15/08/06 17:34:06 INFO Executor: Running task 49.0 in stage 13.0 (TID 866)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 32.0 in stage 13.0 (TID 849) in 501 ms on localhost (34/200)
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 134
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
15/08/06 17:34:06 INFO Executor: Finished task 34.0 in stage 13.0 (TID 851). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 50.0 in stage 13.0 (TID 867, localhost, ANY, 1814 bytes)
15/08/06 17:34:06 INFO Executor: Running task 50.0 in stage 13.0 (TID 867)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 34.0 in stage 13.0 (TID 851) in 285 ms on localhost (35/200)
15/08/06 17:34:06 INFO Executor: Finished task 35.0 in stage 13.0 (TID 852). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 51.0 in stage 13.0 (TID 868, localhost, ANY, 1816 bytes)
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 INFO Executor: Running task 51.0 in stage 13.0 (TID 868)
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO TaskSetManager: Finished task 35.0 in stage 13.0 (TID 852) in 272 ms on localhost (36/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 120
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 95
15/08/06 17:34:06 INFO Executor: Finished task 37.0 in stage 13.0 (TID 854). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 52.0 in stage 13.0 (TID 869, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 52.0 in stage 13.0 (TID 869)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 37.0 in stage 13.0 (TID 854) in 287 ms on localhost (37/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO Executor: Finished task 41.0 in stage 13.0 (TID 858). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 53.0 in stage 13.0 (TID 870, localhost, ANY, 1816 bytes)
15/08/06 17:34:06 INFO Executor: Running task 53.0 in stage 13.0 (TID 870)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 41.0 in stage 13.0 (TID 858) in 297 ms on localhost (38/200)
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:34:06 INFO Executor: Finished task 36.0 in stage 13.0 (TID 853). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 54.0 in stage 13.0 (TID 871, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 54.0 in stage 13.0 (TID 871)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 36.0 in stage 13.0 (TID 853) in 403 ms on localhost (39/200)
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:34:06 INFO Executor: Finished task 44.0 in stage 13.0 (TID 861). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 55.0 in stage 13.0 (TID 872, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 55.0 in stage 13.0 (TID 872)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 44.0 in stage 13.0 (TID 861) in 448 ms on localhost (40/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:34:06 INFO Executor: Finished task 39.0 in stage 13.0 (TID 856). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO Executor: Finished task 40.0 in stage 13.0 (TID 857). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 56.0 in stage 13.0 (TID 873, localhost, ANY, 1814 bytes)
15/08/06 17:34:06 INFO Executor: Running task 56.0 in stage 13.0 (TID 873)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 39.0 in stage 13.0 (TID 856) in 560 ms on localhost (41/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:06 INFO TaskSetManager: Starting task 57.0 in stage 13.0 (TID 874, localhost, ANY, 1814 bytes)
15/08/06 17:34:06 INFO Executor: Running task 57.0 in stage 13.0 (TID 874)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 40.0 in stage 13.0 (TID 857) in 569 ms on localhost (42/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:34:06 INFO Executor: Finished task 45.0 in stage 13.0 (TID 862). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 58.0 in stage 13.0 (TID 875, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 45.0 in stage 13.0 (TID 862) in 664 ms on localhost (43/200)
15/08/06 17:34:06 INFO Executor: Running task 58.0 in stage 13.0 (TID 875)
15/08/06 17:34:06 INFO Executor: Finished task 38.0 in stage 13.0 (TID 855). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:06 INFO TaskSetManager: Starting task 59.0 in stage 13.0 (TID 876, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 59.0 in stage 13.0 (TID 876)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 38.0 in stage 13.0 (TID 855) in 745 ms on localhost (44/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO Executor: Finished task 42.0 in stage 13.0 (TID 859). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO Executor: Finished task 46.0 in stage 13.0 (TID 863). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 60.0 in stage 13.0 (TID 877, localhost, ANY, 1814 bytes)
15/08/06 17:34:06 INFO Executor: Running task 60.0 in stage 13.0 (TID 877)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 42.0 in stage 13.0 (TID 859) in 725 ms on localhost (45/200)
15/08/06 17:34:06 INFO Executor: Finished task 48.0 in stage 13.0 (TID 865). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO Executor: Finished task 43.0 in stage 13.0 (TID 860). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 61.0 in stage 13.0 (TID 878, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 61.0 in stage 13.0 (TID 878)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO TaskSetManager: Finished task 46.0 in stage 13.0 (TID 863) in 696 ms on localhost (46/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO Executor: Finished task 47.0 in stage 13.0 (TID 864). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 62.0 in stage 13.0 (TID 879, localhost, ANY, 1814 bytes)
15/08/06 17:34:06 INFO Executor: Running task 62.0 in stage 13.0 (TID 879)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 48.0 in stage 13.0 (TID 865) in 674 ms on localhost (47/200)
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO Executor: Finished task 49.0 in stage 13.0 (TID 866). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO TaskSetManager: Starting task 63.0 in stage 13.0 (TID 880, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 63.0 in stage 13.0 (TID 880)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 43.0 in stage 13.0 (TID 860) in 747 ms on localhost (48/200)
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:06 INFO TaskSetManager: Starting task 64.0 in stage 13.0 (TID 881, localhost, ANY, 1816 bytes)
15/08/06 17:34:06 INFO Executor: Running task 64.0 in stage 13.0 (TID 881)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:06 INFO TaskSetManager: Finished task 47.0 in stage 13.0 (TID 864) in 709 ms on localhost (49/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO TaskSetManager: Starting task 65.0 in stage 13.0 (TID 882, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 65.0 in stage 13.0 (TID 882)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 49.0 in stage 13.0 (TID 866) in 685 ms on localhost (50/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO Executor: Finished task 50.0 in stage 13.0 (TID 867). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO Executor: Finished task 51.0 in stage 13.0 (TID 868). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 66.0 in stage 13.0 (TID 883, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 66.0 in stage 13.0 (TID 883)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 50.0 in stage 13.0 (TID 867) in 614 ms on localhost (51/200)
15/08/06 17:34:06 INFO TaskSetManager: Starting task 67.0 in stage 13.0 (TID 884, localhost, ANY, 1816 bytes)
15/08/06 17:34:06 INFO Executor: Finished task 52.0 in stage 13.0 (TID 869). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO Executor: Running task 67.0 in stage 13.0 (TID 884)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 51.0 in stage 13.0 (TID 868) in 613 ms on localhost (52/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO TaskSetManager: Starting task 68.0 in stage 13.0 (TID 885, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 68.0 in stage 13.0 (TID 885)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 52.0 in stage 13.0 (TID 869) in 576 ms on localhost (53/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO Executor: Finished task 53.0 in stage 13.0 (TID 870). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 69.0 in stage 13.0 (TID 886, localhost, ANY, 1816 bytes)
15/08/06 17:34:06 INFO Executor: Running task 69.0 in stage 13.0 (TID 886)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 53.0 in stage 13.0 (TID 870) in 544 ms on localhost (54/200)
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO Executor: Finished task 54.0 in stage 13.0 (TID 871). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 70.0 in stage 13.0 (TID 887, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 70.0 in stage 13.0 (TID 887)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 54.0 in stage 13.0 (TID 871) in 513 ms on localhost (55/200)
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 112
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO Executor: Finished task 55.0 in stage 13.0 (TID 872). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 71.0 in stage 13.0 (TID 888, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 71.0 in stage 13.0 (TID 888)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 55.0 in stage 13.0 (TID 872) in 416 ms on localhost (56/200)
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 149
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO Executor: Finished task 56.0 in stage 13.0 (TID 873). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO TaskSetManager: Starting task 72.0 in stage 13.0 (TID 889, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO Executor: Running task 72.0 in stage 13.0 (TID 889)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 56.0 in stage 13.0 (TID 873) in 371 ms on localhost (57/200)
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 203
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 139
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:34:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:06 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 138
15/08/06 17:34:06 INFO Executor: Finished task 57.0 in stage 13.0 (TID 874). 2182 bytes result sent to driver
15/08/06 17:34:06 INFO TaskSetManager: Starting task 73.0 in stage 13.0 (TID 890, localhost, ANY, 1815 bytes)
15/08/06 17:34:06 INFO TaskSetManager: Finished task 57.0 in stage 13.0 (TID 874) in 431 ms on localhost (58/200)
15/08/06 17:34:06 INFO Executor: Running task 73.0 in stage 13.0 (TID 890)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 122
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:34:07 INFO Executor: Finished task 58.0 in stage 13.0 (TID 875). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 74.0 in stage 13.0 (TID 891, localhost, ANY, 1813 bytes)
15/08/06 17:34:07 INFO Executor: Running task 74.0 in stage 13.0 (TID 891)
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 INFO TaskSetManager: Finished task 58.0 in stage 13.0 (TID 875) in 442 ms on localhost (59/200)
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:34:07 INFO Executor: Finished task 59.0 in stage 13.0 (TID 876). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 75.0 in stage 13.0 (TID 892, localhost, ANY, 1815 bytes)
15/08/06 17:34:07 INFO Executor: Running task 75.0 in stage 13.0 (TID 892)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 59.0 in stage 13.0 (TID 876) in 666 ms on localhost (60/200)
15/08/06 17:34:07 INFO Executor: Finished task 60.0 in stage 13.0 (TID 877). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO TaskSetManager: Starting task 76.0 in stage 13.0 (TID 893, localhost, ANY, 1816 bytes)
15/08/06 17:34:07 INFO Executor: Running task 76.0 in stage 13.0 (TID 893)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 60.0 in stage 13.0 (TID 877) in 663 ms on localhost (61/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO Executor: Finished task 65.0 in stage 13.0 (TID 882). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Finished task 65.0 in stage 13.0 (TID 882) in 645 ms on localhost (62/200)
15/08/06 17:34:07 INFO TaskSetManager: Starting task 77.0 in stage 13.0 (TID 894, localhost, ANY, 1815 bytes)
15/08/06 17:34:07 INFO Executor: Running task 77.0 in stage 13.0 (TID 894)
15/08/06 17:34:07 INFO Executor: Finished task 63.0 in stage 13.0 (TID 880). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 78.0 in stage 13.0 (TID 895, localhost, ANY, 1813 bytes)
15/08/06 17:34:07 INFO Executor: Running task 78.0 in stage 13.0 (TID 895)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:07 INFO TaskSetManager: Finished task 63.0 in stage 13.0 (TID 880) in 680 ms on localhost (63/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO Executor: Finished task 67.0 in stage 13.0 (TID 884). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 79.0 in stage 13.0 (TID 896, localhost, ANY, 1814 bytes)
15/08/06 17:34:07 INFO Executor: Running task 79.0 in stage 13.0 (TID 896)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 67.0 in stage 13.0 (TID 884) in 646 ms on localhost (64/200)
15/08/06 17:34:07 INFO Executor: Finished task 61.0 in stage 13.0 (TID 878). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO Executor: Finished task 64.0 in stage 13.0 (TID 881). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO Executor: Finished task 62.0 in stage 13.0 (TID 879). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO TaskSetManager: Starting task 80.0 in stage 13.0 (TID 897, localhost, ANY, 1814 bytes)
15/08/06 17:34:07 INFO Executor: Running task 80.0 in stage 13.0 (TID 897)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 61.0 in stage 13.0 (TID 878) in 735 ms on localhost (65/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO TaskSetManager: Starting task 81.0 in stage 13.0 (TID 898, localhost, ANY, 1814 bytes)
15/08/06 17:34:07 INFO Executor: Running task 81.0 in stage 13.0 (TID 898)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 64.0 in stage 13.0 (TID 881) in 718 ms on localhost (66/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO Executor: Finished task 68.0 in stage 13.0 (TID 885). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 82.0 in stage 13.0 (TID 899, localhost, ANY, 1813 bytes)
15/08/06 17:34:07 INFO Executor: Running task 82.0 in stage 13.0 (TID 899)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 62.0 in stage 13.0 (TID 879) in 743 ms on localhost (67/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 INFO TaskSetManager: Starting task 83.0 in stage 13.0 (TID 900, localhost, ANY, 1814 bytes)
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO Executor: Running task 83.0 in stage 13.0 (TID 900)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 68.0 in stage 13.0 (TID 885) in 679 ms on localhost (68/200)
15/08/06 17:34:07 INFO Executor: Finished task 66.0 in stage 13.0 (TID 883). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 84.0 in stage 13.0 (TID 901, localhost, ANY, 1815 bytes)
15/08/06 17:34:07 INFO Executor: Running task 84.0 in stage 13.0 (TID 901)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 66.0 in stage 13.0 (TID 883) in 696 ms on localhost (69/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:07 INFO Executor: Finished task 71.0 in stage 13.0 (TID 888). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 85.0 in stage 13.0 (TID 902, localhost, ANY, 1815 bytes)
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO Executor: Running task 85.0 in stage 13.0 (TID 902)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 71.0 in stage 13.0 (TID 888) in 632 ms on localhost (70/200)
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO Executor: Finished task 69.0 in stage 13.0 (TID 886). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 86.0 in stage 13.0 (TID 903, localhost, ANY, 1815 bytes)
15/08/06 17:34:07 INFO Executor: Running task 86.0 in stage 13.0 (TID 903)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 69.0 in stage 13.0 (TID 886) in 685 ms on localhost (71/200)
15/08/06 17:34:07 INFO Executor: Finished task 70.0 in stage 13.0 (TID 887). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 87.0 in stage 13.0 (TID 904, localhost, ANY, 1813 bytes)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 70.0 in stage 13.0 (TID 887) in 672 ms on localhost (72/200)
15/08/06 17:34:07 INFO Executor: Running task 87.0 in stage 13.0 (TID 904)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO Executor: Finished task 73.0 in stage 13.0 (TID 890). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO Executor: Finished task 72.0 in stage 13.0 (TID 889). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 88.0 in stage 13.0 (TID 905, localhost, ANY, 1813 bytes)
15/08/06 17:34:07 INFO Executor: Running task 88.0 in stage 13.0 (TID 905)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 73.0 in stage 13.0 (TID 890) in 545 ms on localhost (73/200)
15/08/06 17:34:07 INFO TaskSetManager: Starting task 89.0 in stage 13.0 (TID 906, localhost, ANY, 1814 bytes)
15/08/06 17:34:07 INFO Executor: Running task 89.0 in stage 13.0 (TID 906)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 72.0 in stage 13.0 (TID 889) in 621 ms on localhost (74/200)
15/08/06 17:34:07 INFO Executor: Finished task 74.0 in stage 13.0 (TID 891). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO TaskSetManager: Starting task 90.0 in stage 13.0 (TID 907, localhost, ANY, 1812 bytes)
15/08/06 17:34:07 INFO Executor: Running task 90.0 in stage 13.0 (TID 907)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:07 INFO TaskSetManager: Finished task 74.0 in stage 13.0 (TID 891) in 425 ms on localhost (75/200)
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 191
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 171
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 206
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 200
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO Executor: Finished task 75.0 in stage 13.0 (TID 892). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 91.0 in stage 13.0 (TID 908, localhost, ANY, 1815 bytes)
15/08/06 17:34:07 INFO Executor: Running task 91.0 in stage 13.0 (TID 908)
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 182
15/08/06 17:34:07 INFO TaskSetManager: Finished task 75.0 in stage 13.0 (TID 892) in 403 ms on localhost (76/200)
15/08/06 17:34:07 INFO Executor: Finished task 76.0 in stage 13.0 (TID 893). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 92.0 in stage 13.0 (TID 909, localhost, ANY, 1814 bytes)
15/08/06 17:34:07 INFO Executor: Running task 92.0 in stage 13.0 (TID 909)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 76.0 in stage 13.0 (TID 893) in 384 ms on localhost (77/200)
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 173
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:07 INFO Executor: Finished task 77.0 in stage 13.0 (TID 894). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO Executor: Finished task 78.0 in stage 13.0 (TID 895). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 93.0 in stage 13.0 (TID 910, localhost, ANY, 1814 bytes)
15/08/06 17:34:07 INFO Executor: Running task 93.0 in stage 13.0 (TID 910)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 77.0 in stage 13.0 (TID 894) in 402 ms on localhost (78/200)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 78.0 in stage 13.0 (TID 895) in 385 ms on localhost (79/200)
15/08/06 17:34:07 INFO TaskSetManager: Starting task 94.0 in stage 13.0 (TID 911, localhost, ANY, 1812 bytes)
15/08/06 17:34:07 INFO Executor: Running task 94.0 in stage 13.0 (TID 911)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:07 INFO Executor: Finished task 80.0 in stage 13.0 (TID 897). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO TaskSetManager: Starting task 95.0 in stage 13.0 (TID 912, localhost, ANY, 1816 bytes)
15/08/06 17:34:07 INFO Executor: Running task 95.0 in stage 13.0 (TID 912)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 80.0 in stage 13.0 (TID 897) in 394 ms on localhost (80/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO Executor: Finished task 79.0 in stage 13.0 (TID 896). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO Executor: Finished task 81.0 in stage 13.0 (TID 898). 2182 bytes result sent to driver
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 119
15/08/06 17:34:07 INFO TaskSetManager: Starting task 96.0 in stage 13.0 (TID 913, localhost, ANY, 1815 bytes)
15/08/06 17:34:07 INFO Executor: Running task 96.0 in stage 13.0 (TID 913)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 79.0 in stage 13.0 (TID 896) in 447 ms on localhost (81/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO TaskSetManager: Starting task 97.0 in stage 13.0 (TID 914, localhost, ANY, 1815 bytes)
15/08/06 17:34:07 INFO Executor: Running task 97.0 in stage 13.0 (TID 914)
15/08/06 17:34:07 INFO TaskSetManager: Finished task 81.0 in stage 13.0 (TID 898) in 433 ms on localhost (82/200)
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:34:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:34:08 INFO Executor: Finished task 82.0 in stage 13.0 (TID 899). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 98.0 in stage 13.0 (TID 915, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 98.0 in stage 13.0 (TID 915)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 82.0 in stage 13.0 (TID 899) in 588 ms on localhost (83/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO Executor: Finished task 84.0 in stage 13.0 (TID 901). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Finished task 83.0 in stage 13.0 (TID 900). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 99.0 in stage 13.0 (TID 916, localhost, ANY, 1815 bytes)
15/08/06 17:34:08 INFO Executor: Running task 99.0 in stage 13.0 (TID 916)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 84.0 in stage 13.0 (TID 901) in 586 ms on localhost (84/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO Executor: Finished task 86.0 in stage 13.0 (TID 903). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Finished task 85.0 in stage 13.0 (TID 902). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 100.0 in stage 13.0 (TID 917, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 100.0 in stage 13.0 (TID 917)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 83.0 in stage 13.0 (TID 900) in 607 ms on localhost (85/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 101.0 in stage 13.0 (TID 918, localhost, ANY, 1812 bytes)
15/08/06 17:34:08 INFO Executor: Running task 101.0 in stage 13.0 (TID 918)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 86.0 in stage 13.0 (TID 903) in 580 ms on localhost (86/200)
15/08/06 17:34:08 INFO Executor: Finished task 88.0 in stage 13.0 (TID 905). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 102.0 in stage 13.0 (TID 919, localhost, ANY, 1813 bytes)
15/08/06 17:34:08 INFO Executor: Running task 102.0 in stage 13.0 (TID 919)
15/08/06 17:34:08 INFO Executor: Finished task 89.0 in stage 13.0 (TID 906). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Finished task 85.0 in stage 13.0 (TID 902) in 602 ms on localhost (87/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 103.0 in stage 13.0 (TID 920, localhost, ANY, 1813 bytes)
15/08/06 17:34:08 INFO Executor: Running task 103.0 in stage 13.0 (TID 920)
15/08/06 17:34:08 INFO Executor: Finished task 90.0 in stage 13.0 (TID 907). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Finished task 88.0 in stage 13.0 (TID 905) in 574 ms on localhost (88/200)
15/08/06 17:34:08 INFO Executor: Finished task 87.0 in stage 13.0 (TID 904). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Finished task 89.0 in stage 13.0 (TID 906) in 577 ms on localhost (89/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 104.0 in stage 13.0 (TID 921, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 104.0 in stage 13.0 (TID 921)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO TaskSetManager: Starting task 105.0 in stage 13.0 (TID 922, localhost, ANY, 1815 bytes)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO Executor: Running task 105.0 in stage 13.0 (TID 922)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 90.0 in stage 13.0 (TID 907) in 592 ms on localhost (90/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 106.0 in stage 13.0 (TID 923, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 106.0 in stage 13.0 (TID 923)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 87.0 in stage 13.0 (TID 904) in 620 ms on localhost (91/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO Executor: Finished task 92.0 in stage 13.0 (TID 909). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO TaskSetManager: Starting task 107.0 in stage 13.0 (TID 924, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 107.0 in stage 13.0 (TID 924)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 92.0 in stage 13.0 (TID 909) in 443 ms on localhost (92/200)
15/08/06 17:34:08 INFO Executor: Finished task 91.0 in stage 13.0 (TID 908). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 162
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:34:08 INFO TaskSetManager: Starting task 108.0 in stage 13.0 (TID 925, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 108.0 in stage 13.0 (TID 925)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 91.0 in stage 13.0 (TID 908) in 463 ms on localhost (93/200)
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 155
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO Executor: Finished task 94.0 in stage 13.0 (TID 911). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Finished task 93.0 in stage 13.0 (TID 910). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 109.0 in stage 13.0 (TID 926, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 94.0 in stage 13.0 (TID 911) in 420 ms on localhost (94/200)
15/08/06 17:34:08 INFO Executor: Running task 109.0 in stage 13.0 (TID 926)
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:08 INFO TaskSetManager: Starting task 110.0 in stage 13.0 (TID 927, localhost, ANY, 1813 bytes)
15/08/06 17:34:08 INFO Executor: Running task 110.0 in stage 13.0 (TID 927)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 93.0 in stage 13.0 (TID 910) in 436 ms on localhost (95/200)
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO Executor: Finished task 96.0 in stage 13.0 (TID 913). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 111.0 in stage 13.0 (TID 928, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 111.0 in stage 13.0 (TID 928)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 96.0 in stage 13.0 (TID 913) in 365 ms on localhost (96/200)
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO Executor: Finished task 95.0 in stage 13.0 (TID 912). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 174
15/08/06 17:34:08 INFO TaskSetManager: Starting task 112.0 in stage 13.0 (TID 929, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 112.0 in stage 13.0 (TID 929)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 95.0 in stage 13.0 (TID 912) in 406 ms on localhost (97/200)
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 129
15/08/06 17:34:08 INFO Executor: Finished task 97.0 in stage 13.0 (TID 914). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 113.0 in stage 13.0 (TID 930, localhost, ANY, 1815 bytes)
15/08/06 17:34:08 INFO Executor: Running task 113.0 in stage 13.0 (TID 930)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO TaskSetManager: Finished task 97.0 in stage 13.0 (TID 914) in 367 ms on localhost (98/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO Executor: Finished task 98.0 in stage 13.0 (TID 915). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 178
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO TaskSetManager: Starting task 114.0 in stage 13.0 (TID 931, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 114.0 in stage 13.0 (TID 931)
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:08 INFO TaskSetManager: Finished task 98.0 in stage 13.0 (TID 915) in 324 ms on localhost (99/200)
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:34:08 INFO Executor: Finished task 100.0 in stage 13.0 (TID 917). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Finished task 102.0 in stage 13.0 (TID 919). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Finished task 99.0 in stage 13.0 (TID 916). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 115.0 in stage 13.0 (TID 932, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 115.0 in stage 13.0 (TID 932)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 100.0 in stage 13.0 (TID 917) in 546 ms on localhost (100/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 116.0 in stage 13.0 (TID 933, localhost, ANY, 1815 bytes)
15/08/06 17:34:08 INFO Executor: Running task 116.0 in stage 13.0 (TID 933)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 102.0 in stage 13.0 (TID 919) in 536 ms on localhost (101/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 117.0 in stage 13.0 (TID 934, localhost, ANY, 1813 bytes)
15/08/06 17:34:08 INFO Executor: Running task 117.0 in stage 13.0 (TID 934)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 99.0 in stage 13.0 (TID 916) in 578 ms on localhost (102/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO Executor: Finished task 101.0 in stage 13.0 (TID 918). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 118.0 in stage 13.0 (TID 935, localhost, ANY, 1815 bytes)
15/08/06 17:34:08 INFO Executor: Running task 118.0 in stage 13.0 (TID 935)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 101.0 in stage 13.0 (TID 918) in 573 ms on localhost (103/200)
15/08/06 17:34:08 INFO Executor: Finished task 105.0 in stage 13.0 (TID 922). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO Executor: Finished task 104.0 in stage 13.0 (TID 921). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Finished task 103.0 in stage 13.0 (TID 920). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 119.0 in stage 13.0 (TID 936, localhost, ANY, 1813 bytes)
15/08/06 17:34:08 INFO Executor: Running task 119.0 in stage 13.0 (TID 936)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 105.0 in stage 13.0 (TID 922) in 552 ms on localhost (104/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 120.0 in stage 13.0 (TID 937, localhost, ANY, 1812 bytes)
15/08/06 17:34:08 INFO Executor: Running task 120.0 in stage 13.0 (TID 937)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 104.0 in stage 13.0 (TID 921) in 571 ms on localhost (105/200)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 103.0 in stage 13.0 (TID 920) in 584 ms on localhost (106/200)
15/08/06 17:34:08 INFO Executor: Finished task 106.0 in stage 13.0 (TID 923). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 121.0 in stage 13.0 (TID 938, localhost, ANY, 1812 bytes)
15/08/06 17:34:08 INFO Executor: Running task 121.0 in stage 13.0 (TID 938)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 122.0 in stage 13.0 (TID 939, localhost, ANY, 1812 bytes)
15/08/06 17:34:08 INFO Executor: Running task 122.0 in stage 13.0 (TID 939)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 106.0 in stage 13.0 (TID 923) in 575 ms on localhost (107/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 171
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/06 17:34:08 INFO Executor: Finished task 109.0 in stage 13.0 (TID 926). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 INFO TaskSetManager: Starting task 123.0 in stage 13.0 (TID 940, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 123.0 in stage 13.0 (TID 940)
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 INFO TaskSetManager: Finished task 109.0 in stage 13.0 (TID 926) in 576 ms on localhost (108/200)
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO Executor: Finished task 112.0 in stage 13.0 (TID 929). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 124.0 in stage 13.0 (TID 941, localhost, ANY, 1813 bytes)
15/08/06 17:34:08 INFO Executor: Finished task 108.0 in stage 13.0 (TID 925). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Running task 124.0 in stage 13.0 (TID 941)
15/08/06 17:34:08 INFO Executor: Finished task 111.0 in stage 13.0 (TID 928). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Finished task 112.0 in stage 13.0 (TID 929) in 558 ms on localhost (109/200)
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:34:08 INFO TaskSetManager: Starting task 125.0 in stage 13.0 (TID 942, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO TaskSetManager: Finished task 111.0 in stage 13.0 (TID 928) in 574 ms on localhost (110/200)
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO Executor: Running task 125.0 in stage 13.0 (TID 942)
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:34:08 INFO Executor: Finished task 113.0 in stage 13.0 (TID 930). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Finished task 107.0 in stage 13.0 (TID 924). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 126.0 in stage 13.0 (TID 943, localhost, ANY, 1813 bytes)
15/08/06 17:34:08 INFO Executor: Finished task 114.0 in stage 13.0 (TID 931). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO Executor: Running task 126.0 in stage 13.0 (TID 943)
15/08/06 17:34:08 INFO TaskSetManager: Starting task 127.0 in stage 13.0 (TID 944, localhost, ANY, 1815 bytes)
15/08/06 17:34:08 INFO Executor: Running task 127.0 in stage 13.0 (TID 944)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 113.0 in stage 13.0 (TID 930) in 560 ms on localhost (111/200)
15/08/06 17:34:08 INFO TaskSetManager: Starting task 128.0 in stage 13.0 (TID 945, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO TaskSetManager: Finished task 107.0 in stage 13.0 (TID 924) in 625 ms on localhost (112/200)
15/08/06 17:34:08 INFO Executor: Finished task 110.0 in stage 13.0 (TID 927). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 192
15/08/06 17:34:08 INFO Executor: Running task 128.0 in stage 13.0 (TID 945)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO TaskSetManager: Finished task 114.0 in stage 13.0 (TID 931) in 454 ms on localhost (113/200)
15/08/06 17:34:08 INFO TaskSetManager: Starting task 129.0 in stage 13.0 (TID 946, localhost, ANY, 1815 bytes)
15/08/06 17:34:08 INFO Executor: Running task 129.0 in stage 13.0 (TID 946)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 108.0 in stage 13.0 (TID 925) in 626 ms on localhost (114/200)
15/08/06 17:34:08 INFO TaskSetManager: Starting task 130.0 in stage 13.0 (TID 947, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 130.0 in stage 13.0 (TID 947)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 110.0 in stage 13.0 (TID 927) in 596 ms on localhost (115/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO Executor: Finished task 115.0 in stage 13.0 (TID 932). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO TaskSetManager: Starting task 131.0 in stage 13.0 (TID 948, localhost, ANY, 1815 bytes)
15/08/06 17:34:08 INFO Executor: Running task 131.0 in stage 13.0 (TID 948)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 115.0 in stage 13.0 (TID 932) in 299 ms on localhost (116/200)
15/08/06 17:34:08 INFO Executor: Finished task 116.0 in stage 13.0 (TID 933). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO TaskSetManager: Starting task 132.0 in stage 13.0 (TID 949, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:08 INFO Executor: Running task 132.0 in stage 13.0 (TID 949)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 116.0 in stage 13.0 (TID 933) in 297 ms on localhost (117/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO Executor: Finished task 117.0 in stage 13.0 (TID 934). 2182 bytes result sent to driver
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO TaskSetManager: Starting task 133.0 in stage 13.0 (TID 950, localhost, ANY, 1814 bytes)
15/08/06 17:34:08 INFO Executor: Running task 133.0 in stage 13.0 (TID 950)
15/08/06 17:34:08 INFO TaskSetManager: Finished task 117.0 in stage 13.0 (TID 934) in 294 ms on localhost (118/200)
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 180
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:08 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:09 INFO Executor: Finished task 119.0 in stage 13.0 (TID 936). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 134.0 in stage 13.0 (TID 951, localhost, ANY, 1816 bytes)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 119.0 in stage 13.0 (TID 936) in 364 ms on localhost (119/200)
15/08/06 17:34:09 INFO Executor: Running task 134.0 in stage 13.0 (TID 951)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 142
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO Executor: Finished task 118.0 in stage 13.0 (TID 935). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 135.0 in stage 13.0 (TID 952, localhost, ANY, 1816 bytes)
15/08/06 17:34:09 INFO Executor: Running task 135.0 in stage 13.0 (TID 952)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 118.0 in stage 13.0 (TID 935) in 419 ms on localhost (120/200)
15/08/06 17:34:09 INFO Executor: Finished task 122.0 in stage 13.0 (TID 939). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO TaskSetManager: Starting task 136.0 in stage 13.0 (TID 953, localhost, ANY, 1814 bytes)
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:09 INFO Executor: Running task 136.0 in stage 13.0 (TID 953)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO TaskSetManager: Finished task 122.0 in stage 13.0 (TID 939) in 381 ms on localhost (121/200)
15/08/06 17:34:09 INFO Executor: Finished task 121.0 in stage 13.0 (TID 938). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 137.0 in stage 13.0 (TID 954, localhost, ANY, 1814 bytes)
15/08/06 17:34:09 INFO Executor: Running task 137.0 in stage 13.0 (TID 954)
15/08/06 17:34:09 INFO Executor: Finished task 120.0 in stage 13.0 (TID 937). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Finished task 121.0 in stage 13.0 (TID 938) in 399 ms on localhost (122/200)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO TaskSetManager: Starting task 138.0 in stage 13.0 (TID 955, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO Executor: Running task 138.0 in stage 13.0 (TID 955)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 120.0 in stage 13.0 (TID 937) in 411 ms on localhost (123/200)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 123
15/08/06 17:34:09 INFO Executor: Finished task 124.0 in stage 13.0 (TID 941). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 139.0 in stage 13.0 (TID 956, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO Executor: Running task 139.0 in stage 13.0 (TID 956)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 124.0 in stage 13.0 (TID 941) in 492 ms on localhost (124/200)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO Executor: Finished task 126.0 in stage 13.0 (TID 943). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO Executor: Finished task 123.0 in stage 13.0 (TID 940). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 140.0 in stage 13.0 (TID 957, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 126.0 in stage 13.0 (TID 943) in 543 ms on localhost (125/200)
15/08/06 17:34:09 INFO Executor: Running task 140.0 in stage 13.0 (TID 957)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO TaskSetManager: Starting task 141.0 in stage 13.0 (TID 958, localhost, ANY, 1813 bytes)
15/08/06 17:34:09 INFO Executor: Running task 141.0 in stage 13.0 (TID 958)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 123.0 in stage 13.0 (TID 940) in 599 ms on localhost (126/200)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:09 INFO Executor: Finished task 125.0 in stage 13.0 (TID 942). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO Executor: Finished task 128.0 in stage 13.0 (TID 945). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 142.0 in stage 13.0 (TID 959, localhost, ANY, 1812 bytes)
15/08/06 17:34:09 INFO Executor: Running task 142.0 in stage 13.0 (TID 959)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 125.0 in stage 13.0 (TID 942) in 618 ms on localhost (127/200)
15/08/06 17:34:09 INFO Executor: Finished task 127.0 in stage 13.0 (TID 944). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:09 INFO TaskSetManager: Starting task 143.0 in stage 13.0 (TID 960, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO Executor: Running task 143.0 in stage 13.0 (TID 960)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 128.0 in stage 13.0 (TID 945) in 624 ms on localhost (128/200)
15/08/06 17:34:09 INFO TaskSetManager: Starting task 144.0 in stage 13.0 (TID 961, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 INFO Executor: Finished task 129.0 in stage 13.0 (TID 946). 2182 bytes result sent to driver
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO TaskSetManager: Finished task 127.0 in stage 13.0 (TID 944) in 633 ms on localhost (129/200)
15/08/06 17:34:09 INFO Executor: Running task 144.0 in stage 13.0 (TID 961)
15/08/06 17:34:09 INFO TaskSetManager: Starting task 145.0 in stage 13.0 (TID 962, localhost, ANY, 1814 bytes)
15/08/06 17:34:09 INFO Executor: Running task 145.0 in stage 13.0 (TID 962)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 129.0 in stage 13.0 (TID 946) in 628 ms on localhost (130/200)
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO Executor: Finished task 130.0 in stage 13.0 (TID 947). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO TaskSetManager: Starting task 146.0 in stage 13.0 (TID 963, localhost, ANY, 1814 bytes)
15/08/06 17:34:09 INFO Executor: Running task 146.0 in stage 13.0 (TID 963)
15/08/06 17:34:09 INFO Executor: Finished task 131.0 in stage 13.0 (TID 948). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Finished task 130.0 in stage 13.0 (TID 947) in 660 ms on localhost (131/200)
15/08/06 17:34:09 INFO TaskSetManager: Starting task 147.0 in stage 13.0 (TID 964, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO Executor: Running task 147.0 in stage 13.0 (TID 964)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 131.0 in stage 13.0 (TID 948) in 587 ms on localhost (132/200)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:09 INFO Executor: Finished task 132.0 in stage 13.0 (TID 949). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 148.0 in stage 13.0 (TID 965, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO Executor: Running task 148.0 in stage 13.0 (TID 965)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 132.0 in stage 13.0 (TID 949) in 686 ms on localhost (133/200)
15/08/06 17:34:09 INFO Executor: Finished task 133.0 in stage 13.0 (TID 950). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO TaskSetManager: Starting task 149.0 in stage 13.0 (TID 966, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO Executor: Running task 149.0 in stage 13.0 (TID 966)
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 INFO TaskSetManager: Finished task 133.0 in stage 13.0 (TID 950) in 697 ms on localhost (134/200)
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO Executor: Finished task 134.0 in stage 13.0 (TID 951). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO TaskSetManager: Starting task 150.0 in stage 13.0 (TID 967, localhost, ANY, 1815 bytes)
15/08/06 17:34:09 INFO Executor: Running task 150.0 in stage 13.0 (TID 967)
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO TaskSetManager: Finished task 134.0 in stage 13.0 (TID 951) in 641 ms on localhost (135/200)
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 INFO Executor: Finished task 135.0 in stage 13.0 (TID 952). 2182 bytes result sent to driver
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO TaskSetManager: Starting task 151.0 in stage 13.0 (TID 968, localhost, ANY, 1812 bytes)
15/08/06 17:34:09 INFO Executor: Running task 151.0 in stage 13.0 (TID 968)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO Executor: Finished task 138.0 in stage 13.0 (TID 955). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:34:09 INFO Executor: Finished task 136.0 in stage 13.0 (TID 953). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO Executor: Finished task 137.0 in stage 13.0 (TID 954). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO TaskSetManager: Finished task 135.0 in stage 13.0 (TID 952) in 599 ms on localhost (136/200)
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:09 INFO TaskSetManager: Starting task 152.0 in stage 13.0 (TID 969, localhost, ANY, 1816 bytes)
15/08/06 17:34:09 INFO Executor: Running task 152.0 in stage 13.0 (TID 969)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 138.0 in stage 13.0 (TID 955) in 594 ms on localhost (137/200)
15/08/06 17:34:09 INFO TaskSetManager: Starting task 153.0 in stage 13.0 (TID 970, localhost, ANY, 1814 bytes)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 136.0 in stage 13.0 (TID 953) in 603 ms on localhost (138/200)
15/08/06 17:34:09 INFO Executor: Running task 153.0 in stage 13.0 (TID 970)
15/08/06 17:34:09 INFO TaskSetManager: Starting task 154.0 in stage 13.0 (TID 971, localhost, ANY, 1813 bytes)
15/08/06 17:34:09 INFO Executor: Running task 154.0 in stage 13.0 (TID 971)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO TaskSetManager: Finished task 137.0 in stage 13.0 (TID 954) in 602 ms on localhost (139/200)
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO Executor: Finished task 139.0 in stage 13.0 (TID 956). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 155.0 in stage 13.0 (TID 972, localhost, ANY, 1814 bytes)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 139.0 in stage 13.0 (TID 956) in 445 ms on localhost (140/200)
15/08/06 17:34:09 INFO Executor: Running task 155.0 in stage 13.0 (TID 972)
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 103
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:34:09 INFO Executor: Finished task 140.0 in stage 13.0 (TID 957). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 156.0 in stage 13.0 (TID 973, localhost, ANY, 1816 bytes)
15/08/06 17:34:09 INFO Executor: Running task 156.0 in stage 13.0 (TID 973)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 140.0 in stage 13.0 (TID 957) in 448 ms on localhost (141/200)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 190
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO Executor: Finished task 152.0 in stage 13.0 (TID 969). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 157.0 in stage 13.0 (TID 974, localhost, ANY, 1816 bytes)
15/08/06 17:34:09 INFO Executor: Running task 157.0 in stage 13.0 (TID 974)
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO TaskSetManager: Finished task 152.0 in stage 13.0 (TID 969) in 142 ms on localhost (142/200)
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:34:09 INFO Executor: Finished task 141.0 in stage 13.0 (TID 958). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 158.0 in stage 13.0 (TID 975, localhost, ANY, 1813 bytes)
15/08/06 17:34:09 INFO Executor: Running task 158.0 in stage 13.0 (TID 975)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 141.0 in stage 13.0 (TID 958) in 522 ms on localhost (143/200)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:09 INFO Executor: Finished task 142.0 in stage 13.0 (TID 959). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO TaskSetManager: Starting task 159.0 in stage 13.0 (TID 976, localhost, ANY, 1814 bytes)
15/08/06 17:34:09 INFO Executor: Running task 159.0 in stage 13.0 (TID 976)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 142.0 in stage 13.0 (TID 959) in 483 ms on localhost (144/200)
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:09 INFO Executor: Finished task 144.0 in stage 13.0 (TID 961). 2182 bytes result sent to driver
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:09 INFO TaskSetManager: Starting task 160.0 in stage 13.0 (TID 977, localhost, ANY, 1814 bytes)
15/08/06 17:34:09 INFO Executor: Running task 160.0 in stage 13.0 (TID 977)
15/08/06 17:34:09 INFO TaskSetManager: Finished task 144.0 in stage 13.0 (TID 961) in 473 ms on localhost (145/200)
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:34:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:34:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:34:10 INFO Executor: Finished task 145.0 in stage 13.0 (TID 962). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 161.0 in stage 13.0 (TID 978, localhost, ANY, 1816 bytes)
15/08/06 17:34:10 INFO Executor: Running task 161.0 in stage 13.0 (TID 978)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 145.0 in stage 13.0 (TID 962) in 615 ms on localhost (146/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 143.0 in stage 13.0 (TID 960). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 162.0 in stage 13.0 (TID 979, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 162.0 in stage 13.0 (TID 979)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 143.0 in stage 13.0 (TID 960) in 649 ms on localhost (147/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 146.0 in stage 13.0 (TID 963). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO Executor: Finished task 147.0 in stage 13.0 (TID 964). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 163.0 in stage 13.0 (TID 980, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 163.0 in stage 13.0 (TID 980)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 146.0 in stage 13.0 (TID 963) in 631 ms on localhost (148/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:10 INFO TaskSetManager: Starting task 164.0 in stage 13.0 (TID 981, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 164.0 in stage 13.0 (TID 981)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 147.0 in stage 13.0 (TID 964) in 619 ms on localhost (149/200)
15/08/06 17:34:10 INFO Executor: Finished task 148.0 in stage 13.0 (TID 965). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO TaskSetManager: Starting task 165.0 in stage 13.0 (TID 982, localhost, ANY, 1816 bytes)
15/08/06 17:34:10 INFO Executor: Running task 165.0 in stage 13.0 (TID 982)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 148.0 in stage 13.0 (TID 965) in 524 ms on localhost (150/200)
15/08/06 17:34:10 INFO Executor: Finished task 149.0 in stage 13.0 (TID 966). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO TaskSetManager: Starting task 166.0 in stage 13.0 (TID 983, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 166.0 in stage 13.0 (TID 983)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 149.0 in stage 13.0 (TID 966) in 520 ms on localhost (151/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 150.0 in stage 13.0 (TID 967). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO Executor: Finished task 151.0 in stage 13.0 (TID 968). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO TaskSetManager: Starting task 167.0 in stage 13.0 (TID 984, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 167.0 in stage 13.0 (TID 984)
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 116
15/08/06 17:34:10 INFO TaskSetManager: Starting task 168.0 in stage 13.0 (TID 985, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 168.0 in stage 13.0 (TID 985)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 150.0 in stage 13.0 (TID 967) in 519 ms on localhost (152/200)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 151.0 in stage 13.0 (TID 968) in 515 ms on localhost (153/200)
15/08/06 17:34:10 INFO Executor: Finished task 153.0 in stage 13.0 (TID 970). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 169.0 in stage 13.0 (TID 986, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 169.0 in stage 13.0 (TID 986)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 153.0 in stage 13.0 (TID 970) in 511 ms on localhost (154/200)
15/08/06 17:34:10 INFO Executor: Finished task 154.0 in stage 13.0 (TID 971). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO TaskSetManager: Starting task 170.0 in stage 13.0 (TID 987, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 170.0 in stage 13.0 (TID 987)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 154.0 in stage 13.0 (TID 971) in 518 ms on localhost (155/200)
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:10 INFO Executor: Finished task 155.0 in stage 13.0 (TID 972). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO TaskSetManager: Starting task 171.0 in stage 13.0 (TID 988, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 110
15/08/06 17:34:10 INFO Executor: Running task 171.0 in stage 13.0 (TID 988)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 155.0 in stage 13.0 (TID 972) in 496 ms on localhost (156/200)
15/08/06 17:34:10 INFO Executor: Finished task 163.0 in stage 13.0 (TID 980). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO TaskSetManager: Starting task 172.0 in stage 13.0 (TID 989, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 172.0 in stage 13.0 (TID 989)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 163.0 in stage 13.0 (TID 980) in 130 ms on localhost (157/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO Executor: Finished task 156.0 in stage 13.0 (TID 973). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 173.0 in stage 13.0 (TID 990, localhost, ANY, 1812 bytes)
15/08/06 17:34:10 INFO Executor: Running task 173.0 in stage 13.0 (TID 990)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 156.0 in stage 13.0 (TID 973) in 445 ms on localhost (158/200)
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO Executor: Finished task 157.0 in stage 13.0 (TID 974). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO TaskSetManager: Starting task 174.0 in stage 13.0 (TID 991, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/06 17:34:10 INFO Executor: Running task 174.0 in stage 13.0 (TID 991)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 157.0 in stage 13.0 (TID 974) in 431 ms on localhost (159/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:10 INFO Executor: Finished task 159.0 in stage 13.0 (TID 976). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 INFO TaskSetManager: Starting task 175.0 in stage 13.0 (TID 992, localhost, ANY, 1812 bytes)
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO Executor: Running task 175.0 in stage 13.0 (TID 992)
15/08/06 17:34:10 INFO Executor: Finished task 158.0 in stage 13.0 (TID 975). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO Executor: Finished task 160.0 in stage 13.0 (TID 977). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Finished task 159.0 in stage 13.0 (TID 976) in 376 ms on localhost (160/200)
15/08/06 17:34:10 INFO TaskSetManager: Starting task 176.0 in stage 13.0 (TID 993, localhost, ANY, 1813 bytes)
15/08/06 17:34:10 INFO Executor: Running task 176.0 in stage 13.0 (TID 993)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 158.0 in stage 13.0 (TID 975) in 398 ms on localhost (161/200)
15/08/06 17:34:10 INFO TaskSetManager: Starting task 177.0 in stage 13.0 (TID 994, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 177.0 in stage 13.0 (TID 994)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO TaskSetManager: Finished task 160.0 in stage 13.0 (TID 977) in 366 ms on localhost (162/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 136
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 126
15/08/06 17:34:10 INFO Executor: Finished task 161.0 in stage 13.0 (TID 978). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 178.0 in stage 13.0 (TID 995, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 178.0 in stage 13.0 (TID 995)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 161.0 in stage 13.0 (TID 978) in 265 ms on localhost (163/200)
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:34:10 INFO Executor: Finished task 173.0 in stage 13.0 (TID 990). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 179.0 in stage 13.0 (TID 996, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 179.0 in stage 13.0 (TID 996)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 173.0 in stage 13.0 (TID 990) in 222 ms on localhost (164/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:10 INFO Executor: Finished task 165.0 in stage 13.0 (TID 982). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 180.0 in stage 13.0 (TID 997, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 180.0 in stage 13.0 (TID 997)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 165.0 in stage 13.0 (TID 982) in 345 ms on localhost (165/200)
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO Executor: Finished task 166.0 in stage 13.0 (TID 983). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO TaskSetManager: Starting task 181.0 in stage 13.0 (TID 998, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 181.0 in stage 13.0 (TID 998)
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:34:10 INFO TaskSetManager: Finished task 166.0 in stage 13.0 (TID 983) in 346 ms on localhost (166/200)
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 197
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:10 INFO Executor: Finished task 162.0 in stage 13.0 (TID 979). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 182.0 in stage 13.0 (TID 999, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 182.0 in stage 13.0 (TID 999)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 162.0 in stage 13.0 (TID 979) in 437 ms on localhost (167/200)
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 246
15/08/06 17:34:10 INFO Executor: Finished task 164.0 in stage 13.0 (TID 981). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 183.0 in stage 13.0 (TID 1000, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 183.0 in stage 13.0 (TID 1000)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 164.0 in stage 13.0 (TID 981) in 424 ms on localhost (168/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:34:10 INFO Executor: Finished task 167.0 in stage 13.0 (TID 984). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 184.0 in stage 13.0 (TID 1001, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 184.0 in stage 13.0 (TID 1001)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 167.0 in stage 13.0 (TID 984) in 425 ms on localhost (169/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO Executor: Finished task 169.0 in stage 13.0 (TID 986). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO TaskSetManager: Starting task 185.0 in stage 13.0 (TID 1002, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 185.0 in stage 13.0 (TID 1002)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 169.0 in stage 13.0 (TID 986) in 444 ms on localhost (170/200)
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 135
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 168.0 in stage 13.0 (TID 985). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO Executor: Finished task 170.0 in stage 13.0 (TID 987). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 186.0 in stage 13.0 (TID 1003, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 186.0 in stage 13.0 (TID 1003)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 168.0 in stage 13.0 (TID 985) in 478 ms on localhost (171/200)
15/08/06 17:34:10 INFO TaskSetManager: Starting task 187.0 in stage 13.0 (TID 1004, localhost, ANY, 1812 bytes)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Running task 187.0 in stage 13.0 (TID 1004)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 170.0 in stage 13.0 (TID 987) in 461 ms on localhost (172/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 171.0 in stage 13.0 (TID 988). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 188.0 in stage 13.0 (TID 1005, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 188.0 in stage 13.0 (TID 1005)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 171.0 in stage 13.0 (TID 988) in 467 ms on localhost (173/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:34:10 INFO Executor: Finished task 172.0 in stage 13.0 (TID 989). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 189.0 in stage 13.0 (TID 1006, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 189.0 in stage 13.0 (TID 1006)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 172.0 in stage 13.0 (TID 989) in 504 ms on localhost (174/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 174.0 in stage 13.0 (TID 991). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO TaskSetManager: Starting task 190.0 in stage 13.0 (TID 1007, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 190.0 in stage 13.0 (TID 1007)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 174.0 in stage 13.0 (TID 991) in 486 ms on localhost (175/200)
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 130
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 175.0 in stage 13.0 (TID 992). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 191.0 in stage 13.0 (TID 1008, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 191.0 in stage 13.0 (TID 1008)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 175.0 in stage 13.0 (TID 992) in 497 ms on localhost (176/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO Executor: Finished task 176.0 in stage 13.0 (TID 993). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO TaskSetManager: Starting task 192.0 in stage 13.0 (TID 1009, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 194
15/08/06 17:34:10 INFO Executor: Running task 192.0 in stage 13.0 (TID 1009)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 176.0 in stage 13.0 (TID 993) in 510 ms on localhost (177/200)
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 178
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:34:10 INFO Executor: Finished task 177.0 in stage 13.0 (TID 994). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 193.0 in stage 13.0 (TID 1010, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 193.0 in stage 13.0 (TID 1010)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 177.0 in stage 13.0 (TID 994) in 529 ms on localhost (178/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 178.0 in stage 13.0 (TID 995). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 194.0 in stage 13.0 (TID 1011, localhost, ANY, 1814 bytes)
15/08/06 17:34:10 INFO Executor: Running task 194.0 in stage 13.0 (TID 1011)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 178.0 in stage 13.0 (TID 995) in 494 ms on localhost (179/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO Executor: Finished task 180.0 in stage 13.0 (TID 997). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO TaskSetManager: Starting task 195.0 in stage 13.0 (TID 1012, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 195.0 in stage 13.0 (TID 1012)
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 134
15/08/06 17:34:10 INFO TaskSetManager: Finished task 180.0 in stage 13.0 (TID 997) in 386 ms on localhost (180/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO Executor: Finished task 181.0 in stage 13.0 (TID 998). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 196.0 in stage 13.0 (TID 1013, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 196.0 in stage 13.0 (TID 1013)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 181.0 in stage 13.0 (TID 998) in 396 ms on localhost (181/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:34:10 INFO Executor: Finished task 179.0 in stage 13.0 (TID 996). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO TaskSetManager: Starting task 197.0 in stage 13.0 (TID 1014, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 197.0 in stage 13.0 (TID 1014)
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO TaskSetManager: Finished task 179.0 in stage 13.0 (TID 996) in 444 ms on localhost (182/200)
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:34:10 INFO Executor: Finished task 183.0 in stage 13.0 (TID 1000). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO Executor: Finished task 182.0 in stage 13.0 (TID 999). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Starting task 198.0 in stage 13.0 (TID 1015, localhost, ANY, 1813 bytes)
15/08/06 17:34:10 INFO Executor: Running task 198.0 in stage 13.0 (TID 1015)
15/08/06 17:34:10 INFO TaskSetManager: Finished task 183.0 in stage 13.0 (TID 1000) in 409 ms on localhost (183/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 INFO TaskSetManager: Starting task 199.0 in stage 13.0 (TID 1016, localhost, ANY, 1815 bytes)
15/08/06 17:34:10 INFO Executor: Running task 199.0 in stage 13.0 (TID 1016)
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO TaskSetManager: Finished task 182.0 in stage 13.0 (TID 999) in 435 ms on localhost (184/200)
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:34:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 131
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO Executor: Finished task 184.0 in stage 13.0 (TID 1001). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:10 INFO TaskSetManager: Finished task 184.0 in stage 13.0 (TID 1001) in 404 ms on localhost (185/200)
15/08/06 17:34:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:34:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:34:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:34:10 INFO Executor: Finished task 185.0 in stage 13.0 (TID 1002). 2182 bytes result sent to driver
15/08/06 17:34:10 INFO TaskSetManager: Finished task 185.0 in stage 13.0 (TID 1002) in 374 ms on localhost (186/200)
15/08/06 17:34:11 INFO Executor: Finished task 187.0 in stage 13.0 (TID 1004). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 187.0 in stage 13.0 (TID 1004) in 368 ms on localhost (187/200)
15/08/06 17:34:11 INFO Executor: Finished task 186.0 in stage 13.0 (TID 1003). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 186.0 in stage 13.0 (TID 1003) in 386 ms on localhost (188/200)
15/08/06 17:34:11 INFO Executor: Finished task 188.0 in stage 13.0 (TID 1005). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 188.0 in stage 13.0 (TID 1005) in 368 ms on localhost (189/200)
15/08/06 17:34:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:34:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:34:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:34:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:34:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:34:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:34:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:34:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:34:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:34:11 INFO Executor: Finished task 189.0 in stage 13.0 (TID 1006). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 189.0 in stage 13.0 (TID 1006) in 360 ms on localhost (190/200)
15/08/06 17:34:11 INFO Executor: Finished task 190.0 in stage 13.0 (TID 1007). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 190.0 in stage 13.0 (TID 1007) in 375 ms on localhost (191/200)
15/08/06 17:34:11 INFO Executor: Finished task 191.0 in stage 13.0 (TID 1008). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 191.0 in stage 13.0 (TID 1008) in 362 ms on localhost (192/200)
15/08/06 17:34:11 INFO Executor: Finished task 192.0 in stage 13.0 (TID 1009). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 192.0 in stage 13.0 (TID 1009) in 348 ms on localhost (193/200)
15/08/06 17:34:11 INFO Executor: Finished task 194.0 in stage 13.0 (TID 1011). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 194.0 in stage 13.0 (TID 1011) in 321 ms on localhost (194/200)
15/08/06 17:34:11 INFO Executor: Finished task 195.0 in stage 13.0 (TID 1012). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO Executor: Finished task 193.0 in stage 13.0 (TID 1010). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO Executor: Finished task 196.0 in stage 13.0 (TID 1013). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 195.0 in stage 13.0 (TID 1012) in 328 ms on localhost (195/200)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 196.0 in stage 13.0 (TID 1013) in 317 ms on localhost (196/200)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 193.0 in stage 13.0 (TID 1010) in 375 ms on localhost (197/200)
15/08/06 17:34:11 INFO Executor: Finished task 199.0 in stage 13.0 (TID 1016). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 199.0 in stage 13.0 (TID 1016) in 294 ms on localhost (198/200)
15/08/06 17:34:11 INFO Executor: Finished task 198.0 in stage 13.0 (TID 1015). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO Executor: Finished task 197.0 in stage 13.0 (TID 1014). 2182 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 198.0 in stage 13.0 (TID 1015) in 307 ms on localhost (199/200)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 197.0 in stage 13.0 (TID 1014) in 346 ms on localhost (200/200)
15/08/06 17:34:11 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/08/06 17:34:11 INFO DAGScheduler: Stage 13 (mapPartitions at Exchange.scala:77) finished in 6.664 s
15/08/06 17:34:11 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:34:11 INFO DAGScheduler: running: Set()
15/08/06 17:34:11 INFO DAGScheduler: waiting: Set(Stage 14)
15/08/06 17:34:11 INFO DAGScheduler: failed: Set()
15/08/06 17:34:11 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@25ac8627
15/08/06 17:34:11 INFO StatsReportListener: task runtime:(count: 200, mean: 527.425000, stdev: 147.409445, max: 828.000000, min: 130.000000)
15/08/06 17:34:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:11 INFO StatsReportListener: 	130.0 ms	297.0 ms	346.0 ms	406.0 ms	544.0 ms	628.0 ms	718.0 ms	806.0 ms	828.0 ms
15/08/06 17:34:11 INFO DAGScheduler: Missing parents for Stage 14: List()
15/08/06 17:34:11 INFO DAGScheduler: Submitting Stage 14 (MapPartitionsRDD[80] at mapPartitions at basicOperators.scala:207), which is now runnable
15/08/06 17:34:11 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 3352.795000, stdev: 425.781685, max: 3815.000000, min: 0.000000)
15/08/06 17:34:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:11 INFO StatsReportListener: 	0.0 B	3.2 KB	3.2 KB	3.3 KB	3.3 KB	3.4 KB	3.5 KB	3.5 KB	3.7 KB
15/08/06 17:34:11 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.210000, stdev: 0.495883, max: 3.000000, min: 0.000000)
15/08/06 17:34:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:11 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	3.0 ms
15/08/06 17:34:11 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:34:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:11 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:34:11 INFO StatsReportListener: task result size:(count: 200, mean: 2182.000000, stdev: 0.000000, max: 2182.000000, min: 2182.000000)
15/08/06 17:34:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:11 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/06 17:34:11 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 96.962034, stdev: 1.729903, max: 99.683544, min: 87.692308)
15/08/06 17:34:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:11 INFO StatsReportListener: 	88 %	94 %	95 %	96 %	97 %	98 %	99 %	99 %	100 %
15/08/06 17:34:11 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.045994, stdev: 0.115765, max: 0.704225, min: 0.000000)
15/08/06 17:34:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:11 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/06 17:34:11 INFO StatsReportListener: other time pct: (count: 200, mean: 2.991973, stdev: 1.724052, max: 12.307692, min: 0.316456)
15/08/06 17:34:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:11 INFO StatsReportListener: 	 0 %	 1 %	 1 %	 2 %	 3 %	 4 %	 5 %	 6 %	12 %
15/08/06 17:34:11 INFO MemoryStore: ensureFreeSpace(151624) called with curMem=1632319, maxMem=3333968363
15/08/06 17:34:11 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 148.1 KB, free 3.1 GB)
15/08/06 17:34:11 INFO MemoryStore: ensureFreeSpace(66864) called with curMem=1783943, maxMem=3333968363
15/08/06 17:34:11 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 65.3 KB, free 3.1 GB)
15/08/06 17:34:11 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:42931 (size: 65.3 KB, free: 3.1 GB)
15/08/06 17:34:11 INFO BlockManagerMaster: Updated info of block broadcast_19_piece0
15/08/06 17:34:11 INFO DefaultExecutionContext: Created broadcast 19 from broadcast at DAGScheduler.scala:838
15/08/06 17:34:11 INFO DAGScheduler: Submitting 200 missing tasks from Stage 14 (MapPartitionsRDD[80] at mapPartitions at basicOperators.scala:207)
15/08/06 17:34:11 INFO TaskSchedulerImpl: Adding task set 14.0 with 200 tasks
15/08/06 17:34:11 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 1017, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 1018, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 1019, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 1020, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 1021, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 1022, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 1023, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 7.0 in stage 14.0 (TID 1024, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 8.0 in stage 14.0 (TID 1025, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 9.0 in stage 14.0 (TID 1026, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 10.0 in stage 14.0 (TID 1027, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 11.0 in stage 14.0 (TID 1028, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 12.0 in stage 14.0 (TID 1029, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 13.0 in stage 14.0 (TID 1030, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 14.0 in stage 14.0 (TID 1031, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 15.0 in stage 14.0 (TID 1032, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 0.0 in stage 14.0 (TID 1017)
15/08/06 17:34:11 INFO Executor: Running task 3.0 in stage 14.0 (TID 1020)
15/08/06 17:34:11 INFO Executor: Running task 4.0 in stage 14.0 (TID 1021)
15/08/06 17:34:11 INFO Executor: Running task 8.0 in stage 14.0 (TID 1025)
15/08/06 17:34:11 INFO Executor: Running task 12.0 in stage 14.0 (TID 1029)
15/08/06 17:34:11 INFO Executor: Running task 5.0 in stage 14.0 (TID 1022)
15/08/06 17:34:11 INFO Executor: Running task 2.0 in stage 14.0 (TID 1019)
15/08/06 17:34:11 INFO Executor: Running task 1.0 in stage 14.0 (TID 1018)
15/08/06 17:34:11 INFO Executor: Running task 13.0 in stage 14.0 (TID 1030)
15/08/06 17:34:11 INFO Executor: Running task 15.0 in stage 14.0 (TID 1032)
15/08/06 17:34:11 INFO Executor: Running task 14.0 in stage 14.0 (TID 1031)
15/08/06 17:34:11 INFO Executor: Running task 11.0 in stage 14.0 (TID 1028)
15/08/06 17:34:11 INFO Executor: Running task 10.0 in stage 14.0 (TID 1027)
15/08/06 17:34:11 INFO Executor: Running task 6.0 in stage 14.0 (TID 1023)
15/08/06 17:34:11 INFO Executor: Running task 9.0 in stage 14.0 (TID 1026)
15/08/06 17:34:11 INFO Executor: Running task 7.0 in stage 14.0 (TID 1024)
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ba1453
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000012_1029/part-00012
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@388c5584
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000003_1020/part-00003
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b44989d
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000011_1028/part-00011
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@47f9206
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000004_1021/part-00004
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@534bca38
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000009_1026/part-00009
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14c94d41
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2beef110
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000013_1030/part-00013
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000008_1025/part-00008
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@541467f8
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000010_1027/part-00010
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@cf6b5e9
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000006_1023/part-00006
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2dd308fd
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000015_1032/part-00015
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a209932
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000001_1018/part-00001
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c869262
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000000_1017/part-00000
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@46f46e8f
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000014_1031/part-00014
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d4fef69
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e54c7a6
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79b22491
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e7eea02
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000002_1019/part-00002
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d46a5d4
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000007_1024/part-00007
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2125ead3
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4ca416a5
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000005_1022/part-00005
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@333aa779
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2c7d737a
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@60286567
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f60eae6
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@73ddc28b
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@31474748
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e98e13e
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bf22762
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6686d576
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@61cb6f64
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72fe4064
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@80bfe55
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000012_1029' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000012
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000012_1029: Committed
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000008_1025' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000008
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000008_1025: Committed
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000010_1027' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000010
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000010_1027: Committed
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000013_1030' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000013
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000013_1030: Committed
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000007_1024' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000007
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000007_1024: Committed
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000004_1021' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000004
15/08/06 17:34:11 INFO Executor: Finished task 8.0 in stage 14.0 (TID 1025). 781 bytes result sent to driver
15/08/06 17:34:11 INFO Executor: Finished task 12.0 in stage 14.0 (TID 1029). 781 bytes result sent to driver
15/08/06 17:34:11 INFO Executor: Finished task 13.0 in stage 14.0 (TID 1030). 781 bytes result sent to driver
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000004_1021: Committed
15/08/06 17:34:11 INFO Executor: Finished task 7.0 in stage 14.0 (TID 1024). 781 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Starting task 16.0 in stage 14.0 (TID 1033, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 16.0 in stage 14.0 (TID 1033)
15/08/06 17:34:11 INFO Executor: Finished task 10.0 in stage 14.0 (TID 1027). 781 bytes result sent to driver
15/08/06 17:34:11 INFO Executor: Finished task 4.0 in stage 14.0 (TID 1021). 781 bytes result sent to driver
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000006_1023' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000006
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000003_1020' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000003
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000006_1023: Committed
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000003_1020: Committed
15/08/06 17:34:11 INFO TaskSetManager: Finished task 8.0 in stage 14.0 (TID 1025) in 384 ms on localhost (1/200)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 17.0 in stage 14.0 (TID 1034, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Finished task 6.0 in stage 14.0 (TID 1023). 781 bytes result sent to driver
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000001_1018' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000001
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000001_1018: Committed
15/08/06 17:34:11 INFO TaskSetManager: Starting task 18.0 in stage 14.0 (TID 1035, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Finished task 3.0 in stage 14.0 (TID 1020). 781 bytes result sent to driver
15/08/06 17:34:11 INFO Executor: Running task 17.0 in stage 14.0 (TID 1034)
15/08/06 17:34:11 INFO Executor: Running task 18.0 in stage 14.0 (TID 1035)
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000015_1032' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000015
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000015_1032: Committed
15/08/06 17:34:11 INFO TaskSetManager: Starting task 19.0 in stage 14.0 (TID 1036, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 19.0 in stage 14.0 (TID 1036)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 20.0 in stage 14.0 (TID 1037, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 20.0 in stage 14.0 (TID 1037)
15/08/06 17:34:11 INFO Executor: Finished task 1.0 in stage 14.0 (TID 1018). 781 bytes result sent to driver
15/08/06 17:34:11 INFO Executor: Finished task 15.0 in stage 14.0 (TID 1032). 781 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 12.0 in stage 14.0 (TID 1029) in 387 ms on localhost (2/200)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 21.0 in stage 14.0 (TID 1038, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 21.0 in stage 14.0 (TID 1038)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 22.0 in stage 14.0 (TID 1039, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 22.0 in stage 14.0 (TID 1039)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 23.0 in stage 14.0 (TID 1040, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 23.0 in stage 14.0 (TID 1040)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 24.0 in stage 14.0 (TID 1041, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 24.0 in stage 14.0 (TID 1041)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 25.0 in stage 14.0 (TID 1042, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 25.0 in stage 14.0 (TID 1042)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 7.0 in stage 14.0 (TID 1024) in 392 ms on localhost (3/200)
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000009_1026' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000009
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000009_1026: Committed
15/08/06 17:34:11 INFO TaskSetManager: Finished task 13.0 in stage 14.0 (TID 1030) in 392 ms on localhost (4/200)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 1020) in 394 ms on localhost (5/200)
15/08/06 17:34:11 INFO Executor: Finished task 9.0 in stage 14.0 (TID 1026). 781 bytes result sent to driver
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000005_1022' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000005
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000005_1022: Committed
15/08/06 17:34:11 INFO TaskSetManager: Finished task 6.0 in stage 14.0 (TID 1023) in 395 ms on localhost (6/200)
15/08/06 17:34:11 INFO Executor: Finished task 5.0 in stage 14.0 (TID 1022). 781 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 1021) in 397 ms on localhost (7/200)
15/08/06 17:34:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000000_1017' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000000
15/08/06 17:34:11 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000000_1017: Committed
15/08/06 17:34:11 INFO TaskSetManager: Finished task 10.0 in stage 14.0 (TID 1027) in 397 ms on localhost (8/200)
15/08/06 17:34:11 INFO Executor: Finished task 0.0 in stage 14.0 (TID 1017). 781 bytes result sent to driver
15/08/06 17:34:11 INFO TaskSetManager: Finished task 15.0 in stage 14.0 (TID 1032) in 397 ms on localhost (9/200)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 1018) in 404 ms on localhost (10/200)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 9.0 in stage 14.0 (TID 1026) in 402 ms on localhost (11/200)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 26.0 in stage 14.0 (TID 1043, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 26.0 in stage 14.0 (TID 1043)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 27.0 in stage 14.0 (TID 1044, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 27.0 in stage 14.0 (TID 1044)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 5.0 in stage 14.0 (TID 1022) in 406 ms on localhost (12/200)
15/08/06 17:34:11 INFO TaskSetManager: Starting task 28.0 in stage 14.0 (TID 1045, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:11 INFO Executor: Running task 28.0 in stage 14.0 (TID 1045)
15/08/06 17:34:11 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 1017) in 408 ms on localhost (13/200)
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@115d332f
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000017_1034/part-00017
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31e88b75
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000025_1042/part-00025
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@494d53f6
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000016_1033/part-00016
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1332a209
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000026_1043/part-00026
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@73c36112
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@612c06b7
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@db9eb78
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000022_1039/part-00022
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:11 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:11 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2282e10a
15/08/06 17:34:11 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000023_1040/part-00023
15/08/06 17:34:11 INFO CodecConfig: Compression set to false
15/08/06 17:34:11 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:11 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:11 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14b53fec
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@777a993
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15ef6615
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@bf07ee0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000021_1038/part-00021
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a19aa3f
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000027_1044/part-00027
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000025_1042' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000025
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000025_1042: Committed
15/08/06 17:34:12 INFO Executor: Finished task 25.0 in stage 14.0 (TID 1042). 781 bytes result sent to driver
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16a3dd83
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000020_1037/part-00020
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO TaskSetManager: Starting task 29.0 in stage 14.0 (TID 1046, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 29.0 in stage 14.0 (TID 1046)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 25.0 in stage 14.0 (TID 1042) in 317 ms on localhost (14/200)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4eef9666
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@537b2b06
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000024_1041/part-00024
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@44adab2d
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000019_1036/part-00019
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000017_1034' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000017
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000017_1034: Committed
15/08/06 17:34:12 INFO Executor: Finished task 17.0 in stage 14.0 (TID 1034). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 30.0 in stage 14.0 (TID 1047, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 30.0 in stage 14.0 (TID 1047)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 17.0 in stage 14.0 (TID 1034) in 332 ms on localhost (15/200)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f87ad2a
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51206235
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b44b775
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@744f186e
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6d5aa30c
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000026_1043' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000026
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000026_1043: Committed
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000016_1033' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000016
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000016_1033: Committed
15/08/06 17:34:12 INFO Executor: Finished task 26.0 in stage 14.0 (TID 1043). 781 bytes result sent to driver
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000022_1039' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000022
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000022_1039: Committed
15/08/06 17:34:12 INFO Executor: Finished task 16.0 in stage 14.0 (TID 1033). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 31.0 in stage 14.0 (TID 1048, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2f4f2254
15/08/06 17:34:12 INFO Executor: Running task 31.0 in stage 14.0 (TID 1048)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000028_1045/part-00028
15/08/06 17:34:12 INFO Executor: Finished task 22.0 in stage 14.0 (TID 1039). 781 bytes result sent to driver
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO TaskSetManager: Finished task 26.0 in stage 14.0 (TID 1043) in 332 ms on localhost (16/200)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 32.0 in stage 14.0 (TID 1049, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 32.0 in stage 14.0 (TID 1049)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 16.0 in stage 14.0 (TID 1033) in 355 ms on localhost (17/200)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 33.0 in stage 14.0 (TID 1050, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 33.0 in stage 14.0 (TID 1050)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 22.0 in stage 14.0 (TID 1039) in 351 ms on localhost (18/200)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000023_1040' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000023
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000023_1040: Committed
15/08/06 17:34:12 INFO Executor: Finished task 23.0 in stage 14.0 (TID 1040). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 34.0 in stage 14.0 (TID 1051, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 34.0 in stage 14.0 (TID 1051)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000020_1037' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000020
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000020_1037: Committed
15/08/06 17:34:12 INFO TaskSetManager: Finished task 23.0 in stage 14.0 (TID 1040) in 359 ms on localhost (19/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO Executor: Finished task 20.0 in stage 14.0 (TID 1037). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 35.0 in stage 14.0 (TID 1052, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 35.0 in stage 14.0 (TID 1052)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 20.0 in stage 14.0 (TID 1037) in 363 ms on localhost (20/200)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000021_1038' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000021
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000021_1038: Committed
15/08/06 17:34:12 INFO Executor: Finished task 21.0 in stage 14.0 (TID 1038). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 36.0 in stage 14.0 (TID 1053, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 36.0 in stage 14.0 (TID 1053)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 21.0 in stage 14.0 (TID 1038) in 377 ms on localhost (21/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3cf5d095
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000024_1041' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000024
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000024_1041: Committed
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO Executor: Finished task 24.0 in stage 14.0 (TID 1041). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 37.0 in stage 14.0 (TID 1054, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000019_1036' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000019
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000019_1036: Committed
15/08/06 17:34:12 INFO TaskSetManager: Finished task 24.0 in stage 14.0 (TID 1041) in 390 ms on localhost (22/200)
15/08/06 17:34:12 INFO Executor: Running task 37.0 in stage 14.0 (TID 1054)
15/08/06 17:34:12 INFO Executor: Finished task 19.0 in stage 14.0 (TID 1036). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 38.0 in stage 14.0 (TID 1055, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 19.0 in stage 14.0 (TID 1036) in 395 ms on localhost (23/200)
15/08/06 17:34:12 INFO Executor: Running task 38.0 in stage 14.0 (TID 1055)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000002_1019' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000002
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000002_1019: Committed
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000014_1031' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000014
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000014_1031: Committed
15/08/06 17:34:12 INFO Executor: Finished task 2.0 in stage 14.0 (TID 1019). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 39.0 in stage 14.0 (TID 1056, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Finished task 14.0 in stage 14.0 (TID 1031). 781 bytes result sent to driver
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000011_1028' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000011
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000011_1028: Committed
15/08/06 17:34:12 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 1019) in 794 ms on localhost (24/200)
15/08/06 17:34:12 INFO Executor: Running task 39.0 in stage 14.0 (TID 1056)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 40.0 in stage 14.0 (TID 1057, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 40.0 in stage 14.0 (TID 1057)
15/08/06 17:34:12 INFO Executor: Finished task 11.0 in stage 14.0 (TID 1028). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Finished task 14.0 in stage 14.0 (TID 1031) in 792 ms on localhost (25/200)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 41.0 in stage 14.0 (TID 1058, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@47a55958
15/08/06 17:34:12 INFO Executor: Running task 41.0 in stage 14.0 (TID 1058)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000018_1035/part-00018
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:34:12 INFO TaskSetManager: Finished task 11.0 in stage 14.0 (TID 1028) in 795 ms on localhost (26/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000028_1045' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000028
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000028_1045: Committed
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e524711
15/08/06 17:34:12 INFO Executor: Finished task 28.0 in stage 14.0 (TID 1045). 781 bytes result sent to driver
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO TaskSetManager: Starting task 42.0 in stage 14.0 (TID 1059, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 42.0 in stage 14.0 (TID 1059)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO TaskSetManager: Finished task 28.0 in stage 14.0 (TID 1045) in 406 ms on localhost (27/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000018_1035' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000018
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000018_1035: Committed
15/08/06 17:34:12 INFO Executor: Finished task 18.0 in stage 14.0 (TID 1035). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 43.0 in stage 14.0 (TID 1060, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 43.0 in stage 14.0 (TID 1060)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 18.0 in stage 14.0 (TID 1035) in 454 ms on localhost (28/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@37ad4409
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000030_1047/part-00030
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d26dbf8
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7bc11429
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000033_1050/part-00033
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4112cf38
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000029_1046/part-00029
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f3d2fdd
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000035_1052/part-00035
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2a53c89e
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000032_1049/part-00032
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@510cad39
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@147d38d4
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000036_1053/part-00036
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000030_1047' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000030
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000030_1047: Committed
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6cac7978
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000031_1048/part-00031
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO Executor: Finished task 30.0 in stage 14.0 (TID 1047). 781 bytes result sent to driver
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@54484d4b
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000034_1051/part-00034
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO TaskSetManager: Starting task 44.0 in stage 14.0 (TID 1061, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 44.0 in stage 14.0 (TID 1061)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4eddc9
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO TaskSetManager: Finished task 30.0 in stage 14.0 (TID 1047) in 188 ms on localhost (29/200)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f0ea88f
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@ae7462b
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d695ec5
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f65c2da
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e17d93d
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6fe5abbc
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000039_1056/part-00039
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@cfd79f6
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000040_1057/part-00040
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d9af997
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000038_1055/part-00038
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000033_1050' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000033
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000033_1050: Committed
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO Executor: Finished task 33.0 in stage 14.0 (TID 1050). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 45.0 in stage 14.0 (TID 1062, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 45.0 in stage 14.0 (TID 1062)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 33.0 in stage 14.0 (TID 1050) in 196 ms on localhost (30/200)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72a3d639
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6edbe104
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000037_1054/part-00037
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000035_1052' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000035
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000035_1052: Committed
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000034_1051' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000034
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000034_1051: Committed
15/08/06 17:34:12 INFO Executor: Finished task 34.0 in stage 14.0 (TID 1051). 781 bytes result sent to driver
15/08/06 17:34:12 INFO Executor: Finished task 35.0 in stage 14.0 (TID 1052). 781 bytes result sent to driver
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@314b8a13
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO TaskSetManager: Starting task 46.0 in stage 14.0 (TID 1063, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 47.0 in stage 14.0 (TID 1064, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 47.0 in stage 14.0 (TID 1064)
15/08/06 17:34:12 INFO Executor: Running task 46.0 in stage 14.0 (TID 1063)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO TaskSetManager: Finished task 35.0 in stage 14.0 (TID 1052) in 197 ms on localhost (31/200)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 34.0 in stage 14.0 (TID 1051) in 200 ms on localhost (32/200)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@839c8dd
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000042_1059/part-00042
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@22ab9b40
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2dbe43d
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000040_1057' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000040
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000040_1057: Committed
15/08/06 17:34:12 INFO Executor: Finished task 40.0 in stage 14.0 (TID 1057). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 48.0 in stage 14.0 (TID 1065, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 48.0 in stage 14.0 (TID 1065)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO TaskSetManager: Finished task 40.0 in stage 14.0 (TID 1057) in 177 ms on localhost (33/200)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c895021
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000043_1060/part-00043
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000038_1055' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000038
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000038_1055: Committed
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@436a004c
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO Executor: Finished task 38.0 in stage 14.0 (TID 1055). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 49.0 in stage 14.0 (TID 1066, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 49.0 in stage 14.0 (TID 1066)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO TaskSetManager: Finished task 38.0 in stage 14.0 (TID 1055) in 199 ms on localhost (34/200)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69ee4d85
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000041_1058/part-00041
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@282f21ee
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000037_1054' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000037
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000037_1054: Committed
15/08/06 17:34:12 INFO Executor: Finished task 37.0 in stage 14.0 (TID 1054). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 50.0 in stage 14.0 (TID 1067, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 50.0 in stage 14.0 (TID 1067)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ce0f65d
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO TaskSetManager: Finished task 37.0 in stage 14.0 (TID 1054) in 413 ms on localhost (35/200)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000042_1059' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000042
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000042_1059: Committed
15/08/06 17:34:12 INFO Executor: Finished task 42.0 in stage 14.0 (TID 1059). 781 bytes result sent to driver
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000027_1044' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000027
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000027_1044: Committed
15/08/06 17:34:12 INFO TaskSetManager: Starting task 51.0 in stage 14.0 (TID 1068, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 51.0 in stage 14.0 (TID 1068)
15/08/06 17:34:12 INFO Executor: Finished task 27.0 in stage 14.0 (TID 1044). 781 bytes result sent to driver
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO TaskSetManager: Finished task 42.0 in stage 14.0 (TID 1059) in 394 ms on localhost (36/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO TaskSetManager: Starting task 52.0 in stage 14.0 (TID 1069, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 52.0 in stage 14.0 (TID 1069)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 27.0 in stage 14.0 (TID 1044) in 803 ms on localhost (37/200)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000043_1060' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000043
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000043_1060: Committed
15/08/06 17:34:12 INFO Executor: Finished task 43.0 in stage 14.0 (TID 1060). 781 bytes result sent to driver
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO TaskSetManager: Starting task 53.0 in stage 14.0 (TID 1070, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 53.0 in stage 14.0 (TID 1070)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 43.0 in stage 14.0 (TID 1060) in 371 ms on localhost (38/200)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000041_1058' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000041
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000041_1058: Committed
15/08/06 17:34:12 INFO Executor: Finished task 41.0 in stage 14.0 (TID 1058). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 54.0 in stage 14.0 (TID 1071, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 54.0 in stage 14.0 (TID 1071)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 41.0 in stage 14.0 (TID 1058) in 421 ms on localhost (39/200)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5fede0c9
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000044_1061/part-00044
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@156a880c
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d31b36e
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000045_1062/part-00045
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000044_1061' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000044
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000044_1061: Committed
15/08/06 17:34:12 INFO Executor: Finished task 44.0 in stage 14.0 (TID 1061). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 55.0 in stage 14.0 (TID 1072, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 55.0 in stage 14.0 (TID 1072)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 44.0 in stage 14.0 (TID 1061) in 363 ms on localhost (40/200)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2c0051f3
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1caddf83
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000047_1064/part-00047
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7e6ca7f0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000046_1063/part-00046
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@614e6f5d
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34147eeb
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000045_1062' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000045
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000045_1062: Committed
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO Executor: Finished task 45.0 in stage 14.0 (TID 1062). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 56.0 in stage 14.0 (TID 1073, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 56.0 in stage 14.0 (TID 1073)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 45.0 in stage 14.0 (TID 1062) in 359 ms on localhost (41/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78cd1dfd
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000048_1065/part-00048
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34e49e92
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000049_1066/part-00049
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000047_1064' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000047
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000047_1064: Committed
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000046_1063' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000046
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000046_1063: Committed
15/08/06 17:34:12 INFO Executor: Finished task 47.0 in stage 14.0 (TID 1064). 781 bytes result sent to driver
15/08/06 17:34:12 INFO Executor: Finished task 46.0 in stage 14.0 (TID 1063). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 57.0 in stage 14.0 (TID 1074, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 57.0 in stage 14.0 (TID 1074)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 58.0 in stage 14.0 (TID 1075, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 58.0 in stage 14.0 (TID 1075)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 47.0 in stage 14.0 (TID 1064) in 363 ms on localhost (42/200)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 46.0 in stage 14.0 (TID 1063) in 365 ms on localhost (43/200)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72a2b8fd
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4babac23
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000032_1049' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000032
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000032_1049: Committed
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO Executor: Finished task 32.0 in stage 14.0 (TID 1049). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 59.0 in stage 14.0 (TID 1076, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 59.0 in stage 14.0 (TID 1076)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 32.0 in stage 14.0 (TID 1049) in 594 ms on localhost (44/200)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000029_1046' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000029
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000029_1046: Committed
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000036_1053' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000036
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000036_1053: Committed
15/08/06 17:34:12 INFO Executor: Finished task 29.0 in stage 14.0 (TID 1046). 781 bytes result sent to driver
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53fbde18
15/08/06 17:34:12 INFO TaskSetManager: Starting task 60.0 in stage 14.0 (TID 1077, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000051_1068/part-00051
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000031_1048' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000031
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000031_1048: Committed
15/08/06 17:34:12 INFO Executor: Finished task 36.0 in stage 14.0 (TID 1053). 781 bytes result sent to driver
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ed98aaf
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000050_1067/part-00050
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO TaskSetManager: Finished task 29.0 in stage 14.0 (TID 1046) in 631 ms on localhost (45/200)
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO Executor: Finished task 31.0 in stage 14.0 (TID 1048). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 61.0 in stage 14.0 (TID 1078, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000049_1066' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000049
15/08/06 17:34:12 INFO Executor: Running task 60.0 in stage 14.0 (TID 1077)
15/08/06 17:34:12 INFO Executor: Running task 61.0 in stage 14.0 (TID 1078)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 62.0 in stage 14.0 (TID 1079, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 62.0 in stage 14.0 (TID 1079)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 31.0 in stage 14.0 (TID 1048) in 604 ms on localhost (46/200)
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000049_1066: Committed
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@9754d51
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000048_1065' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000048
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO TaskSetManager: Finished task 36.0 in stage 14.0 (TID 1053) in 575 ms on localhost (47/200)
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000048_1065: Committed
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000052_1069/part-00052
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO Executor: Finished task 48.0 in stage 14.0 (TID 1065). 781 bytes result sent to driver
15/08/06 17:34:12 INFO Executor: Finished task 49.0 in stage 14.0 (TID 1066). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 63.0 in stage 14.0 (TID 1080, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 63.0 in stage 14.0 (TID 1080)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 48.0 in stage 14.0 (TID 1065) in 376 ms on localhost (48/200)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 64.0 in stage 14.0 (TID 1081, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO Executor: Running task 64.0 in stage 14.0 (TID 1081)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO TaskSetManager: Finished task 49.0 in stage 14.0 (TID 1066) in 365 ms on localhost (49/200)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@41dd101a
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000053_1070/part-00053
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@43b4290f
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78e3e2dd
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000054_1071/part-00054
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64673e78
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2eddf19f
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000039_1056' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000039
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000039_1056: Committed
15/08/06 17:34:12 INFO Executor: Finished task 39.0 in stage 14.0 (TID 1056). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 65.0 in stage 14.0 (TID 1082, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 65.0 in stage 14.0 (TID 1082)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 39.0 in stage 14.0 (TID 1056) in 578 ms on localhost (50/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@24db4c39
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e36c4cc
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000052_1069' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000052
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000052_1069: Committed
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000050_1067' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000050
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000050_1067: Committed
15/08/06 17:34:12 INFO Executor: Finished task 52.0 in stage 14.0 (TID 1069). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 66.0 in stage 14.0 (TID 1083, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Finished task 50.0 in stage 14.0 (TID 1067). 781 bytes result sent to driver
15/08/06 17:34:12 INFO Executor: Running task 66.0 in stage 14.0 (TID 1083)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 67.0 in stage 14.0 (TID 1084, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 67.0 in stage 14.0 (TID 1084)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a4b50a4
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000055_1072/part-00055
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000051_1068' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000051
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000051_1068: Committed
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO TaskSetManager: Finished task 52.0 in stage 14.0 (TID 1069) in 182 ms on localhost (51/200)
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO Executor: Finished task 51.0 in stage 14.0 (TID 1068). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Finished task 50.0 in stage 14.0 (TID 1067) in 200 ms on localhost (52/200)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 68.0 in stage 14.0 (TID 1085, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 68.0 in stage 14.0 (TID 1085)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 51.0 in stage 14.0 (TID 1068) in 190 ms on localhost (53/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000054_1071' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000054
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000054_1071: Committed
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000053_1070' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000053
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@566d0a07
15/08/06 17:34:12 INFO Executor: Finished task 54.0 in stage 14.0 (TID 1071). 781 bytes result sent to driver
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000053_1070: Committed
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO TaskSetManager: Starting task 69.0 in stage 14.0 (TID 1086, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO Executor: Running task 69.0 in stage 14.0 (TID 1086)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO Executor: Finished task 53.0 in stage 14.0 (TID 1070). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Finished task 54.0 in stage 14.0 (TID 1071) in 188 ms on localhost (54/200)
15/08/06 17:34:12 INFO TaskSetManager: Starting task 70.0 in stage 14.0 (TID 1087, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 70.0 in stage 14.0 (TID 1087)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 53.0 in stage 14.0 (TID 1070) in 195 ms on localhost (55/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@760911bc
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000057_1074/part-00057
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@270f96
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000056_1073/part-00056
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000055_1072' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000055
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000055_1072: Committed
15/08/06 17:34:12 INFO Executor: Finished task 55.0 in stage 14.0 (TID 1072). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 71.0 in stage 14.0 (TID 1088, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 71.0 in stage 14.0 (TID 1088)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 55.0 in stage 14.0 (TID 1072) in 184 ms on localhost (56/200)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@55654bec
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@781a6e20
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000059_1076/part-00059
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@666e17e8
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@13ec2caa
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000058_1075/part-00058
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@56e463b4
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4bee2e6a
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000062_1079/part-00062
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000057_1074' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000057
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000057_1074: Committed
15/08/06 17:34:12 INFO Executor: Finished task 57.0 in stage 14.0 (TID 1074). 781 bytes result sent to driver
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@64b6b1e9
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000064_1081/part-00064
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO TaskSetManager: Starting task 72.0 in stage 14.0 (TID 1089, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 72.0 in stage 14.0 (TID 1089)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@780de522
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO TaskSetManager: Finished task 57.0 in stage 14.0 (TID 1074) in 170 ms on localhost (57/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3587a28e
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000061_1078/part-00061
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6e9b68d0
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000056_1073' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000056
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000056_1073: Committed
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@46f5298f
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO Executor: Finished task 56.0 in stage 14.0 (TID 1073). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 73.0 in stage 14.0 (TID 1090, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 73.0 in stage 14.0 (TID 1090)
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@294398db
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO TaskSetManager: Finished task 56.0 in stage 14.0 (TID 1073) in 300 ms on localhost (58/200)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25c4074a
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000063_1080/part-00063
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000059_1076' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000059
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000059_1076: Committed
15/08/06 17:34:12 INFO Executor: Finished task 59.0 in stage 14.0 (TID 1076). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 74.0 in stage 14.0 (TID 1091, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 74.0 in stage 14.0 (TID 1091)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 59.0 in stage 14.0 (TID 1076) in 266 ms on localhost (59/200)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c840c3f
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000067_1084/part-00067
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@723b36b1
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000060_1077/part-00060
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6bc79bf7
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000065_1082/part-00065
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000061_1078' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000061
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000061_1078: Committed
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000058_1075' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000058
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000058_1075: Committed
15/08/06 17:34:12 INFO Executor: Finished task 61.0 in stage 14.0 (TID 1078). 781 bytes result sent to driver
15/08/06 17:34:12 INFO Executor: Finished task 58.0 in stage 14.0 (TID 1075). 781 bytes result sent to driver
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12c648ae
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO TaskSetManager: Starting task 75.0 in stage 14.0 (TID 1092, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53e5f2ea
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000066_1083/part-00066
15/08/06 17:34:12 INFO Executor: Running task 75.0 in stage 14.0 (TID 1092)
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@385bd621
15/08/06 17:34:12 INFO TaskSetManager: Finished task 61.0 in stage 14.0 (TID 1078) in 279 ms on localhost (60/200)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15e36df9
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO TaskSetManager: Starting task 76.0 in stage 14.0 (TID 1093, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 76.0 in stage 14.0 (TID 1093)
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO TaskSetManager: Finished task 58.0 in stage 14.0 (TID 1075) in 314 ms on localhost (61/200)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000064_1081' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000064
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000064_1081: Committed
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@69347f41
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO Executor: Finished task 64.0 in stage 14.0 (TID 1081). 781 bytes result sent to driver
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO TaskSetManager: Starting task 77.0 in stage 14.0 (TID 1094, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO Executor: Running task 77.0 in stage 14.0 (TID 1094)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 64.0 in stage 14.0 (TID 1081) in 282 ms on localhost (62/200)
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@45167d3f
15/08/06 17:34:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000060_1077' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000060
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000060_1077: Committed
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c558db0
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000068_1085/part-00068
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO Executor: Finished task 60.0 in stage 14.0 (TID 1077). 781 bytes result sent to driver
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1f75ad85
15/08/06 17:34:12 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000070_1087/part-00070
15/08/06 17:34:12 INFO TaskSetManager: Starting task 78.0 in stage 14.0 (TID 1095, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:12 INFO CodecConfig: Compression set to false
15/08/06 17:34:12 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:12 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:12 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:12 INFO Executor: Running task 78.0 in stage 14.0 (TID 1095)
15/08/06 17:34:12 INFO TaskSetManager: Finished task 60.0 in stage 14.0 (TID 1077) in 305 ms on localhost (63/200)
15/08/06 17:34:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000066_1083' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000066
15/08/06 17:34:12 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000066_1083: Committed
15/08/06 17:34:12 INFO Executor: Finished task 66.0 in stage 14.0 (TID 1083). 781 bytes result sent to driver
15/08/06 17:34:12 INFO TaskSetManager: Starting task 79.0 in stage 14.0 (TID 1096, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 79.0 in stage 14.0 (TID 1096)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a1470e6
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000069_1086/part-00069
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO TaskSetManager: Finished task 66.0 in stage 14.0 (TID 1083) in 262 ms on localhost (64/200)
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@201a6e9
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d9c6ec0
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35ed8feb
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000070_1087' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000070
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000070_1087: Committed
15/08/06 17:34:13 INFO Executor: Finished task 70.0 in stage 14.0 (TID 1087). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO TaskSetManager: Starting task 80.0 in stage 14.0 (TID 1097, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 80.0 in stage 14.0 (TID 1097)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000068_1085' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000068
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000068_1085: Committed
15/08/06 17:34:13 INFO TaskSetManager: Finished task 70.0 in stage 14.0 (TID 1087) in 280 ms on localhost (65/200)
15/08/06 17:34:13 INFO Executor: Finished task 68.0 in stage 14.0 (TID 1085). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 81.0 in stage 14.0 (TID 1098, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 81.0 in stage 14.0 (TID 1098)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@eee2993
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000071_1088/part-00071
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO TaskSetManager: Finished task 68.0 in stage 14.0 (TID 1085) in 294 ms on localhost (66/200)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b16e0c5
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31e5028e
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000072_1089/part-00072
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000071_1088' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000071
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000071_1088: Committed
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@646fb5d9
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000074_1091/part-00074
15/08/06 17:34:13 INFO Executor: Finished task 71.0 in stage 14.0 (TID 1088). 781 bytes result sent to driver
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO TaskSetManager: Starting task 82.0 in stage 14.0 (TID 1099, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 82.0 in stage 14.0 (TID 1099)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d166610
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000073_1090/part-00073
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO TaskSetManager: Finished task 71.0 in stage 14.0 (TID 1088) in 281 ms on localhost (67/200)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@af91b65
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@290da81f
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@576047aa
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25a548f8
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000076_1093/part-00076
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5fa2339d
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000075_1092/part-00075
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000072_1089' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000072
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000072_1089: Committed
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000073_1090' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000073
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000073_1090: Committed
15/08/06 17:34:13 INFO Executor: Finished task 72.0 in stage 14.0 (TID 1089). 781 bytes result sent to driver
15/08/06 17:34:13 INFO Executor: Finished task 73.0 in stage 14.0 (TID 1090). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 83.0 in stage 14.0 (TID 1100, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000074_1091' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000074
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000074_1091: Committed
15/08/06 17:34:13 INFO Executor: Running task 83.0 in stage 14.0 (TID 1100)
15/08/06 17:34:13 INFO Executor: Finished task 74.0 in stage 14.0 (TID 1091). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Finished task 72.0 in stage 14.0 (TID 1089) in 292 ms on localhost (68/200)
15/08/06 17:34:13 INFO TaskSetManager: Starting task 84.0 in stage 14.0 (TID 1101, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 84.0 in stage 14.0 (TID 1101)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 73.0 in stage 14.0 (TID 1090) in 179 ms on localhost (69/200)
15/08/06 17:34:13 INFO TaskSetManager: Starting task 85.0 in stage 14.0 (TID 1102, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 85.0 in stage 14.0 (TID 1102)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO TaskSetManager: Finished task 74.0 in stage 14.0 (TID 1091) in 173 ms on localhost (70/200)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66d5fe70
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b339fce
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000077_1094/part-00077
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7be37ad5
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@200ed49e
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000078_1095/part-00078
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3bd0f352
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6376d552
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000079_1096/part-00079
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000076_1093' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000076
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000076_1093: Committed
15/08/06 17:34:13 INFO Executor: Finished task 76.0 in stage 14.0 (TID 1093). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 86.0 in stage 14.0 (TID 1103, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO Executor: Running task 86.0 in stage 14.0 (TID 1103)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5e516bd6
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO TaskSetManager: Finished task 76.0 in stage 14.0 (TID 1093) in 180 ms on localhost (71/200)
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@91b57ce
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000075_1092' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000075
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000075_1092: Committed
15/08/06 17:34:13 INFO Executor: Finished task 75.0 in stage 14.0 (TID 1092). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO TaskSetManager: Starting task 87.0 in stage 14.0 (TID 1104, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 87.0 in stage 14.0 (TID 1104)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 75.0 in stage 14.0 (TID 1092) in 193 ms on localhost (72/200)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@610e9216
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000081_1098/part-00081
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000077_1094' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000077
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000077_1094: Committed
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@258d3fb8
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000080_1097/part-00080
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO Executor: Finished task 77.0 in stage 14.0 (TID 1094). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 88.0 in stage 14.0 (TID 1105, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 88.0 in stage 14.0 (TID 1105)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 77.0 in stage 14.0 (TID 1094) in 199 ms on localhost (73/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000078_1095' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000078
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000078_1095: Committed
15/08/06 17:34:13 INFO Executor: Finished task 78.0 in stage 14.0 (TID 1095). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14f74c89
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO TaskSetManager: Starting task 89.0 in stage 14.0 (TID 1106, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 89.0 in stage 14.0 (TID 1106)
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO TaskSetManager: Finished task 78.0 in stage 14.0 (TID 1095) in 198 ms on localhost (74/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000079_1096' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000079
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000079_1096: Committed
15/08/06 17:34:13 INFO Executor: Finished task 79.0 in stage 14.0 (TID 1096). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 90.0 in stage 14.0 (TID 1107, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 90.0 in stage 14.0 (TID 1107)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ad870cb
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO TaskSetManager: Finished task 79.0 in stage 14.0 (TID 1096) in 195 ms on localhost (75/200)
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000081_1098' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000081
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000081_1098: Committed
15/08/06 17:34:13 INFO Executor: Finished task 81.0 in stage 14.0 (TID 1098). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 91.0 in stage 14.0 (TID 1108, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 91.0 in stage 14.0 (TID 1108)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 81.0 in stage 14.0 (TID 1098) in 180 ms on localhost (76/200)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2f12bda4
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000082_1099/part-00082
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7c54b7c5
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c6c5051
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000084_1101/part-00084
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b6825b7
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000083_1100/part-00083
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000082_1099' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000082
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000082_1099: Committed
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b2a3f12
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO Executor: Finished task 82.0 in stage 14.0 (TID 1099). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO TaskSetManager: Starting task 92.0 in stage 14.0 (TID 1109, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 92.0 in stage 14.0 (TID 1109)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 82.0 in stage 14.0 (TID 1099) in 277 ms on localhost (77/200)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1291d6a1
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000085_1102/part-00085
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ff10
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000062_1079' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000062
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000062_1079: Committed
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO Executor: Finished task 62.0 in stage 14.0 (TID 1079). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 93.0 in stage 14.0 (TID 1110, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 93.0 in stage 14.0 (TID 1110)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 62.0 in stage 14.0 (TID 1079) in 672 ms on localhost (78/200)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d42e65e
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000084_1101' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000084
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000084_1101: Committed
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO Executor: Finished task 84.0 in stage 14.0 (TID 1101). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 94.0 in stage 14.0 (TID 1111, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 94.0 in stage 14.0 (TID 1111)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 84.0 in stage 14.0 (TID 1101) in 255 ms on localhost (79/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000083_1100' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000083
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000083_1100: Committed
15/08/06 17:34:13 INFO Executor: Finished task 83.0 in stage 14.0 (TID 1100). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 95.0 in stage 14.0 (TID 1112, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 95.0 in stage 14.0 (TID 1112)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 83.0 in stage 14.0 (TID 1100) in 269 ms on localhost (80/200)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2b3b1530
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@33ec187c
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000088_1105/part-00088
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000086_1103/part-00086
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6161b1aa
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000087_1104/part-00087
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000085_1102' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000085
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000085_1102: Committed
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO Executor: Finished task 85.0 in stage 14.0 (TID 1102). 781 bytes result sent to driver
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000067_1084' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000067
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000067_1084: Committed
15/08/06 17:34:13 INFO TaskSetManager: Starting task 96.0 in stage 14.0 (TID 1113, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Finished task 67.0 in stage 14.0 (TID 1084). 781 bytes result sent to driver
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000063_1080' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000063
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000063_1080: Committed
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000065_1082' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000065
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000065_1082: Committed
15/08/06 17:34:13 INFO TaskSetManager: Finished task 85.0 in stage 14.0 (TID 1102) in 278 ms on localhost (81/200)
15/08/06 17:34:13 INFO Executor: Finished task 63.0 in stage 14.0 (TID 1080). 781 bytes result sent to driver
15/08/06 17:34:13 INFO Executor: Running task 96.0 in stage 14.0 (TID 1113)
15/08/06 17:34:13 INFO Executor: Finished task 65.0 in stage 14.0 (TID 1082). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 97.0 in stage 14.0 (TID 1114, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 97.0 in stage 14.0 (TID 1114)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 67.0 in stage 14.0 (TID 1084) in 662 ms on localhost (82/200)
15/08/06 17:34:13 INFO TaskSetManager: Starting task 98.0 in stage 14.0 (TID 1115, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 98.0 in stage 14.0 (TID 1115)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 63.0 in stage 14.0 (TID 1080) in 708 ms on localhost (83/200)
15/08/06 17:34:13 INFO TaskSetManager: Starting task 99.0 in stage 14.0 (TID 1116, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 99.0 in stage 14.0 (TID 1116)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@161d2a83
15/08/06 17:34:13 INFO TaskSetManager: Finished task 65.0 in stage 14.0 (TID 1082) in 685 ms on localhost (84/200)
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@354c176e
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@bd6e963
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1489ed81
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000090_1107/part-00090
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@15f09246
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000089_1106/part-00089
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000086_1103' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000086
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000086_1103: Committed
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000088_1105' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000088
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000088_1105: Committed
15/08/06 17:34:13 INFO Executor: Finished task 86.0 in stage 14.0 (TID 1103). 781 bytes result sent to driver
15/08/06 17:34:13 INFO Executor: Finished task 88.0 in stage 14.0 (TID 1105). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7924f043
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO TaskSetManager: Starting task 100.0 in stage 14.0 (TID 1117, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO Executor: Running task 100.0 in stage 14.0 (TID 1117)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 86.0 in stage 14.0 (TID 1103) in 277 ms on localhost (85/200)
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO TaskSetManager: Starting task 101.0 in stage 14.0 (TID 1118, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 101.0 in stage 14.0 (TID 1118)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 88.0 in stage 14.0 (TID 1105) in 255 ms on localhost (86/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000087_1104' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000087
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000087_1104: Committed
15/08/06 17:34:13 INFO Executor: Finished task 87.0 in stage 14.0 (TID 1104). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 102.0 in stage 14.0 (TID 1119, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 102.0 in stage 14.0 (TID 1119)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO TaskSetManager: Finished task 87.0 in stage 14.0 (TID 1104) in 272 ms on localhost (87/200)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42886342
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000069_1086' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000069
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000069_1086: Committed
15/08/06 17:34:13 INFO Executor: Finished task 69.0 in stage 14.0 (TID 1086). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@774d2fb5
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000091_1108/part-00091
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO TaskSetManager: Starting task 103.0 in stage 14.0 (TID 1120, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 103.0 in stage 14.0 (TID 1120)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 69.0 in stage 14.0 (TID 1086) in 692 ms on localhost (88/200)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17639837
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000089_1106' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000089
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000089_1106: Committed
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000090_1107' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000090
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000090_1107: Committed
15/08/06 17:34:13 INFO Executor: Finished task 89.0 in stage 14.0 (TID 1106). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 104.0 in stage 14.0 (TID 1121, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Finished task 90.0 in stage 14.0 (TID 1107). 781 bytes result sent to driver
15/08/06 17:34:13 INFO Executor: Running task 104.0 in stage 14.0 (TID 1121)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 89.0 in stage 14.0 (TID 1106) in 276 ms on localhost (89/200)
15/08/06 17:34:13 INFO TaskSetManager: Starting task 105.0 in stage 14.0 (TID 1122, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 105.0 in stage 14.0 (TID 1122)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 90.0 in stage 14.0 (TID 1107) in 274 ms on localhost (90/200)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000091_1108' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000091
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000091_1108: Committed
15/08/06 17:34:13 INFO Executor: Finished task 91.0 in stage 14.0 (TID 1108). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 106.0 in stage 14.0 (TID 1123, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 106.0 in stage 14.0 (TID 1123)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 91.0 in stage 14.0 (TID 1108) in 268 ms on localhost (91/200)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ef3daf8
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000093_1110/part-00093
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@9a9df44
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000092_1109/part-00092
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6f80ee50
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2806fc06
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000094_1111/part-00094
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@628c0271
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000096_1113/part-00096
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@21158993
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000099_1116/part-00099
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14c50241
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000098_1115/part-00098
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@22c85934
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f5876c3
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78dd0413
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000093_1110' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000093
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000093_1110: Committed
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO Executor: Finished task 93.0 in stage 14.0 (TID 1110). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 107.0 in stage 14.0 (TID 1124, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 107.0 in stage 14.0 (TID 1124)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@607d79dc
15/08/06 17:34:13 INFO TaskSetManager: Finished task 93.0 in stage 14.0 (TID 1110) in 167 ms on localhost (92/200)
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1f95e84a
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000095_1112/part-00095
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@62e75fea
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5ba0922
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000097_1114/part-00097
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4da22790
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000100_1117/part-00100
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000094_1111' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000094
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000094_1111: Committed
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000096_1113' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000096
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000096_1113: Committed
15/08/06 17:34:13 INFO Executor: Finished task 94.0 in stage 14.0 (TID 1111). 781 bytes result sent to driver
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a2fd8e4
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000101_1118/part-00101
15/08/06 17:34:13 INFO TaskSetManager: Starting task 108.0 in stage 14.0 (TID 1125, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO Executor: Running task 108.0 in stage 14.0 (TID 1125)
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO Executor: Finished task 96.0 in stage 14.0 (TID 1113). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d8d7a16
15/08/06 17:34:13 INFO TaskSetManager: Finished task 94.0 in stage 14.0 (TID 1111) in 184 ms on localhost (93/200)
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO TaskSetManager: Starting task 109.0 in stage 14.0 (TID 1126, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 109.0 in stage 14.0 (TID 1126)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@643ad77a
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5318bdfe
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000102_1119/part-00102
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO TaskSetManager: Finished task 96.0 in stage 14.0 (TID 1113) in 161 ms on localhost (94/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000092_1109' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000092
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000092_1109: Committed
15/08/06 17:34:13 INFO Executor: Finished task 92.0 in stage 14.0 (TID 1109). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 110.0 in stage 14.0 (TID 1127, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 110.0 in stage 14.0 (TID 1127)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 92.0 in stage 14.0 (TID 1109) in 210 ms on localhost (95/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000098_1115' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000098
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000098_1115: Committed
15/08/06 17:34:13 INFO Executor: Finished task 98.0 in stage 14.0 (TID 1115). 781 bytes result sent to driver
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22fabfac
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@74e8154e
15/08/06 17:34:13 INFO TaskSetManager: Starting task 111.0 in stage 14.0 (TID 1128, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000103_1120/part-00103
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO Executor: Running task 111.0 in stage 14.0 (TID 1128)
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO TaskSetManager: Finished task 98.0 in stage 14.0 (TID 1115) in 171 ms on localhost (96/200)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d7abbe
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5aa7055e
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000097_1114' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000097
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000097_1114: Committed
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@136149fc
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000095_1112' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000095
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000095_1112: Committed
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000104_1121/part-00104
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d7d7248
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO Executor: Finished task 95.0 in stage 14.0 (TID 1112). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO Executor: Finished task 97.0 in stage 14.0 (TID 1114). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 112.0 in stage 14.0 (TID 1129, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 112.0 in stage 14.0 (TID 1129)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e9284e0
15/08/06 17:34:13 INFO TaskSetManager: Finished task 95.0 in stage 14.0 (TID 1112) in 330 ms on localhost (97/200)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000105_1122/part-00105
15/08/06 17:34:13 INFO TaskSetManager: Starting task 113.0 in stage 14.0 (TID 1130, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO Executor: Running task 113.0 in stage 14.0 (TID 1130)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 97.0 in stage 14.0 (TID 1114) in 317 ms on localhost (98/200)
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000080_1097' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000080
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000080_1097: Committed
15/08/06 17:34:13 INFO Executor: Finished task 80.0 in stage 14.0 (TID 1097). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 114.0 in stage 14.0 (TID 1131, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 114.0 in stage 14.0 (TID 1131)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000101_1118' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000101
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000101_1118: Committed
15/08/06 17:34:13 INFO TaskSetManager: Finished task 80.0 in stage 14.0 (TID 1097) in 688 ms on localhost (99/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000100_1117' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000100
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000100_1117: Committed
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO Executor: Finished task 101.0 in stage 14.0 (TID 1118). 781 bytes result sent to driver
15/08/06 17:34:13 INFO Executor: Finished task 100.0 in stage 14.0 (TID 1117). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 115.0 in stage 14.0 (TID 1132, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 115.0 in stage 14.0 (TID 1132)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 101.0 in stage 14.0 (TID 1118) in 296 ms on localhost (100/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000102_1119' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000102
15/08/06 17:34:13 INFO TaskSetManager: Starting task 116.0 in stage 14.0 (TID 1133, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000102_1119: Committed
15/08/06 17:34:13 INFO TaskSetManager: Finished task 100.0 in stage 14.0 (TID 1117) in 299 ms on localhost (101/200)
15/08/06 17:34:13 INFO Executor: Running task 116.0 in stage 14.0 (TID 1133)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@37a82187
15/08/06 17:34:13 INFO Executor: Finished task 102.0 in stage 14.0 (TID 1119). 781 bytes result sent to driver
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO TaskSetManager: Starting task 117.0 in stage 14.0 (TID 1134, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 117.0 in stage 14.0 (TID 1134)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO TaskSetManager: Finished task 102.0 in stage 14.0 (TID 1119) in 300 ms on localhost (102/200)
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a6b8a4c
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c347d87
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000106_1123/part-00106
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000103_1120' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000103
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000103_1120: Committed
15/08/06 17:34:13 INFO Executor: Finished task 103.0 in stage 14.0 (TID 1120). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 118.0 in stage 14.0 (TID 1135, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 118.0 in stage 14.0 (TID 1135)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 103.0 in stage 14.0 (TID 1120) in 300 ms on localhost (103/200)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6aa77c72
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000106_1123' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000106
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000106_1123: Committed
15/08/06 17:34:13 INFO Executor: Finished task 106.0 in stage 14.0 (TID 1123). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO TaskSetManager: Starting task 119.0 in stage 14.0 (TID 1136, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 119.0 in stage 14.0 (TID 1136)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 106.0 in stage 14.0 (TID 1123) in 291 ms on localhost (104/200)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@718b6658
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000107_1124/part-00107
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@15eabefb
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000108_1125/part-00108
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@29f6aeb9
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000110_1127/part-00110
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3572a134
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000109_1126/part-00109
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51ef525c
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000111_1128/part-00111
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f336d27
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1445b2d
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ebde801
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35cab1a0
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12e2d57a
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@38966eb0
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000114_1131/part-00114
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4531dce9
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000115_1132/part-00115
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4cf1cc9e
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000113_1130/part-00113
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000110_1127' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000110
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000110_1127: Committed
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d865cfe
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000112_1129/part-00112
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO Executor: Finished task 110.0 in stage 14.0 (TID 1127). 781 bytes result sent to driver
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000108_1125' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000108
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000108_1125: Committed
15/08/06 17:34:13 INFO TaskSetManager: Starting task 120.0 in stage 14.0 (TID 1137, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 120.0 in stage 14.0 (TID 1137)
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3b106825
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO TaskSetManager: Finished task 110.0 in stage 14.0 (TID 1127) in 301 ms on localhost (105/200)
15/08/06 17:34:13 INFO Executor: Finished task 108.0 in stage 14.0 (TID 1125). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 121.0 in stage 14.0 (TID 1138, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 121.0 in stage 14.0 (TID 1138)
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d2257a5
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000116_1133/part-00116
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO TaskSetManager: Finished task 108.0 in stage 14.0 (TID 1125) in 310 ms on localhost (106/200)
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6e688311
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@f7509b8
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000117_1134/part-00117
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000118_1135/part-00118
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b68b5c8
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@353db862
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@25c7f94a
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1e4367f
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@785818aa
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5233c70
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000114_1131' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000114
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000114_1131: Committed
15/08/06 17:34:13 INFO Executor: Finished task 114.0 in stage 14.0 (TID 1131). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 122.0 in stage 14.0 (TID 1139, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 122.0 in stage 14.0 (TID 1139)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 114.0 in stage 14.0 (TID 1131) in 175 ms on localhost (107/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000115_1132' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000115
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000115_1132: Committed
15/08/06 17:34:13 INFO Executor: Finished task 115.0 in stage 14.0 (TID 1132). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 123.0 in stage 14.0 (TID 1140, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 123.0 in stage 14.0 (TID 1140)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 115.0 in stage 14.0 (TID 1132) in 177 ms on localhost (108/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000113_1130' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000113
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000113_1130: Committed
15/08/06 17:34:13 INFO Executor: Finished task 113.0 in stage 14.0 (TID 1130). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO TaskSetManager: Starting task 124.0 in stage 14.0 (TID 1141, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO Executor: Running task 124.0 in stage 14.0 (TID 1141)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO TaskSetManager: Finished task 113.0 in stage 14.0 (TID 1130) in 188 ms on localhost (109/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000117_1134' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000117
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000117_1134: Committed
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42e7906b
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000119_1136/part-00119
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO Executor: Finished task 117.0 in stage 14.0 (TID 1134). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO TaskSetManager: Starting task 125.0 in stage 14.0 (TID 1142, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 125.0 in stage 14.0 (TID 1142)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 117.0 in stage 14.0 (TID 1134) in 175 ms on localhost (110/200)
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000118_1135' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000118
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000118_1135: Committed
15/08/06 17:34:13 INFO Executor: Finished task 118.0 in stage 14.0 (TID 1135). 781 bytes result sent to driver
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@708cd7a1
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000116_1133' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000116
15/08/06 17:34:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:13 INFO TaskSetManager: Starting task 126.0 in stage 14.0 (TID 1143, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000116_1133: Committed
15/08/06 17:34:13 INFO Executor: Running task 126.0 in stage 14.0 (TID 1143)
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:13 INFO TaskSetManager: Finished task 118.0 in stage 14.0 (TID 1135) in 175 ms on localhost (111/200)
15/08/06 17:34:13 INFO Executor: Finished task 116.0 in stage 14.0 (TID 1133). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 127.0 in stage 14.0 (TID 1144, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 127.0 in stage 14.0 (TID 1144)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 116.0 in stage 14.0 (TID 1133) in 193 ms on localhost (112/200)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000119_1136' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000119
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000119_1136: Committed
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO Executor: Finished task 119.0 in stage 14.0 (TID 1136). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 128.0 in stage 14.0 (TID 1145, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 128.0 in stage 14.0 (TID 1145)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 119.0 in stage 14.0 (TID 1136) in 165 ms on localhost (113/200)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000099_1116' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000099
15/08/06 17:34:13 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000099_1116: Committed
15/08/06 17:34:13 INFO Executor: Finished task 99.0 in stage 14.0 (TID 1116). 781 bytes result sent to driver
15/08/06 17:34:13 INFO TaskSetManager: Starting task 129.0 in stage 14.0 (TID 1146, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:13 INFO Executor: Running task 129.0 in stage 14.0 (TID 1146)
15/08/06 17:34:13 INFO TaskSetManager: Finished task 99.0 in stage 14.0 (TID 1116) in 563 ms on localhost (114/200)
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@357a7324
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000120_1137/part-00120
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:13 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d7d5ca1
15/08/06 17:34:13 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000121_1138/part-00121
15/08/06 17:34:13 INFO CodecConfig: Compression set to false
15/08/06 17:34:13 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:13 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:13 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@190c7853
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30ad8f8f
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@52ac0e40
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000122_1139/part-00122
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000121_1138' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000121
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000121_1138: Committed
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34ae1f02
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000123_1140/part-00123
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO Executor: Finished task 121.0 in stage 14.0 (TID 1138). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c04d68c
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO TaskSetManager: Starting task 130.0 in stage 14.0 (TID 1147, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 130.0 in stage 14.0 (TID 1147)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 121.0 in stage 14.0 (TID 1138) in 169 ms on localhost (115/200)
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@341a8f09
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000124_1141/part-00124
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@465d9130
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000127_1144/part-00127
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@157775f
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000126_1143/part-00126
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@155dfea9
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000125_1142/part-00125
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3eba917c
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ba160e2
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76d537e2
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@13f059f0
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000128_1145/part-00128
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000122_1139' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000122
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000122_1139: Committed
15/08/06 17:34:14 INFO Executor: Finished task 122.0 in stage 14.0 (TID 1139). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 131.0 in stage 14.0 (TID 1148, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 131.0 in stage 14.0 (TID 1148)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 122.0 in stage 14.0 (TID 1139) in 247 ms on localhost (116/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7980c72c
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@403d53dd
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000123_1140' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000123
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000123_1140: Committed
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000105_1122' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000105
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000105_1122: Committed
15/08/06 17:34:14 INFO Executor: Finished task 123.0 in stage 14.0 (TID 1140). 781 bytes result sent to driver
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000104_1121' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000104
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000104_1121: Committed
15/08/06 17:34:14 INFO TaskSetManager: Starting task 132.0 in stage 14.0 (TID 1149, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Finished task 105.0 in stage 14.0 (TID 1122). 781 bytes result sent to driver
15/08/06 17:34:14 INFO Executor: Finished task 104.0 in stage 14.0 (TID 1121). 781 bytes result sent to driver
15/08/06 17:34:14 INFO Executor: Running task 132.0 in stage 14.0 (TID 1149)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 123.0 in stage 14.0 (TID 1140) in 261 ms on localhost (117/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 133.0 in stage 14.0 (TID 1150, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 133.0 in stage 14.0 (TID 1150)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 105.0 in stage 14.0 (TID 1122) in 697 ms on localhost (118/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 134.0 in stage 14.0 (TID 1151, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 134.0 in stage 14.0 (TID 1151)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@466944d1
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO TaskSetManager: Finished task 104.0 in stage 14.0 (TID 1121) in 699 ms on localhost (119/200)
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000125_1142' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000125
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000125_1142: Committed
15/08/06 17:34:14 INFO Executor: Finished task 125.0 in stage 14.0 (TID 1142). 781 bytes result sent to driver
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a031813
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000128_1145' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000128
15/08/06 17:34:14 INFO TaskSetManager: Starting task 135.0 in stage 14.0 (TID 1152, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000129_1146/part-00129
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000128_1145: Committed
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000124_1141' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000124
15/08/06 17:34:14 INFO Executor: Running task 135.0 in stage 14.0 (TID 1152)
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000124_1141: Committed
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO Executor: Finished task 124.0 in stage 14.0 (TID 1141). 781 bytes result sent to driver
15/08/06 17:34:14 INFO Executor: Finished task 128.0 in stage 14.0 (TID 1145). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Finished task 125.0 in stage 14.0 (TID 1142) in 279 ms on localhost (120/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 136.0 in stage 14.0 (TID 1153, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000127_1144' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000127
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000127_1144: Committed
15/08/06 17:34:14 INFO Executor: Running task 136.0 in stage 14.0 (TID 1153)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 124.0 in stage 14.0 (TID 1141) in 285 ms on localhost (121/200)
15/08/06 17:34:14 INFO Executor: Finished task 127.0 in stage 14.0 (TID 1144). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 137.0 in stage 14.0 (TID 1154, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 137.0 in stage 14.0 (TID 1154)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 128.0 in stage 14.0 (TID 1145) in 251 ms on localhost (122/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 138.0 in stage 14.0 (TID 1155, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 138.0 in stage 14.0 (TID 1155)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 127.0 in stage 14.0 (TID 1144) in 273 ms on localhost (123/200)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76594fb2
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000129_1146' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000129
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000129_1146: Committed
15/08/06 17:34:14 INFO Executor: Finished task 129.0 in stage 14.0 (TID 1146). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO TaskSetManager: Starting task 139.0 in stage 14.0 (TID 1156, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 139.0 in stage 14.0 (TID 1156)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO TaskSetManager: Finished task 129.0 in stage 14.0 (TID 1146) in 252 ms on localhost (124/200)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@189d09f1
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000130_1147/part-00130
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000107_1124' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000107
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000107_1124: Committed
15/08/06 17:34:14 INFO Executor: Finished task 107.0 in stage 14.0 (TID 1124). 781 bytes result sent to driver
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@453f7ddd
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000131_1148/part-00131
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO TaskSetManager: Starting task 140.0 in stage 14.0 (TID 1157, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO Executor: Running task 140.0 in stage 14.0 (TID 1157)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 107.0 in stage 14.0 (TID 1124) in 737 ms on localhost (125/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@554b9a7
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000109_1126' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000109
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000109_1126: Committed
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000111_1128' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000111
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000111_1128: Committed
15/08/06 17:34:14 INFO Executor: Finished task 109.0 in stage 14.0 (TID 1126). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 141.0 in stage 14.0 (TID 1158, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Finished task 111.0 in stage 14.0 (TID 1128). 781 bytes result sent to driver
15/08/06 17:34:14 INFO Executor: Running task 141.0 in stage 14.0 (TID 1158)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 109.0 in stage 14.0 (TID 1126) in 713 ms on localhost (126/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 142.0 in stage 14.0 (TID 1159, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 142.0 in stage 14.0 (TID 1159)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 111.0 in stage 14.0 (TID 1128) in 701 ms on localhost (127/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d0ae432
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@73db586d
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000132_1149/part-00132
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2473681f
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000133_1150/part-00133
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@74f179ed
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@187362ec
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000134_1151/part-00134
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@26abb8f6
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000135_1152/part-00135
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4c47ad44
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000138_1155/part-00138
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31d708c4
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000136_1153/part-00136
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000131_1148' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000131
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000131_1148: Committed
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22eee973
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000137_1154/part-00137
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000112_1129' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000112
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000112_1129: Committed
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO Executor: Finished task 112.0 in stage 14.0 (TID 1129). 781 bytes result sent to driver
15/08/06 17:34:14 INFO Executor: Finished task 131.0 in stage 14.0 (TID 1148). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 143.0 in stage 14.0 (TID 1160, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 143.0 in stage 14.0 (TID 1160)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@258a569
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO TaskSetManager: Finished task 112.0 in stage 14.0 (TID 1129) in 590 ms on localhost (128/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 144.0 in stage 14.0 (TID 1161, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 144.0 in stage 14.0 (TID 1161)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 131.0 in stage 14.0 (TID 1148) in 166 ms on localhost (129/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@40478422
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@38865cfe
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000132_1149' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000132
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000132_1149: Committed
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77e9ff2d
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO Executor: Finished task 132.0 in stage 14.0 (TID 1149). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@65462c98
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO TaskSetManager: Starting task 145.0 in stage 14.0 (TID 1162, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6219e319
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO Executor: Running task 145.0 in stage 14.0 (TID 1162)
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO TaskSetManager: Finished task 132.0 in stage 14.0 (TID 1149) in 155 ms on localhost (130/200)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000138_1155' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000138
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000138_1155: Committed
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000135_1152' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000135
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000135_1152: Committed
15/08/06 17:34:14 INFO Executor: Finished task 138.0 in stage 14.0 (TID 1155). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 146.0 in stage 14.0 (TID 1163, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Finished task 135.0 in stage 14.0 (TID 1152). 781 bytes result sent to driver
15/08/06 17:34:14 INFO Executor: Running task 146.0 in stage 14.0 (TID 1163)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 138.0 in stage 14.0 (TID 1155) in 146 ms on localhost (131/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 147.0 in stage 14.0 (TID 1164, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 147.0 in stage 14.0 (TID 1164)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 135.0 in stage 14.0 (TID 1152) in 154 ms on localhost (132/200)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000137_1154' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000137
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000137_1154: Committed
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO Executor: Finished task 137.0 in stage 14.0 (TID 1154). 781 bytes result sent to driver
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61d3b319
15/08/06 17:34:14 INFO TaskSetManager: Starting task 148.0 in stage 14.0 (TID 1165, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000139_1156/part-00139
15/08/06 17:34:14 INFO Executor: Running task 148.0 in stage 14.0 (TID 1165)
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO TaskSetManager: Finished task 137.0 in stage 14.0 (TID 1154) in 154 ms on localhost (133/200)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d173b53
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000139_1156' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000139
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000139_1156: Committed
15/08/06 17:34:14 INFO Executor: Finished task 139.0 in stage 14.0 (TID 1156). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 149.0 in stage 14.0 (TID 1166, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 149.0 in stage 14.0 (TID 1166)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 139.0 in stage 14.0 (TID 1156) in 166 ms on localhost (134/200)
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c089d3b
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000141_1158/part-00141
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d227343
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000142_1159/part-00142
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3507a05c
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@eef0a56
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000140_1157/part-00140
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@795b2a3a
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@611cd23f
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000145_1162/part-00145
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@631a836e
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000120_1137' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000120
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000120_1137: Committed
15/08/06 17:34:14 INFO Executor: Finished task 120.0 in stage 14.0 (TID 1137). 781 bytes result sent to driver
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@59ac3dd4
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000144_1161/part-00144
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO TaskSetManager: Starting task 150.0 in stage 14.0 (TID 1167, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 150.0 in stage 14.0 (TID 1167)
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO TaskSetManager: Finished task 120.0 in stage 14.0 (TID 1137) in 586 ms on localhost (135/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@71d1507e
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6df21065
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000147_1164/part-00147
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@74b82cbd
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000146_1163/part-00146
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a9ca132
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000143_1160/part-00143
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@19202991
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000126_1143' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000126
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000126_1143: Committed
15/08/06 17:34:14 INFO Executor: Finished task 126.0 in stage 14.0 (TID 1143). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@234d060a
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO TaskSetManager: Starting task 151.0 in stage 14.0 (TID 1168, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 151.0 in stage 14.0 (TID 1168)
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000140_1157' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000140
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000140_1157: Committed
15/08/06 17:34:14 INFO TaskSetManager: Finished task 126.0 in stage 14.0 (TID 1143) in 727 ms on localhost (136/200)
15/08/06 17:34:14 INFO Executor: Finished task 140.0 in stage 14.0 (TID 1157). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 152.0 in stage 14.0 (TID 1169, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@439a50a5
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4824a34f
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO Executor: Running task 152.0 in stage 14.0 (TID 1169)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000148_1165/part-00148
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO TaskSetManager: Finished task 140.0 in stage 14.0 (TID 1157) in 382 ms on localhost (137/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4e0ff950
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000145_1162' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000145
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000145_1162: Committed
15/08/06 17:34:14 INFO Executor: Finished task 145.0 in stage 14.0 (TID 1162). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@9df7ad6
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO TaskSetManager: Starting task 153.0 in stage 14.0 (TID 1170, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 153.0 in stage 14.0 (TID 1170)
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000144_1161' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000144
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000144_1161: Committed
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO TaskSetManager: Finished task 145.0 in stage 14.0 (TID 1162) in 345 ms on localhost (138/200)
15/08/06 17:34:14 INFO Executor: Finished task 144.0 in stage 14.0 (TID 1161). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 154.0 in stage 14.0 (TID 1171, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 154.0 in stage 14.0 (TID 1171)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 144.0 in stage 14.0 (TID 1161) in 356 ms on localhost (139/200)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000147_1164' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000147
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000147_1164: Committed
15/08/06 17:34:14 INFO Executor: Finished task 147.0 in stage 14.0 (TID 1164). 781 bytes result sent to driver
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000143_1160' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000143
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000143_1160: Committed
15/08/06 17:34:14 INFO TaskSetManager: Starting task 155.0 in stage 14.0 (TID 1172, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 155.0 in stage 14.0 (TID 1172)
15/08/06 17:34:14 INFO Executor: Finished task 143.0 in stage 14.0 (TID 1160). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Finished task 147.0 in stage 14.0 (TID 1164) in 334 ms on localhost (140/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 156.0 in stage 14.0 (TID 1173, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 156.0 in stage 14.0 (TID 1173)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 143.0 in stage 14.0 (TID 1160) in 371 ms on localhost (141/200)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4cb4716b
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000146_1163' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000146
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000149_1166/part-00149
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000146_1163: Committed
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO Executor: Finished task 146.0 in stage 14.0 (TID 1163). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 157.0 in stage 14.0 (TID 1174, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 157.0 in stage 14.0 (TID 1174)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000148_1165' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000148
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000148_1165: Committed
15/08/06 17:34:14 INFO TaskSetManager: Finished task 146.0 in stage 14.0 (TID 1163) in 351 ms on localhost (142/200)
15/08/06 17:34:14 INFO Executor: Finished task 148.0 in stage 14.0 (TID 1165). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 158.0 in stage 14.0 (TID 1175, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 158.0 in stage 14.0 (TID 1175)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 148.0 in stage 14.0 (TID 1165) in 348 ms on localhost (143/200)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000130_1147' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000130
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000130_1147: Committed
15/08/06 17:34:14 INFO Executor: Finished task 130.0 in stage 14.0 (TID 1147). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO TaskSetManager: Starting task 159.0 in stage 14.0 (TID 1176, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 159.0 in stage 14.0 (TID 1176)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 130.0 in stage 14.0 (TID 1147) in 662 ms on localhost (144/200)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@505885a4
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000149_1166' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000149
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000149_1166: Committed
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO Executor: Finished task 149.0 in stage 14.0 (TID 1166). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 160.0 in stage 14.0 (TID 1177, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 160.0 in stage 14.0 (TID 1177)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 149.0 in stage 14.0 (TID 1166) in 342 ms on localhost (145/200)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000133_1150' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000133
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000133_1150: Committed
15/08/06 17:34:14 INFO Executor: Finished task 133.0 in stage 14.0 (TID 1150). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 161.0 in stage 14.0 (TID 1178, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 161.0 in stage 14.0 (TID 1178)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 133.0 in stage 14.0 (TID 1150) in 574 ms on localhost (146/200)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000136_1153' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000136
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000136_1153: Committed
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000134_1151' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000134
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000134_1151: Committed
15/08/06 17:34:14 INFO Executor: Finished task 136.0 in stage 14.0 (TID 1153). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 162.0 in stage 14.0 (TID 1179, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 162.0 in stage 14.0 (TID 1179)
15/08/06 17:34:14 INFO Executor: Finished task 134.0 in stage 14.0 (TID 1151). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Finished task 136.0 in stage 14.0 (TID 1153) in 555 ms on localhost (147/200)
15/08/06 17:34:14 INFO TaskSetManager: Starting task 163.0 in stage 14.0 (TID 1180, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 163.0 in stage 14.0 (TID 1180)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 134.0 in stage 14.0 (TID 1151) in 581 ms on localhost (148/200)
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@64bcc4c2
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000150_1167/part-00150
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16360f0c
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000151_1168/part-00151
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7f40d0c8
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000152_1169/part-00152
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@784b3bbc
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@52f16e9b
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@388671ce
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000153_1170/part-00153
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@543f0b7a
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000154_1171/part-00154
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7904deeb
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000156_1173/part-00156
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4747c820
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000150_1167' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000150
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000150_1167: Committed
15/08/06 17:34:14 INFO Executor: Finished task 150.0 in stage 14.0 (TID 1167). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 164.0 in stage 14.0 (TID 1181, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 150.0 in stage 14.0 (TID 1167) in 357 ms on localhost (149/200)
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@fd917ab
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000155_1172/part-00155
15/08/06 17:34:14 INFO Executor: Running task 164.0 in stage 14.0 (TID 1181)
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15c3af30
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000151_1168' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000151
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000151_1168: Committed
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@47e5af44
15/08/06 17:34:14 INFO Executor: Finished task 151.0 in stage 14.0 (TID 1168). 781 bytes result sent to driver
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO TaskSetManager: Starting task 165.0 in stage 14.0 (TID 1182, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 165.0 in stage 14.0 (TID 1182)
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO TaskSetManager: Finished task 151.0 in stage 14.0 (TID 1168) in 170 ms on localhost (150/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e0d0a59
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@330e737d
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000141_1158' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000141
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000141_1158: Committed
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5bf38c32
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000158_1175/part-00158
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@527955c7
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000159_1176/part-00159
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3eb906b4
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000157_1174/part-00157
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO Executor: Finished task 141.0 in stage 14.0 (TID 1158). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO TaskSetManager: Starting task 166.0 in stage 14.0 (TID 1183, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 166.0 in stage 14.0 (TID 1183)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 141.0 in stage 14.0 (TID 1158) in 561 ms on localhost (151/200)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000152_1169' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000152
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000152_1169: Committed
15/08/06 17:34:14 INFO Executor: Finished task 152.0 in stage 14.0 (TID 1169). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 167.0 in stage 14.0 (TID 1184, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 167.0 in stage 14.0 (TID 1184)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 152.0 in stage 14.0 (TID 1169) in 188 ms on localhost (152/200)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000153_1170' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000153
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000153_1170: Committed
15/08/06 17:34:14 INFO Executor: Finished task 153.0 in stage 14.0 (TID 1170). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@513c8a1c
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO TaskSetManager: Starting task 168.0 in stage 14.0 (TID 1185, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 153.0 in stage 14.0 (TID 1170) in 187 ms on localhost (153/200)
15/08/06 17:34:14 INFO Executor: Running task 168.0 in stage 14.0 (TID 1185)
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000142_1159' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000142
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000142_1159: Committed
15/08/06 17:34:14 INFO Executor: Finished task 142.0 in stage 14.0 (TID 1159). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 169.0 in stage 14.0 (TID 1186, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 169.0 in stage 14.0 (TID 1186)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 142.0 in stage 14.0 (TID 1159) in 583 ms on localhost (154/200)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6bfdaa4c
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000154_1171' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000154
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000154_1171: Committed
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO Executor: Finished task 154.0 in stage 14.0 (TID 1171). 781 bytes result sent to driver
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO TaskSetManager: Starting task 170.0 in stage 14.0 (TID 1187, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000155_1172' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000155
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000155_1172: Committed
15/08/06 17:34:14 INFO Executor: Running task 170.0 in stage 14.0 (TID 1187)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 154.0 in stage 14.0 (TID 1171) in 194 ms on localhost (155/200)
15/08/06 17:34:14 INFO Executor: Finished task 155.0 in stage 14.0 (TID 1172). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 171.0 in stage 14.0 (TID 1188, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO Executor: Running task 171.0 in stage 14.0 (TID 1188)
15/08/06 17:34:14 INFO TaskSetManager: Finished task 155.0 in stage 14.0 (TID 1172) in 189 ms on localhost (156/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6a900702
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1eaa4168
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3bac2292
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000161_1178/part-00161
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000163_1180/part-00163
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000156_1173' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000156
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000156_1173: Committed
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:14 INFO Executor: Finished task 156.0 in stage 14.0 (TID 1173). 781 bytes result sent to driver
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c3ff770
15/08/06 17:34:14 INFO TaskSetManager: Starting task 172.0 in stage 14.0 (TID 1189, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000162_1179/part-00162
15/08/06 17:34:14 INFO Executor: Running task 172.0 in stage 14.0 (TID 1189)
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO TaskSetManager: Finished task 156.0 in stage 14.0 (TID 1173) in 202 ms on localhost (157/200)
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1a08d60d
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@49d8e18
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@532f9fe4
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:14 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4cebf184
15/08/06 17:34:14 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000160_1177/part-00160
15/08/06 17:34:14 INFO CodecConfig: Compression set to false
15/08/06 17:34:14 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:14 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:14 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:14 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000158_1175' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000158
15/08/06 17:34:14 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000158_1175: Committed
15/08/06 17:34:14 INFO Executor: Finished task 158.0 in stage 14.0 (TID 1175). 781 bytes result sent to driver
15/08/06 17:34:14 INFO TaskSetManager: Starting task 173.0 in stage 14.0 (TID 1190, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:14 INFO TaskSetManager: Finished task 158.0 in stage 14.0 (TID 1175) in 206 ms on localhost (158/200)
15/08/06 17:34:14 INFO Executor: Running task 173.0 in stage 14.0 (TID 1190)
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51965c17
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000162_1179' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000162
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000162_1179: Committed
15/08/06 17:34:15 INFO Executor: Finished task 162.0 in stage 14.0 (TID 1179). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Starting task 174.0 in stage 14.0 (TID 1191, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 174.0 in stage 14.0 (TID 1191)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 162.0 in stage 14.0 (TID 1179) in 286 ms on localhost (159/200)
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000160_1177' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000160
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000160_1177: Committed
15/08/06 17:34:15 INFO Executor: Finished task 160.0 in stage 14.0 (TID 1177). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Starting task 175.0 in stage 14.0 (TID 1192, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 175.0 in stage 14.0 (TID 1192)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 160.0 in stage 14.0 (TID 1177) in 317 ms on localhost (160/200)
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2116c5f4
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e1e2724
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000165_1182/part-00165
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000164_1181/part-00164
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a66e539
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000166_1183/part-00166
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6eb6f803
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e54c5be
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f8254b5
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d9ca028
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@45d6da50
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000170_1187/part-00170
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000167_1184/part-00167
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@509946b3
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000169_1186/part-00169
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6e8c2ab8
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17fc71ce
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3448cebb
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000172_1189/part-00172
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000168_1185/part-00168
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e9397af
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000171_1188/part-00171
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4bcb8241
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000173_1190/part-00173
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1dd4a047
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000174_1191/part-00174
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7266a49d
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000175_1192/part-00175
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30c17a18
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000166_1183' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000166
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000166_1183: Committed
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000165_1182' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000165
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30411db
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000165_1182: Committed
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@95af394
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@322d7e58
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO Executor: Finished task 165.0 in stage 14.0 (TID 1182). 781 bytes result sent to driver
15/08/06 17:34:15 INFO Executor: Finished task 166.0 in stage 14.0 (TID 1183). 781 bytes result sent to driver
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14d58a41
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@661aea37
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO TaskSetManager: Starting task 176.0 in stage 14.0 (TID 1193, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 176.0 in stage 14.0 (TID 1193)
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000159_1176' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000159
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000159_1176: Committed
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@481f2bfc
15/08/06 17:34:15 INFO TaskSetManager: Finished task 165.0 in stage 14.0 (TID 1182) in 703 ms on localhost (161/200)
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3741b0f
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000163_1180' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000163
15/08/06 17:34:15 INFO TaskSetManager: Starting task 177.0 in stage 14.0 (TID 1194, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Finished task 159.0 in stage 14.0 (TID 1176). 781 bytes result sent to driver
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000163_1180: Committed
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO Executor: Running task 177.0 in stage 14.0 (TID 1194)
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO Executor: Finished task 163.0 in stage 14.0 (TID 1180). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Finished task 166.0 in stage 14.0 (TID 1183) in 688 ms on localhost (162/200)
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000161_1178' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000161
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000161_1178: Committed
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO TaskSetManager: Starting task 178.0 in stage 14.0 (TID 1195, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 178.0 in stage 14.0 (TID 1195)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 179.0 in stage 14.0 (TID 1196, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Finished task 161.0 in stage 14.0 (TID 1178). 781 bytes result sent to driver
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000164_1181' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000164
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000164_1181: Committed
15/08/06 17:34:15 INFO TaskSetManager: Starting task 180.0 in stage 14.0 (TID 1197, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 179.0 in stage 14.0 (TID 1196)
15/08/06 17:34:15 INFO Executor: Running task 180.0 in stage 14.0 (TID 1197)
15/08/06 17:34:15 INFO Executor: Finished task 164.0 in stage 14.0 (TID 1181). 781 bytes result sent to driver
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000157_1174' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000157
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000157_1174: Committed
15/08/06 17:34:15 INFO TaskSetManager: Finished task 161.0 in stage 14.0 (TID 1178) in 784 ms on localhost (163/200)
15/08/06 17:34:15 INFO Executor: Finished task 157.0 in stage 14.0 (TID 1174). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Finished task 163.0 in stage 14.0 (TID 1180) in 777 ms on localhost (164/200)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 159.0 in stage 14.0 (TID 1176) in 829 ms on localhost (165/200)
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000170_1187' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000170
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000170_1187: Committed
15/08/06 17:34:15 INFO TaskSetManager: Starting task 181.0 in stage 14.0 (TID 1198, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 181.0 in stage 14.0 (TID 1198)
15/08/06 17:34:15 INFO Executor: Finished task 170.0 in stage 14.0 (TID 1187). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Finished task 164.0 in stage 14.0 (TID 1181) in 722 ms on localhost (166/200)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 182.0 in stage 14.0 (TID 1199, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 182.0 in stage 14.0 (TID 1199)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 157.0 in stage 14.0 (TID 1174) in 841 ms on localhost (167/200)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 183.0 in stage 14.0 (TID 1200, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 183.0 in stage 14.0 (TID 1200)
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000167_1184' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000167
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000167_1184: Committed
15/08/06 17:34:15 INFO TaskSetManager: Finished task 170.0 in stage 14.0 (TID 1187) in 672 ms on localhost (168/200)
15/08/06 17:34:15 INFO Executor: Finished task 167.0 in stage 14.0 (TID 1184). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Starting task 184.0 in stage 14.0 (TID 1201, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 184.0 in stage 14.0 (TID 1201)
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000169_1186' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000169
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000169_1186: Committed
15/08/06 17:34:15 INFO TaskSetManager: Finished task 167.0 in stage 14.0 (TID 1184) in 698 ms on localhost (169/200)
15/08/06 17:34:15 INFO Executor: Finished task 169.0 in stage 14.0 (TID 1186). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Starting task 185.0 in stage 14.0 (TID 1202, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 185.0 in stage 14.0 (TID 1202)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 169.0 in stage 14.0 (TID 1186) in 681 ms on localhost (170/200)
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000171_1188' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000171
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000171_1188: Committed
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000174_1191' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000174
15/08/06 17:34:15 INFO Executor: Finished task 171.0 in stage 14.0 (TID 1188). 781 bytes result sent to driver
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000172_1189' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000172
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000172_1189: Committed
15/08/06 17:34:15 INFO TaskSetManager: Starting task 186.0 in stage 14.0 (TID 1203, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Finished task 172.0 in stage 14.0 (TID 1189). 781 bytes result sent to driver
15/08/06 17:34:15 INFO Executor: Running task 186.0 in stage 14.0 (TID 1203)
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000174_1191: Committed
15/08/06 17:34:15 INFO TaskSetManager: Finished task 171.0 in stage 14.0 (TID 1188) in 682 ms on localhost (171/200)
15/08/06 17:34:15 INFO Executor: Finished task 174.0 in stage 14.0 (TID 1191). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Starting task 187.0 in stage 14.0 (TID 1204, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 187.0 in stage 14.0 (TID 1204)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 172.0 in stage 14.0 (TID 1189) in 670 ms on localhost (172/200)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 188.0 in stage 14.0 (TID 1205, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 188.0 in stage 14.0 (TID 1205)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 174.0 in stage 14.0 (TID 1191) in 517 ms on localhost (173/200)
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000168_1185' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000168
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000168_1185: Committed
15/08/06 17:34:15 INFO Executor: Finished task 168.0 in stage 14.0 (TID 1185). 781 bytes result sent to driver
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000173_1190' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000173
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000175_1192' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000175
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000173_1190: Committed
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000175_1192: Committed
15/08/06 17:34:15 INFO TaskSetManager: Starting task 189.0 in stage 14.0 (TID 1206, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO Executor: Finished task 173.0 in stage 14.0 (TID 1190). 781 bytes result sent to driver
15/08/06 17:34:15 INFO Executor: Finished task 175.0 in stage 14.0 (TID 1192). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Finished task 168.0 in stage 14.0 (TID 1185) in 709 ms on localhost (174/200)
15/08/06 17:34:15 INFO Executor: Running task 189.0 in stage 14.0 (TID 1206)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 190.0 in stage 14.0 (TID 1207, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 190.0 in stage 14.0 (TID 1207)
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO TaskSetManager: Finished task 173.0 in stage 14.0 (TID 1190) in 656 ms on localhost (175/200)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 191.0 in stage 14.0 (TID 1208, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 191.0 in stage 14.0 (TID 1208)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 175.0 in stage 14.0 (TID 1192) in 515 ms on localhost (176/200)
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4c146b25
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000180_1197/part-00180
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@50867128
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000178_1195/part-00178
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@663b2aa6
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000182_1199/part-00182
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@366d2cb9
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000183_1200/part-00183
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ed11886
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000181_1198/part-00181
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23549e2c
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000184_1201/part-00184
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@209c53e7
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000188_1205/part-00188
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64dc8e12
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4bf37379
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000191_1208/part-00191
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c2233e3
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@46faef55
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@20a59fa
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@75992d21
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000176_1193/part-00176
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b620dd8
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@427aedf3
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d038ef7
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000179_1196/part-00179
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1183b1a4
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000186_1203/part-00186
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@54eb4749
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@24adc964
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@d3c9f38
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000189_1206/part-00189
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@382ed1da
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000185_1202/part-00185
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ade0cc
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000187_1204/part-00187
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4da59bb8
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5435f050
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e5a95ab
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000182_1199' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000182
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000182_1199: Committed
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO Executor: Finished task 182.0 in stage 14.0 (TID 1199). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Starting task 192.0 in stage 14.0 (TID 1209, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 192.0 in stage 14.0 (TID 1209)
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5947ccbf
15/08/06 17:34:15 INFO TaskSetManager: Finished task 182.0 in stage 14.0 (TID 1199) in 274 ms on localhost (177/200)
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@409ab8cb
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@360d9415
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7b8521ed
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000190_1207/part-00190
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2e6201d0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000177_1194/part-00177
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000183_1200' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000183
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000178_1195' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000178
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000183_1200: Committed
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000178_1195: Committed
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000191_1208' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000191
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000191_1208: Committed
15/08/06 17:34:15 INFO Executor: Finished task 191.0 in stage 14.0 (TID 1208). 781 bytes result sent to driver
15/08/06 17:34:15 INFO Executor: Finished task 178.0 in stage 14.0 (TID 1195). 781 bytes result sent to driver
15/08/06 17:34:15 INFO Executor: Finished task 183.0 in stage 14.0 (TID 1200). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Starting task 193.0 in stage 14.0 (TID 1210, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 193.0 in stage 14.0 (TID 1210)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 194.0 in stage 14.0 (TID 1211, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 194.0 in stage 14.0 (TID 1211)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 195.0 in stage 14.0 (TID 1212, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 195.0 in stage 14.0 (TID 1212)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 191.0 in stage 14.0 (TID 1208) in 269 ms on localhost (178/200)
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000188_1205' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000188
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000188_1205: Committed
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000176_1193' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000176
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000176_1193: Committed
15/08/06 17:34:15 INFO TaskSetManager: Finished task 178.0 in stage 14.0 (TID 1195) in 305 ms on localhost (179/200)
15/08/06 17:34:15 INFO Executor: Finished task 176.0 in stage 14.0 (TID 1193). 781 bytes result sent to driver
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@25909881
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e5209c
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO TaskSetManager: Finished task 183.0 in stage 14.0 (TID 1200) in 297 ms on localhost (180/200)
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000179_1196' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000179
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000179_1196: Committed
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO TaskSetManager: Starting task 196.0 in stage 14.0 (TID 1213, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 196.0 in stage 14.0 (TID 1213)
15/08/06 17:34:15 INFO Executor: Finished task 179.0 in stage 14.0 (TID 1196). 781 bytes result sent to driver
15/08/06 17:34:15 INFO Executor: Finished task 188.0 in stage 14.0 (TID 1205). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Finished task 176.0 in stage 14.0 (TID 1193) in 314 ms on localhost (181/200)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 197.0 in stage 14.0 (TID 1214, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 197.0 in stage 14.0 (TID 1214)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 179.0 in stage 14.0 (TID 1196) in 311 ms on localhost (182/200)
15/08/06 17:34:15 INFO TaskSetManager: Starting task 198.0 in stage 14.0 (TID 1215, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 198.0 in stage 14.0 (TID 1215)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 188.0 in stage 14.0 (TID 1205) in 288 ms on localhost (183/200)
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000186_1203' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000186
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000186_1203: Committed
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000185_1202' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000185
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000185_1202: Committed
15/08/06 17:34:15 INFO Executor: Finished task 185.0 in stage 14.0 (TID 1202). 781 bytes result sent to driver
15/08/06 17:34:15 INFO Executor: Finished task 186.0 in stage 14.0 (TID 1203). 781 bytes result sent to driver
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:34:15 INFO TaskSetManager: Starting task 199.0 in stage 14.0 (TID 1216, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:34:15 INFO Executor: Running task 199.0 in stage 14.0 (TID 1216)
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO TaskSetManager: Finished task 185.0 in stage 14.0 (TID 1202) in 320 ms on localhost (184/200)
15/08/06 17:34:15 INFO TaskSetManager: Finished task 186.0 in stage 14.0 (TID 1203) in 315 ms on localhost (185/200)
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000190_1207' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000190
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000190_1207: Committed
15/08/06 17:34:15 INFO Executor: Finished task 190.0 in stage 14.0 (TID 1207). 781 bytes result sent to driver
15/08/06 17:34:15 INFO TaskSetManager: Finished task 190.0 in stage 14.0 (TID 1207) in 314 ms on localhost (186/200)
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:34:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14bd9409
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000192_1209/part-00192
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@364379b3
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7eaf29a
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000195_1212/part-00195
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1cebe5c0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000194_1211/part-00194
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4338af15
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000196_1213/part-00196
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e6eae2d
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@546d2c60
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000197_1214/part-00197
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000198_1215/part-00198
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ddad0cd
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@28c28348
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7a3900df
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2aab43a
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000193_1210/part-00193
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@785e9bc0
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1aa19353
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000192_1209' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000192
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000192_1209: Committed
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3db159bd
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:34:15 INFO Executor: Finished task 192.0 in stage 14.0 (TID 1209). 781 bytes result sent to driver
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000195_1212' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000195
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000195_1212: Committed
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO Executor: Finished task 195.0 in stage 14.0 (TID 1212). 781 bytes result sent to driver
15/08/06 17:34:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@657ce094
15/08/06 17:34:15 INFO TaskSetManager: Finished task 192.0 in stage 14.0 (TID 1209) in 174 ms on localhost (187/200)
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/_temporary/attempt_201508061734_0014_m_000199_1216/part-00199
15/08/06 17:34:15 INFO CodecConfig: Compression set to false
15/08/06 17:34:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:34:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:34:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:34:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:34:15 INFO TaskSetManager: Finished task 195.0 in stage 14.0 (TID 1212) in 155 ms on localhost (188/200)
15/08/06 17:34:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b8025c0
15/08/06 17:34:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/06 17:34:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000193_1210' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000193
15/08/06 17:34:15 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000193_1210: Committed
15/08/06 17:34:15 INFO Executor: Finished task 193.0 in stage 14.0 (TID 1210). 781 bytes result sent to driver
15/08/06 17:34:16 INFO TaskSetManager: Finished task 193.0 in stage 14.0 (TID 1210) in 179 ms on localhost (189/200)
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000199_1216' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000199
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000199_1216: Committed
15/08/06 17:34:16 INFO Executor: Finished task 199.0 in stage 14.0 (TID 1216). 781 bytes result sent to driver
15/08/06 17:34:16 INFO TaskSetManager: Finished task 199.0 in stage 14.0 (TID 1216) in 158 ms on localhost (190/200)
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000181_1198' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000181
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000180_1197' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000180
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000181_1198: Committed
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000180_1197: Committed
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000184_1201' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000184
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000184_1201: Committed
15/08/06 17:34:16 INFO Executor: Finished task 180.0 in stage 14.0 (TID 1197). 781 bytes result sent to driver
15/08/06 17:34:16 INFO Executor: Finished task 181.0 in stage 14.0 (TID 1198). 781 bytes result sent to driver
15/08/06 17:34:16 INFO Executor: Finished task 184.0 in stage 14.0 (TID 1201). 781 bytes result sent to driver
15/08/06 17:34:16 INFO TaskSetManager: Finished task 180.0 in stage 14.0 (TID 1197) in 685 ms on localhost (191/200)
15/08/06 17:34:16 INFO TaskSetManager: Finished task 181.0 in stage 14.0 (TID 1198) in 682 ms on localhost (192/200)
15/08/06 17:34:16 INFO TaskSetManager: Finished task 184.0 in stage 14.0 (TID 1201) in 676 ms on localhost (193/200)
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000187_1204' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000187
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000187_1204: Committed
15/08/06 17:34:16 INFO Executor: Finished task 187.0 in stage 14.0 (TID 1204). 781 bytes result sent to driver
15/08/06 17:34:16 INFO TaskSetManager: Finished task 187.0 in stage 14.0 (TID 1204) in 691 ms on localhost (194/200)
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000189_1206' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000189
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000189_1206: Committed
15/08/06 17:34:16 INFO Executor: Finished task 189.0 in stage 14.0 (TID 1206). 781 bytes result sent to driver
15/08/06 17:34:16 INFO TaskSetManager: Finished task 189.0 in stage 14.0 (TID 1206) in 690 ms on localhost (195/200)
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000177_1194' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000177
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000177_1194: Committed
15/08/06 17:34:16 INFO Executor: Finished task 177.0 in stage 14.0 (TID 1194). 781 bytes result sent to driver
15/08/06 17:34:16 INFO TaskSetManager: Finished task 177.0 in stage 14.0 (TID 1194) in 749 ms on localhost (196/200)
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000196_1213' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000196
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000196_1213: Committed
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000198_1215' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000198
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000198_1215: Committed
15/08/06 17:34:16 INFO Executor: Finished task 196.0 in stage 14.0 (TID 1213). 781 bytes result sent to driver
15/08/06 17:34:16 INFO Executor: Finished task 198.0 in stage 14.0 (TID 1215). 781 bytes result sent to driver
15/08/06 17:34:16 INFO TaskSetManager: Finished task 196.0 in stage 14.0 (TID 1213) in 551 ms on localhost (197/200)
15/08/06 17:34:16 INFO TaskSetManager: Finished task 198.0 in stage 14.0 (TID 1215) in 549 ms on localhost (198/200)
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000194_1211' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000194
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000194_1211: Committed
15/08/06 17:34:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061734_0014_m_000197_1214' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_temporary/0/task_201508061734_0014_m_000197
15/08/06 17:34:16 INFO SparkHiveWriterContainer: attempt_201508061734_0014_m_000197_1214: Committed
15/08/06 17:34:16 INFO Executor: Finished task 194.0 in stage 14.0 (TID 1211). 781 bytes result sent to driver
15/08/06 17:34:16 INFO Executor: Finished task 197.0 in stage 14.0 (TID 1214). 781 bytes result sent to driver
15/08/06 17:34:16 INFO TaskSetManager: Finished task 194.0 in stage 14.0 (TID 1211) in 563 ms on localhost (199/200)
15/08/06 17:34:16 INFO TaskSetManager: Finished task 197.0 in stage 14.0 (TID 1214) in 555 ms on localhost (200/200)
15/08/06 17:34:16 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/08/06 17:34:16 INFO DAGScheduler: Stage 14 (runJob at InsertIntoHiveTable.scala:93) finished in 5.039 s
15/08/06 17:34:16 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4fec5c0
15/08/06 17:34:16 INFO DAGScheduler: Job 8 finished: runJob at InsertIntoHiveTable.scala:93, took 11.887704 s
15/08/06 17:34:16 INFO StatsReportListener: task runtime:(count: 200, mean: 385.865000, stdev: 193.519345, max: 841.000000, min: 146.000000)
15/08/06 17:34:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:16 INFO StatsReportListener: 	146.0 ms	167.0 ms	177.0 ms	210.0 ms	330.0 ms	561.0 ms	697.0 ms	737.0 ms	841.0 ms
15/08/06 17:34:16 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.225000, stdev: 0.484123, max: 2.000000, min: 0.000000)
15/08/06 17:34:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:16 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/06 17:34:16 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:34:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:16 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:34:16 INFO StatsReportListener: task result size:(count: 200, mean: 781.000000, stdev: 0.000000, max: 781.000000, min: 781.000000)
15/08/06 17:34:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:16 INFO StatsReportListener: 	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B
15/08/06 17:34:16 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.956549, stdev: 4.161041, max: 99.746835, min: 51.573850)
15/08/06 17:34:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:16 INFO StatsReportListener: 	52 %	95 %	97 %	98 %	99 %	99 %	100 %	100 %	100 %
15/08/06 17:34:16 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.071708, stdev: 0.174597, max: 1.290323, min: 0.000000)
15/08/06 17:34:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:16 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %
15/08/06 17:34:16 INFO StatsReportListener: other time pct: (count: 200, mean: 1.971743, stdev: 4.160154, max: 48.426150, min: 0.158479)
15/08/06 17:34:16 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:16 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 1 %	 1 %	 2 %	 3 %	 5 %	48 %
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/_SUCCESS;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/_SUCCESS;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00000;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00000;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00001;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00001;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00002;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00002;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00003;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00003;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00004;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00004;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00005;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00005;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00006;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00006;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00007;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00007;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00008;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00008;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00009;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00009;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00010;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00010;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00011;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00011;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00012;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00012;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00013;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00013;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00014;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00014;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00015;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00015;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00016;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00016;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00017;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00017;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00018;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00018;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00019;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00019;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00020;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00020;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00021;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00021;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00022;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00022;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00023;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00023;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00024;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00024;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00025;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00025;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00026;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00026;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00027;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00027;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00028;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00028;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00029;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00029;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00030;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00030;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00031;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00031;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00032;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00032;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00033;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00033;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00034;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00034;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00035;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00035;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00036;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00036;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00037;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00037;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00038;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00038;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00039;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00039;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00040;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00040;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00041;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00041;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00042;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00042;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00043;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00043;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00044;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00044;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00045;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00045;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00046;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00046;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00047;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00047;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00048;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00048;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00049;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00049;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00050;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00050;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00051;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00051;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00052;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00052;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00053;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00053;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00054;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00054;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00055;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00055;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00056;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00056;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00057;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00057;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00058;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00058;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00059;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00059;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00060;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00060;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00061;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00061;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00062;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00062;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00063;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00063;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00064;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00064;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00065;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00065;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00066;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00066;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00067;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00067;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00068;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00068;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00069;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00069;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00070;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00070;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00071;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00071;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00072;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00072;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00073;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00073;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00074;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00074;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00075;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00075;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00076;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00076;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00077;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00077;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00078;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00078;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00079;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00079;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00080;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00080;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00081;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00081;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00082;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00082;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00083;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00083;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00084;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00084;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00085;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00085;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00086;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00086;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00087;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00087;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00088;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00088;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00089;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00089;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00090;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00090;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00091;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00091;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00092;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00092;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00093;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00093;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00094;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00094;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00095;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00095;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00096;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00096;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00097;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00097;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00098;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00098;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00099;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00099;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00100;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00100;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00101;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00101;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00102;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00102;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00103;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00103;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00104;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00104;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00105;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00105;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00106;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00106;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00107;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00107;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00108;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00108;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00109;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00109;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00110;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00110;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00111;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00111;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00112;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00112;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00113;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00113;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00114;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00114;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00115;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00115;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00116;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00116;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00117;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00117;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00118;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00118;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00119;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00119;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00120;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00120;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00121;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00121;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00122;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00122;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00123;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00123;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00124;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00124;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00125;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00125;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00126;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00126;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00127;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00127;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00128;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00128;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00129;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00129;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00130;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00130;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00131;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00131;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00132;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00132;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00133;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00133;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00134;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00134;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00135;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00135;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00136;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00136;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00137;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00137;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00138;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00138;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00139;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00139;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00140;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00140;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00141;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00141;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00142;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00142;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00143;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00143;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00144;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00144;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00145;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00145;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00146;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00146;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00147;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00147;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00148;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00148;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00149;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00149;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00150;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00150;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00151;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00151;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00152;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00152;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00153;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00153;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00154;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00154;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00155;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00155;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00156;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00156;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00157;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00157;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00158;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00158;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00159;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00159;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00160;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00160;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00161;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00161;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00162;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00162;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00163;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00163;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00164;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00164;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00165;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00165;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00166;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00166;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00167;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00167;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00168;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00168;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00169;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00169;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00170;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00170;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00171;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00171;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00172;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00172;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00173;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00173;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00174;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00174;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00175;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00175;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00176;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00176;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00177;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00177;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00178;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00178;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00179;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00179;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00180;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00180;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00181;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00181;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00182;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00182;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00183;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00183;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00184;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00184;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00185;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00185;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00186;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00186;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00187;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00187;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00188;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00188;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00189;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00189;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00190;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00190;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00191;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00191;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00192;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00192;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00193;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00193;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00194;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00194;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00195;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00195;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00196;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00196;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00197;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00197;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00198;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00198;Status:true
15/08/06 17:34:17 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-34-00_311_9088387764538709730-1/-ext-10000/part-00199;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00199;Status:true
15/08/06 17:34:17 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:34:17 INFO DAGScheduler: Got job 9 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:34:17 INFO DAGScheduler: Final stage: Stage 15(collect at SparkPlan.scala:84)
15/08/06 17:34:17 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:34:17 INFO DAGScheduler: Missing parents: List()
15/08/06 17:34:17 INFO DAGScheduler: Submitting Stage 15 (MappedRDD[82] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:34:17 INFO MemoryStore: ensureFreeSpace(3256) called with curMem=1850807, maxMem=3333968363
15/08/06 17:34:17 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 3.2 KB, free 3.1 GB)
15/08/06 17:34:17 INFO MemoryStore: ensureFreeSpace(1958) called with curMem=1854063, maxMem=3333968363
15/08/06 17:34:17 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 1958.0 B, free 3.1 GB)
15/08/06 17:34:17 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:42931 (size: 1958.0 B, free: 3.1 GB)
15/08/06 17:34:17 INFO BlockManagerMaster: Updated info of block broadcast_20_piece0
15/08/06 17:34:17 INFO DefaultExecutionContext: Created broadcast 20 from broadcast at DAGScheduler.scala:838
15/08/06 17:34:17 INFO DAGScheduler: Submitting 1 missing tasks from Stage 15 (MappedRDD[82] at map at SparkPlan.scala:84)
15/08/06 17:34:17 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
15/08/06 17:34:17 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 1217, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:34:17 INFO Executor: Running task 0.0 in stage 15.0 (TID 1217)
15/08/06 17:34:17 INFO Executor: Finished task 0.0 in stage 15.0 (TID 1217). 618 bytes result sent to driver
15/08/06 17:34:17 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 1217) in 5 ms on localhost (1/1)
15/08/06 17:34:17 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/08/06 17:34:17 INFO DAGScheduler: Stage 15 (collect at SparkPlan.scala:84) finished in 0.006 s
15/08/06 17:34:17 INFO DAGScheduler: Job 9 finished: collect at SparkPlan.scala:84, took 0.017728 s
15/08/06 17:34:17 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@16902371
15/08/06 17:34:17 INFO StatsReportListener: task runtime:(count: 1, mean: 5.000000, stdev: 0.000000, max: 5.000000, min: 5.000000)
15/08/06 17:34:17 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:17 INFO StatsReportListener: 	5.0 ms	5.0 ms	5.0 ms	5.0 ms	5.0 ms	5.0 ms	5.0 ms	5.0 ms	5.0 ms
Time taken: 17.959 seconds
15/08/06 17:34:17 INFO CliDriver: Time taken: 17.959 seconds
15/08/06 17:34:17 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:34:17 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:17 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:34:17 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:34:17 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438882457944 end=1438882457944 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:34:17 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 20.000000, stdev: 0.000000, max: 20.000000, min: 20.000000)
15/08/06 17:34:17 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:17 INFO StatsReportListener: 	20 %	20 %	20 %	20 %	20 %	20 %	20 %	20 %	20 %
15/08/06 17:34:17 INFO StatsReportListener: other time pct: (count: 1, mean: 80.000000, stdev: 0.000000, max: 80.000000, min: 80.000000)
15/08/06 17:34:17 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:34:17 INFO StatsReportListener: 	80 %	80 %	80 %	80 %	80 %	80 %	80 %	80 %	80 %
15/08/06 17:34:18 INFO SparkUI: Stopped Spark web UI at http://sandbox.hortonworks.com:4040
15/08/06 17:34:18 INFO DAGScheduler: Stopping DAGScheduler
15/08/06 17:34:19 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
15/08/06 17:34:19 INFO MemoryStore: MemoryStore cleared
15/08/06 17:34:19 INFO BlockManager: BlockManager stopped
15/08/06 17:34:19 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/06 17:34:19 INFO SparkContext: Successfully stopped SparkContext
