-- number of partitions when shuffling data for aggregates and joins
--set spark.sql.shuffle.partitions=1024;

DROP TABLE q11_important_stock_par_spark;
DROP TABLE q11_part_tmp_par_spark;

-- create the target table
create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet;
create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet;

-- the query
insert into table q11_part_tmp_par_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_par_spark
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par_spark) sum_tmp
        join q11_part_tmp_par_spark
where part_value > total_value * 0.0001
order by value desc;

Spark assembly has been built with Hive, including Datanucleus jars on classpath
15/08/06 17:54:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/06 17:54:03 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/06 17:54:03 INFO metastore: Connected to metastore.
15/08/06 17:54:04 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/06 17:54:04 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/06 17:54:04 INFO SecurityManager: Changing view acls to: hive
15/08/06 17:54:04 INFO SecurityManager: Changing modify acls to: hive
15/08/06 17:54:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/06 17:54:05 INFO Slf4jLogger: Slf4jLogger started
15/08/06 17:54:05 INFO Remoting: Starting remoting
15/08/06 17:54:05 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@sandbox.hortonworks.com:42537]
15/08/06 17:54:05 INFO Utils: Successfully started service 'sparkDriver' on port 42537.
15/08/06 17:54:05 INFO SparkEnv: Registering MapOutputTracker
15/08/06 17:54:05 INFO SparkEnv: Registering BlockManagerMaster
15/08/06 17:54:05 INFO DiskBlockManager: Created local directory at /tmp/spark-bc46fa30-85df-481b-a0b2-cfd5e053daa4/spark-ef72262a-5ee5-41e9-bf2b-fbc3cfec4b00
15/08/06 17:54:05 INFO MemoryStore: MemoryStore started with capacity 3.1 GB
15/08/06 17:54:05 INFO HttpFileServer: HTTP File server directory is /tmp/spark-f8b7d7bc-9464-4372-9628-953b1592a478/spark-30ecfa88-4863-4300-a517-728eb93dcfbd
15/08/06 17:54:05 INFO HttpServer: Starting HTTP Server
15/08/06 17:54:05 INFO Utils: Successfully started service 'HTTP file server' on port 54721.
15/08/06 17:54:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/06 17:54:05 INFO SparkUI: Started SparkUI at http://sandbox.hortonworks.com:4040
15/08/06 17:54:06 INFO Executor: Starting executor ID <driver> on host localhost
15/08/06 17:54:06 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@sandbox.hortonworks.com:42537/user/HeartbeatReceiver
15/08/06 17:54:06 INFO NettyBlockTransferService: Server created on 37948
15/08/06 17:54:06 INFO BlockManagerMaster: Trying to register BlockManager
15/08/06 17:54:06 INFO BlockManagerMasterActor: Registering block manager localhost:37948 with 3.1 GB RAM, BlockManagerId(<driver>, localhost, 37948)
15/08/06 17:54:06 INFO BlockManagerMaster: Registered BlockManager
SET spark.sql.hive.version=0.13.1
15/08/06 17:54:07 INFO ParseDriver: Parsing command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:54:07 INFO ParseDriver: Parse Completed
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO ParseDriver: Parsing command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:54:08 INFO ParseDriver: Parse Completed
15/08/06 17:54:08 INFO PerfLogger: </PERFLOG method=parse start=1438883648537 end=1438883648538 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO Driver: Semantic Analysis Completed
15/08/06 17:54:08 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438883648538 end=1438883648683 duration=145 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:54:08 INFO PerfLogger: </PERFLOG method=compile start=1438883648507 end=1438883648696 duration=189 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO Driver: Starting command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:54:08 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438883648498 end=1438883648718 duration=220 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: </PERFLOG method=runTasks start=1438883648719 end=1438883648893 duration=174 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438883648697 end=1438883648893 duration=196 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:54:08 INFO Driver: OK
15/08/06 17:54:08 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883648894 end=1438883648894 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:08 INFO PerfLogger: </PERFLOG method=Driver.run start=1438883648497 end=1438883648894 duration=397 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:54:09 INFO DAGScheduler: Got job 0 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:54:09 INFO DAGScheduler: Final stage: Stage 0(collect at SparkPlan.scala:84)
15/08/06 17:54:09 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:54:09 INFO DAGScheduler: Missing parents: List()
15/08/06 17:54:09 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[2] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:54:09 INFO MemoryStore: ensureFreeSpace(1896) called with curMem=0, maxMem=3333968363
15/08/06 17:54:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1896.0 B, free 3.1 GB)
15/08/06 17:54:09 INFO MemoryStore: ensureFreeSpace(1208) called with curMem=1896, maxMem=3333968363
15/08/06 17:54:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1208.0 B, free 3.1 GB)
15/08/06 17:54:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:37948 (size: 1208.0 B, free: 3.1 GB)
15/08/06 17:54:09 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/08/06 17:54:09 INFO DefaultExecutionContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:09 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[2] at map at SparkPlan.scala:84)
15/08/06 17:54:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/08/06 17:54:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:54:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/06 17:54:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 618 bytes result sent to driver
15/08/06 17:54:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 75 ms on localhost (1/1)
15/08/06 17:54:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/06 17:54:09 INFO DAGScheduler: Stage 0 (collect at SparkPlan.scala:84) finished in 0.094 s
15/08/06 17:54:09 INFO DAGScheduler: Job 0 finished: collect at SparkPlan.scala:84, took 0.467336 s
Time taken: 2.668 seconds
15/08/06 17:54:09 INFO CliDriver: Time taken: 2.668 seconds
15/08/06 17:54:09 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@787f1da3
15/08/06 17:54:09 INFO StatsReportListener: task runtime:(count: 1, mean: 75.000000, stdev: 0.000000, max: 75.000000, min: 75.000000)
15/08/06 17:54:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:09 INFO StatsReportListener: 	75.0 ms	75.0 ms	75.0 ms	75.0 ms	75.0 ms	75.0 ms	75.0 ms	75.0 ms	75.0 ms
15/08/06 17:54:09 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:54:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:09 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:54:09 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 20.000000, stdev: 0.000000, max: 20.000000, min: 20.000000)
15/08/06 17:54:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:09 INFO StatsReportListener: 	20 %	20 %	20 %	20 %	20 %	20 %	20 %	20 %	20 %
15/08/06 17:54:09 INFO StatsReportListener: other time pct: (count: 1, mean: 80.000000, stdev: 0.000000, max: 80.000000, min: 80.000000)
15/08/06 17:54:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:09 INFO StatsReportListener: 	80 %	80 %	80 %	80 %	80 %	80 %	80 %	80 %	80 %
15/08/06 17:54:09 INFO ParseDriver: Parsing command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:54:09 INFO ParseDriver: Parse Completed
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO ParseDriver: Parsing command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:54:09 INFO ParseDriver: Parse Completed
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=parse start=1438883649594 end=1438883649595 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO Driver: Semantic Analysis Completed
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438883649595 end=1438883649616 duration=21 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=compile start=1438883649594 end=1438883649617 duration=23 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO Driver: Starting command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438883649593 end=1438883649617 duration=24 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=runTasks start=1438883649617 end=1438883649692 duration=75 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438883649617 end=1438883649692 duration=75 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:54:09 INFO Driver: OK
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883649692 end=1438883649692 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=Driver.run start=1438883649593 end=1438883649692 duration=99 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:54:09 INFO DAGScheduler: Got job 1 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:54:09 INFO DAGScheduler: Final stage: Stage 1(collect at SparkPlan.scala:84)
15/08/06 17:54:09 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:54:09 INFO DAGScheduler: Missing parents: List()
15/08/06 17:54:09 INFO DAGScheduler: Submitting Stage 1 (MappedRDD[5] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:54:09 INFO MemoryStore: ensureFreeSpace(1896) called with curMem=3104, maxMem=3333968363
15/08/06 17:54:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 1896.0 B, free 3.1 GB)
15/08/06 17:54:09 INFO MemoryStore: ensureFreeSpace(1207) called with curMem=5000, maxMem=3333968363
15/08/06 17:54:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1207.0 B, free 3.1 GB)
15/08/06 17:54:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:37948 (size: 1207.0 B, free: 3.1 GB)
15/08/06 17:54:09 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/08/06 17:54:09 INFO DefaultExecutionContext: Created broadcast 1 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:09 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1 (MappedRDD[5] at map at SparkPlan.scala:84)
15/08/06 17:54:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/08/06 17:54:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:54:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/08/06 17:54:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 618 bytes result sent to driver
15/08/06 17:54:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 26 ms on localhost (1/1)
15/08/06 17:54:09 INFO DAGScheduler: Stage 1 (collect at SparkPlan.scala:84) finished in 0.032 s
15/08/06 17:54:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/06 17:54:09 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@474d3be5
15/08/06 17:54:09 INFO DAGScheduler: Job 1 finished: collect at SparkPlan.scala:84, took 0.062867 s
Time taken: 0.294 seconds
15/08/06 17:54:09 INFO CliDriver: Time taken: 0.294 seconds
15/08/06 17:54:09 INFO StatsReportListener: task runtime:(count: 1, mean: 26.000000, stdev: 0.000000, max: 26.000000, min: 26.000000)
15/08/06 17:54:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:09 INFO StatsReportListener: 	26.0 ms	26.0 ms	26.0 ms	26.0 ms	26.0 ms	26.0 ms	26.0 ms	26.0 ms	26.0 ms
15/08/06 17:54:09 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:54:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:09 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:54:09 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 11.538462, stdev: 0.000000, max: 11.538462, min: 11.538462)
15/08/06 17:54:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:09 INFO StatsReportListener: 	12 %	12 %	12 %	12 %	12 %	12 %	12 %	12 %	12 %
15/08/06 17:54:09 INFO StatsReportListener: other time pct: (count: 1, mean: 88.461538, stdev: 0.000000, max: 88.461538, min: 88.461538)
15/08/06 17:54:09 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:09 INFO StatsReportListener: 	88 %	88 %	88 %	88 %	88 %	88 %	88 %	88 %	88 %
15/08/06 17:54:09 INFO ParseDriver: Parsing command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:54:09 INFO ParseDriver: Parse Completed
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO ParseDriver: Parsing command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:54:09 INFO ParseDriver: Parse Completed
15/08/06 17:54:09 INFO PerfLogger: </PERFLOG method=parse start=1438883649954 end=1438883649954 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:09 INFO SemanticAnalyzer: Starting Semantic Analysis
15/08/06 17:54:09 INFO SemanticAnalyzer: Creating table q11_important_stock_par_spark position=13
15/08/06 17:54:10 INFO Driver: Semantic Analysis Completed
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438883649954 end=1438883650001 duration=47 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=compile start=1438883649953 end=1438883650002 duration=49 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO Driver: Starting command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438883649953 end=1438883650004 duration=51 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=runTasks start=1438883650004 end=1438883650051 duration=47 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438883650002 end=1438883650051 duration=49 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:54:10 INFO Driver: OK
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883650052 end=1438883650052 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=Driver.run start=1438883649953 end=1438883650052 duration=99 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:54:10 INFO DAGScheduler: Got job 2 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:54:10 INFO DAGScheduler: Final stage: Stage 2(collect at SparkPlan.scala:84)
15/08/06 17:54:10 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:54:10 INFO DAGScheduler: Missing parents: List()
15/08/06 17:54:10 INFO DAGScheduler: Submitting Stage 2 (MappedRDD[8] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:54:10 INFO MemoryStore: ensureFreeSpace(2560) called with curMem=6207, maxMem=3333968363
15/08/06 17:54:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 3.1 GB)
15/08/06 17:54:10 INFO MemoryStore: ensureFreeSpace(1562) called with curMem=8767, maxMem=3333968363
15/08/06 17:54:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1562.0 B, free 3.1 GB)
15/08/06 17:54:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:37948 (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:54:10 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/08/06 17:54:10 INFO DefaultExecutionContext: Created broadcast 2 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:10 INFO DAGScheduler: Submitting 1 missing tasks from Stage 2 (MappedRDD[8] at map at SparkPlan.scala:84)
15/08/06 17:54:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/06 17:54:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:54:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
15/08/06 17:54:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 618 bytes result sent to driver
15/08/06 17:54:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 30 ms on localhost (1/1)
15/08/06 17:54:10 INFO DAGScheduler: Stage 2 (collect at SparkPlan.scala:84) finished in 0.037 s
15/08/06 17:54:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/06 17:54:10 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2fdd84ca
15/08/06 17:54:10 INFO DAGScheduler: Job 2 finished: collect at SparkPlan.scala:84, took 0.065075 s
15/08/06 17:54:10 INFO StatsReportListener: task runtime:(count: 1, mean: 30.000000, stdev: 0.000000, max: 30.000000, min: 30.000000)
15/08/06 17:54:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:10 INFO StatsReportListener: 	30.0 ms	30.0 ms	30.0 ms	30.0 ms	30.0 ms	30.0 ms	30.0 ms	30.0 ms	30.0 ms
15/08/06 17:54:10 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:54:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:10 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:54:10 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 20.000000, stdev: 0.000000, max: 20.000000, min: 20.000000)
15/08/06 17:54:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:10 INFO StatsReportListener: 	20 %	20 %	20 %	20 %	20 %	20 %	20 %	20 %	20 %
Time taken: 0.377 seconds
15/08/06 17:54:10 INFO CliDriver: Time taken: 0.377 seconds
15/08/06 17:54:10 INFO StatsReportListener: other time pct: (count: 1, mean: 80.000000, stdev: 0.000000, max: 80.000000, min: 80.000000)
15/08/06 17:54:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:10 INFO StatsReportListener: 	80 %	80 %	80 %	80 %	80 %	80 %	80 %	80 %	80 %
15/08/06 17:54:10 INFO ParseDriver: Parsing command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:54:10 INFO ParseDriver: Parse Completed
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO ParseDriver: Parsing command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:54:10 INFO ParseDriver: Parse Completed
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=parse start=1438883650271 end=1438883650272 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO SemanticAnalyzer: Starting Semantic Analysis
15/08/06 17:54:10 INFO SemanticAnalyzer: Creating table q11_part_tmp_par_spark position=13
15/08/06 17:54:10 INFO Driver: Semantic Analysis Completed
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438883650272 end=1438883650287 duration=15 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=compile start=1438883650271 end=1438883650288 duration=17 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO Driver: Starting command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438883650270 end=1438883650288 duration=18 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=runTasks start=1438883650288 end=1438883650328 duration=40 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438883650288 end=1438883650329 duration=41 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:54:10 INFO Driver: OK
15/08/06 17:54:10 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883650329 end=1438883650329 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO PerfLogger: </PERFLOG method=Driver.run start=1438883650270 end=1438883650329 duration=59 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:10 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:54:10 INFO DAGScheduler: Got job 3 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:54:10 INFO DAGScheduler: Final stage: Stage 3(collect at SparkPlan.scala:84)
15/08/06 17:54:10 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:54:10 INFO DAGScheduler: Missing parents: List()
15/08/06 17:54:10 INFO DAGScheduler: Submitting Stage 3 (MappedRDD[11] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:54:10 INFO MemoryStore: ensureFreeSpace(2560) called with curMem=10329, maxMem=3333968363
15/08/06 17:54:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 3.1 GB)
15/08/06 17:54:10 INFO MemoryStore: ensureFreeSpace(1562) called with curMem=12889, maxMem=3333968363
15/08/06 17:54:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1562.0 B, free 3.1 GB)
15/08/06 17:54:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:37948 (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:54:10 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/08/06 17:54:10 INFO DefaultExecutionContext: Created broadcast 3 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:10 INFO DAGScheduler: Submitting 1 missing tasks from Stage 3 (MappedRDD[11] at map at SparkPlan.scala:84)
15/08/06 17:54:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
15/08/06 17:54:10 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:54:10 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
15/08/06 17:54:10 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 618 bytes result sent to driver
15/08/06 17:54:10 INFO DAGScheduler: Stage 3 (collect at SparkPlan.scala:84) finished in 0.031 s
15/08/06 17:54:10 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@dfa60d7
15/08/06 17:54:10 INFO DAGScheduler: Job 3 finished: collect at SparkPlan.scala:84, took 0.063099 s
15/08/06 17:54:10 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 27 ms on localhost (1/1)
Time taken: 0.258 seconds
15/08/06 17:54:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/06 17:54:10 INFO CliDriver: Time taken: 0.258 seconds
15/08/06 17:54:10 INFO StatsReportListener: task runtime:(count: 1, mean: 27.000000, stdev: 0.000000, max: 27.000000, min: 27.000000)
15/08/06 17:54:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:10 INFO StatsReportListener: 	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms
15/08/06 17:54:10 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:54:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:10 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:54:10 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 14.814815, stdev: 0.000000, max: 14.814815, min: 14.814815)
15/08/06 17:54:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:10 INFO StatsReportListener: 	15 %	15 %	15 %	15 %	15 %	15 %	15 %	15 %	15 %
15/08/06 17:54:10 INFO StatsReportListener: other time pct: (count: 1, mean: 85.185185, stdev: 0.000000, max: 85.185185, min: 85.185185)
15/08/06 17:54:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:10 INFO StatsReportListener: 	85 %	85 %	85 %	85 %	85 %	85 %	85 %	85 %	85 %
15/08/06 17:54:10 INFO ParseDriver: Parsing command: insert into table q11_part_tmp_par_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey
15/08/06 17:54:10 INFO ParseDriver: Parse Completed
15/08/06 17:54:11 INFO BlockManager: Removing broadcast 3
15/08/06 17:54:11 INFO BlockManager: Removing block broadcast_3_piece0
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_3_piece0 of size 1562 dropped from memory (free 3333955474)
15/08/06 17:54:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:37948 in memory (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:54:11 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/08/06 17:54:11 INFO BlockManager: Removing block broadcast_3
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_3 of size 2560 dropped from memory (free 3333958034)
15/08/06 17:54:11 INFO ContextCleaner: Cleaned broadcast 3
15/08/06 17:54:11 INFO BlockManager: Removing broadcast 2
15/08/06 17:54:11 INFO BlockManager: Removing block broadcast_2
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_2 of size 2560 dropped from memory (free 3333960594)
15/08/06 17:54:11 INFO BlockManager: Removing block broadcast_2_piece0
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_2_piece0 of size 1562 dropped from memory (free 3333962156)
15/08/06 17:54:11 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:37948 in memory (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:54:11 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/08/06 17:54:11 INFO ContextCleaner: Cleaned broadcast 2
15/08/06 17:54:11 INFO BlockManager: Removing broadcast 1
15/08/06 17:54:11 INFO BlockManager: Removing block broadcast_1_piece0
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_1_piece0 of size 1207 dropped from memory (free 3333963363)
15/08/06 17:54:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:37948 in memory (size: 1207.0 B, free: 3.1 GB)
15/08/06 17:54:11 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/08/06 17:54:11 INFO BlockManager: Removing block broadcast_1
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_1 of size 1896 dropped from memory (free 3333965259)
15/08/06 17:54:11 INFO ContextCleaner: Cleaned broadcast 1
15/08/06 17:54:11 INFO BlockManager: Removing broadcast 0
15/08/06 17:54:11 INFO BlockManager: Removing block broadcast_0
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_0 of size 1896 dropped from memory (free 3333967155)
15/08/06 17:54:11 INFO BlockManager: Removing block broadcast_0_piece0
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_0_piece0 of size 1208 dropped from memory (free 3333968363)
15/08/06 17:54:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:37948 in memory (size: 1208.0 B, free: 3.1 GB)
15/08/06 17:54:11 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/08/06 17:54:11 INFO ContextCleaner: Cleaned broadcast 0
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/06 17:54:11 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(n_nationkey#31, n_name#32, n_regionkey#33, n_comment#34)
15/08/06 17:54:11 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(s_suppkey#35, s_name#36, s_address#37, s_nationkey#38, s_phone#39, s_acctbal#40, s_comment#41)
15/08/06 17:54:11 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#42, ps_suppkey#43, ps_availqty#44, ps_supplycost#45, ps_comment#46)
15/08/06 17:54:11 INFO MemoryStore: ensureFreeSpace(281594) called with curMem=0, maxMem=3333968363
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 275.0 KB, free 3.1 GB)
15/08/06 17:54:11 INFO MemoryStore: ensureFreeSpace(281634) called with curMem=281594, maxMem=3333968363
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 275.0 KB, free 3.1 GB)
15/08/06 17:54:11 INFO MemoryStore: ensureFreeSpace(31895) called with curMem=563228, maxMem=3333968363
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:54:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:37948 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:54:11 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/08/06 17:54:11 INFO DefaultExecutionContext: Created broadcast 4 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:54:11 INFO MemoryStore: ensureFreeSpace(31949) called with curMem=595123, maxMem=3333968363
15/08/06 17:54:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 31.2 KB, free 3.1 GB)
15/08/06 17:54:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:37948 (size: 31.2 KB, free: 3.1 GB)
15/08/06 17:54:11 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0
15/08/06 17:54:11 INFO DefaultExecutionContext: Created broadcast 5 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:54:11 INFO FileInputFormat: Total input paths to process : 1
15/08/06 17:54:11 INFO ParquetInputFormat: Total input paths to process : 1
15/08/06 17:54:11 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:54:11 INFO ParquetFileReader: reading another 1 footers
15/08/06 17:54:11 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:54:11 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0; isDirectory=false; length=1493206; replication=1; blocksize=134217728; modification_time=1438802778074; access_time=1438881494768; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 17 ms
15/08/06 17:54:11 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/06 17:54:11 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/06 17:54:12 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:54:12 INFO DefaultExecutionContext: Starting job: collect at BroadcastHashJoin.scala:53
15/08/06 17:54:12 INFO DAGScheduler: Got job 4 (collect at BroadcastHashJoin.scala:53) with 1 output partitions (allowLocal=false)
15/08/06 17:54:12 INFO DAGScheduler: Final stage: Stage 4(collect at BroadcastHashJoin.scala:53)
15/08/06 17:54:12 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:54:12 INFO DAGScheduler: Missing parents: List()
15/08/06 17:54:12 INFO DAGScheduler: Submitting Stage 4 (MappedRDD[29] at map at BroadcastHashJoin.scala:53), which has no missing parents
15/08/06 17:54:12 INFO MemoryStore: ensureFreeSpace(2488) called with curMem=627072, maxMem=3333968363
15/08/06 17:54:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.4 KB, free 3.1 GB)
15/08/06 17:54:12 INFO MemoryStore: ensureFreeSpace(1467) called with curMem=629560, maxMem=3333968363
15/08/06 17:54:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1467.0 B, free 3.1 GB)
15/08/06 17:54:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37948 (size: 1467.0 B, free: 3.1 GB)
15/08/06 17:54:12 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/08/06 17:54:12 INFO DefaultExecutionContext: Created broadcast 6 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:12 INFO DAGScheduler: Submitting 1 missing tasks from Stage 4 (MappedRDD[29] at map at BroadcastHashJoin.scala:53)
15/08/06 17:54:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15/08/06 17:54:12 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, ANY, 1586 bytes)
15/08/06 17:54:12 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
15/08/06 17:54:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0 start: 0 end: 1493206 length: 1493206 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 10000 records.
15/08/06 17:54:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:12 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 10000
15/08/06 17:54:12 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 144335 bytes result sent to driver
15/08/06 17:54:12 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 469 ms on localhost (1/1)
15/08/06 17:54:12 INFO DAGScheduler: Stage 4 (collect at BroadcastHashJoin.scala:53) finished in 0.662 s
15/08/06 17:54:12 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/06 17:54:12 INFO DAGScheduler: Job 4 finished: collect at BroadcastHashJoin.scala:53, took 0.704847 s
15/08/06 17:54:12 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@31684f41
15/08/06 17:54:12 INFO StatsReportListener: task runtime:(count: 1, mean: 469.000000, stdev: 0.000000, max: 469.000000, min: 469.000000)
15/08/06 17:54:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:12 INFO StatsReportListener: 	469.0 ms	469.0 ms	469.0 ms	469.0 ms	469.0 ms	469.0 ms	469.0 ms	469.0 ms	469.0 ms
15/08/06 17:54:12 INFO StatsReportListener: task result size:(count: 1, mean: 144335.000000, stdev: 0.000000, max: 144335.000000, min: 144335.000000)
15/08/06 17:54:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:12 INFO StatsReportListener: 	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB
15/08/06 17:54:12 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 71.002132, stdev: 0.000000, max: 71.002132, min: 71.002132)
15/08/06 17:54:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:12 INFO StatsReportListener: 	71 %	71 %	71 %	71 %	71 %	71 %	71 %	71 %	71 %
15/08/06 17:54:12 INFO StatsReportListener: other time pct: (count: 1, mean: 28.997868, stdev: 0.000000, max: 28.997868, min: 28.997868)
15/08/06 17:54:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:12 INFO StatsReportListener: 	29 %	29 %	29 %	29 %	29 %	29 %	29 %	29 %	29 %
15/08/06 17:54:12 INFO MemoryStore: ensureFreeSpace(65648) called with curMem=631027, maxMem=3333968363
15/08/06 17:54:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 64.1 KB, free 3.1 GB)
15/08/06 17:54:12 INFO MemoryStore: ensureFreeSpace(55237) called with curMem=696675, maxMem=3333968363
15/08/06 17:54:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 53.9 KB, free 3.1 GB)
15/08/06 17:54:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37948 (size: 53.9 KB, free: 3.1 GB)
15/08/06 17:54:12 INFO BlockManagerMaster: Updated info of block broadcast_7_piece0
15/08/06 17:54:12 INFO DefaultExecutionContext: Created broadcast 7 from broadcast at BroadcastHashJoin.scala:55
15/08/06 17:54:12 INFO MemoryStore: ensureFreeSpace(281194) called with curMem=751912, maxMem=3333968363
15/08/06 17:54:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 274.6 KB, free 3.1 GB)
15/08/06 17:54:13 INFO MemoryStore: ensureFreeSpace(31858) called with curMem=1033106, maxMem=3333968363
15/08/06 17:54:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:54:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:37948 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:54:13 INFO BlockManagerMaster: Updated info of block broadcast_8_piece0
15/08/06 17:54:13 INFO DefaultExecutionContext: Created broadcast 8 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:54:13 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/06 17:54:13 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/06 17:54:13 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/06 17:54:13 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/06 17:54:13 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/06 17:54:13 INFO DefaultExecutionContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/08/06 17:54:13 INFO FileInputFormat: Total input paths to process : 1
15/08/06 17:54:13 INFO ParquetInputFormat: Total input paths to process : 1
15/08/06 17:54:13 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:54:13 INFO ParquetFileReader: reading another 1 footers
15/08/06 17:54:13 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:54:13 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0; isDirectory=false; length=3216; replication=1; blocksize=134217728; modification_time=1438802774354; access_time=1438881494331; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 20 ms
15/08/06 17:54:13 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:54:13 INFO FileInputFormat: Total input paths to process : 10
15/08/06 17:54:13 INFO ParquetInputFormat: Total input paths to process : 10
15/08/06 17:54:13 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:54:13 INFO ParquetFileReader: reading another 10 footers
15/08/06 17:54:13 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:54:13 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0; isDirectory=false; length=11478369; replication=1; blocksize=134217728; modification_time=1438802779757; access_time=1438881494861; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0; isDirectory=false; length=11437027; replication=1; blocksize=134217728; modification_time=1438802780428; access_time=1438881496406; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0; isDirectory=false; length=11437121; replication=1; blocksize=134217728; modification_time=1438802779669; access_time=1438881496411; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0; isDirectory=false; length=11437895; replication=1; blocksize=134217728; modification_time=1438802779902; access_time=1438881496416; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0; isDirectory=false; length=11436112; replication=1; blocksize=134217728; modification_time=1438802779829; access_time=1438881496416; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0; isDirectory=false; length=11361987; replication=1; blocksize=134217728; modification_time=1438802780059; access_time=1438881496419; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0; isDirectory=false; length=11361789; replication=1; blocksize=134217728; modification_time=1438802780860; access_time=1438881496423; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0; isDirectory=false; length=11362156; replication=1; blocksize=134217728; modification_time=1438802781041; access_time=1438881496427; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0; isDirectory=false; length=11362379; replication=1; blocksize=134217728; modification_time=1438802780967; access_time=1438881496428; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0; isDirectory=false; length=11361401; replication=1; blocksize=134217728; modification_time=1438802781090; access_time=1438881496428; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 35 ms
15/08/06 17:54:13 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:54:13 INFO DAGScheduler: Registering RDD 30 (mapPartitions at Exchange.scala:64)
15/08/06 17:54:13 INFO DAGScheduler: Registering RDD 39 (mapPartitions at Exchange.scala:64)
15/08/06 17:54:13 INFO DAGScheduler: Registering RDD 45 (mapPartitions at Exchange.scala:64)
15/08/06 17:54:13 INFO DAGScheduler: Got job 5 (runJob at InsertIntoHiveTable.scala:93) with 200 output partitions (allowLocal=false)
15/08/06 17:54:13 INFO DAGScheduler: Final stage: Stage 8(runJob at InsertIntoHiveTable.scala:93)
15/08/06 17:54:13 INFO DAGScheduler: Parents of final stage: List(Stage 7)
15/08/06 17:54:13 INFO DAGScheduler: Missing parents: List(Stage 7)
15/08/06 17:54:13 INFO DAGScheduler: Submitting Stage 5 (MapPartitionsRDD[30] at mapPartitions at Exchange.scala:64), which has no missing parents
15/08/06 17:54:13 INFO MemoryStore: ensureFreeSpace(7264) called with curMem=1064964, maxMem=3333968363
15/08/06 17:54:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.1 KB, free 3.1 GB)
15/08/06 17:54:13 INFO MemoryStore: ensureFreeSpace(4177) called with curMem=1072228, maxMem=3333968363
15/08/06 17:54:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 3.1 GB)
15/08/06 17:54:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:37948 (size: 4.1 KB, free: 3.1 GB)
15/08/06 17:54:13 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/08/06 17:54:13 INFO DefaultExecutionContext: Created broadcast 9 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:13 INFO DAGScheduler: Submitting 10 missing tasks from Stage 5 (MapPartitionsRDD[30] at mapPartitions at Exchange.scala:64)
15/08/06 17:54:13 INFO TaskSchedulerImpl: Adding task set 5.0 with 10 tasks
15/08/06 17:54:13 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, ANY, 1581 bytes)
15/08/06 17:54:13 INFO DAGScheduler: Submitting Stage 6 (MapPartitionsRDD[39] at mapPartitions at Exchange.scala:64), which has no missing parents
15/08/06 17:54:13 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, ANY, 1585 bytes)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 7, localhost, ANY, 1584 bytes)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 8, localhost, ANY, 1584 bytes)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 9, localhost, ANY, 1584 bytes)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 10, localhost, ANY, 1584 bytes)
15/08/06 17:54:13 INFO MemoryStore: ensureFreeSpace(10096) called with curMem=1076405, maxMem=3333968363
15/08/06 17:54:13 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.9 KB, free 3.1 GB)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 11, localhost, ANY, 1583 bytes)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 12, localhost, ANY, 1582 bytes)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 13, localhost, ANY, 1583 bytes)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 14, localhost, ANY, 1582 bytes)
15/08/06 17:54:13 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
15/08/06 17:54:13 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
15/08/06 17:54:13 INFO Executor: Running task 2.0 in stage 5.0 (TID 7)
15/08/06 17:54:13 INFO Executor: Running task 3.0 in stage 5.0 (TID 8)
15/08/06 17:54:13 INFO MemoryStore: ensureFreeSpace(5665) called with curMem=1086501, maxMem=3333968363
15/08/06 17:54:13 INFO Executor: Running task 5.0 in stage 5.0 (TID 10)
15/08/06 17:54:13 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.5 KB, free 3.1 GB)
15/08/06 17:54:13 INFO Executor: Running task 6.0 in stage 5.0 (TID 11)
15/08/06 17:54:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:37948 (size: 5.5 KB, free: 3.1 GB)
15/08/06 17:54:13 INFO Executor: Running task 4.0 in stage 5.0 (TID 9)
15/08/06 17:54:13 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/08/06 17:54:13 INFO Executor: Running task 7.0 in stage 5.0 (TID 12)
15/08/06 17:54:13 INFO DefaultExecutionContext: Created broadcast 10 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:13 INFO Executor: Running task 8.0 in stage 5.0 (TID 13)
15/08/06 17:54:13 INFO DAGScheduler: Submitting 1 missing tasks from Stage 6 (MapPartitionsRDD[39] at mapPartitions at Exchange.scala:64)
15/08/06 17:54:13 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/08/06 17:54:13 INFO Executor: Running task 9.0 in stage 5.0 (TID 14)
15/08/06 17:54:13 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 15, localhost, ANY, 1552 bytes)
15/08/06 17:54:13 INFO Executor: Running task 0.0 in stage 6.0 (TID 15)
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 0 end: 11478369 length: 11478369 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 0 end: 11437895 length: 11437895 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 0 end: 11437121 length: 11437121 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 0 end: 11437027 length: 11437027 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 0 end: 11362156 length: 11362156 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 0 end: 11436112 length: 11436112 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 0 end: 11361987 length: 11361987 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 0 end: 11361401 length: 11361401 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 0 end: 11362379 length: 11362379 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 0 end: 11361789 length: 11361789 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80195 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80260 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80615 records.
15/08/06 17:54:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0 start: 0 end: 3216 length: 3216 hosts: [] requestedSchema: message root {
  optional int32 n_nationkey;
  optional binary n_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}},{"name":"n_regionkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80265 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79671 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79706 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80206 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79826 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79589 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 25 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79667 records.
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 25
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 80265
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 80260
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 79671
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 79826
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 79706
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 80195
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 79589
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 80615
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 80206
15/08/06 17:54:13 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 79667
15/08/06 17:54:14 INFO Executor: Finished task 0.0 in stage 6.0 (TID 15). 2019 bytes result sent to driver
15/08/06 17:54:14 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 15) in 703 ms on localhost (1/1)
15/08/06 17:54:14 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/06 17:54:14 INFO DAGScheduler: Stage 6 (mapPartitions at Exchange.scala:64) finished in 0.710 s
15/08/06 17:54:14 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5c9819fc
15/08/06 17:54:14 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:54:14 INFO DAGScheduler: running: Set(Stage 5)
15/08/06 17:54:14 INFO DAGScheduler: waiting: Set(Stage 7, Stage 8)
15/08/06 17:54:14 INFO StatsReportListener: task runtime:(count: 1, mean: 703.000000, stdev: 0.000000, max: 703.000000, min: 703.000000)
15/08/06 17:54:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:14 INFO StatsReportListener: 	703.0 ms	703.0 ms	703.0 ms	703.0 ms	703.0 ms	703.0 ms	703.0 ms	703.0 ms	703.0 ms
15/08/06 17:54:14 INFO DAGScheduler: failed: Set()
15/08/06 17:54:14 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 13231.000000, stdev: 0.000000, max: 13231.000000, min: 13231.000000)
15/08/06 17:54:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:14 INFO StatsReportListener: 	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB
15/08/06 17:54:14 INFO StatsReportListener: task result size:(count: 1, mean: 2019.000000, stdev: 0.000000, max: 2019.000000, min: 2019.000000)
15/08/06 17:54:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:14 INFO StatsReportListener: 	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B
15/08/06 17:54:14 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 97.439545, stdev: 0.000000, max: 97.439545, min: 97.439545)
15/08/06 17:54:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:14 INFO StatsReportListener: 	97 %	97 %	97 %	97 %	97 %	97 %	97 %	97 %	97 %
15/08/06 17:54:14 INFO StatsReportListener: other time pct: (count: 1, mean: 2.560455, stdev: 0.000000, max: 2.560455, min: 2.560455)
15/08/06 17:54:14 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:14 INFO StatsReportListener: 	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %
15/08/06 17:54:14 INFO DAGScheduler: Missing parents for Stage 7: List(Stage 5)
15/08/06 17:54:14 INFO DAGScheduler: Missing parents for Stage 8: List(Stage 7)
15/08/06 17:54:15 INFO Executor: Finished task 9.0 in stage 5.0 (TID 14). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 14) in 1814 ms on localhost (1/10)
15/08/06 17:54:15 INFO Executor: Finished task 2.0 in stage 5.0 (TID 7). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 7) in 1849 ms on localhost (2/10)
15/08/06 17:54:15 INFO Executor: Finished task 8.0 in stage 5.0 (TID 13). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 13) in 1875 ms on localhost (3/10)
15/08/06 17:54:15 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1905 ms on localhost (4/10)
15/08/06 17:54:15 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 1906 ms on localhost (5/10)
15/08/06 17:54:15 INFO Executor: Finished task 5.0 in stage 5.0 (TID 10). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO Executor: Finished task 7.0 in stage 5.0 (TID 12). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO Executor: Finished task 3.0 in stage 5.0 (TID 8). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 10) in 1917 ms on localhost (6/10)
15/08/06 17:54:15 INFO Executor: Finished task 6.0 in stage 5.0 (TID 11). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 12) in 1919 ms on localhost (7/10)
15/08/06 17:54:15 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 8) in 1925 ms on localhost (8/10)
15/08/06 17:54:15 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 11) in 1925 ms on localhost (9/10)
15/08/06 17:54:15 INFO Executor: Finished task 4.0 in stage 5.0 (TID 9). 2019 bytes result sent to driver
15/08/06 17:54:15 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 9) in 1937 ms on localhost (10/10)
15/08/06 17:54:15 INFO DAGScheduler: Stage 5 (mapPartitions at Exchange.scala:64) finished in 1.944 s
15/08/06 17:54:15 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/06 17:54:15 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:54:15 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1a014a8a
15/08/06 17:54:15 INFO DAGScheduler: running: Set()
15/08/06 17:54:15 INFO DAGScheduler: waiting: Set(Stage 7, Stage 8)
15/08/06 17:54:15 INFO DAGScheduler: failed: Set()
15/08/06 17:54:15 INFO StatsReportListener: task runtime:(count: 10, mean: 1897.200000, stdev: 37.247282, max: 1937.000000, min: 1814.000000)
15/08/06 17:54:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:15 INFO StatsReportListener: 	1.8 s	1.8 s	1.8 s	1.9 s	1.9 s	1.9 s	1.9 s	1.9 s	1.9 s
15/08/06 17:54:15 INFO StatsReportListener: shuffle bytes written:(count: 10, mean: 7582991.200000, stdev: 26756.903116, max: 7616204.000000, min: 7549451.000000)
15/08/06 17:54:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:15 INFO StatsReportListener: 	7.2 MB	7.2 MB	7.2 MB	7.2 MB	7.3 MB	7.3 MB	7.3 MB	7.3 MB	7.3 MB
15/08/06 17:54:15 INFO StatsReportListener: task result size:(count: 10, mean: 2019.000000, stdev: 0.000000, max: 2019.000000, min: 2019.000000)
15/08/06 17:54:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:15 INFO StatsReportListener: 	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B
15/08/06 17:54:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 10, mean: 98.479089, stdev: 0.293936, max: 98.950131, min: 97.739802)
15/08/06 17:54:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:15 INFO StatsReportListener: 	98 %	98 %	98 %	98 %	99 %	99 %	99 %	99 %	99 %
15/08/06 17:54:15 INFO StatsReportListener: other time pct: (count: 10, mean: 1.520911, stdev: 0.293936, max: 2.260198, min: 1.049869)
15/08/06 17:54:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:15 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 2 %	 2 %	 2 %	 2 %	 2 %
15/08/06 17:54:15 INFO DAGScheduler: Missing parents for Stage 7: List()
15/08/06 17:54:15 INFO DAGScheduler: Missing parents for Stage 8: List(Stage 7)
15/08/06 17:54:15 INFO DAGScheduler: Submitting Stage 7 (MapPartitionsRDD[45] at mapPartitions at Exchange.scala:64), which is now runnable
15/08/06 17:54:15 INFO MemoryStore: ensureFreeSpace(13544) called with curMem=1092166, maxMem=3333968363
15/08/06 17:54:15 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.2 KB, free 3.1 GB)
15/08/06 17:54:15 INFO MemoryStore: ensureFreeSpace(7369) called with curMem=1105710, maxMem=3333968363
15/08/06 17:54:15 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.2 KB, free 3.1 GB)
15/08/06 17:54:15 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:37948 (size: 7.2 KB, free: 3.1 GB)
15/08/06 17:54:15 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/08/06 17:54:15 INFO DefaultExecutionContext: Created broadcast 11 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:15 INFO DAGScheduler: Submitting 200 missing tasks from Stage 7 (MapPartitionsRDD[45] at mapPartitions at Exchange.scala:64)
15/08/06 17:54:15 INFO TaskSchedulerImpl: Adding task set 7.0 with 200 tasks
15/08/06 17:54:15 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 16, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 17, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 18, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 19, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 20, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 21, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 22, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 23, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 24, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 25, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 26, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 27, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 28, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 29, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 30, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 31, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:15 INFO Executor: Running task 1.0 in stage 7.0 (TID 17)
15/08/06 17:54:15 INFO Executor: Running task 4.0 in stage 7.0 (TID 20)
15/08/06 17:54:15 INFO Executor: Running task 9.0 in stage 7.0 (TID 25)
15/08/06 17:54:15 INFO Executor: Running task 8.0 in stage 7.0 (TID 24)
15/08/06 17:54:15 INFO Executor: Running task 10.0 in stage 7.0 (TID 26)
15/08/06 17:54:15 INFO Executor: Running task 5.0 in stage 7.0 (TID 21)
15/08/06 17:54:15 INFO Executor: Running task 7.0 in stage 7.0 (TID 23)
15/08/06 17:54:15 INFO Executor: Running task 3.0 in stage 7.0 (TID 19)
15/08/06 17:54:15 INFO Executor: Running task 2.0 in stage 7.0 (TID 18)
15/08/06 17:54:15 INFO Executor: Running task 6.0 in stage 7.0 (TID 22)
15/08/06 17:54:15 INFO Executor: Running task 0.0 in stage 7.0 (TID 16)
15/08/06 17:54:15 INFO Executor: Running task 12.0 in stage 7.0 (TID 28)
15/08/06 17:54:15 INFO Executor: Running task 11.0 in stage 7.0 (TID 27)
15/08/06 17:54:15 INFO Executor: Running task 13.0 in stage 7.0 (TID 29)
15/08/06 17:54:15 INFO Executor: Running task 14.0 in stage 7.0 (TID 30)
15/08/06 17:54:15 INFO Executor: Running task 15.0 in stage 7.0 (TID 31)
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:54:16 INFO Executor: Finished task 15.0 in stage 7.0 (TID 31). 1124 bytes result sent to driver
15/08/06 17:54:16 INFO Executor: Finished task 12.0 in stage 7.0 (TID 28). 1124 bytes result sent to driver
15/08/06 17:54:16 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 32, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:16 INFO Executor: Running task 16.0 in stage 7.0 (TID 32)
15/08/06 17:54:16 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 31) in 873 ms on localhost (1/200)
15/08/06 17:54:16 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 33, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:16 INFO Executor: Running task 17.0 in stage 7.0 (TID 33)
15/08/06 17:54:16 INFO Executor: Finished task 7.0 in stage 7.0 (TID 23). 1124 bytes result sent to driver
15/08/06 17:54:16 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 28) in 882 ms on localhost (2/200)
15/08/06 17:54:16 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 34, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:16 INFO Executor: Running task 18.0 in stage 7.0 (TID 34)
15/08/06 17:54:16 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 23) in 899 ms on localhost (3/200)
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:16 INFO BlockManager: Removing broadcast 10
15/08/06 17:54:16 INFO BlockManager: Removing block broadcast_10
15/08/06 17:54:16 INFO MemoryStore: Block broadcast_10 of size 10096 dropped from memory (free 3332865380)
15/08/06 17:54:16 INFO BlockManager: Removing block broadcast_10_piece0
15/08/06 17:54:16 INFO MemoryStore: Block broadcast_10_piece0 of size 5665 dropped from memory (free 3332871045)
15/08/06 17:54:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:37948 in memory (size: 5.5 KB, free: 3.1 GB)
15/08/06 17:54:16 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/08/06 17:54:16 INFO ContextCleaner: Cleaned broadcast 10
15/08/06 17:54:16 INFO BlockManager: Removing broadcast 9
15/08/06 17:54:16 INFO BlockManager: Removing block broadcast_9
15/08/06 17:54:16 INFO MemoryStore: Block broadcast_9 of size 7264 dropped from memory (free 3332878309)
15/08/06 17:54:16 INFO BlockManager: Removing block broadcast_9_piece0
15/08/06 17:54:16 INFO MemoryStore: Block broadcast_9_piece0 of size 4177 dropped from memory (free 3332882486)
15/08/06 17:54:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:37948 in memory (size: 4.1 KB, free: 3.1 GB)
15/08/06 17:54:16 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/08/06 17:54:16 INFO ContextCleaner: Cleaned broadcast 9
15/08/06 17:54:16 INFO BlockManager: Removing broadcast 6
15/08/06 17:54:16 INFO BlockManager: Removing block broadcast_6_piece0
15/08/06 17:54:16 INFO MemoryStore: Block broadcast_6_piece0 of size 1467 dropped from memory (free 3332883953)
15/08/06 17:54:16 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:37948 in memory (size: 1467.0 B, free: 3.1 GB)
15/08/06 17:54:16 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/08/06 17:54:16 INFO BlockManager: Removing block broadcast_6
15/08/06 17:54:16 INFO MemoryStore: Block broadcast_6 of size 2488 dropped from memory (free 3332886441)
15/08/06 17:54:16 INFO ContextCleaner: Cleaned broadcast 6
15/08/06 17:54:16 INFO BlockManager: Removing broadcast 4
15/08/06 17:54:16 INFO BlockManager: Removing block broadcast_4
15/08/06 17:54:16 INFO MemoryStore: Block broadcast_4 of size 281594 dropped from memory (free 3333168035)
15/08/06 17:54:16 INFO BlockManager: Removing block broadcast_4_piece0
15/08/06 17:54:16 INFO MemoryStore: Block broadcast_4_piece0 of size 31895 dropped from memory (free 3333199930)
15/08/06 17:54:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:37948 in memory (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:54:16 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/08/06 17:54:16 INFO ContextCleaner: Cleaned broadcast 4
15/08/06 17:54:17 INFO Executor: Finished task 13.0 in stage 7.0 (TID 29). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 35, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 19.0 in stage 7.0 (TID 35)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 29) in 1745 ms on localhost (4/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO Executor: Finished task 9.0 in stage 7.0 (TID 25). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 36, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 20.0 in stage 7.0 (TID 36)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 25) in 1793 ms on localhost (5/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO Executor: Finished task 10.0 in stage 7.0 (TID 26). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 37, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 21.0 in stage 7.0 (TID 37)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 26) in 1850 ms on localhost (6/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO Executor: Finished task 2.0 in stage 7.0 (TID 18). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO Executor: Finished task 6.0 in stage 7.0 (TID 22). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO Executor: Finished task 14.0 in stage 7.0 (TID 30). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 38, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 22.0 in stage 7.0 (TID 38)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 18) in 1879 ms on localhost (7/200)
15/08/06 17:54:17 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 39, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 23.0 in stage 7.0 (TID 39)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 22) in 1879 ms on localhost (8/200)
15/08/06 17:54:17 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 40, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 24.0 in stage 7.0 (TID 40)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 30) in 1876 ms on localhost (9/200)
15/08/06 17:54:17 INFO Executor: Finished task 8.0 in stage 7.0 (TID 24). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 41, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 25.0 in stage 7.0 (TID 41)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 24) in 1886 ms on localhost (10/200)
15/08/06 17:54:17 INFO Executor: Finished task 1.0 in stage 7.0 (TID 17). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/06 17:54:17 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 42, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO Executor: Finished task 3.0 in stage 7.0 (TID 19). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO Executor: Finished task 4.0 in stage 7.0 (TID 20). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO Executor: Finished task 5.0 in stage 7.0 (TID 21). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO Executor: Running task 26.0 in stage 7.0 (TID 42)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 17) in 1895 ms on localhost (11/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/06 17:54:17 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 43, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 27.0 in stage 7.0 (TID 43)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 19) in 1903 ms on localhost (12/200)
15/08/06 17:54:17 INFO Executor: Finished task 0.0 in stage 7.0 (TID 16). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 44, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 45, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 29.0 in stage 7.0 (TID 45)
15/08/06 17:54:17 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 46, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 30.0 in stage 7.0 (TID 46)
15/08/06 17:54:17 INFO Executor: Running task 28.0 in stage 7.0 (TID 44)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO Executor: Finished task 11.0 in stage 7.0 (TID 27). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 20) in 1907 ms on localhost (13/200)
15/08/06 17:54:17 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 47, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/06 17:54:17 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 16) in 1913 ms on localhost (14/200)
15/08/06 17:54:17 INFO Executor: Finished task 17.0 in stage 7.0 (TID 33). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 21) in 1910 ms on localhost (15/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO Executor: Running task 31.0 in stage 7.0 (TID 47)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO Executor: Finished task 18.0 in stage 7.0 (TID 34). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 27) in 1908 ms on localhost (16/200)
15/08/06 17:54:17 INFO Executor: Finished task 16.0 in stage 7.0 (TID 32). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 33) in 1030 ms on localhost (17/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 48, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 49, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 33.0 in stage 7.0 (TID 49)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO Executor: Running task 32.0 in stage 7.0 (TID 48)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 34) in 1017 ms on localhost (18/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 50, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 34.0 in stage 7.0 (TID 50)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 32) in 1041 ms on localhost (19/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO Executor: Finished task 21.0 in stage 7.0 (TID 37). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 51, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 37) in 91 ms on localhost (20/200)
15/08/06 17:54:17 INFO Executor: Running task 35.0 in stage 7.0 (TID 51)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO Executor: Finished task 32.0 in stage 7.0 (TID 48). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 52, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 36.0 in stage 7.0 (TID 52)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 48) in 76 ms on localhost (21/200)
15/08/06 17:54:17 INFO Executor: Finished task 31.0 in stage 7.0 (TID 47). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 53, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 37.0 in stage 7.0 (TID 53)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 47) in 90 ms on localhost (22/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO Executor: Finished task 19.0 in stage 7.0 (TID 35). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 54, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 38.0 in stage 7.0 (TID 54)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 35) in 267 ms on localhost (23/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO Executor: Finished task 20.0 in stage 7.0 (TID 36). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 55, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 39.0 in stage 7.0 (TID 55)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 36) in 503 ms on localhost (24/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:17 INFO Executor: Finished task 22.0 in stage 7.0 (TID 38). 1124 bytes result sent to driver
15/08/06 17:54:17 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 56, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:17 INFO Executor: Running task 40.0 in stage 7.0 (TID 56)
15/08/06 17:54:17 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 38) in 598 ms on localhost (25/200)
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 23.0 in stage 7.0 (TID 39). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO Executor: Finished task 26.0 in stage 7.0 (TID 42). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 57, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 41.0 in stage 7.0 (TID 57)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 39) in 638 ms on localhost (26/200)
15/08/06 17:54:18 INFO Executor: Finished task 29.0 in stage 7.0 (TID 45). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 58, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 42.0 in stage 7.0 (TID 58)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 42) in 627 ms on localhost (27/200)
15/08/06 17:54:18 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 59, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 43.0 in stage 7.0 (TID 59)
15/08/06 17:54:18 INFO Executor: Finished task 24.0 in stage 7.0 (TID 40). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 45) in 618 ms on localhost (28/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 60, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Running task 44.0 in stage 7.0 (TID 60)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 25.0 in stage 7.0 (TID 41). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO Executor: Finished task 27.0 in stage 7.0 (TID 43). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 30.0 in stage 7.0 (TID 46). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 40) in 642 ms on localhost (29/200)
15/08/06 17:54:18 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 61, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Finished task 28.0 in stage 7.0 (TID 44). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO Executor: Running task 45.0 in stage 7.0 (TID 61)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 41) in 649 ms on localhost (30/200)
15/08/06 17:54:18 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 62, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 46.0 in stage 7.0 (TID 62)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 43) in 638 ms on localhost (31/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 46) in 636 ms on localhost (32/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 63, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Running task 47.0 in stage 7.0 (TID 63)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 64, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 48.0 in stage 7.0 (TID 64)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 44) in 641 ms on localhost (33/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 33.0 in stage 7.0 (TID 49). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 65, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 49.0 in stage 7.0 (TID 65)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 49) in 644 ms on localhost (34/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 34.0 in stage 7.0 (TID 50). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO Executor: Finished task 43.0 in stage 7.0 (TID 59). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 66, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 50) in 648 ms on localhost (35/200)
15/08/06 17:54:18 INFO Executor: Running task 50.0 in stage 7.0 (TID 66)
15/08/06 17:54:18 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 67, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 51.0 in stage 7.0 (TID 67)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 59) in 51 ms on localhost (36/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO Executor: Finished task 36.0 in stage 7.0 (TID 52). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO Executor: Finished task 46.0 in stage 7.0 (TID 62). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 68, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 52.0 in stage 7.0 (TID 68)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 52) in 587 ms on localhost (37/200)
15/08/06 17:54:18 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 69, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 53.0 in stage 7.0 (TID 69)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 62) in 41 ms on localhost (38/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 35.0 in stage 7.0 (TID 51). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 70, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 54.0 in stage 7.0 (TID 70)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 51) in 652 ms on localhost (39/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 37.0 in stage 7.0 (TID 53). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO Executor: Finished task 38.0 in stage 7.0 (TID 54). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 71, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 55.0 in stage 7.0 (TID 71)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 53) in 614 ms on localhost (40/200)
15/08/06 17:54:18 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 72, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 54) in 609 ms on localhost (41/200)
15/08/06 17:54:18 INFO Executor: Running task 56.0 in stage 7.0 (TID 72)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 50.0 in stage 7.0 (TID 66). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 73, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 66) in 68 ms on localhost (42/200)
15/08/06 17:54:18 INFO Executor: Running task 57.0 in stage 7.0 (TID 73)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 52.0 in stage 7.0 (TID 68). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 74, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 58.0 in stage 7.0 (TID 74)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 68) in 69 ms on localhost (43/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 39.0 in stage 7.0 (TID 55). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 75, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 59.0 in stage 7.0 (TID 75)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 55) in 367 ms on localhost (44/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 56.0 in stage 7.0 (TID 72). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 76, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 60.0 in stage 7.0 (TID 76)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 72) in 59 ms on localhost (45/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 59.0 in stage 7.0 (TID 75). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 77, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 61.0 in stage 7.0 (TID 77)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 75) in 42 ms on localhost (46/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 40.0 in stage 7.0 (TID 56). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 78, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 56) in 276 ms on localhost (47/200)
15/08/06 17:54:18 INFO Executor: Running task 62.0 in stage 7.0 (TID 78)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 44.0 in stage 7.0 (TID 60). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 79, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 63.0 in stage 7.0 (TID 79)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 60) in 244 ms on localhost (48/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO Executor: Finished task 42.0 in stage 7.0 (TID 58). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 80, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 64.0 in stage 7.0 (TID 80)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 58) in 290 ms on localhost (49/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 41.0 in stage 7.0 (TID 57). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 81, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 65.0 in stage 7.0 (TID 81)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 57) in 310 ms on localhost (50/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 48.0 in stage 7.0 (TID 64). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 82, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 66.0 in stage 7.0 (TID 82)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 64) in 597 ms on localhost (51/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO Executor: Finished task 45.0 in stage 7.0 (TID 61). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 83, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 67.0 in stage 7.0 (TID 83)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 61) in 649 ms on localhost (52/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 47.0 in stage 7.0 (TID 63). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 84, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 68.0 in stage 7.0 (TID 84)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 63) in 801 ms on localhost (53/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 49.0 in stage 7.0 (TID 65). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 85, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 69.0 in stage 7.0 (TID 85)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 65) in 820 ms on localhost (54/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 54.0 in stage 7.0 (TID 70). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 86, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 70.0 in stage 7.0 (TID 86)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 70) in 804 ms on localhost (55/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 69.0 in stage 7.0 (TID 85). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO Executor: Finished task 53.0 in stage 7.0 (TID 69). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 87, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 71.0 in stage 7.0 (TID 87)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 85) in 42 ms on localhost (56/200)
15/08/06 17:54:18 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 88, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 69) in 842 ms on localhost (57/200)
15/08/06 17:54:18 INFO Executor: Running task 72.0 in stage 7.0 (TID 88)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 70.0 in stage 7.0 (TID 86). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 89, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 73.0 in stage 7.0 (TID 89)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 86) in 41 ms on localhost (58/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 51.0 in stage 7.0 (TID 67). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 90, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 74.0 in stage 7.0 (TID 90)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 67) in 866 ms on localhost (59/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 57.0 in stage 7.0 (TID 73). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 91, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 75.0 in stage 7.0 (TID 91)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 73) in 821 ms on localhost (60/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 55.0 in stage 7.0 (TID 71). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 92, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 76.0 in stage 7.0 (TID 92)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 71) in 863 ms on localhost (61/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 58.0 in stage 7.0 (TID 74). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 93, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 77.0 in stage 7.0 (TID 93)
15/08/06 17:54:18 INFO Executor: Finished task 60.0 in stage 7.0 (TID 76). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 74) in 844 ms on localhost (62/200)
15/08/06 17:54:18 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 94, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 78.0 in stage 7.0 (TID 94)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 76) in 820 ms on localhost (63/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO Executor: Finished task 61.0 in stage 7.0 (TID 77). 1124 bytes result sent to driver
15/08/06 17:54:18 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 95, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:18 INFO Executor: Running task 79.0 in stage 7.0 (TID 95)
15/08/06 17:54:18 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 77) in 798 ms on localhost (64/200)
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 62.0 in stage 7.0 (TID 78). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO Executor: Finished task 63.0 in stage 7.0 (TID 79). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 96, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 80.0 in stage 7.0 (TID 96)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 78) in 804 ms on localhost (65/200)
15/08/06 17:54:19 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 97, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 79) in 786 ms on localhost (66/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 65.0 in stage 7.0 (TID 81). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO Executor: Running task 81.0 in stage 7.0 (TID 97)
15/08/06 17:54:19 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 98, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 81) in 734 ms on localhost (67/200)
15/08/06 17:54:19 INFO Executor: Running task 82.0 in stage 7.0 (TID 98)
15/08/06 17:54:19 INFO Executor: Finished task 64.0 in stage 7.0 (TID 80). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 99, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:19 INFO Executor: Running task 83.0 in stage 7.0 (TID 99)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 80) in 756 ms on localhost (68/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 66.0 in stage 7.0 (TID 82). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 100, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 84.0 in stage 7.0 (TID 100)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 82) in 486 ms on localhost (69/200)
15/08/06 17:54:19 INFO Executor: Finished task 67.0 in stage 7.0 (TID 83). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 101, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO Executor: Running task 85.0 in stage 7.0 (TID 101)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 83) in 449 ms on localhost (70/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 68.0 in stage 7.0 (TID 84). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 102, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 86.0 in stage 7.0 (TID 102)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 84) in 398 ms on localhost (71/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 72.0 in stage 7.0 (TID 88). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 103, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 87.0 in stage 7.0 (TID 103)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 88) in 560 ms on localhost (72/200)
15/08/06 17:54:19 INFO Executor: Finished task 71.0 in stage 7.0 (TID 87). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 104, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 88.0 in stage 7.0 (TID 104)
15/08/06 17:54:19 INFO Executor: Finished task 74.0 in stage 7.0 (TID 90). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 87) in 575 ms on localhost (73/200)
15/08/06 17:54:19 INFO Executor: Finished task 73.0 in stage 7.0 (TID 89). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 105, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 89.0 in stage 7.0 (TID 105)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 90) in 560 ms on localhost (74/200)
15/08/06 17:54:19 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 106, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 90.0 in stage 7.0 (TID 106)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 89) in 572 ms on localhost (75/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 75.0 in stage 7.0 (TID 91). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 107, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 91.0 in stage 7.0 (TID 107)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 91) in 567 ms on localhost (76/200)
15/08/06 17:54:19 INFO Executor: Finished task 76.0 in stage 7.0 (TID 92). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 108, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 92.0 in stage 7.0 (TID 108)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 92) in 561 ms on localhost (77/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 77.0 in stage 7.0 (TID 93). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 109, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 93.0 in stage 7.0 (TID 109)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 93) in 577 ms on localhost (78/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 79.0 in stage 7.0 (TID 95). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 110, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 94.0 in stage 7.0 (TID 110)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 95) in 592 ms on localhost (79/200)
15/08/06 17:54:19 INFO Executor: Finished task 78.0 in stage 7.0 (TID 94). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 111, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 95.0 in stage 7.0 (TID 111)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 94) in 618 ms on localhost (80/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 94.0 in stage 7.0 (TID 110). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 112, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 96.0 in stage 7.0 (TID 112)
15/08/06 17:54:19 INFO Executor: Finished task 83.0 in stage 7.0 (TID 99). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 110) in 52 ms on localhost (81/200)
15/08/06 17:54:19 INFO Executor: Finished task 84.0 in stage 7.0 (TID 100). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 113, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 97.0 in stage 7.0 (TID 113)
15/08/06 17:54:19 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 114, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 98.0 in stage 7.0 (TID 114)
15/08/06 17:54:19 INFO Executor: Finished task 82.0 in stage 7.0 (TID 98). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 100) in 526 ms on localhost (82/200)
15/08/06 17:54:19 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 115, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 98) in 596 ms on localhost (83/200)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 99) in 592 ms on localhost (84/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 80.0 in stage 7.0 (TID 96). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 116, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 96) in 618 ms on localhost (85/200)
15/08/06 17:54:19 INFO Executor: Running task 100.0 in stage 7.0 (TID 116)
15/08/06 17:54:19 INFO Executor: Finished task 81.0 in stage 7.0 (TID 97). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 117, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 97) in 618 ms on localhost (86/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Running task 99.0 in stage 7.0 (TID 115)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 85.0 in stage 7.0 (TID 101). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 118, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 102.0 in stage 7.0 (TID 118)
15/08/06 17:54:19 INFO Executor: Running task 101.0 in stage 7.0 (TID 117)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 101) in 550 ms on localhost (87/200)
15/08/06 17:54:19 INFO Executor: Finished task 86.0 in stage 7.0 (TID 102). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 119, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 102) in 459 ms on localhost (88/200)
15/08/06 17:54:19 INFO Executor: Running task 103.0 in stage 7.0 (TID 119)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 87.0 in stage 7.0 (TID 103). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 120, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 104.0 in stage 7.0 (TID 120)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 103) in 257 ms on localhost (89/200)
15/08/06 17:54:19 INFO Executor: Finished task 97.0 in stage 7.0 (TID 113). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 121, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 113) in 96 ms on localhost (90/200)
15/08/06 17:54:19 INFO Executor: Running task 105.0 in stage 7.0 (TID 121)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO Executor: Finished task 88.0 in stage 7.0 (TID 104). 1124 bytes result sent to driver
15/08/06 17:54:19 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 122, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:19 INFO Executor: Running task 106.0 in stage 7.0 (TID 122)
15/08/06 17:54:19 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 104) in 311 ms on localhost (91/200)
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 90.0 in stage 7.0 (TID 106). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 123, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 107.0 in stage 7.0 (TID 123)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 106) in 584 ms on localhost (92/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 89.0 in stage 7.0 (TID 105). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 124, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 108.0 in stage 7.0 (TID 124)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 105) in 644 ms on localhost (93/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 92.0 in stage 7.0 (TID 108). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 125, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 109.0 in stage 7.0 (TID 125)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 108) in 656 ms on localhost (94/200)
15/08/06 17:54:20 INFO Executor: Finished task 91.0 in stage 7.0 (TID 107). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 126, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 110.0 in stage 7.0 (TID 126)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 107) in 669 ms on localhost (95/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 95.0 in stage 7.0 (TID 111). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 127, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 111.0 in stage 7.0 (TID 127)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 111) in 608 ms on localhost (96/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 93.0 in stage 7.0 (TID 109). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 128, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 112.0 in stage 7.0 (TID 128)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 109) in 688 ms on localhost (97/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 96.0 in stage 7.0 (TID 112). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 129, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 113.0 in stage 7.0 (TID 129)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 112) in 655 ms on localhost (98/200)
15/08/06 17:54:20 INFO Executor: Finished task 98.0 in stage 7.0 (TID 114). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 130, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 114.0 in stage 7.0 (TID 130)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 114) in 659 ms on localhost (99/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 103.0 in stage 7.0 (TID 119). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 131, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 115.0 in stage 7.0 (TID 131)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 119) in 622 ms on localhost (100/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 102.0 in stage 7.0 (TID 118). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 132, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 116.0 in stage 7.0 (TID 132)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 118) in 654 ms on localhost (101/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 99.0 in stage 7.0 (TID 115). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 133, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 117.0 in stage 7.0 (TID 133)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 115) in 697 ms on localhost (102/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO Executor: Finished task 104.0 in stage 7.0 (TID 120). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 134, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Finished task 101.0 in stage 7.0 (TID 117). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO Executor: Running task 118.0 in stage 7.0 (TID 134)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 120) in 619 ms on localhost (103/200)
15/08/06 17:54:20 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 135, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 119.0 in stage 7.0 (TID 135)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 117) in 686 ms on localhost (104/200)
15/08/06 17:54:20 INFO Executor: Finished task 100.0 in stage 7.0 (TID 116). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 136, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 120.0 in stage 7.0 (TID 136)
15/08/06 17:54:20 INFO Executor: Finished task 106.0 in stage 7.0 (TID 122). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO Executor: Finished task 105.0 in stage 7.0 (TID 121). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 116) in 698 ms on localhost (105/200)
15/08/06 17:54:20 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 137, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 121.0 in stage 7.0 (TID 137)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 122) in 572 ms on localhost (106/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 138, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 122.0 in stage 7.0 (TID 138)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 121) in 635 ms on localhost (107/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 107.0 in stage 7.0 (TID 123). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 139, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 123) in 502 ms on localhost (108/200)
15/08/06 17:54:20 INFO Executor: Running task 123.0 in stage 7.0 (TID 139)
15/08/06 17:54:20 INFO Executor: Finished task 108.0 in stage 7.0 (TID 124). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 140, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 124.0 in stage 7.0 (TID 140)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 124) in 458 ms on localhost (109/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 109.0 in stage 7.0 (TID 125). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 141, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 125.0 in stage 7.0 (TID 141)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 125) in 494 ms on localhost (110/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 110.0 in stage 7.0 (TID 126). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 142, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 126.0 in stage 7.0 (TID 142)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 126) in 515 ms on localhost (111/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 111.0 in stage 7.0 (TID 127). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 143, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 127.0 in stage 7.0 (TID 143)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 127) in 511 ms on localhost (112/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO Executor: Finished task 113.0 in stage 7.0 (TID 129). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 144, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 128.0 in stage 7.0 (TID 144)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 129) in 535 ms on localhost (113/200)
15/08/06 17:54:20 INFO Executor: Finished task 112.0 in stage 7.0 (TID 128). 1124 bytes result sent to driver
15/08/06 17:54:20 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 145, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:20 INFO Executor: Running task 129.0 in stage 7.0 (TID 145)
15/08/06 17:54:20 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 128) in 588 ms on localhost (114/200)
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:21 INFO Executor: Finished task 114.0 in stage 7.0 (TID 130). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 146, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 130.0 in stage 7.0 (TID 146)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 130) in 761 ms on localhost (115/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 119.0 in stage 7.0 (TID 135). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 147, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 131.0 in stage 7.0 (TID 147)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 135) in 743 ms on localhost (116/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 116.0 in stage 7.0 (TID 132). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 132) in 801 ms on localhost (117/200)
15/08/06 17:54:21 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 148, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 132.0 in stage 7.0 (TID 148)
15/08/06 17:54:21 INFO Executor: Finished task 115.0 in stage 7.0 (TID 131). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 149, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 133.0 in stage 7.0 (TID 149)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 131) in 827 ms on localhost (118/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 122.0 in stage 7.0 (TID 138). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 150, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 134.0 in stage 7.0 (TID 150)
15/08/06 17:54:21 INFO Executor: Finished task 118.0 in stage 7.0 (TID 134). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 138) in 809 ms on localhost (119/200)
15/08/06 17:54:21 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 151, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 135.0 in stage 7.0 (TID 151)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 134) in 835 ms on localhost (120/200)
15/08/06 17:54:21 INFO Executor: Finished task 123.0 in stage 7.0 (TID 139). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO Executor: Finished task 121.0 in stage 7.0 (TID 137). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 152, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 136.0 in stage 7.0 (TID 152)
15/08/06 17:54:21 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 153, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 137.0 in stage 7.0 (TID 153)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 139) in 617 ms on localhost (121/200)
15/08/06 17:54:21 INFO Executor: Finished task 120.0 in stage 7.0 (TID 136). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 137) in 826 ms on localhost (122/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 154, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 138.0 in stage 7.0 (TID 154)
15/08/06 17:54:21 INFO Executor: Finished task 117.0 in stage 7.0 (TID 133). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 136) in 840 ms on localhost (123/200)
15/08/06 17:54:21 INFO Executor: Finished task 124.0 in stage 7.0 (TID 140). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 155, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 139.0 in stage 7.0 (TID 155)
15/08/06 17:54:21 INFO Executor: Finished task 133.0 in stage 7.0 (TID 149). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 156, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 140.0 in stage 7.0 (TID 156)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 133) in 861 ms on localhost (124/200)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 140) in 615 ms on localhost (125/200)
15/08/06 17:54:21 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 157, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Running task 141.0 in stage 7.0 (TID 157)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 126.0 in stage 7.0 (TID 142). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 149) in 75 ms on localhost (126/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 158, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 142.0 in stage 7.0 (TID 158)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 142) in 527 ms on localhost (127/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 125.0 in stage 7.0 (TID 141). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 159, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO Executor: Running task 143.0 in stage 7.0 (TID 159)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 141) in 556 ms on localhost (128/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 127.0 in stage 7.0 (TID 143). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 160, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 144.0 in stage 7.0 (TID 160)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 143) in 539 ms on localhost (129/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO Executor: Finished task 129.0 in stage 7.0 (TID 145). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 161, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Finished task 128.0 in stage 7.0 (TID 144). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO Executor: Running task 145.0 in stage 7.0 (TID 161)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 145) in 445 ms on localhost (130/200)
15/08/06 17:54:21 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 162, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 146.0 in stage 7.0 (TID 162)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 144) in 452 ms on localhost (131/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 144.0 in stage 7.0 (TID 160). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 163, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 147.0 in stage 7.0 (TID 163)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 160) in 65 ms on localhost (132/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 130.0 in stage 7.0 (TID 146). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 164, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 148.0 in stage 7.0 (TID 164)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 146) in 371 ms on localhost (133/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 131.0 in stage 7.0 (TID 147). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 165, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 149.0 in stage 7.0 (TID 165)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 147) in 420 ms on localhost (134/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:21 INFO Executor: Finished task 132.0 in stage 7.0 (TID 148). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 166, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 150.0 in stage 7.0 (TID 166)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 148) in 520 ms on localhost (135/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO Executor: Finished task 134.0 in stage 7.0 (TID 150). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 167, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 151.0 in stage 7.0 (TID 167)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 150) in 516 ms on localhost (136/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 150.0 in stage 7.0 (TID 166). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 168, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 152.0 in stage 7.0 (TID 168)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 166) in 64 ms on localhost (137/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO Executor: Finished task 136.0 in stage 7.0 (TID 152). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 169, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 153.0 in stage 7.0 (TID 169)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 152) in 541 ms on localhost (138/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO Executor: Finished task 137.0 in stage 7.0 (TID 153). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 170, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 153) in 568 ms on localhost (139/200)
15/08/06 17:54:21 INFO Executor: Running task 154.0 in stage 7.0 (TID 170)
15/08/06 17:54:21 INFO Executor: Finished task 139.0 in stage 7.0 (TID 155). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 171, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 155.0 in stage 7.0 (TID 171)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 155) in 573 ms on localhost (140/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 153.0 in stage 7.0 (TID 169). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 172, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 156.0 in stage 7.0 (TID 172)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 169) in 56 ms on localhost (141/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 140.0 in stage 7.0 (TID 156). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 173, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 157.0 in stage 7.0 (TID 173)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 156) in 591 ms on localhost (142/200)
15/08/06 17:54:21 INFO Executor: Finished task 135.0 in stage 7.0 (TID 151). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 174, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 151) in 616 ms on localhost (143/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Running task 158.0 in stage 7.0 (TID 174)
15/08/06 17:54:21 INFO Executor: Finished task 142.0 in stage 7.0 (TID 158). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 175, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 159.0 in stage 7.0 (TID 175)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 158) in 586 ms on localhost (144/200)
15/08/06 17:54:21 INFO Executor: Finished task 143.0 in stage 7.0 (TID 159). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO Executor: Finished task 141.0 in stage 7.0 (TID 157). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 159) in 588 ms on localhost (145/200)
15/08/06 17:54:21 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 176, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 160.0 in stage 7.0 (TID 176)
15/08/06 17:54:21 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 177, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 161.0 in stage 7.0 (TID 177)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 157) in 616 ms on localhost (146/200)
15/08/06 17:54:21 INFO Executor: Finished task 138.0 in stage 7.0 (TID 154). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 178, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 162.0 in stage 7.0 (TID 178)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 154) in 631 ms on localhost (147/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/06 17:54:21 INFO Executor: Finished task 147.0 in stage 7.0 (TID 163). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 179, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 163.0 in stage 7.0 (TID 179)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 163) in 540 ms on localhost (148/200)
15/08/06 17:54:21 INFO Executor: Finished task 145.0 in stage 7.0 (TID 161). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 180, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 164.0 in stage 7.0 (TID 180)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 161) in 585 ms on localhost (149/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 146.0 in stage 7.0 (TID 162). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 181, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 165.0 in stage 7.0 (TID 181)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 162) in 613 ms on localhost (150/200)
15/08/06 17:54:21 INFO Executor: Finished task 148.0 in stage 7.0 (TID 164). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 182, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 166.0 in stage 7.0 (TID 182)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 164) in 470 ms on localhost (151/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO Executor: Finished task 162.0 in stage 7.0 (TID 178). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 183, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 167.0 in stage 7.0 (TID 183)
15/08/06 17:54:21 INFO Executor: Finished task 164.0 in stage 7.0 (TID 180). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 178) in 87 ms on localhost (152/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 184, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 168.0 in stage 7.0 (TID 184)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 180) in 58 ms on localhost (153/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO Executor: Finished task 149.0 in stage 7.0 (TID 165). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 185, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO Executor: Running task 169.0 in stage 7.0 (TID 185)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 165) in 429 ms on localhost (154/200)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO Executor: Finished task 151.0 in stage 7.0 (TID 167). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 170.0 in stage 7.0 (TID 186, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Finished task 167.0 in stage 7.0 (TID 183). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO Executor: Running task 170.0 in stage 7.0 (TID 186)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 167) in 292 ms on localhost (155/200)
15/08/06 17:54:21 INFO TaskSetManager: Starting task 171.0 in stage 7.0 (TID 187, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 171.0 in stage 7.0 (TID 187)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 183) in 79 ms on localhost (156/200)
15/08/06 17:54:21 INFO Executor: Finished task 169.0 in stage 7.0 (TID 185). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO TaskSetManager: Starting task 172.0 in stage 7.0 (TID 188, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 172.0 in stage 7.0 (TID 188)
15/08/06 17:54:21 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 185) in 59 ms on localhost (157/200)
15/08/06 17:54:21 INFO Executor: Finished task 152.0 in stage 7.0 (TID 168). 1124 bytes result sent to driver
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:21 INFO TaskSetManager: Starting task 173.0 in stage 7.0 (TID 189, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:21 INFO Executor: Running task 173.0 in stage 7.0 (TID 189)
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:21 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 168) in 296 ms on localhost (158/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 173.0 in stage 7.0 (TID 189). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 174.0 in stage 7.0 (TID 190, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 174.0 in stage 7.0 (TID 190)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 173.0 in stage 7.0 (TID 189) in 214 ms on localhost (159/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO Executor: Finished task 154.0 in stage 7.0 (TID 170). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 175.0 in stage 7.0 (TID 191, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 175.0 in stage 7.0 (TID 191)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 170) in 545 ms on localhost (160/200)
15/08/06 17:54:22 INFO Executor: Finished task 157.0 in stage 7.0 (TID 173). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 176.0 in stage 7.0 (TID 192, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 176.0 in stage 7.0 (TID 192)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 173) in 531 ms on localhost (161/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 160.0 in stage 7.0 (TID 176). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO Executor: Finished task 156.0 in stage 7.0 (TID 172). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 177.0 in stage 7.0 (TID 193, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 177.0 in stage 7.0 (TID 193)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 176) in 689 ms on localhost (162/200)
15/08/06 17:54:22 INFO TaskSetManager: Starting task 178.0 in stage 7.0 (TID 194, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 178.0 in stage 7.0 (TID 194)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 172) in 727 ms on localhost (163/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 163.0 in stage 7.0 (TID 179). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 179.0 in stage 7.0 (TID 195, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 179.0 in stage 7.0 (TID 195)
15/08/06 17:54:22 INFO Executor: Finished task 155.0 in stage 7.0 (TID 171). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 179) in 688 ms on localhost (164/200)
15/08/06 17:54:22 INFO TaskSetManager: Starting task 180.0 in stage 7.0 (TID 196, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 180.0 in stage 7.0 (TID 196)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 171) in 777 ms on localhost (165/200)
15/08/06 17:54:22 INFO Executor: Finished task 161.0 in stage 7.0 (TID 177). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 181.0 in stage 7.0 (TID 197, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 181.0 in stage 7.0 (TID 197)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 177) in 733 ms on localhost (166/200)
15/08/06 17:54:22 INFO Executor: Finished task 158.0 in stage 7.0 (TID 174). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 182.0 in stage 7.0 (TID 198, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 182.0 in stage 7.0 (TID 198)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 174) in 768 ms on localhost (167/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:22 INFO Executor: Finished task 159.0 in stage 7.0 (TID 175). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 183.0 in stage 7.0 (TID 199, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Finished task 180.0 in stage 7.0 (TID 196). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO Executor: Running task 183.0 in stage 7.0 (TID 199)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 175) in 804 ms on localhost (168/200)
15/08/06 17:54:22 INFO TaskSetManager: Starting task 184.0 in stage 7.0 (TID 200, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 184.0 in stage 7.0 (TID 200)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 180.0 in stage 7.0 (TID 196) in 68 ms on localhost (169/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 168.0 in stage 7.0 (TID 184). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 185.0 in stage 7.0 (TID 201, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 185.0 in stage 7.0 (TID 201)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 184) in 755 ms on localhost (170/200)
15/08/06 17:54:22 INFO Executor: Finished task 184.0 in stage 7.0 (TID 200). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 186.0 in stage 7.0 (TID 202, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 186.0 in stage 7.0 (TID 202)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 184.0 in stage 7.0 (TID 200) in 63 ms on localhost (171/200)
15/08/06 17:54:22 INFO Executor: Finished task 166.0 in stage 7.0 (TID 182). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 187.0 in stage 7.0 (TID 203, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 187.0 in stage 7.0 (TID 203)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 182) in 787 ms on localhost (172/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 165.0 in stage 7.0 (TID 181). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 188.0 in stage 7.0 (TID 204, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 181) in 802 ms on localhost (173/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO Executor: Running task 188.0 in stage 7.0 (TID 204)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO Executor: Finished task 171.0 in stage 7.0 (TID 187). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 189.0 in stage 7.0 (TID 205, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Finished task 170.0 in stage 7.0 (TID 186). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO Executor: Running task 189.0 in stage 7.0 (TID 205)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 171.0 in stage 7.0 (TID 187) in 720 ms on localhost (174/200)
15/08/06 17:54:22 INFO TaskSetManager: Starting task 190.0 in stage 7.0 (TID 206, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 170.0 in stage 7.0 (TID 186) in 734 ms on localhost (175/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 174.0 in stage 7.0 (TID 190). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO Executor: Running task 190.0 in stage 7.0 (TID 206)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO TaskSetManager: Starting task 191.0 in stage 7.0 (TID 207, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 191.0 in stage 7.0 (TID 207)
15/08/06 17:54:22 INFO Executor: Finished task 172.0 in stage 7.0 (TID 188). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Finished task 174.0 in stage 7.0 (TID 190) in 499 ms on localhost (176/200)
15/08/06 17:54:22 INFO TaskSetManager: Starting task 192.0 in stage 7.0 (TID 208, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 192.0 in stage 7.0 (TID 208)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 172.0 in stage 7.0 (TID 188) in 726 ms on localhost (177/200)
15/08/06 17:54:22 INFO Executor: Finished task 186.0 in stage 7.0 (TID 202). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 193.0 in stage 7.0 (TID 209, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 193.0 in stage 7.0 (TID 209)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 186.0 in stage 7.0 (TID 202) in 61 ms on localhost (178/200)
15/08/06 17:54:22 INFO Executor: Finished task 175.0 in stage 7.0 (TID 191). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO TaskSetManager: Starting task 194.0 in stage 7.0 (TID 210, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 194.0 in stage 7.0 (TID 210)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO TaskSetManager: Finished task 175.0 in stage 7.0 (TID 191) in 439 ms on localhost (179/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 176.0 in stage 7.0 (TID 192). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 195.0 in stage 7.0 (TID 211, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 195.0 in stage 7.0 (TID 211)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 176.0 in stage 7.0 (TID 192) in 431 ms on localhost (180/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 190.0 in stage 7.0 (TID 206). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 196.0 in stage 7.0 (TID 212, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 190.0 in stage 7.0 (TID 206) in 70 ms on localhost (181/200)
15/08/06 17:54:22 INFO Executor: Running task 196.0 in stage 7.0 (TID 212)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 178.0 in stage 7.0 (TID 194). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 197.0 in stage 7.0 (TID 213, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 197.0 in stage 7.0 (TID 213)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 178.0 in stage 7.0 (TID 194) in 318 ms on localhost (182/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 177.0 in stage 7.0 (TID 193). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO TaskSetManager: Starting task 198.0 in stage 7.0 (TID 214, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 198.0 in stage 7.0 (TID 214)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 177.0 in stage 7.0 (TID 193) in 354 ms on localhost (183/200)
15/08/06 17:54:22 INFO Executor: Finished task 195.0 in stage 7.0 (TID 211). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Starting task 199.0 in stage 7.0 (TID 215, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:54:22 INFO Executor: Running task 199.0 in stage 7.0 (TID 215)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 195.0 in stage 7.0 (TID 211) in 104 ms on localhost (184/200)
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:54:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:22 INFO Executor: Finished task 197.0 in stage 7.0 (TID 213). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO Executor: Finished task 181.0 in stage 7.0 (TID 197). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Finished task 197.0 in stage 7.0 (TID 213) in 65 ms on localhost (185/200)
15/08/06 17:54:22 INFO TaskSetManager: Finished task 181.0 in stage 7.0 (TID 197) in 341 ms on localhost (186/200)
15/08/06 17:54:22 INFO Executor: Finished task 179.0 in stage 7.0 (TID 195). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Finished task 179.0 in stage 7.0 (TID 195) in 375 ms on localhost (187/200)
15/08/06 17:54:22 INFO Executor: Finished task 182.0 in stage 7.0 (TID 198). 1124 bytes result sent to driver
15/08/06 17:54:22 INFO TaskSetManager: Finished task 182.0 in stage 7.0 (TID 198) in 364 ms on localhost (188/200)
15/08/06 17:54:23 INFO Executor: Finished task 183.0 in stage 7.0 (TID 199). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 183.0 in stage 7.0 (TID 199) in 401 ms on localhost (189/200)
15/08/06 17:54:23 INFO Executor: Finished task 185.0 in stage 7.0 (TID 201). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 185.0 in stage 7.0 (TID 201) in 385 ms on localhost (190/200)
15/08/06 17:54:23 INFO Executor: Finished task 187.0 in stage 7.0 (TID 203). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 187.0 in stage 7.0 (TID 203) in 386 ms on localhost (191/200)
15/08/06 17:54:23 INFO Executor: Finished task 188.0 in stage 7.0 (TID 204). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 188.0 in stage 7.0 (TID 204) in 382 ms on localhost (192/200)
15/08/06 17:54:23 INFO Executor: Finished task 189.0 in stage 7.0 (TID 205). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 189.0 in stage 7.0 (TID 205) in 388 ms on localhost (193/200)
15/08/06 17:54:23 INFO Executor: Finished task 194.0 in stage 7.0 (TID 210). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO Executor: Finished task 192.0 in stage 7.0 (TID 208). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 194.0 in stage 7.0 (TID 210) in 366 ms on localhost (194/200)
15/08/06 17:54:23 INFO Executor: Finished task 193.0 in stage 7.0 (TID 209). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 192.0 in stage 7.0 (TID 208) in 387 ms on localhost (195/200)
15/08/06 17:54:23 INFO TaskSetManager: Finished task 193.0 in stage 7.0 (TID 209) in 378 ms on localhost (196/200)
15/08/06 17:54:23 INFO Executor: Finished task 191.0 in stage 7.0 (TID 207). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 191.0 in stage 7.0 (TID 207) in 406 ms on localhost (197/200)
15/08/06 17:54:23 INFO Executor: Finished task 196.0 in stage 7.0 (TID 212). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 196.0 in stage 7.0 (TID 212) in 355 ms on localhost (198/200)
15/08/06 17:54:23 INFO Executor: Finished task 198.0 in stage 7.0 (TID 214). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 198.0 in stage 7.0 (TID 214) in 293 ms on localhost (199/200)
15/08/06 17:54:23 INFO Executor: Finished task 199.0 in stage 7.0 (TID 215). 1124 bytes result sent to driver
15/08/06 17:54:23 INFO TaskSetManager: Finished task 199.0 in stage 7.0 (TID 215) in 292 ms on localhost (200/200)
15/08/06 17:54:23 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/06 17:54:23 INFO DAGScheduler: Stage 7 (mapPartitions at Exchange.scala:64) finished in 7.655 s
15/08/06 17:54:23 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:54:23 INFO DAGScheduler: running: Set()
15/08/06 17:54:23 INFO DAGScheduler: waiting: Set(Stage 8)
15/08/06 17:54:23 INFO DAGScheduler: failed: Set()
15/08/06 17:54:23 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4c2cc7ce
15/08/06 17:54:23 INFO StatsReportListener: task runtime:(count: 200, mean: 605.640000, stdev: 409.642137, max: 1913.000000, min: 41.000000)
15/08/06 17:54:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:23 INFO StatsReportListener: 	41.0 ms	61.0 ms	76.0 ms	385.0 ms	588.0 ms	733.0 ms	866.0 ms	1.9 s	1.9 s
15/08/06 17:54:23 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 7580.820000, stdev: 3876.470167, max: 17153.000000, min: 0.000000)
15/08/06 17:54:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:23 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	6.3 KB	8.1 KB	9.9 KB	11.7 KB	13.4 KB	16.8 KB
15/08/06 17:54:23 INFO DAGScheduler: Missing parents for Stage 8: List()
15/08/06 17:54:23 INFO DAGScheduler: Submitting Stage 8 (MapPartitionsRDD[48] at mapPartitions at Aggregate.scala:151), which is now runnable
15/08/06 17:54:23 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.065000, stdev: 0.246526, max: 1.000000, min: 0.000000)
15/08/06 17:54:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:23 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms
15/08/06 17:54:23 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:54:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:23 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:54:23 INFO StatsReportListener: task result size:(count: 200, mean: 1124.000000, stdev: 0.000000, max: 1124.000000, min: 1124.000000)
15/08/06 17:54:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:23 INFO StatsReportListener: 	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B
15/08/06 17:54:23 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 96.934154, stdev: 3.955846, max: 99.538106, min: 78.666667)
15/08/06 17:54:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:23 INFO StatsReportListener: 	79 %	88 %	90 %	97 %	99 %	99 %	99 %	99 %	100 %
15/08/06 17:54:23 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.026781, stdev: 0.158647, max: 1.587302, min: 0.000000)
15/08/06 17:54:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:23 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 2 %
15/08/06 17:54:23 INFO StatsReportListener: other time pct: (count: 200, mean: 3.039065, stdev: 3.928560, max: 21.333333, min: 0.461894)
15/08/06 17:54:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:23 INFO StatsReportListener: 	 0 %	 1 %	 1 %	 1 %	 1 %	 3 %	10 %	12 %	21 %
15/08/06 17:54:23 INFO MemoryStore: ensureFreeSpace(149960) called with curMem=768433, maxMem=3333968363
15/08/06 17:54:23 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 146.4 KB, free 3.1 GB)
15/08/06 17:54:23 INFO MemoryStore: ensureFreeSpace(65191) called with curMem=918393, maxMem=3333968363
15/08/06 17:54:23 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 63.7 KB, free 3.1 GB)
15/08/06 17:54:23 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:37948 (size: 63.7 KB, free: 3.1 GB)
15/08/06 17:54:23 INFO BlockManagerMaster: Updated info of block broadcast_12_piece0
15/08/06 17:54:23 INFO DefaultExecutionContext: Created broadcast 12 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:23 INFO DAGScheduler: Submitting 200 missing tasks from Stage 8 (MapPartitionsRDD[48] at mapPartitions at Aggregate.scala:151)
15/08/06 17:54:23 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/06 17:54:23 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 216, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 217, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 218, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 219, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 220, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 221, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 222, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 223, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 224, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 225, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 226, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 227, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 228, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 229, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 230, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 231, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:23 INFO Executor: Running task 1.0 in stage 8.0 (TID 217)
15/08/06 17:54:23 INFO Executor: Running task 0.0 in stage 8.0 (TID 216)
15/08/06 17:54:23 INFO Executor: Running task 2.0 in stage 8.0 (TID 218)
15/08/06 17:54:23 INFO Executor: Running task 3.0 in stage 8.0 (TID 219)
15/08/06 17:54:23 INFO Executor: Running task 4.0 in stage 8.0 (TID 220)
15/08/06 17:54:23 INFO Executor: Running task 5.0 in stage 8.0 (TID 221)
15/08/06 17:54:23 INFO Executor: Running task 7.0 in stage 8.0 (TID 223)
15/08/06 17:54:23 INFO Executor: Running task 10.0 in stage 8.0 (TID 226)
15/08/06 17:54:23 INFO Executor: Running task 8.0 in stage 8.0 (TID 224)
15/08/06 17:54:23 INFO Executor: Running task 6.0 in stage 8.0 (TID 222)
15/08/06 17:54:23 INFO Executor: Running task 9.0 in stage 8.0 (TID 225)
15/08/06 17:54:23 INFO Executor: Running task 11.0 in stage 8.0 (TID 227)
15/08/06 17:54:23 INFO Executor: Running task 15.0 in stage 8.0 (TID 231)
15/08/06 17:54:23 INFO Executor: Running task 13.0 in stage 8.0 (TID 229)
15/08/06 17:54:23 INFO Executor: Running task 14.0 in stage 8.0 (TID 230)
15/08/06 17:54:23 INFO Executor: Running task 12.0 in stage 8.0 (TID 228)
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48e25145
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@245ae65
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a1cb608
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000004_220/part-00004
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000008_224/part-00008
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000014_230/part-00014
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23c8bc5d
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000010_226/part-00010
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4e0b5542
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000001_217/part-00001
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24636a6
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000007_223/part-00007
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ef8d281
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000000_216/part-00000
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@38f8ae2b
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000002_218/part-00002
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4902243f
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4e72a645
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000012_228/part-00012
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000015_231/part-00015
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d494a6f
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000013_229/part-00013
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@98c442e
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000009_225/part-00009
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17806a9f
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000003_219/part-00003
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1ce83e13
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000011_227/part-00011
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d9bfab7
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000005_221/part-00005
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3dea4762
15/08/06 17:54:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000006_222/part-00006
15/08/06 17:54:23 INFO CodecConfig: Compression set to false
15/08/06 17:54:23 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:23 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:23 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bc2df54
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@36c607f2
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1294732e
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@65cc996b
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c817380
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1efab11b
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@8b791c4
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e0b6883
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@219409bd
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4869c9d
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@733c6eae
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66d42aa7
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4c1d12b6
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5015da89
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@39babc1e
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@333bcf6
15/08/06 17:54:24 INFO BlockManager: Removing broadcast 11
15/08/06 17:54:24 INFO BlockManager: Removing block broadcast_11
15/08/06 17:54:24 INFO MemoryStore: Block broadcast_11 of size 13544 dropped from memory (free 3332998323)
15/08/06 17:54:24 INFO BlockManager: Removing block broadcast_11_piece0
15/08/06 17:54:24 INFO MemoryStore: Block broadcast_11_piece0 of size 7369 dropped from memory (free 3333005692)
15/08/06 17:54:24 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:37948 in memory (size: 7.2 KB, free: 3.1 GB)
15/08/06 17:54:24 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/08/06 17:54:24 INFO ContextCleaner: Cleaned broadcast 11
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,076
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,396
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,836
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,156
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,136
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,616
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,136
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,356
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,816
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 555B for [ps_partkey] INT32: 128 values, 519B raw, 519B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 615B for [ps_partkey] INT32: 143 values, 579B raw, 579B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,075B for [part_value] DOUBLE: 128 values, 1,031B raw, 1,031B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 603B for [ps_partkey] INT32: 140 values, 567B raw, 567B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,171B for [part_value] DOUBLE: 140 values, 1,127B raw, 1,127B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 667B for [ps_partkey] INT32: 156 values, 631B raw, 631B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 659B for [ps_partkey] INT32: 154 values, 623B raw, 623B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 751B for [ps_partkey] INT32: 177 values, 715B raw, 715B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 711B for [ps_partkey] INT32: 167 values, 675B raw, 675B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,283B for [part_value] DOUBLE: 154 values, 1,239B raw, 1,239B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 815B for [ps_partkey] INT32: 193 values, 779B raw, 779B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,195B for [part_value] DOUBLE: 143 values, 1,151B raw, 1,151B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,387B for [part_value] DOUBLE: 167 values, 1,343B raw, 1,343B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,467B for [part_value] DOUBLE: 177 values, 1,423B raw, 1,423B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,299B for [part_value] DOUBLE: 156 values, 1,255B raw, 1,255B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 619B for [ps_partkey] INT32: 144 values, 583B raw, 583B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,595B for [part_value] DOUBLE: 193 values, 1,551B raw, 1,551B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,203B for [part_value] DOUBLE: 144 values, 1,159B raw, 1,159B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000006_222' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000006
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000004_220' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000004
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000002_218' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000002
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000006_222: Committed
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000007_223' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000007
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000004_220: Committed
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000007_223: Committed
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000002_218: Committed
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000005_221' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000005
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000008_224' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000008
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000005_221: Committed
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000008_224: Committed
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000000_216' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000000
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000000_216: Committed
15/08/06 17:54:24 INFO Executor: Finished task 4.0 in stage 8.0 (TID 220). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 2.0 in stage 8.0 (TID 218). 781 bytes result sent to driver
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000015_231' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000015
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000015_231: Committed
15/08/06 17:54:24 INFO Executor: Finished task 6.0 in stage 8.0 (TID 222). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 0.0 in stage 8.0 (TID 216). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 5.0 in stage 8.0 (TID 221). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 8.0 in stage 8.0 (TID 224). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 7.0 in stage 8.0 (TID 223). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 15.0 in stage 8.0 (TID 231). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 232, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 16.0 in stage 8.0 (TID 232)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000012_228' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000012
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000012_228: Committed
15/08/06 17:54:24 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 220) in 1037 ms on localhost (1/200)
15/08/06 17:54:24 INFO Executor: Finished task 12.0 in stage 8.0 (TID 228). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 233, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 17.0 in stage 8.0 (TID 233)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 234, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 18.0 in stage 8.0 (TID 234)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 222) in 1041 ms on localhost (2/200)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000010_226' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000010
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000010_226: Committed
15/08/06 17:54:24 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 218) in 1043 ms on localhost (3/200)
15/08/06 17:54:24 INFO Executor: Finished task 10.0 in stage 8.0 (TID 226). 781 bytes result sent to driver
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000011_227' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000011
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000011_227: Committed
15/08/06 17:54:24 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 235, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 19.0 in stage 8.0 (TID 235)
15/08/06 17:54:24 INFO Executor: Finished task 11.0 in stage 8.0 (TID 227). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 216) in 1047 ms on localhost (4/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 236, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 20.0 in stage 8.0 (TID 236)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000003_219' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000003
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000003_219: Committed
15/08/06 17:54:24 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 221) in 1045 ms on localhost (5/200)
15/08/06 17:54:24 INFO Executor: Finished task 3.0 in stage 8.0 (TID 219). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 237, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 21.0 in stage 8.0 (TID 237)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 238, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 239, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 23.0 in stage 8.0 (TID 239)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 240, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 24.0 in stage 8.0 (TID 240)
15/08/06 17:54:24 INFO Executor: Running task 22.0 in stage 8.0 (TID 238)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 241, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 25.0 in stage 8.0 (TID 241)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 242, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 26.0 in stage 8.0 (TID 242)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000001_217' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000001
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000001_217: Committed
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000009_225' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000009
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000013_229' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000013
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000009_225: Committed
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000013_229: Committed
15/08/06 17:54:24 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 224) in 1050 ms on localhost (6/200)
15/08/06 17:54:24 INFO Executor: Finished task 1.0 in stage 8.0 (TID 217). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 13.0 in stage 8.0 (TID 229). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 228) in 1049 ms on localhost (7/200)
15/08/06 17:54:24 INFO Executor: Finished task 9.0 in stage 8.0 (TID 225). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 223) in 1053 ms on localhost (8/200)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000014_230' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000014
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000014_230: Committed
15/08/06 17:54:24 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 231) in 1051 ms on localhost (9/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 243, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Finished task 14.0 in stage 8.0 (TID 230). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Running task 27.0 in stage 8.0 (TID 243)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 219) in 1058 ms on localhost (10/200)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 227) in 1056 ms on localhost (11/200)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 226) in 1057 ms on localhost (12/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 244, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 28.0 in stage 8.0 (TID 244)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 217) in 1063 ms on localhost (13/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 245, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 29.0 in stage 8.0 (TID 245)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 229) in 1060 ms on localhost (14/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 246, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 30.0 in stage 8.0 (TID 246)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 225) in 1072 ms on localhost (15/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 247, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 31.0 in stage 8.0 (TID 247)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 230) in 1072 ms on localhost (16/200)
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@636d7716
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000026_242/part-00026
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15e5f8e3
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d370a50
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000027_243/part-00027
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2df96371
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78862160
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000023_239/part-00023
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4fefa74b
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000020_236/part-00020
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fb939ff
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000018_234/part-00018
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@43d99e97
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000025_241/part-00025
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000016_232/part-00016
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@398ea49e
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000029_245/part-00029
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66f08b4b
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a491db9
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5ec1ec0
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@533bf759
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5fe195f4
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@163e7a94
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000024_240/part-00024
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000021_237/part-00021
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000030_246/part-00030
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4787e015
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000019_235/part-00019
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2988a763
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31ca9ab7
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@124fc546
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000031_247/part-00031
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@71095b10
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000022_238/part-00022
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000028_244/part-00028
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3320800e
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@55242e42
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000017_233/part-00017
15/08/06 17:54:24 INFO CodecConfig: Compression set to false
15/08/06 17:54:24 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:24 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@480d1354
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3be1ff89
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67c8a4b2
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@724f9a15
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@138b55bd
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3678b4be
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ea241e2
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1e937459
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7984cef
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,276
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,036
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,356
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,856
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 643B for [ps_partkey] INT32: 150 values, 607B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 395B for [ps_partkey] INT32: 88 values, 359B raw, 359B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 659B for [ps_partkey] INT32: 154 values, 623B raw, 623B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,251B for [part_value] DOUBLE: 150 values, 1,207B raw, 1,207B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,283B for [part_value] DOUBLE: 154 values, 1,239B raw, 1,239B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000026_242' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000026
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000026_242: Committed
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 755B for [part_value] DOUBLE: 88 values, 711B raw, 711B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 759B for [ps_partkey] INT32: 179 values, 723B raw, 723B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,296
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,483B for [part_value] DOUBLE: 179 values, 1,439B raw, 1,439B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO Executor: Finished task 26.0 in stage 8.0 (TID 242). 781 bytes result sent to driver
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:54:24 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d94c1
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:54:24 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 248, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 647B for [ps_partkey] INT32: 151 values, 611B raw, 611B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO Executor: Running task 32.0 in stage 8.0 (TID 248)
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,259B for [part_value] DOUBLE: 151 values, 1,215B raw, 1,215B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 242) in 500 ms on localhost (17/200)
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000023_239' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000023
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000023_239: Committed
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO Executor: Finished task 23.0 in stage 8.0 (TID 239). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 249, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 33.0 in stage 8.0 (TID 249)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 239) in 508 ms on localhost (18/200)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000016_232' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000016
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000016_232: Committed
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000018_234' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000018
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000018_234: Committed
15/08/06 17:54:24 INFO Executor: Finished task 16.0 in stage 8.0 (TID 232). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 18.0 in stage 8.0 (TID 234). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 250, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000020_236' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000020
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000020_236: Committed
15/08/06 17:54:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:54:24 INFO Executor: Running task 34.0 in stage 8.0 (TID 250)
15/08/06 17:54:24 INFO Executor: Finished task 20.0 in stage 8.0 (TID 236). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 232) in 536 ms on localhost (19/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 251, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 35.0 in stage 8.0 (TID 251)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 234) in 537 ms on localhost (20/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 252, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO Executor: Running task 36.0 in stage 8.0 (TID 252)
15/08/06 17:54:24 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:24 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 236) in 535 ms on localhost (21/200)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000027_243' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000027
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000027_243: Committed
15/08/06 17:54:24 INFO Executor: Finished task 27.0 in stage 8.0 (TID 243). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 253, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000021_237' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000021
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000021_237: Committed
15/08/06 17:54:24 INFO Executor: Running task 37.0 in stage 8.0 (TID 253)
15/08/06 17:54:24 INFO Executor: Finished task 21.0 in stage 8.0 (TID 237). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 243) in 539 ms on localhost (22/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 254, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 38.0 in stage 8.0 (TID 254)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000024_240' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000024
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000024_240: Committed
15/08/06 17:54:24 INFO Executor: Finished task 24.0 in stage 8.0 (TID 240). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 237) in 551 ms on localhost (23/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 255, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 39.0 in stage 8.0 (TID 255)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 240) in 551 ms on localhost (24/200)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000025_241' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000025
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000025_241: Committed
15/08/06 17:54:24 INFO Executor: Finished task 25.0 in stage 8.0 (TID 241). 781 bytes result sent to driver
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000022_238' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000022
15/08/06 17:54:24 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 256, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000022_238: Committed
15/08/06 17:54:24 INFO Executor: Running task 40.0 in stage 8.0 (TID 256)
15/08/06 17:54:24 INFO Executor: Finished task 22.0 in stage 8.0 (TID 238). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 241) in 559 ms on localhost (25/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 257, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 41.0 in stage 8.0 (TID 257)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 238) in 561 ms on localhost (26/200)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000031_247' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000031
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000031_247: Committed
15/08/06 17:54:24 INFO Executor: Finished task 31.0 in stage 8.0 (TID 247). 781 bytes result sent to driver
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000029_245' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000029
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000029_245: Committed
15/08/06 17:54:24 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 258, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000028_244' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000028
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000028_244: Committed
15/08/06 17:54:24 INFO Executor: Finished task 29.0 in stage 8.0 (TID 245). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Running task 42.0 in stage 8.0 (TID 258)
15/08/06 17:54:24 INFO Executor: Finished task 28.0 in stage 8.0 (TID 244). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 247) in 537 ms on localhost (27/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 259, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 43.0 in stage 8.0 (TID 259)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 245) in 555 ms on localhost (28/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 260, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 44.0 in stage 8.0 (TID 260)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 244) in 559 ms on localhost (29/200)
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000030_246' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000030
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000019_235' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000019
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000030_246: Committed
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000019_235: Committed
15/08/06 17:54:24 INFO Executor: Finished task 19.0 in stage 8.0 (TID 235). 781 bytes result sent to driver
15/08/06 17:54:24 INFO Executor: Finished task 30.0 in stage 8.0 (TID 246). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 261, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000017_233' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000017
15/08/06 17:54:24 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000017_233: Committed
15/08/06 17:54:24 INFO Executor: Running task 45.0 in stage 8.0 (TID 261)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 235) in 581 ms on localhost (30/200)
15/08/06 17:54:24 INFO Executor: Finished task 17.0 in stage 8.0 (TID 233). 781 bytes result sent to driver
15/08/06 17:54:24 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 262, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 46.0 in stage 8.0 (TID 262)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 246) in 554 ms on localhost (31/200)
15/08/06 17:54:24 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 263, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:24 INFO Executor: Running task 47.0 in stage 8.0 (TID 263)
15/08/06 17:54:24 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 233) in 591 ms on localhost (32/200)
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c6a7bb5
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000036_252/part-00036
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@12dfb6a6
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000040_256/part-00040
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@18ff9807
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000039_255/part-00039
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ab12321
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@337971d6
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000037_253/part-00037
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a20b300
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000035_251/part-00035
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14c38ea
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000032_248/part-00032
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@332c8b50
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@316e4b6f
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000041_257/part-00041
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@401a7dc3
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78837287
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000034_250/part-00034
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7dd43ae0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000033_249/part-00033
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35a8235f
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@b1f9ba4
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e933ecf
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@689472f
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b303591
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4a0ce927
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@159f256c
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000043_259/part-00043
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@351e08b
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000042_258/part-00042
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,576
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 703B for [ps_partkey] INT32: 165 values, 667B raw, 667B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,371B for [part_value] DOUBLE: 165 values, 1,327B raw, 1,327B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d3e0eb7
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000038_254/part-00038
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@55381f60
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000045_261/part-00045
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57940355
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000044_260/part-00044
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,636
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@13b47914
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000047_263/part-00047
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 515B for [ps_partkey] INT32: 118 values, 479B raw, 479B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 995B for [part_value] DOUBLE: 118 values, 951B raw, 951B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000040_256' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000040
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000040_256: Committed
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000036_252' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000036
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000036_252: Committed
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,116
15/08/06 17:54:25 INFO Executor: Finished task 36.0 in stage 8.0 (TID 252). 781 bytes result sent to driver
15/08/06 17:54:25 INFO Executor: Finished task 40.0 in stage 8.0 (TID 256). 781 bytes result sent to driver
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000039_255' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000039
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000039_255: Committed
15/08/06 17:54:25 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 264, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 811B for [ps_partkey] INT32: 192 values, 775B raw, 775B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@901b69e
15/08/06 17:54:25 INFO Executor: Finished task 39.0 in stage 8.0 (TID 255). 781 bytes result sent to driver
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,587B for [part_value] DOUBLE: 192 values, 1,543B raw, 1,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 265, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 48.0 in stage 8.0 (TID 264)
15/08/06 17:54:25 INFO Executor: Running task 49.0 in stage 8.0 (TID 265)
15/08/06 17:54:25 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 266, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO Executor: Running task 50.0 in stage 8.0 (TID 266)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 256) in 225 ms on localhost (33/200)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@19944a87
15/08/06 17:54:25 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 255) in 235 ms on localhost (34/200)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000037_253' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000037
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b5b0ab
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000037_253: Committed
15/08/06 17:54:25 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 252) in 259 ms on localhost (35/200)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e81ab91
15/08/06 17:54:25 INFO Executor: Finished task 37.0 in stage 8.0 (TID 253). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 267, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@36e18539
15/08/06 17:54:25 INFO Executor: Running task 51.0 in stage 8.0 (TID 267)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 253) in 249 ms on localhost (36/200)
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,676
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5dd17719
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000046_262/part-00046
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000032_248' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000032
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000032_248: Committed
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 523B for [ps_partkey] INT32: 120 values, 487B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,816
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2211a7e5
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,011B for [part_value] DOUBLE: 120 values, 967B raw, 967B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO Executor: Finished task 32.0 in stage 8.0 (TID 248). 781 bytes result sent to driver
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 268, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 551B for [ps_partkey] INT32: 127 values, 515B raw, 515B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO Executor: Running task 52.0 in stage 8.0 (TID 268)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,067B for [part_value] DOUBLE: 127 values, 1,023B raw, 1,023B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 248) in 311 ms on localhost (37/200)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000033_249' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000033
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000033_249: Committed
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000035_251' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000035
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000041_257' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000041
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000034_250' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000034
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000034_250: Committed
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO Executor: Finished task 33.0 in stage 8.0 (TID 249). 781 bytes result sent to driver
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000035_251: Committed
15/08/06 17:54:25 INFO Executor: Finished task 35.0 in stage 8.0 (TID 251). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 269, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Finished task 34.0 in stage 8.0 (TID 250). 781 bytes result sent to driver
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000041_257: Committed
15/08/06 17:54:25 INFO Executor: Running task 53.0 in stage 8.0 (TID 269)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 249) in 337 ms on localhost (38/200)
15/08/06 17:54:25 INFO Executor: Finished task 41.0 in stage 8.0 (TID 257). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 270, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO Executor: Running task 54.0 in stage 8.0 (TID 270)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 251) in 319 ms on localhost (39/200)
15/08/06 17:54:25 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 271, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 272, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 56.0 in stage 8.0 (TID 272)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 250) in 331 ms on localhost (40/200)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e250aaf
15/08/06 17:54:25 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 257) in 287 ms on localhost (41/200)
15/08/06 17:54:25 INFO Executor: Running task 55.0 in stage 8.0 (TID 271)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000042_258' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000042
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000042_258: Committed
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000044_260' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000044
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000044_260: Committed
15/08/06 17:54:25 INFO Executor: Finished task 42.0 in stage 8.0 (TID 258). 781 bytes result sent to driver
15/08/06 17:54:25 INFO Executor: Finished task 44.0 in stage 8.0 (TID 260). 781 bytes result sent to driver
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000045_261' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000045
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000045_261: Committed
15/08/06 17:54:25 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 273, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Finished task 45.0 in stage 8.0 (TID 261). 781 bytes result sent to driver
15/08/06 17:54:25 INFO Executor: Running task 57.0 in stage 8.0 (TID 273)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 258) in 292 ms on localhost (42/200)
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 274, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 58.0 in stage 8.0 (TID 274)
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:54:25 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 260) in 288 ms on localhost (43/200)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 275, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 59.0 in stage 8.0 (TID 275)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 261) in 287 ms on localhost (44/200)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000043_259' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000043
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000043_259: Committed
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000038_254' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000038
15/08/06 17:54:25 INFO Executor: Finished task 43.0 in stage 8.0 (TID 259). 781 bytes result sent to driver
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000038_254: Committed
15/08/06 17:54:25 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 276, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 60.0 in stage 8.0 (TID 276)
15/08/06 17:54:25 INFO Executor: Finished task 38.0 in stage 8.0 (TID 254). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 259) in 303 ms on localhost (45/200)
15/08/06 17:54:25 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 277, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 61.0 in stage 8.0 (TID 277)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 254) in 409 ms on localhost (46/200)
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@33d68fdb
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000049_265/part-00049
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b7e8d73
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000051_267/part-00051
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@149c9739
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000048_264/part-00048
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5e0958da
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@37a88071
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@882bbc7
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000052_268/part-00052
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,176
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b16724d
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000050_266/part-00050
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 423B for [ps_partkey] INT32: 95 values, 387B raw, 387B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@183e5e0f
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 811B for [part_value] DOUBLE: 95 values, 767B raw, 767B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@631a5f29
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@585daf93
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@249c6f53
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000054_270/part-00054
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8f9d51b
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000061_277/part-00061
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@699e4ea2
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000051_267' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000051
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000059_275/part-00059
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000051_267: Committed
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@29a1c940
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000058_274/part-00058
15/08/06 17:54:25 INFO Executor: Finished task 51.0 in stage 8.0 (TID 267). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 278, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 62.0 in stage 8.0 (TID 278)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000049_265' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000049
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000049_265: Committed
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d52377a
15/08/06 17:54:25 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 267) in 300 ms on localhost (47/200)
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000053_269/part-00053
15/08/06 17:54:25 INFO Executor: Finished task 49.0 in stage 8.0 (TID 265). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 279, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 63.0 in stage 8.0 (TID 279)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 265) in 314 ms on localhost (48/200)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6d2b12e6
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1dc2801d
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a411e7c
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000055_271/part-00055
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2a914bd6
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000057_273/part-00057
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4c8ef26d
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000048_264' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000048
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000048_264: Committed
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO Executor: Finished task 48.0 in stage 8.0 (TID 264). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 280, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 64.0 in stage 8.0 (TID 280)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1a898633
15/08/06 17:54:25 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 264) in 333 ms on localhost (49/200)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@73f3e55
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7eb81e4e
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@67f00532
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000056_272/part-00056
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f9a2ddd
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000050_266' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000050
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000050_266: Committed
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,516
15/08/06 17:54:25 INFO Executor: Finished task 50.0 in stage 8.0 (TID 266). 781 bytes result sent to driver
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 281, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO Executor: Running task 65.0 in stage 8.0 (TID 281)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 266) in 342 ms on localhost (50/200)
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 491B for [ps_partkey] INT32: 112 values, 455B raw, 455B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 947B for [part_value] DOUBLE: 112 values, 903B raw, 903B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,636
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 515B for [ps_partkey] INT32: 118 values, 479B raw, 479B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,336
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000061_277' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000061
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@59653639
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000061_277: Committed
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 995B for [part_value] DOUBLE: 118 values, 951B raw, 951B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO Executor: Finished task 61.0 in stage 8.0 (TID 277). 781 bytes result sent to driver
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 282, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 455B for [ps_partkey] INT32: 103 values, 419B raw, 419B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO Executor: Running task 66.0 in stage 8.0 (TID 282)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 277) in 181 ms on localhost (51/200)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000054_270' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000054
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000054_270: Committed
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 875B for [part_value] DOUBLE: 103 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO Executor: Finished task 54.0 in stage 8.0 (TID 270). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 283, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 67.0 in stage 8.0 (TID 283)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 270) in 299 ms on localhost (52/200)
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000059_275' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000059
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000059_275: Committed
15/08/06 17:54:25 INFO Executor: Finished task 59.0 in stage 8.0 (TID 275). 781 bytes result sent to driver
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000058_274' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000058
15/08/06 17:54:25 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 284, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000058_274: Committed
15/08/06 17:54:25 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 275) in 291 ms on localhost (53/200)
15/08/06 17:54:25 INFO Executor: Finished task 58.0 in stage 8.0 (TID 274). 781 bytes result sent to driver
15/08/06 17:54:25 INFO Executor: Running task 68.0 in stage 8.0 (TID 284)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000057_273' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000057
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000057_273: Committed
15/08/06 17:54:25 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 285, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 69.0 in stage 8.0 (TID 285)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000053_269' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000053
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000053_269: Committed
15/08/06 17:54:25 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 274) in 297 ms on localhost (54/200)
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO Executor: Finished task 57.0 in stage 8.0 (TID 273). 781 bytes result sent to driver
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4df463ef
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 286, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000060_276/part-00060
15/08/06 17:54:25 INFO Executor: Finished task 53.0 in stage 8.0 (TID 269). 781 bytes result sent to driver
15/08/06 17:54:25 INFO Executor: Running task 70.0 in stage 8.0 (TID 286)
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 273) in 303 ms on localhost (55/200)
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 287, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 71.0 in stage 8.0 (TID 287)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 269) in 317 ms on localhost (56/200)
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000055_271' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000055
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000055_271: Committed
15/08/06 17:54:25 INFO Executor: Finished task 55.0 in stage 8.0 (TID 271). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 288, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 72.0 in stage 8.0 (TID 288)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 271) in 320 ms on localhost (57/200)
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15f18e5
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,336
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 855B for [ps_partkey] INT32: 203 values, 819B raw, 819B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,675B for [part_value] DOUBLE: 203 values, 1,631B raw, 1,631B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000060_276' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000060
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000060_276: Committed
15/08/06 17:54:25 INFO Executor: Finished task 60.0 in stage 8.0 (TID 276). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 289, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 73.0 in stage 8.0 (TID 289)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 276) in 351 ms on localhost (58/200)
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3eb82839
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000063_279/part-00063
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2a5d1d5f
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@379559e3
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000065_281/part-00065
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000064_280/part-00064
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2e406297
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000062_278/part-00062
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@31d225dc
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,056
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 599B for [ps_partkey] INT32: 139 values, 563B raw, 563B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,163B for [part_value] DOUBLE: 139 values, 1,119B raw, 1,119B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@736c6a99
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@744bbfc4
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6608ee4e
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000047_263' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000047
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000047_263: Committed
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000046_262' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000046
15/08/06 17:54:25 INFO Executor: Finished task 47.0 in stage 8.0 (TID 263). 781 bytes result sent to driver
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000046_262: Committed
15/08/06 17:54:25 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 290, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 74.0 in stage 8.0 (TID 290)
15/08/06 17:54:25 INFO Executor: Finished task 46.0 in stage 8.0 (TID 262). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 263) in 799 ms on localhost (59/200)
15/08/06 17:54:25 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 291, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 75.0 in stage 8.0 (TID 291)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000063_279' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000063
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000063_279: Committed
15/08/06 17:54:25 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 262) in 804 ms on localhost (60/200)
15/08/06 17:54:25 INFO Executor: Finished task 63.0 in stage 8.0 (TID 279). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 292, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 76.0 in stage 8.0 (TID 292)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 279) in 288 ms on localhost (61/200)
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@9cbbeae
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000068_284/part-00068
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@28bfcac9
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000067_283/part-00067
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000065_281' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000065
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000065_281: Committed
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO Executor: Finished task 65.0 in stage 8.0 (TID 281). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 293, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 77.0 in stage 8.0 (TID 293)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 281) in 265 ms on localhost (62/200)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000064_280' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000064
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000064_280: Committed
15/08/06 17:54:25 INFO Executor: Finished task 64.0 in stage 8.0 (TID 280). 781 bytes result sent to driver
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000062_278' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000062
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000062_278: Committed
15/08/06 17:54:25 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 294, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 78.0 in stage 8.0 (TID 294)
15/08/06 17:54:25 INFO Executor: Finished task 62.0 in stage 8.0 (TID 278). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 280) in 280 ms on localhost (63/200)
15/08/06 17:54:25 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 295, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 79.0 in stage 8.0 (TID 295)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 278) in 304 ms on localhost (64/200)
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@43bd9fe4
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000071_287/part-00071
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d160856
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6ae66c1f
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14b976fd
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c32e6ac
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000072_288/part-00072
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000069_285/part-00069
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,716
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 531B for [ps_partkey] INT32: 122 values, 495B raw, 495B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,027B for [part_value] DOUBLE: 122 values, 983B raw, 983B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@65d10bcd
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000066_282/part-00066
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@467c7c33
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c191c05
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e6291f7
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000070_286/part-00070
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c8c1b4c
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76e47eb7
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,816
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 751B for [ps_partkey] INT32: 177 values, 715B raw, 715B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,467B for [part_value] DOUBLE: 177 values, 1,423B raw, 1,423B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000067_283' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000067
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000067_283: Committed
15/08/06 17:54:25 INFO Executor: Finished task 67.0 in stage 8.0 (TID 283). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 296, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO Executor: Running task 80.0 in stage 8.0 (TID 296)
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 283) in 298 ms on localhost (65/200)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d5d3111
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000071_287' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000071
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000071_287: Committed
15/08/06 17:54:25 INFO Executor: Finished task 71.0 in stage 8.0 (TID 287). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 297, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 81.0 in stage 8.0 (TID 297)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 287) in 296 ms on localhost (66/200)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000069_285' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000069
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000069_285: Committed
15/08/06 17:54:25 INFO Executor: Finished task 69.0 in stage 8.0 (TID 285). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 298, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 82.0 in stage 8.0 (TID 298)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 285) in 308 ms on localhost (67/200)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000066_282' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000066
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000066_282: Committed
15/08/06 17:54:25 INFO Executor: Finished task 66.0 in stage 8.0 (TID 282). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 299, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a868b99
15/08/06 17:54:25 INFO Executor: Running task 83.0 in stage 8.0 (TID 299)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000073_289/part-00073
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 282) in 329 ms on localhost (68/200)
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000070_286' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000070
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000070_286: Committed
15/08/06 17:54:25 INFO Executor: Finished task 70.0 in stage 8.0 (TID 286). 781 bytes result sent to driver
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 300, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 84.0 in stage 8.0 (TID 300)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 286) in 320 ms on localhost (69/200)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@713f080
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000073_289' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000073
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000073_289: Committed
15/08/06 17:54:25 INFO Executor: Finished task 73.0 in stage 8.0 (TID 289). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 301, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 85.0 in stage 8.0 (TID 301)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 289) in 291 ms on localhost (70/200)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000052_268' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000052
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000052_268: Committed
15/08/06 17:54:25 INFO Executor: Finished task 52.0 in stage 8.0 (TID 268). 781 bytes result sent to driver
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3f31ea9
15/08/06 17:54:25 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 302, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 86.0 in stage 8.0 (TID 302)
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000075_291/part-00075
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 268) in 706 ms on localhost (71/200)
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24c97703
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000074_290/part-00074
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25bb5f3e
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000076_292/part-00076
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@232a63ff
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1ede54be
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000077_293/part-00077
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d124a28
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000078_294/part-00078
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ac4b5d4
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@73bf44b6
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,236
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 635B for [ps_partkey] INT32: 148 values, 599B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,235B for [part_value] DOUBLE: 148 values, 1,191B raw, 1,191B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16dcd8c2
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000079_295/part-00079
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@136fd7fd
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67f980e6
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1707a9a7
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000080_296/part-00080
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,096
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 807B for [ps_partkey] INT32: 191 values, 771B raw, 771B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,579B for [part_value] DOUBLE: 191 values, 1,535B raw, 1,535B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000075_291' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000075
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000075_291: Committed
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7177458b
15/08/06 17:54:25 INFO Executor: Finished task 75.0 in stage 8.0 (TID 291). 781 bytes result sent to driver
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000076_292' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000076
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000076_292: Committed
15/08/06 17:54:25 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 303, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 87.0 in stage 8.0 (TID 303)
15/08/06 17:54:25 INFO Executor: Finished task 76.0 in stage 8.0 (TID 292). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 291) in 196 ms on localhost (72/200)
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000074_290' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000074
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000074_290: Committed
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000056_272' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000056
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000056_272: Committed
15/08/06 17:54:25 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 304, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Finished task 74.0 in stage 8.0 (TID 290). 781 bytes result sent to driver
15/08/06 17:54:25 INFO Executor: Running task 88.0 in stage 8.0 (TID 304)
15/08/06 17:54:25 INFO Executor: Finished task 56.0 in stage 8.0 (TID 272). 781 bytes result sent to driver
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@318ac7ec
15/08/06 17:54:25 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 292) in 197 ms on localhost (73/200)
15/08/06 17:54:25 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 305, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 89.0 in stage 8.0 (TID 305)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 290) in 203 ms on localhost (74/200)
15/08/06 17:54:25 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 306, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 90.0 in stage 8.0 (TID 306)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 272) in 736 ms on localhost (75/200)
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48778ca2
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000082_298/part-00082
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1aac48f5
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000081_297/part-00081
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000077_293' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000077
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000077_293: Committed
15/08/06 17:54:25 INFO Executor: Finished task 77.0 in stage 8.0 (TID 293). 781 bytes result sent to driver
15/08/06 17:54:25 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 307, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:25 INFO Executor: Running task 91.0 in stage 8.0 (TID 307)
15/08/06 17:54:25 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 293) in 214 ms on localhost (76/200)
15/08/06 17:54:25 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@27150816
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000083_299/part-00083
15/08/06 17:54:25 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b4f2aba
15/08/06 17:54:25 INFO CodecConfig: Compression set to false
15/08/06 17:54:25 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:25 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000078_294' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000078
15/08/06 17:54:25 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000078_294: Committed
15/08/06 17:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:54:26 INFO Executor: Finished task 78.0 in stage 8.0 (TID 294). 781 bytes result sent to driver
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@bc1e54e
15/08/06 17:54:26 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 308, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 92.0 in stage 8.0 (TID 308)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 294) in 219 ms on localhost (77/200)
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67e2a360
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3602a299
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000084_300/part-00084
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b115946
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000086_302/part-00086
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@23f2b5a1
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000081_297' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000081
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000081_297: Committed
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c0e39b2
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000085_301/part-00085
15/08/06 17:54:26 INFO Executor: Finished task 81.0 in stage 8.0 (TID 297). 781 bytes result sent to driver
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 309, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO Executor: Running task 93.0 in stage 8.0 (TID 309)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 297) in 302 ms on localhost (78/200)
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000083_299' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000083
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000083_299: Committed
15/08/06 17:54:26 INFO Executor: Finished task 83.0 in stage 8.0 (TID 299). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 310, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 94.0 in stage 8.0 (TID 310)
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@53aaed2
15/08/06 17:54:26 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 299) in 297 ms on localhost (79/200)
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3286516b
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000084_300' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000084
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000084_300: Committed
15/08/06 17:54:26 INFO Executor: Finished task 84.0 in stage 8.0 (TID 300). 781 bytes result sent to driver
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 311, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO Executor: Running task 95.0 in stage 8.0 (TID 311)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 300) in 301 ms on localhost (80/200)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000086_302' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000086
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000086_302: Committed
15/08/06 17:54:26 INFO Executor: Finished task 86.0 in stage 8.0 (TID 302). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 312, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 96.0 in stage 8.0 (TID 312)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 302) in 275 ms on localhost (81/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000085_301' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000085
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000085_301: Committed
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO Executor: Finished task 85.0 in stage 8.0 (TID 301). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 313, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 97.0 in stage 8.0 (TID 313)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 301) in 286 ms on localhost (82/200)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1030f879
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7f08d83c
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2b2ee0cc
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000088_304/part-00088
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000090_306/part-00090
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000089_305/part-00089
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@37a9b7dd
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000087_303/part-00087
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15a9a5fd
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,276
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@396d9aac
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e2c480d
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000092_308/part-00092
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 843B for [ps_partkey] INT32: 200 values, 807B raw, 807B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d2e54bf
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,651B for [part_value] DOUBLE: 200 values, 1,607B raw, 1,607B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,736
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 735B for [ps_partkey] INT32: 173 values, 699B raw, 699B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,435B for [part_value] DOUBLE: 173 values, 1,391B raw, 1,391B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12adad5c
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,316
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 651B for [ps_partkey] INT32: 152 values, 615B raw, 615B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bae5214
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,267B for [part_value] DOUBLE: 152 values, 1,223B raw, 1,223B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000068_284' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000068
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000068_284: Committed
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO Executor: Finished task 68.0 in stage 8.0 (TID 284). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 314, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 98.0 in stage 8.0 (TID 314)
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@d1d4782
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000091_307/part-00091
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 284) in 702 ms on localhost (83/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000072_288' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000072
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000072_288: Committed
15/08/06 17:54:26 INFO Executor: Finished task 72.0 in stage 8.0 (TID 288). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 315, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 99.0 in stage 8.0 (TID 315)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 288) in 699 ms on localhost (84/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000088_304' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000088
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000089_305' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000089
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000088_304: Committed
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000089_305: Committed
15/08/06 17:54:26 INFO Executor: Finished task 88.0 in stage 8.0 (TID 304). 781 bytes result sent to driver
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000090_306' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000090
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000090_306: Committed
15/08/06 17:54:26 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 316, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 100.0 in stage 8.0 (TID 316)
15/08/06 17:54:26 INFO Executor: Finished task 89.0 in stage 8.0 (TID 305). 781 bytes result sent to driver
15/08/06 17:54:26 INFO Executor: Finished task 90.0 in stage 8.0 (TID 306). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 304) in 291 ms on localhost (85/200)
15/08/06 17:54:26 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 317, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 101.0 in stage 8.0 (TID 317)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 305) in 291 ms on localhost (86/200)
15/08/06 17:54:26 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 318, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 102.0 in stage 8.0 (TID 318)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 306) in 291 ms on localhost (87/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000092_308' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000092
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000092_308: Committed
15/08/06 17:54:26 INFO Executor: Finished task 92.0 in stage 8.0 (TID 308). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 319, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 103.0 in stage 8.0 (TID 319)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 308) in 266 ms on localhost (88/200)
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@da26707
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000094_310/part-00094
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3c6de7c3
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@aee9cd9
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000093_309/part-00093
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,656
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 519B for [ps_partkey] INT32: 119 values, 483B raw, 483B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,003B for [part_value] DOUBLE: 119 values, 959B raw, 959B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@55f148fe
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000095_311/part-00095
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000087_303' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000087
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000087_303: Committed
15/08/06 17:54:26 INFO Executor: Finished task 87.0 in stage 8.0 (TID 303). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 320, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 104.0 in stage 8.0 (TID 320)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 303) in 328 ms on localhost (89/200)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a18305b
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@40e01f50
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000091_307' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000091
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000091_307: Committed
15/08/06 17:54:26 INFO Executor: Finished task 91.0 in stage 8.0 (TID 307). 781 bytes result sent to driver
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:54:26 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 321, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 105.0 in stage 8.0 (TID 321)
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e33ad67
15/08/06 17:54:26 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 307) in 318 ms on localhost (90/200)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a50328a
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000096_312/part-00096
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78f541f2
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000097_313/part-00097
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000095_311' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000095
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000095_311: Committed
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO Executor: Finished task 95.0 in stage 8.0 (TID 311). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 322, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 106.0 in stage 8.0 (TID 322)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 311) in 158 ms on localhost (91/200)
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@259433c9
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,556
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 499B for [ps_partkey] INT32: 114 values, 463B raw, 463B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 963B for [part_value] DOUBLE: 114 values, 919B raw, 919B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000093_309' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000093
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000093_309: Committed
15/08/06 17:54:26 INFO Executor: Finished task 93.0 in stage 8.0 (TID 309). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 323, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 107.0 in stage 8.0 (TID 323)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000094_310' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000094
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000094_310: Committed
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@70318ce6
15/08/06 17:54:26 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 309) in 196 ms on localhost (92/200)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO Executor: Finished task 94.0 in stage 8.0 (TID 310). 781 bytes result sent to driver
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,396
15/08/06 17:54:26 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 324, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 108.0 in stage 8.0 (TID 324)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 667B for [ps_partkey] INT32: 156 values, 631B raw, 631B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 310) in 192 ms on localhost (93/200)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,299B for [part_value] DOUBLE: 156 values, 1,255B raw, 1,255B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000096_312' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000096
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000096_312: Committed
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO Executor: Finished task 96.0 in stage 8.0 (TID 312). 781 bytes result sent to driver
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 325, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 109.0 in stage 8.0 (TID 325)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 312) in 174 ms on localhost (94/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000097_313' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000097
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000097_313: Committed
15/08/06 17:54:26 INFO Executor: Finished task 97.0 in stage 8.0 (TID 313). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 326, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 110.0 in stage 8.0 (TID 326)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 313) in 181 ms on localhost (95/200)
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@70021311
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1831fa8a
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000099_315/part-00099
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000098_314/part-00098
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@72ed7a50
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000100_316/part-00100
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@44f95522
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000101_317/part-00101
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5575ed44
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000103_319/part-00103
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e622373
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d0593d7
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,516
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c7c9c21
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 691B for [ps_partkey] INT32: 162 values, 655B raw, 655B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,347B for [part_value] DOUBLE: 162 values, 1,303B raw, 1,303B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,756
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c769243
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 739B for [ps_partkey] INT32: 174 values, 703B raw, 703B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b0035f0
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,443B for [part_value] DOUBLE: 174 values, 1,399B raw, 1,399B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000102_318/part-00102
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@55765661
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000079_295' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000079
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000079_295: Committed
15/08/06 17:54:26 INFO Executor: Finished task 79.0 in stage 8.0 (TID 295). 781 bytes result sent to driver
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000080_296' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000080
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000080_296: Committed
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@21c3de91
15/08/06 17:54:26 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 327, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:54:26 INFO Executor: Running task 111.0 in stage 8.0 (TID 327)
15/08/06 17:54:26 INFO Executor: Finished task 80.0 in stage 8.0 (TID 296). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 295) in 748 ms on localhost (96/200)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 328, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 112.0 in stage 8.0 (TID 328)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 296) in 704 ms on localhost (97/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000082_298' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000082
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000082_298: Committed
15/08/06 17:54:26 INFO Executor: Finished task 82.0 in stage 8.0 (TID 298). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 329, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 113.0 in stage 8.0 (TID 329)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 298) in 685 ms on localhost (98/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000099_315' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000099
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000099_315: Committed
15/08/06 17:54:26 INFO Executor: Finished task 99.0 in stage 8.0 (TID 315). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 330, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 114.0 in stage 8.0 (TID 330)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 315) in 293 ms on localhost (99/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000100_316' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000100
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000100_316: Committed
15/08/06 17:54:26 INFO Executor: Finished task 100.0 in stage 8.0 (TID 316). 781 bytes result sent to driver
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000103_319' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000103
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000103_319: Committed
15/08/06 17:54:26 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 331, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 115.0 in stage 8.0 (TID 331)
15/08/06 17:54:26 INFO Executor: Finished task 103.0 in stage 8.0 (TID 319). 781 bytes result sent to driver
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000101_317' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000101
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000101_317: Committed
15/08/06 17:54:26 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 316) in 291 ms on localhost (100/200)
15/08/06 17:54:26 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 332, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 116.0 in stage 8.0 (TID 332)
15/08/06 17:54:26 INFO Executor: Finished task 101.0 in stage 8.0 (TID 317). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 319) in 285 ms on localhost (101/200)
15/08/06 17:54:26 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 333, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 117.0 in stage 8.0 (TID 333)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 317) in 292 ms on localhost (102/200)
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d889d8b
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000104_320/part-00104
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000102_318' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000102
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000102_318: Committed
15/08/06 17:54:26 INFO Executor: Finished task 102.0 in stage 8.0 (TID 318). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 334, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 118.0 in stage 8.0 (TID 334)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 318) in 296 ms on localhost (103/200)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b3bc16d
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000105_321/part-00105
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1617568c
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000106_322/part-00106
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f78bbbf
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,796
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 747B for [ps_partkey] INT32: 176 values, 711B raw, 711B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,459B for [part_value] DOUBLE: 176 values, 1,415B raw, 1,415B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2acd91d7
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ccf5ae8
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000109_325/part-00109
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@96c8057
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000107_323/part-00107
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2e2fddaf
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000108_324/part-00108
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@75cc020e
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66fa6173
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f777177
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000105_321' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000105
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000105_321: Committed
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,616
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO Executor: Finished task 105.0 in stage 8.0 (TID 321). 781 bytes result sent to driver
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 711B for [ps_partkey] INT32: 167 values, 675B raw, 675B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000104_320' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000104
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,387B for [part_value] DOUBLE: 167 values, 1,343B raw, 1,343B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000104_320: Committed
15/08/06 17:54:26 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 335, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Finished task 104.0 in stage 8.0 (TID 320). 781 bytes result sent to driver
15/08/06 17:54:26 INFO Executor: Running task 119.0 in stage 8.0 (TID 335)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 321) in 293 ms on localhost (104/200)
15/08/06 17:54:26 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 336, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 120.0 in stage 8.0 (TID 336)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 320) in 314 ms on localhost (105/200)
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ec2de5
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d5971ea
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000110_326/part-00110
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000107_323' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000107
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000107_323: Committed
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000106_322' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000106
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000106_322: Committed
15/08/06 17:54:26 INFO Executor: Finished task 106.0 in stage 8.0 (TID 322). 781 bytes result sent to driver
15/08/06 17:54:26 INFO Executor: Finished task 107.0 in stage 8.0 (TID 323). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 337, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000108_324' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000108
15/08/06 17:54:26 INFO Executor: Running task 121.0 in stage 8.0 (TID 337)
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000108_324: Committed
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78405cd1
15/08/06 17:54:26 INFO Executor: Finished task 108.0 in stage 8.0 (TID 324). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 322) in 310 ms on localhost (106/200)
15/08/06 17:54:26 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 338, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:54:26 INFO Executor: Running task 122.0 in stage 8.0 (TID 338)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 323) in 297 ms on localhost (107/200)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 339, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO Executor: Running task 123.0 in stage 8.0 (TID 339)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 324) in 294 ms on localhost (108/200)
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4431cc7f
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2168c8d3
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000111_327/part-00111
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000113_329/part-00113
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000109_325' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000109
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000109_325: Committed
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO Executor: Finished task 109.0 in stage 8.0 (TID 325). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 340, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 124.0 in stage 8.0 (TID 340)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 325) in 299 ms on localhost (109/200)
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3ef680da
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@628eadc9
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@49f88be3
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000118_334/part-00118
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000110_326' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000110
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000110_326: Committed
15/08/06 17:54:26 INFO Executor: Finished task 110.0 in stage 8.0 (TID 326). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 341, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 326) in 304 ms on localhost (110/200)
15/08/06 17:54:26 INFO Executor: Running task 125.0 in stage 8.0 (TID 341)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3aee29d0
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000112_328/part-00112
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b4ee35a
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000117_333/part-00117
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@46d74328
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000114_330/part-00114
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@623ca21e
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@684a88e8
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000115_331/part-00115
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@76a9bc10
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000116_332/part-00116
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f06425e
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000113_329' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000113
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000113_329: Committed
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@47019902
15/08/06 17:54:26 INFO Executor: Finished task 113.0 in stage 8.0 (TID 329). 781 bytes result sent to driver
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,836
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 342, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 555B for [ps_partkey] INT32: 128 values, 519B raw, 519B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO Executor: Running task 126.0 in stage 8.0 (TID 342)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,075B for [part_value] DOUBLE: 128 values, 1,031B raw, 1,031B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 329) in 158 ms on localhost (111/200)
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f332eaf
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,056
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@65cc878a
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 799B for [ps_partkey] INT32: 189 values, 763B raw, 763B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2c659cd4
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,563B for [part_value] DOUBLE: 189 values, 1,519B raw, 1,519B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000111_327' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000111
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000111_327: Committed
15/08/06 17:54:26 INFO Executor: Finished task 111.0 in stage 8.0 (TID 327). 781 bytes result sent to driver
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 343, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO Executor: Running task 127.0 in stage 8.0 (TID 343)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 327) in 178 ms on localhost (112/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000118_334' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000118
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000118_334: Committed
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000112_328' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000112
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000112_328: Committed
15/08/06 17:54:26 INFO Executor: Finished task 118.0 in stage 8.0 (TID 334). 781 bytes result sent to driver
15/08/06 17:54:26 INFO Executor: Finished task 112.0 in stage 8.0 (TID 328). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 344, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 128.0 in stage 8.0 (TID 344)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 334) in 166 ms on localhost (113/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000115_331' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000115
15/08/06 17:54:26 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 345, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000115_331: Committed
15/08/06 17:54:26 INFO Executor: Running task 129.0 in stage 8.0 (TID 345)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 328) in 191 ms on localhost (114/200)
15/08/06 17:54:26 INFO Executor: Finished task 115.0 in stage 8.0 (TID 331). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 346, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 130.0 in stage 8.0 (TID 346)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 331) in 178 ms on localhost (115/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000116_332' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000116
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000116_332: Committed
15/08/06 17:54:26 INFO Executor: Finished task 116.0 in stage 8.0 (TID 332). 781 bytes result sent to driver
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d8fde01
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000119_335/part-00119
15/08/06 17:54:26 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 347, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000117_333' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000117
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000117_333: Committed
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO Executor: Running task 131.0 in stage 8.0 (TID 347)
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO Executor: Finished task 117.0 in stage 8.0 (TID 333). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 332) in 182 ms on localhost (116/200)
15/08/06 17:54:26 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 348, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 132.0 in stage 8.0 (TID 348)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 333) in 181 ms on localhost (117/200)
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@72c0e2f5
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000120_336/part-00120
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4721b830
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000122_338/part-00122
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@418fd51a
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000123_339/part-00123
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@40fbbe3f
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@57ef64c9
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,116
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 811B for [ps_partkey] INT32: 192 values, 775B raw, 775B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1dfa59e7
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000121_337/part-00121
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,587B for [part_value] DOUBLE: 192 values, 1,543B raw, 1,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f44cd90
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,876
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7e13ea36
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d75bfe4
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000124_340/part-00124
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,776
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 763B for [ps_partkey] INT32: 180 values, 727B raw, 727B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 743B for [ps_partkey] INT32: 175 values, 707B raw, 707B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,491B for [part_value] DOUBLE: 180 values, 1,447B raw, 1,447B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,451B for [part_value] DOUBLE: 175 values, 1,407B raw, 1,407B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2c404f21
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@547e5ad2
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000125_341/part-00125
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@701616de
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,236
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 635B for [ps_partkey] INT32: 148 values, 599B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,235B for [part_value] DOUBLE: 148 values, 1,191B raw, 1,191B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000123_339' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000123
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000123_339: Committed
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000121_337' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000121
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000121_337: Committed
15/08/06 17:54:26 INFO Executor: Finished task 123.0 in stage 8.0 (TID 339). 781 bytes result sent to driver
15/08/06 17:54:26 INFO Executor: Finished task 121.0 in stage 8.0 (TID 337). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 349, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 133.0 in stage 8.0 (TID 349)
15/08/06 17:54:26 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 350, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 134.0 in stage 8.0 (TID 350)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 339) in 236 ms on localhost (118/200)
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a66c8fd
15/08/06 17:54:26 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 337) in 241 ms on localhost (119/200)
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000126_342/part-00126
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@20c849f2
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,736
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 535B for [ps_partkey] INT32: 123 values, 499B raw, 499B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,035B for [part_value] DOUBLE: 123 values, 991B raw, 991B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@16f48ee2
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,656
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 719B for [ps_partkey] INT32: 169 values, 683B raw, 683B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,403B for [part_value] DOUBLE: 169 values, 1,359B raw, 1,359B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000124_340' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000124
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000124_340: Committed
15/08/06 17:54:26 INFO Executor: Finished task 124.0 in stage 8.0 (TID 340). 781 bytes result sent to driver
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@59a0b187
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000127_343/part-00127
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 351, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO Executor: Running task 135.0 in stage 8.0 (TID 351)
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 340) in 244 ms on localhost (120/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000125_341' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000125
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000125_341: Committed
15/08/06 17:54:26 INFO Executor: Finished task 125.0 in stage 8.0 (TID 341). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 352, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 136.0 in stage 8.0 (TID 352)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 341) in 235 ms on localhost (121/200)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c0a15e3
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57921a7
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000126_342' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000126
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000128_344/part-00128
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000126_342: Committed
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2afad175
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000129_345/part-00129
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO Executor: Finished task 126.0 in stage 8.0 (TID 342). 781 bytes result sent to driver
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 353, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO Executor: Running task 137.0 in stage 8.0 (TID 353)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 342) in 220 ms on localhost (122/200)
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@46907ce2
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000131_347/part-00131
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@79b65caf
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000132_348/part-00132
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4e50bd56
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000130_346/part-00130
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5be8dbba
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@288a75f3
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@752f896d
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000127_343' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000127
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@576b243a
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000127_343: Committed
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO Executor: Finished task 127.0 in stage 8.0 (TID 343). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 354, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO Executor: Running task 138.0 in stage 8.0 (TID 354)
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 343) in 241 ms on localhost (123/200)
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@8114ae8
15/08/06 17:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000098_314' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000098
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000098_314: Committed
15/08/06 17:54:26 INFO Executor: Finished task 98.0 in stage 8.0 (TID 314). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 355, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 139.0 in stage 8.0 (TID 355)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 314) in 717 ms on localhost (124/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000129_345' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000129
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000129_345: Committed
15/08/06 17:54:26 INFO Executor: Finished task 129.0 in stage 8.0 (TID 345). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 356, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 140.0 in stage 8.0 (TID 356)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 345) in 241 ms on localhost (125/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000132_348' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000132
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000132_348: Committed
15/08/06 17:54:26 INFO Executor: Finished task 132.0 in stage 8.0 (TID 348). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 357, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 141.0 in stage 8.0 (TID 357)
15/08/06 17:54:26 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 348) in 237 ms on localhost (126/200)
15/08/06 17:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000130_346' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000130
15/08/06 17:54:26 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000130_346: Committed
15/08/06 17:54:26 INFO Executor: Finished task 130.0 in stage 8.0 (TID 346). 781 bytes result sent to driver
15/08/06 17:54:26 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 358, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:26 INFO Executor: Running task 142.0 in stage 8.0 (TID 358)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 346) in 252 ms on localhost (127/200)
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24c9b4b9
15/08/06 17:54:26 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3677f08d
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000134_350/part-00134
15/08/06 17:54:26 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000133_349/part-00133
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression set to false
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:26 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@eac7918
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d421e2e
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000136_352/part-00136
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ccd2e8f
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000135_351/part-00135
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d1edb7d
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,056
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 599B for [ps_partkey] INT32: 139 values, 563B raw, 563B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,163B for [part_value] DOUBLE: 139 values, 1,119B raw, 1,119B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67d78b9a
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@45c2eaa4
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000133_349' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000133
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000133_349: Committed
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO Executor: Finished task 133.0 in stage 8.0 (TID 349). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 359, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 143.0 in stage 8.0 (TID 359)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 349) in 161 ms on localhost (128/200)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22257f97
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000134_350' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000134
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000137_353/part-00137
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000134_350: Committed
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO Executor: Finished task 134.0 in stage 8.0 (TID 350). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 360, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 144.0 in stage 8.0 (TID 360)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 350) in 169 ms on localhost (129/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000135_351' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000135
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000135_351: Committed
15/08/06 17:54:27 INFO Executor: Finished task 135.0 in stage 8.0 (TID 351). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 361, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 145.0 in stage 8.0 (TID 361)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 351) in 154 ms on localhost (130/200)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@70de4391
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000138_354/part-00138
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000136_352' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000136
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000136_352: Committed
15/08/06 17:54:27 INFO Executor: Finished task 136.0 in stage 8.0 (TID 352). 781 bytes result sent to driver
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@346dea06
15/08/06 17:54:27 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 362, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,736
15/08/06 17:54:27 INFO Executor: Running task 146.0 in stage 8.0 (TID 362)
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 535B for [ps_partkey] INT32: 123 values, 499B raw, 499B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 352) in 157 ms on localhost (131/200)
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,035B for [part_value] DOUBLE: 123 values, 991B raw, 991B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@41f37515
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000141_357/part-00141
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@642ad3c1
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000139_355/part-00139
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e53ef13
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,536
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 495B for [ps_partkey] INT32: 113 values, 459B raw, 459B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 955B for [part_value] DOUBLE: 113 values, 911B raw, 911B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@776f3281
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51e45322
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000142_358/part-00142
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000137_353' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000137
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000137_353: Committed
15/08/06 17:54:27 INFO Executor: Finished task 137.0 in stage 8.0 (TID 353). 781 bytes result sent to driver
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000138_354' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000138
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000138_354: Committed
15/08/06 17:54:27 INFO Executor: Finished task 138.0 in stage 8.0 (TID 354). 781 bytes result sent to driver
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@587b8584
15/08/06 17:54:27 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 363, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 147.0 in stage 8.0 (TID 363)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@74aa795b
15/08/06 17:54:27 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 364, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 148.0 in stage 8.0 (TID 364)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 354) in 273 ms on localhost (132/200)
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 353) in 301 ms on localhost (133/200)
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,656
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3633d70a
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000140_356/part-00140
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 719B for [ps_partkey] INT32: 169 values, 683B raw, 683B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,403B for [part_value] DOUBLE: 169 values, 1,359B raw, 1,359B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000114_330' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000114
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000114_330: Committed
15/08/06 17:54:27 INFO Executor: Finished task 114.0 in stage 8.0 (TID 330). 781 bytes result sent to driver
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000141_357' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000141
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000141_357: Committed
15/08/06 17:54:27 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 365, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Finished task 141.0 in stage 8.0 (TID 357). 781 bytes result sent to driver
15/08/06 17:54:27 INFO Executor: Running task 149.0 in stage 8.0 (TID 365)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 330) in 678 ms on localhost (134/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 366, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 150.0 in stage 8.0 (TID 366)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 357) in 256 ms on localhost (135/200)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5581b9c4
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000120_336' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000120
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000120_336: Committed
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000142_358' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000142
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000142_358: Committed
15/08/06 17:54:27 INFO Executor: Finished task 120.0 in stage 8.0 (TID 336). 781 bytes result sent to driver
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000119_335' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000119
15/08/06 17:54:27 INFO Executor: Finished task 142.0 in stage 8.0 (TID 358). 781 bytes result sent to driver
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000119_335: Committed
15/08/06 17:54:27 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 367, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 151.0 in stage 8.0 (TID 367)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 336) in 652 ms on localhost (136/200)
15/08/06 17:54:27 INFO Executor: Finished task 119.0 in stage 8.0 (TID 335). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 368, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO Executor: Running task 152.0 in stage 8.0 (TID 368)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 369, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO Executor: Running task 153.0 in stage 8.0 (TID 369)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 335) in 657 ms on localhost (137/200)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 358) in 282 ms on localhost (138/200)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000122_338' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000122
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000122_338: Committed
15/08/06 17:54:27 INFO Executor: Finished task 122.0 in stage 8.0 (TID 338). 781 bytes result sent to driver
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a81551b
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000143_359/part-00143
15/08/06 17:54:27 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 370, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO Executor: Running task 154.0 in stage 8.0 (TID 370)
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 338) in 633 ms on localhost (139/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000140_356' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000140
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000140_356: Committed
15/08/06 17:54:27 INFO Executor: Finished task 140.0 in stage 8.0 (TID 356). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 371, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 155.0 in stage 8.0 (TID 371)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 356) in 313 ms on localhost (140/200)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@434de008
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2daf4fbd
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000145_361/part-00145
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,336
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 455B for [ps_partkey] INT32: 103 values, 419B raw, 419B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 875B for [part_value] DOUBLE: 103 values, 831B raw, 831B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c718285
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000144_360/part-00144
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d5aca0b
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@386bfda
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000143_359' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000143
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000143_359: Committed
15/08/06 17:54:27 INFO Executor: Finished task 143.0 in stage 8.0 (TID 359). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 372, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 359) in 291 ms on localhost (141/200)
15/08/06 17:54:27 INFO Executor: Running task 156.0 in stage 8.0 (TID 372)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61140f8e
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000149_365/part-00149
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c50f0ce
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000146_362/part-00146
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000144_360' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000144
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000144_360: Committed
15/08/06 17:54:27 INFO Executor: Finished task 144.0 in stage 8.0 (TID 360). 781 bytes result sent to driver
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@301b75bf
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000150_366/part-00150
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 373, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 157.0 in stage 8.0 (TID 373)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 360) in 290 ms on localhost (142/200)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@40b0e822
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000148_364/part-00148
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@47370cda
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000147_363/part-00147
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@49cad6f3
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@23e6115e
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@507c4808
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e97dd61
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ed165fc
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7bd6ae73
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b573f89
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000151_367/part-00151
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000152_368/part-00152
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000131_347' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000131
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000131_347: Committed
15/08/06 17:54:27 INFO Executor: Finished task 131.0 in stage 8.0 (TID 347). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 374, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000128_344' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000128
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000128_344: Committed
15/08/06 17:54:27 INFO Executor: Running task 158.0 in stage 8.0 (TID 374)
15/08/06 17:54:27 INFO Executor: Finished task 128.0 in stage 8.0 (TID 344). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 347) in 640 ms on localhost (143/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 375, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 159.0 in stage 8.0 (TID 375)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 344) in 650 ms on localhost (144/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000148_364' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000148
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000149_365' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000149
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000148_364: Committed
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000149_365: Committed
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000146_362' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000146
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000146_362: Committed
15/08/06 17:54:27 INFO Executor: Finished task 149.0 in stage 8.0 (TID 365). 781 bytes result sent to driver
15/08/06 17:54:27 INFO Executor: Finished task 148.0 in stage 8.0 (TID 364). 781 bytes result sent to driver
15/08/06 17:54:27 INFO Executor: Finished task 146.0 in stage 8.0 (TID 362). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 376, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 160.0 in stage 8.0 (TID 376)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 365) in 152 ms on localhost (145/200)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5fbc85ec
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16b1572f
15/08/06 17:54:27 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 377, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,076
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000154_370/part-00154
15/08/06 17:54:27 INFO Executor: Running task 161.0 in stage 8.0 (TID 377)
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 803B for [ps_partkey] INT32: 190 values, 767B raw, 767B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,571B for [part_value] DOUBLE: 190 values, 1,527B raw, 1,527B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 364) in 167 ms on localhost (146/200)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@332b3bab
15/08/06 17:54:27 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 378, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000153_369/part-00153
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO Executor: Running task 162.0 in stage 8.0 (TID 378)
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c5845d0
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,536
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000147_363' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000147
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000147_363: Committed
15/08/06 17:54:27 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 362) in 323 ms on localhost (147/200)
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 495B for [ps_partkey] INT32: 113 values, 459B raw, 459B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 955B for [part_value] DOUBLE: 113 values, 911B raw, 911B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO Executor: Finished task 147.0 in stage 8.0 (TID 363). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 379, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 163.0 in stage 8.0 (TID 379)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 363) in 294 ms on localhost (148/200)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e8fdfa7
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000155_371/part-00155
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@680f8428
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,056
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 799B for [ps_partkey] INT32: 189 values, 763B raw, 763B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,563B for [part_value] DOUBLE: 189 values, 1,519B raw, 1,519B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@39bef1a8
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,816
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 551B for [ps_partkey] INT32: 127 values, 515B raw, 515B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,067B for [part_value] DOUBLE: 127 values, 1,023B raw, 1,023B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@31639d1
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000151_367' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000151
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000151_367: Committed
15/08/06 17:54:27 INFO Executor: Finished task 151.0 in stage 8.0 (TID 367). 781 bytes result sent to driver
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000152_368' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000152
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000152_368: Committed
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 380, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 164.0 in stage 8.0 (TID 380)
15/08/06 17:54:27 INFO Executor: Finished task 152.0 in stage 8.0 (TID 368). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 367) in 150 ms on localhost (149/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 381, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 165.0 in stage 8.0 (TID 381)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 368) in 149 ms on localhost (150/200)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000154_370' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000154
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000154_370: Committed
15/08/06 17:54:27 INFO Executor: Finished task 154.0 in stage 8.0 (TID 370). 781 bytes result sent to driver
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000153_369' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000153
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000153_369: Committed
15/08/06 17:54:27 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 382, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 166.0 in stage 8.0 (TID 382)
15/08/06 17:54:27 INFO Executor: Finished task 153.0 in stage 8.0 (TID 369). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 370) in 162 ms on localhost (151/200)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23ae749e
15/08/06 17:54:27 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 383, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000157_373/part-00157
15/08/06 17:54:27 INFO Executor: Running task 167.0 in stage 8.0 (TID 383)
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 369) in 173 ms on localhost (152/200)
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000155_371' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000155
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000155_371: Committed
15/08/06 17:54:27 INFO Executor: Finished task 155.0 in stage 8.0 (TID 371). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 384, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4c87a5f
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000156_372/part-00156
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 371) in 160 ms on localhost (153/200)
15/08/06 17:54:27 INFO Executor: Running task 168.0 in stage 8.0 (TID 384)
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@11e14d7d
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@9cd8d26
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6fee3ba5
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000158_374/part-00158
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8873f52
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000159_375/part-00159
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@674e24dd
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000160_376/part-00160
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000157_373' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000157
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000157_373: Committed
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO Executor: Finished task 157.0 in stage 8.0 (TID 373). 781 bytes result sent to driver
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6d8ed688
15/08/06 17:54:27 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 385, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 169.0 in stage 8.0 (TID 385)
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,636
15/08/06 17:54:27 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 373) in 147 ms on localhost (154/200)
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 715B for [ps_partkey] INT32: 168 values, 679B raw, 679B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,395B for [part_value] DOUBLE: 168 values, 1,351B raw, 1,351B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ee54a0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6cf5d5e0
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3aa6a7b1
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000162_378/part-00162
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1bc73845
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000161_377/part-00161
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000156_372' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000156
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000156_372: Committed
15/08/06 17:54:27 INFO Executor: Finished task 156.0 in stage 8.0 (TID 372). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 386, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 170.0 in stage 8.0 (TID 386)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 372) in 311 ms on localhost (155/200)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a30f467
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000163_379/part-00163
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2c876407
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a461228
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000158_374' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000158
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,596
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000158_374: Committed
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,996
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000159_375' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000159
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000159_375: Committed
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 587B for [ps_partkey] INT32: 136 values, 551B raw, 551B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 507B for [ps_partkey] INT32: 116 values, 471B raw, 471B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,139B for [part_value] DOUBLE: 136 values, 1,095B raw, 1,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 979B for [part_value] DOUBLE: 116 values, 935B raw, 935B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO Executor: Finished task 159.0 in stage 8.0 (TID 375). 781 bytes result sent to driver
15/08/06 17:54:27 INFO Executor: Finished task 158.0 in stage 8.0 (TID 374). 781 bytes result sent to driver
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000160_376' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000160
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000160_376: Committed
15/08/06 17:54:27 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 387, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 171.0 in stage 8.0 (TID 387)
15/08/06 17:54:27 INFO Executor: Finished task 160.0 in stage 8.0 (TID 376). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 375) in 269 ms on localhost (156/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 388, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 172.0 in stage 8.0 (TID 388)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 374) in 274 ms on localhost (157/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 389, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 173.0 in stage 8.0 (TID 389)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 376) in 271 ms on localhost (158/200)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@41b2ff8a
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,476
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 483B for [ps_partkey] INT32: 110 values, 447B raw, 447B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 931B for [part_value] DOUBLE: 110 values, 887B raw, 887B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000139_355' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000139
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000139_355: Committed
15/08/06 17:54:27 INFO Executor: Finished task 139.0 in stage 8.0 (TID 355). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 390, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 174.0 in stage 8.0 (TID 390)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 355) in 705 ms on localhost (159/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000162_378' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000162
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000162_378: Committed
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@229f7106
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000164_380/part-00164
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO Executor: Finished task 162.0 in stage 8.0 (TID 378). 781 bytes result sent to driver
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000161_377' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000161
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000161_377: Committed
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 391, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 175.0 in stage 8.0 (TID 391)
15/08/06 17:54:27 INFO Executor: Finished task 161.0 in stage 8.0 (TID 377). 781 bytes result sent to driver
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a59a22
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000167_383/part-00167
15/08/06 17:54:27 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 378) in 285 ms on localhost (160/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 392, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23c9aedb
15/08/06 17:54:27 INFO Executor: Running task 176.0 in stage 8.0 (TID 392)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000165_381/part-00165
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 377) in 288 ms on localhost (161/200)
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d112786
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000168_384/part-00168
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d59380f
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1dfafb44
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000166_382/part-00166
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@48863cbc
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4e0ca6a
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000163_379' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000163
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000163_379: Committed
15/08/06 17:54:27 INFO Executor: Finished task 163.0 in stage 8.0 (TID 379). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 393, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d4c6578
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,576
15/08/06 17:54:27 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 379) in 307 ms on localhost (162/200)
15/08/06 17:54:27 INFO Executor: Running task 177.0 in stage 8.0 (TID 393)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 903B for [ps_partkey] INT32: 215 values, 867B raw, 867B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,771B for [part_value] DOUBLE: 215 values, 1,727B raw, 1,727B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76f0923f
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,556
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 499B for [ps_partkey] INT32: 114 values, 463B raw, 463B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 963B for [part_value] DOUBLE: 114 values, 919B raw, 919B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000164_380' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000164
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000164_380: Committed
15/08/06 17:54:27 INFO Executor: Finished task 164.0 in stage 8.0 (TID 380). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 394, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 178.0 in stage 8.0 (TID 394)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000168_384' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000168
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000168_384: Committed
15/08/06 17:54:27 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 380) in 314 ms on localhost (163/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000165_381' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000165
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000165_381: Committed
15/08/06 17:54:27 INFO Executor: Finished task 168.0 in stage 8.0 (TID 384). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 395, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 179.0 in stage 8.0 (TID 395)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 384) in 284 ms on localhost (164/200)
15/08/06 17:54:27 INFO Executor: Finished task 165.0 in stage 8.0 (TID 381). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 396, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 381) in 318 ms on localhost (165/200)
15/08/06 17:54:27 INFO Executor: Running task 180.0 in stage 8.0 (TID 396)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000145_361' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000145
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000145_361: Committed
15/08/06 17:54:27 INFO Executor: Finished task 145.0 in stage 8.0 (TID 361). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 397, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 181.0 in stage 8.0 (TID 397)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 361) in 683 ms on localhost (166/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000166_382' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000166
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000166_382: Committed
15/08/06 17:54:27 INFO Executor: Finished task 166.0 in stage 8.0 (TID 382). 781 bytes result sent to driver
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 398, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 182.0 in stage 8.0 (TID 398)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@9ec7174
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000170_386/part-00170
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 382) in 303 ms on localhost (167/200)
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@390093e5
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000169_385/part-00169
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1a040038
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,996
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 787B for [ps_partkey] INT32: 186 values, 751B raw, 751B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@194b8109
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,539B for [part_value] DOUBLE: 186 values, 1,495B raw, 1,495B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000173_389/part-00173
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@75c7e549
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000172_388/part-00172
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@44ac55ae
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@38bce918
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a6b43c1
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000171_387/part-00171
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5510e792
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,596
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 707B for [ps_partkey] INT32: 166 values, 671B raw, 671B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,379B for [part_value] DOUBLE: 166 values, 1,335B raw, 1,335B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000170_386' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000170
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000170_386: Committed
15/08/06 17:54:27 INFO Executor: Finished task 170.0 in stage 8.0 (TID 386). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 399, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 183.0 in stage 8.0 (TID 399)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c69d284
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000150_366' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000150
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000150_366: Committed
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000175_391/part-00175
15/08/06 17:54:27 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 386) in 148 ms on localhost (168/200)
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO Executor: Finished task 150.0 in stage 8.0 (TID 366). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 400, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 184.0 in stage 8.0 (TID 400)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61130377
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000176_392/part-00176
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@66f11134
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000174_390/part-00174
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 366) in 551 ms on localhost (169/200)
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e6129aa
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7efbb6ea
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,216
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 831B for [ps_partkey] INT32: 197 values, 795B raw, 795B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@b2a6737
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,627B for [part_value] DOUBLE: 197 values, 1,583B raw, 1,583B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,756
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 939B for [ps_partkey] INT32: 224 values, 903B raw, 903B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,843B for [part_value] DOUBLE: 224 values, 1,799B raw, 1,799B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000173_389' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000173
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4620ef51
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000173_389: Committed
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000172_388' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000172
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000172_388: Committed
15/08/06 17:54:27 INFO Executor: Finished task 173.0 in stage 8.0 (TID 389). 781 bytes result sent to driver
15/08/06 17:54:27 INFO Executor: Finished task 172.0 in stage 8.0 (TID 388). 781 bytes result sent to driver
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 401, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 185.0 in stage 8.0 (TID 401)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 389) in 151 ms on localhost (170/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 402, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 186.0 in stage 8.0 (TID 402)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 388) in 155 ms on localhost (171/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000171_387' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000171
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000171_387: Committed
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000174_390' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000174
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000174_390: Committed
15/08/06 17:54:27 INFO Executor: Finished task 171.0 in stage 8.0 (TID 387). 781 bytes result sent to driver
15/08/06 17:54:27 INFO Executor: Finished task 174.0 in stage 8.0 (TID 390). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 403, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO Executor: Running task 187.0 in stage 8.0 (TID 403)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 404, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO Executor: Running task 188.0 in stage 8.0 (TID 404)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000176_392' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000176
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000176_392: Committed
15/08/06 17:54:27 INFO Executor: Finished task 176.0 in stage 8.0 (TID 392). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 390) in 148 ms on localhost (172/200)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 387) in 169 ms on localhost (173/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 405, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 189.0 in stage 8.0 (TID 405)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 392) in 147 ms on localhost (174/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000175_391' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000175
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000175_391: Committed
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e3d2b3
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000177_393/part-00177
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO Executor: Finished task 175.0 in stage 8.0 (TID 391). 781 bytes result sent to driver
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 406, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 190.0 in stage 8.0 (TID 406)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 391) in 152 ms on localhost (175/200)
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@77ae6df5
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000178_394/part-00178
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f2ec153
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,996
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@56ef5351
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000179_395/part-00179
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a47176d
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000180_396/part-00180
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d6f49b4
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000181_397/part-00181
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 787B for [ps_partkey] INT32: 186 values, 751B raw, 751B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,539B for [part_value] DOUBLE: 186 values, 1,495B raw, 1,495B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@791a27d6
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,462,196
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@242d90b7
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b77f0ef
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4f6af134
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7601b5a7
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,027B for [ps_partkey] INT32: 246 values, 991B raw, 991B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,796
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 2,019B for [part_value] DOUBLE: 246 values, 1,975B raw, 1,975B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000182_398/part-00182
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 747B for [ps_partkey] INT32: 176 values, 711B raw, 711B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,459B for [part_value] DOUBLE: 176 values, 1,415B raw, 1,415B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@50106ad2
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000177_393' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000177
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000177_393: Committed
15/08/06 17:54:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:54:27 INFO Executor: Finished task 177.0 in stage 8.0 (TID 393). 781 bytes result sent to driver
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:27 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 407, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 191.0 in stage 8.0 (TID 407)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 393) in 281 ms on localhost (176/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000180_396' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000180
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000180_396: Committed
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000178_394' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000178
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000178_394: Committed
15/08/06 17:54:27 INFO Executor: Finished task 180.0 in stage 8.0 (TID 396). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 408, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Finished task 178.0 in stage 8.0 (TID 394). 781 bytes result sent to driver
15/08/06 17:54:27 INFO Executor: Running task 192.0 in stage 8.0 (TID 408)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 396) in 252 ms on localhost (177/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000179_395' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000179
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000179_395: Committed
15/08/06 17:54:27 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 409, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 193.0 in stage 8.0 (TID 409)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000181_397' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000181
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000181_397: Committed
15/08/06 17:54:27 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 394) in 264 ms on localhost (178/200)
15/08/06 17:54:27 INFO Executor: Finished task 181.0 in stage 8.0 (TID 397). 781 bytes result sent to driver
15/08/06 17:54:27 INFO Executor: Finished task 179.0 in stage 8.0 (TID 395). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 410, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 194.0 in stage 8.0 (TID 410)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 397) in 253 ms on localhost (179/200)
15/08/06 17:54:27 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 411, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 195.0 in stage 8.0 (TID 411)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 395) in 261 ms on localhost (180/200)
15/08/06 17:54:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000182_398' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000182
15/08/06 17:54:27 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000182_398: Committed
15/08/06 17:54:27 INFO Executor: Finished task 182.0 in stage 8.0 (TID 398). 781 bytes result sent to driver
15/08/06 17:54:27 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 412, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:27 INFO Executor: Running task 196.0 in stage 8.0 (TID 412)
15/08/06 17:54:27 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 398) in 261 ms on localhost (181/200)
15/08/06 17:54:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48d16701
15/08/06 17:54:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000184_400/part-00184
15/08/06 17:54:27 INFO CodecConfig: Compression set to false
15/08/06 17:54:27 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:27 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:27 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@669a9957
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000183_399/part-00183
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1654fabc
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000185_401/part-00185
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4980ca9e
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2199d3de
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000186_402/part-00186
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@22d35561
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6413f918
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000189_405/part-00189
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5b46c9fc
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16d29890
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000187_403/part-00187
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@694f2bbd
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1492540
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000190_406/part-00190
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a9612ee
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000188_404/part-00188
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@62b06a09
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@25c8daa4
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,156
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000183_399' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000183
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000183_399: Committed
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 819B for [ps_partkey] INT32: 194 values, 783B raw, 783B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,603B for [part_value] DOUBLE: 194 values, 1,559B raw, 1,559B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO Executor: Finished task 183.0 in stage 8.0 (TID 399). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 413, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:28 INFO Executor: Running task 197.0 in stage 8.0 (TID 413)
15/08/06 17:54:28 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 399) in 269 ms on localhost (182/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000185_401' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000185
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000185_401: Committed
15/08/06 17:54:28 INFO Executor: Finished task 185.0 in stage 8.0 (TID 401). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 414, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:28 INFO Executor: Running task 198.0 in stage 8.0 (TID 414)
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7be34d61
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:54:28 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 401) in 254 ms on localhost (183/200)
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b6e209a
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000186_402' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000186
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000186_402: Committed
15/08/06 17:54:28 INFO Executor: Finished task 186.0 in stage 8.0 (TID 402). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 415, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:28 INFO Executor: Running task 199.0 in stage 8.0 (TID 415)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000189_405' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000189
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000189_405: Committed
15/08/06 17:54:28 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 402) in 272 ms on localhost (184/200)
15/08/06 17:54:28 INFO Executor: Finished task 189.0 in stage 8.0 (TID 405). 781 bytes result sent to driver
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000187_403' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000187
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000187_403: Committed
15/08/06 17:54:28 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 405) in 259 ms on localhost (185/200)
15/08/06 17:54:28 INFO Executor: Finished task 187.0 in stage 8.0 (TID 403). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 403) in 267 ms on localhost (186/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000188_404' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000188
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000188_404: Committed
15/08/06 17:54:28 INFO Executor: Finished task 188.0 in stage 8.0 (TID 404). 781 bytes result sent to driver
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000190_406' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000190
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000190_406: Committed
15/08/06 17:54:28 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 404) in 276 ms on localhost (187/200)
15/08/06 17:54:28 INFO Executor: Finished task 190.0 in stage 8.0 (TID 406). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 406) in 271 ms on localhost (188/200)
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@32e8c37
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@64f77082
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@30fb9a97
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000191_407/part-00191
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000193_409/part-00193
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000192_408/part-00192
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:54:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1bd64a20
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000196_412/part-00196
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3bf06879
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@245afead
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000194_410/part-00194
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000195_411/part-00195
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@52062280
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@10c32b92
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d6186d6
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,136
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 615B for [ps_partkey] INT32: 143 values, 579B raw, 579B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,195B for [part_value] DOUBLE: 143 values, 1,151B raw, 1,151B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@edeff75
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,896
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@b1a79b2
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 567B for [ps_partkey] INT32: 131 values, 531B raw, 531B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,099B for [part_value] DOUBLE: 131 values, 1,055B raw, 1,055B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1bc669b6
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,276
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000167_383' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000167
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 643B for [ps_partkey] INT32: 150 values, 607B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000167_383: Committed
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,251B for [part_value] DOUBLE: 150 values, 1,207B raw, 1,207B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO Executor: Finished task 167.0 in stage 8.0 (TID 383). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 383) in 698 ms on localhost (189/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000191_407' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000191
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000191_407: Committed
15/08/06 17:54:28 INFO Executor: Finished task 191.0 in stage 8.0 (TID 407). 781 bytes result sent to driver
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000192_408' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000192
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000192_408: Committed
15/08/06 17:54:28 INFO Executor: Finished task 192.0 in stage 8.0 (TID 408). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 407) in 167 ms on localhost (190/200)
15/08/06 17:54:28 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 408) in 162 ms on localhost (191/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000194_410' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000194
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000194_410: Committed
15/08/06 17:54:28 INFO Executor: Finished task 194.0 in stage 8.0 (TID 410). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 410) in 166 ms on localhost (192/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000196_412' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000196
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000196_412: Committed
15/08/06 17:54:28 INFO Executor: Finished task 196.0 in stage 8.0 (TID 412). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 412) in 157 ms on localhost (193/200)
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c4d0376
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000198_414/part-00198
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5fa5fc4e
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000197_413/part-00197
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c0614dd
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0008_m_000199_415/part-00199
15/08/06 17:54:28 INFO CodecConfig: Compression set to false
15/08/06 17:54:28 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:28 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:28 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000169_385' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000169
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000169_385: Committed
15/08/06 17:54:28 INFO Executor: Finished task 169.0 in stage 8.0 (TID 385). 781 bytes result sent to driver
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@149f121c
15/08/06 17:54:28 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 385) in 695 ms on localhost (194/200)
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,776
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@575df6d7
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 743B for [ps_partkey] INT32: 175 values, 707B raw, 707B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,451B for [part_value] DOUBLE: 175 values, 1,407B raw, 1,407B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@59f8ca10
15/08/06 17:54:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000197_413' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000197
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000197_413: Committed
15/08/06 17:54:28 INFO Executor: Finished task 197.0 in stage 8.0 (TID 413). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 413) in 167 ms on localhost (195/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000184_400' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000184
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000184_400: Committed
15/08/06 17:54:28 INFO Executor: Finished task 184.0 in stage 8.0 (TID 400). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 400) in 660 ms on localhost (196/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000193_409' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000193
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000193_409: Committed
15/08/06 17:54:28 INFO Executor: Finished task 193.0 in stage 8.0 (TID 409). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 409) in 562 ms on localhost (197/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000195_411' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000195
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000195_411: Committed
15/08/06 17:54:28 INFO Executor: Finished task 195.0 in stage 8.0 (TID 411). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 411) in 564 ms on localhost (198/200)
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000198_414' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000198
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000198_414: Committed
15/08/06 17:54:28 INFO Executor: Finished task 198.0 in stage 8.0 (TID 414). 781 bytes result sent to driver
15/08/06 17:54:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0008_m_000199_415' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_temporary/0/task_201508061754_0008_m_000199
15/08/06 17:54:28 INFO SparkHiveWriterContainer: attempt_201508061754_0008_m_000199_415: Committed
15/08/06 17:54:28 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 414) in 555 ms on localhost (199/200)
15/08/06 17:54:28 INFO Executor: Finished task 199.0 in stage 8.0 (TID 415). 781 bytes result sent to driver
15/08/06 17:54:28 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 415) in 543 ms on localhost (200/200)
15/08/06 17:54:28 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/06 17:54:28 INFO DAGScheduler: Stage 8 (runJob at InsertIntoHiveTable.scala:93) finished in 5.271 s
15/08/06 17:54:28 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@33aaef2b
15/08/06 17:54:28 INFO DAGScheduler: Job 5 finished: runJob at InsertIntoHiveTable.scala:93, took 15.396575 s
15/08/06 17:54:28 INFO StatsReportListener: task runtime:(count: 200, mean: 394.975000, stdev: 252.786895, max: 1072.000000, min: 147.000000)
15/08/06 17:54:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:28 INFO StatsReportListener: 	147.0 ms	155.0 ms	166.0 ms	241.0 ms	294.0 ms	551.0 ms	736.0 ms	1.1 s	1.1 s
15/08/06 17:54:28 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.485000, stdev: 1.009839, max: 11.000000, min: 0.000000)
15/08/06 17:54:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:28 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	11.0 ms
15/08/06 17:54:28 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:54:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:28 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:54:28 INFO StatsReportListener: task result size:(count: 200, mean: 781.000000, stdev: 0.000000, max: 781.000000, min: 781.000000)
15/08/06 17:54:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:28 INFO StatsReportListener: 	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B
15/08/06 17:54:28 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.535297, stdev: 4.530910, max: 99.713467, min: 57.823129)
15/08/06 17:54:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:28 INFO StatsReportListener: 	58 %	94 %	97 %	97 %	98 %	99 %	99 %	100 %	100 %
15/08/06 17:54:28 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.122438, stdev: 0.229777, max: 1.967800, min: 0.000000)
15/08/06 17:54:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:28 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 2 %
15/08/06 17:54:28 INFO StatsReportListener: other time pct: (count: 200, mean: 2.342265, stdev: 4.538433, max: 42.176871, min: 0.286533)
15/08/06 17:54:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:28 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 2 %	 2 %	 3 %	 5 %	42 %
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/_SUCCESS;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/_SUCCESS;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00000;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00001;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00002;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00003;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00004;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00005;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00006;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00007;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00008;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00009;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00010;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00011;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00012;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00013;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00014;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00015;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00016;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00017;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00018;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00019;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00020;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00021;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00022;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00023;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00024;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00025;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00026;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00027;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00028;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00029;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00030;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00031;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00032;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00033;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00034;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00035;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00036;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00037;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00038;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00039;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00040;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00041;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00042;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00043;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00044;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00045;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00046;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00047;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00048;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00049;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00050;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00051;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00052;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00053;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00054;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00055;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00056;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00057;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00058;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00059;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00060;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00061;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00062;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00063;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00064;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00065;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00066;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00067;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00068;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00069;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00070;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00071;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00072;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00073;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00074;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00075;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00076;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00077;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00078;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00079;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00080;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00081;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00082;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00083;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00084;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00085;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00086;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00087;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00088;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00089;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00090;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00091;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00092;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00093;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00094;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00095;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00096;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00097;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00098;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00099;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00100;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00101;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00102;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00103;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00104;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00105;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00106;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00107;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00108;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00109;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00110;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00111;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00112;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00113;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00114;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00115;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00116;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00117;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00118;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00119;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00120;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00121;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00122;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00123;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00124;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00125;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00126;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00127;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00128;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00129;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00130;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00131;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00132;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00133;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00134;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00135;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00136;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00137;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00138;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00139;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00140;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00141;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00142;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00143;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00144;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00145;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00146;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00147;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00148;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00149;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00150;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00151;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00152;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00153;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00154;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00155;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00156;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00157;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00158;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00159;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00160;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00161;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00162;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00163;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00164;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00165;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00166;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00167;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00168;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00169;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00170;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00171;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00172;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00173;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00174;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00175;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00176;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00177;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00178;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00179;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00180;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00181;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00182;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00183;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00184;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00185;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00186;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00187;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00188;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00189;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00190;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00191;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00192;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00193;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00194;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00195;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00196;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00197;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00198;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198;Status:true
15/08/06 17:54:30 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-11_803_6361891443120332127-1/-ext-10000/part-00199;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199;Status:true
15/08/06 17:54:30 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:54:30 INFO DAGScheduler: Got job 6 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:54:30 INFO DAGScheduler: Final stage: Stage 9(collect at SparkPlan.scala:84)
15/08/06 17:54:30 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:54:30 INFO DAGScheduler: Missing parents: List()
15/08/06 17:54:30 INFO DAGScheduler: Submitting Stage 9 (MappedRDD[50] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:54:30 INFO MemoryStore: ensureFreeSpace(3240) called with curMem=962671, maxMem=3333968363
15/08/06 17:54:30 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 3.2 KB, free 3.1 GB)
15/08/06 17:54:30 INFO MemoryStore: ensureFreeSpace(1941) called with curMem=965911, maxMem=3333968363
15/08/06 17:54:30 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1941.0 B, free 3.1 GB)
15/08/06 17:54:30 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:37948 (size: 1941.0 B, free: 3.1 GB)
15/08/06 17:54:30 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/08/06 17:54:30 INFO DefaultExecutionContext: Created broadcast 13 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:30 INFO DAGScheduler: Submitting 1 missing tasks from Stage 9 (MappedRDD[50] at map at SparkPlan.scala:84)
15/08/06 17:54:30 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/08/06 17:54:30 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 416, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:54:30 INFO Executor: Running task 0.0 in stage 9.0 (TID 416)
15/08/06 17:54:30 INFO Executor: Finished task 0.0 in stage 9.0 (TID 416). 618 bytes result sent to driver
15/08/06 17:54:30 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 416) in 6 ms on localhost (1/1)
15/08/06 17:54:30 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/06 17:54:30 INFO DAGScheduler: Stage 9 (collect at SparkPlan.scala:84) finished in 0.007 s
15/08/06 17:54:30 INFO DAGScheduler: Job 6 finished: collect at SparkPlan.scala:84, took 0.016454 s
15/08/06 17:54:30 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3b63697b
Time taken: 20.201 seconds
15/08/06 17:54:30 INFO CliDriver: Time taken: 20.201 seconds
15/08/06 17:54:30 INFO StatsReportListener: task runtime:(count: 1, mean: 6.000000, stdev: 0.000000, max: 6.000000, min: 6.000000)
15/08/06 17:54:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:30 INFO StatsReportListener: 	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms
15/08/06 17:54:30 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:54:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:30 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:54:30 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 33.333333, stdev: 0.000000, max: 33.333333, min: 33.333333)
15/08/06 17:54:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:30 INFO StatsReportListener: 	33 %	33 %	33 %	33 %	33 %	33 %	33 %	33 %	33 %
15/08/06 17:54:30 INFO StatsReportListener: other time pct: (count: 1, mean: 66.666667, stdev: 0.000000, max: 66.666667, min: 66.666667)
15/08/06 17:54:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:30 INFO StatsReportListener: 	67 %	67 %	67 %	67 %	67 %	67 %	67 %	67 %	67 %
15/08/06 17:54:30 INFO BlockManager: Removing broadcast 13
15/08/06 17:54:30 INFO BlockManager: Removing block broadcast_13_piece0
15/08/06 17:54:30 INFO MemoryStore: Block broadcast_13_piece0 of size 1941 dropped from memory (free 3333002452)
15/08/06 17:54:30 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:37948 in memory (size: 1941.0 B, free: 3.1 GB)
15/08/06 17:54:30 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/08/06 17:54:30 INFO BlockManager: Removing block broadcast_13
15/08/06 17:54:30 INFO MemoryStore: Block broadcast_13 of size 3240 dropped from memory (free 3333005692)
15/08/06 17:54:30 INFO ContextCleaner: Cleaned broadcast 13
15/08/06 17:54:30 INFO ParseDriver: Parsing command: insert into table q11_important_stock_par_spark
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par_spark) sum_tmp
        join q11_part_tmp_par_spark
where part_value > total_value * 0.0001
order by value desc
15/08/06 17:54:30 INFO ParseDriver: Parse Completed
15/08/06 17:54:30 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#114, part_value#115)
15/08/06 17:54:30 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#118, part_value#119)
15/08/06 17:54:31 INFO MemoryStore: ensureFreeSpace(280818) called with curMem=962671, maxMem=3333968363
15/08/06 17:54:31 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 274.2 KB, free 3.1 GB)
15/08/06 17:54:31 INFO MemoryStore: ensureFreeSpace(31760) called with curMem=1243489, maxMem=3333968363
15/08/06 17:54:31 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 31.0 KB, free 3.1 GB)
15/08/06 17:54:31 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:37948 (size: 31.0 KB, free: 3.1 GB)
15/08/06 17:54:31 INFO BlockManagerMaster: Updated info of block broadcast_14_piece0
15/08/06 17:54:31 INFO DefaultExecutionContext: Created broadcast 14 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:54:31 INFO MemoryStore: ensureFreeSpace(280962) called with curMem=1275249, maxMem=3333968363
15/08/06 17:54:31 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 274.4 KB, free 3.1 GB)
15/08/06 17:54:31 INFO MemoryStore: ensureFreeSpace(31842) called with curMem=1556211, maxMem=3333968363
15/08/06 17:54:31 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:54:31 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:37948 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:54:31 INFO BlockManagerMaster: Updated info of block broadcast_15_piece0
15/08/06 17:54:31 INFO DefaultExecutionContext: Created broadcast 15 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:54:31 INFO FileInputFormat: Total input paths to process : 200
15/08/06 17:54:31 INFO ParquetInputFormat: Total input paths to process : 200
15/08/06 17:54:31 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:54:31 INFO ParquetFileReader: reading another 200 footers
15/08/06 17:54:31 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:54:31 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000; isDirectory=false; length=2638; replication=1; blocksize=134217728; modification_time=1438883664348; access_time=1438883663879; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001; isDirectory=false; length=2326; replication=1; blocksize=134217728; modification_time=1438883664351; access_time=1438883663860; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438883664350; access_time=1438883663880; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883664350; access_time=1438883663905; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004; isDirectory=false; length=2194; replication=1; blocksize=134217728; modification_time=1438883664349; access_time=1438883663856; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005; isDirectory=false; length=2446; replication=1; blocksize=134217728; modification_time=1438883664348; access_time=1438883663910; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006; isDirectory=false; length=1858; replication=1; blocksize=134217728; modification_time=1438883664348; access_time=1438883664146; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883664348; access_time=1438883663867; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883664349; access_time=1438883663856; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009; isDirectory=false; length=2050; replication=1; blocksize=134217728; modification_time=1438883664348; access_time=1438883663903; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010; isDirectory=false; length=2002; replication=1; blocksize=134217728; modification_time=1438883664351; access_time=1438883663855; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883664352; access_time=1438883663905; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012; isDirectory=false; length=2038; replication=1; blocksize=134217728; modification_time=1438883664348; access_time=1438883663883; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013; isDirectory=false; length=2170; replication=1; blocksize=134217728; modification_time=1438883664347; access_time=1438883663898; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438883664351; access_time=1438883663857; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883664351; access_time=1438883663883; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438883664875; access_time=1438883664611; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438883664942; access_time=1438883664866; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438883664882; access_time=1438883664608; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883664935; access_time=1438883664661; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438883664884; access_time=1438883664609; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021; isDirectory=false; length=2122; replication=1; blocksize=134217728; modification_time=1438883664901; access_time=1438883664651; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022; isDirectory=false; length=2170; replication=1; blocksize=134217728; modification_time=1438883664921; access_time=1438883664654; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438883664691; access_time=1438883664601; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024; isDirectory=false; length=2470; replication=1; blocksize=134217728; modification_time=1438883664917; access_time=1438883664649; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438883664917; access_time=1438883664609; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438883664660; access_time=1438883664581; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438883664898; access_time=1438883664607; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028; isDirectory=false; length=2134; replication=1; blocksize=134217728; modification_time=1438883664929; access_time=1438883664662; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438883664931; access_time=1438883664622; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438883664939; access_time=1438883664650; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031; isDirectory=false; length=1378; replication=1; blocksize=134217728; modification_time=1438883664926; access_time=1438883664661; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032; isDirectory=false; length=2302; replication=1; blocksize=134217728; modification_time=1438883665171; access_time=1438883665105; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438883665208; access_time=1438883665117; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034; isDirectory=false; length=2626; replication=1; blocksize=134217728; modification_time=1438883665204; access_time=1438883665115; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883665200; access_time=1438883665105; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438883665148; access_time=1438883665087; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883665162; access_time=1438883665103; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883665244; access_time=1438883665162; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438883665150; access_time=1438883665093; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438883665150; access_time=1438883665088; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041; isDirectory=false; length=1738; replication=1; blocksize=134217728; modification_time=1438883665203; access_time=1438883665116; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042; isDirectory=false; length=1762; replication=1; blocksize=134217728; modification_time=1438883665214; access_time=1438883665149; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438883665242; access_time=1438883665150; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044; isDirectory=false; length=1846; replication=1; blocksize=134217728; modification_time=1438883665213; access_time=1438883665168; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438883665214; access_time=1438883665168; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438883665755; access_time=1438883665211; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438883665745; access_time=1438883665172; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438883665491; access_time=1438883665431; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049; isDirectory=false; length=1462; replication=1; blocksize=134217728; modification_time=1438883665471; access_time=1438883665418; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883665504; access_time=1438883665449; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438883665468; access_time=1438883665419; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438883665897; access_time=1438883665445; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053; isDirectory=false; length=1738; replication=1; blocksize=134217728; modification_time=1438883665537; access_time=1438883665494; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438883665517; access_time=1438883665471; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055; isDirectory=false; length=1558; replication=1; blocksize=134217728; modification_time=1438883665545; access_time=1438883665492; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883665949; access_time=1438883665512; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883665534; access_time=1438883665494; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058; isDirectory=false; length=1666; replication=1; blocksize=134217728; modification_time=1438883665532; access_time=1438883665492; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438883665529; access_time=1438883665480; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060; isDirectory=false; length=2758; replication=1; blocksize=134217728; modification_time=1438883665594; access_time=1438883665553; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438883665514; access_time=1438883665475; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883665771; access_time=1438883665633; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063; isDirectory=false; length=1990; replication=1; blocksize=134217728; modification_time=1438883665762; access_time=1438883665623; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438883665768; access_time=1438883665633; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438883665768; access_time=1438883665632; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066; isDirectory=false; length=2446; replication=1; blocksize=134217728; modification_time=1438883665838; access_time=1438883665809; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883665818; access_time=1438883665782; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068; isDirectory=false; length=1786; replication=1; blocksize=134217728; modification_time=1438883666220; access_time=1438883665782; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438883665834; access_time=1438883665805; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438883665857; access_time=1438883665824; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438883665831; access_time=1438883665794; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883666244; access_time=1438883665804; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438883665887; access_time=1438883665860; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074; isDirectory=false; length=2098; replication=1; blocksize=134217728; modification_time=1438883665949; access_time=1438883665917; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438883665943; access_time=1438883665910; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438883665947; access_time=1438883665920; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438883665976; access_time=1438883665928; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078; isDirectory=false; length=2614; replication=1; blocksize=134217728; modification_time=1438883665981; access_time=1438883665927; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883666511; access_time=1438883665948; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438883666512; access_time=1438883665953; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883666130; access_time=1438883665986; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438883666524; access_time=1438883665979; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438883666139; access_time=1438883666004; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438883666156; access_time=1438883666127; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883666174; access_time=1438883666149; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438883666169; access_time=1438883666143; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087; isDirectory=false; length=2146; replication=1; blocksize=134217728; modification_time=1438883666255; access_time=1438883666212; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088; isDirectory=false; length=2722; replication=1; blocksize=134217728; modification_time=1438883666244; access_time=1438883666203; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438883666244; access_time=1438883666207; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090; isDirectory=false; length=2398; replication=1; blocksize=134217728; modification_time=1438883666244; access_time=1438883666205; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091; isDirectory=false; length=1750; replication=1; blocksize=134217728; modification_time=1438883666296; access_time=1438883666246; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883666251; access_time=1438883666222; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438883666321; access_time=1438883666278; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438883666323; access_time=1438883666276; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883666314; access_time=1438883666290; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096; isDirectory=false; length=1690; replication=1; blocksize=134217728; modification_time=1438883666344; access_time=1438883666318; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097; isDirectory=false; length=2194; replication=1; blocksize=134217728; modification_time=1438883666356; access_time=1438883666325; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438883666939; access_time=1438883666372; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099; isDirectory=false; length=2266; replication=1; blocksize=134217728; modification_time=1438883666537; access_time=1438883666373; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883666540; access_time=1438883666373; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101; isDirectory=false; length=2410; replication=1; blocksize=134217728; modification_time=1438883666539; access_time=1438883666375; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883666546; access_time=1438883666521; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883666539; access_time=1438883666380; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104; isDirectory=false; length=2434; replication=1; blocksize=134217728; modification_time=1438883666593; access_time=1438883666557; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438883666592; access_time=1438883666569; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438883666611; access_time=1438883666577; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107; isDirectory=false; length=2326; replication=1; blocksize=134217728; modification_time=1438883666612; access_time=1438883666589; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438883666619; access_time=1438883666591; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883666639; access_time=1438883666596; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883666648; access_time=1438883666621; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883666683; access_time=1438883666647; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438883666707; access_time=1438883666678; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438883666682; access_time=1438883666652; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438883667204; access_time=1438883666685; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115; isDirectory=false; length=1858; replication=1; blocksize=134217728; modification_time=1438883666710; access_time=1438883666682; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438883666719; access_time=1438883666686; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117; isDirectory=false; length=2590; replication=1; blocksize=134217728; modification_time=1438883666723; access_time=1438883666680; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883666699; access_time=1438883666668; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883667249; access_time=1438883666739; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120; isDirectory=false; length=2626; replication=1; blocksize=134217728; modification_time=1438883667249; access_time=1438883666742; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438883666860; access_time=1438883666837; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122; isDirectory=false; length=2422; replication=1; blocksize=134217728; modification_time=1438883667261; access_time=1438883666836; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123; isDirectory=false; length=2482; replication=1; blocksize=134217728; modification_time=1438883666858; access_time=1438883666830; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124; isDirectory=false; length=2098; replication=1; blocksize=134217728; modification_time=1438883666879; access_time=1438883666852; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125; isDirectory=false; length=1798; replication=1; blocksize=134217728; modification_time=1438883666892; access_time=1438883666866; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126; isDirectory=false; length=2350; replication=1; blocksize=134217728; modification_time=1438883666902; access_time=1438883666878; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883666924; access_time=1438883666895; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438883667357; access_time=1438883666919; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438883666956; access_time=1438883666915; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883666967; access_time=1438883666928; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883667357; access_time=1438883666923; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438883666961; access_time=1438883666924; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883667022; access_time=1438883666997; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134; isDirectory=false; length=1990; replication=1; blocksize=134217728; modification_time=1438883667033; access_time=1438883666996; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883667037; access_time=1438883667012; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438883667042; access_time=1438883667016; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137; isDirectory=false; length=1798; replication=1; blocksize=134217728; modification_time=1438883667076; access_time=1438883667042; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138; isDirectory=false; length=1678; replication=1; blocksize=134217728; modification_time=1438883667089; access_time=1438883667053; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438883667651; access_time=1438883667072; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438883667265; access_time=1438883667220; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438883667214; access_time=1438883667070; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142; isDirectory=false; length=2350; replication=1; blocksize=134217728; modification_time=1438883667249; access_time=1438883667086; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143; isDirectory=false; length=1558; replication=1; blocksize=134217728; modification_time=1438883667300; access_time=1438883667273; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883667314; access_time=1438883667293; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438883667721; access_time=1438883667288; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438883667364; access_time=1438883667330; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883667372; access_time=1438883667341; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883667364; access_time=1438883667340; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438883667362; access_time=1438883667329; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438883667766; access_time=1438883667334; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151; isDirectory=false; length=2602; replication=1; blocksize=134217728; modification_time=1438883667391; access_time=1438883667369; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152; isDirectory=false; length=1678; replication=1; blocksize=134217728; modification_time=1438883667393; access_time=1438883667371; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153; isDirectory=false; length=1846; replication=1; blocksize=134217728; modification_time=1438883667413; access_time=1438883667383; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154; isDirectory=false; length=2590; replication=1; blocksize=134217728; modification_time=1438883667411; access_time=1438883667381; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438883667417; access_time=1438883667391; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883667476; access_time=1438883667442; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438883667465; access_time=1438883667435; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158; isDirectory=false; length=2338; replication=1; blocksize=134217728; modification_time=1438883667629; access_time=1438883667466; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438883667630; access_time=1438883667477; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438883667633; access_time=1438883667478; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161; isDirectory=false; length=1714; replication=1; blocksize=134217728; modification_time=1438883667655; access_time=1438883667624; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162; isDirectory=false; length=1954; replication=1; blocksize=134217728; modification_time=1438883667653; access_time=1438883667624; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163; isDirectory=false; length=1642; replication=1; blocksize=134217728; modification_time=1438883667672; access_time=1438883667638; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883667702; access_time=1438883667668; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883667705; access_time=1438883667671; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166; isDirectory=false; length=1690; replication=1; blocksize=134217728; modification_time=1438883667723; access_time=1438883667688; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167; isDirectory=false; length=2902; replication=1; blocksize=134217728; modification_time=1438883668116; access_time=1438883667674; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438883667703; access_time=1438883667674; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883668163; access_time=1438883667741; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170; isDirectory=false; length=2554; replication=1; blocksize=134217728; modification_time=1438883667761; access_time=1438883667735; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438883667799; access_time=1438883667767; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883667780; access_time=1438883667753; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173; isDirectory=false; length=2314; replication=1; blocksize=134217728; modification_time=1438883667780; access_time=1438883667752; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174; isDirectory=false; length=3010; replication=1; blocksize=134217728; modification_time=1438883667800; access_time=1438883667780; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438883667806; access_time=1438883667778; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176; isDirectory=false; length=2686; replication=1; blocksize=134217728; modification_time=1438883667802; access_time=1438883667779; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177; isDirectory=false; length=2554; replication=1; blocksize=134217728; modification_time=1438883667961; access_time=1438883667823; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178; isDirectory=false; length=3274; replication=1; blocksize=134217728; modification_time=1438883667965; access_time=1438883667835; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883667969; access_time=1438883667842; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883667967; access_time=1438883667844; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181; isDirectory=false; length=2434; replication=1; blocksize=134217728; modification_time=1438883667969; access_time=1438883667846; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438883667984; access_time=1438883667956; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438883668030; access_time=1438883668003; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438883668421; access_time=1438883667999; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438883668036; access_time=1438883668013; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883668047; access_time=1438883668017; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187; isDirectory=false; length=2650; replication=1; blocksize=134217728; modification_time=1438883668053; access_time=1438883668027; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438883668074; access_time=1438883668038; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438883668048; access_time=1438883668024; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883668076; access_time=1438883668034; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883668124; access_time=1438883668098; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192; isDirectory=false; length=2038; replication=1; blocksize=134217728; modification_time=1438883668128; access_time=1438883668098; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438883668528; access_time=1438883668096; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194; isDirectory=false; length=1894; replication=1; blocksize=134217728; modification_time=1438883668135; access_time=1438883668110; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195; isDirectory=false; length=2122; replication=1; blocksize=134217728; modification_time=1438883668539; access_time=1438883668110; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883668137; access_time=1438883668107; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883668196; access_time=1438883668165; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198; isDirectory=false; length=2422; replication=1; blocksize=134217728; modification_time=1438883668596; access_time=1438883668162; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438883668598; access_time=1438883668170; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 585 ms
15/08/06 17:54:31 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:54:31 INFO DefaultExecutionContext: Starting job: RangePartitioner at Exchange.scala:88
15/08/06 17:54:32 INFO FileInputFormat: Total input paths to process : 200
15/08/06 17:54:32 INFO ParquetInputFormat: Total input paths to process : 200
15/08/06 17:54:32 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:54:32 INFO DAGScheduler: Registering RDD 63 (mapPartitions at Exchange.scala:100)
15/08/06 17:54:32 INFO DAGScheduler: Got job 7 (RangePartitioner at Exchange.scala:88) with 200 output partitions (allowLocal=false)
15/08/06 17:54:32 INFO DAGScheduler: Final stage: Stage 11(RangePartitioner at Exchange.scala:88)
15/08/06 17:54:32 INFO DAGScheduler: Parents of final stage: List(Stage 10)
15/08/06 17:54:32 INFO DAGScheduler: Missing parents: List(Stage 10)
15/08/06 17:54:32 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[63] at mapPartitions at Exchange.scala:100), which has no missing parents
15/08/06 17:54:32 INFO MemoryStore: ensureFreeSpace(8200) called with curMem=1588053, maxMem=3333968363
15/08/06 17:54:32 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 8.0 KB, free 3.1 GB)
15/08/06 17:54:32 INFO MemoryStore: ensureFreeSpace(4450) called with curMem=1596253, maxMem=3333968363
15/08/06 17:54:32 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.1 GB)
15/08/06 17:54:32 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:37948 (size: 4.3 KB, free: 3.1 GB)
15/08/06 17:54:32 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/08/06 17:54:32 INFO DefaultExecutionContext: Created broadcast 16 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:32 INFO DAGScheduler: Submitting 200 missing tasks from Stage 10 (MapPartitionsRDD[63] at mapPartitions at Exchange.scala:100)
15/08/06 17:54:32 INFO TaskSchedulerImpl: Adding task set 10.0 with 200 tasks
15/08/06 17:54:32 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 417, localhost, ANY, 1525 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 418, localhost, ANY, 1526 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 419, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 420, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 421, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 422, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 423, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 424, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 425, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 426, localhost, ANY, 1524 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 427, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 428, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 429, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 430, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 431, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 15.0 in stage 10.0 (TID 432, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 0.0 in stage 10.0 (TID 417)
15/08/06 17:54:32 INFO Executor: Running task 2.0 in stage 10.0 (TID 419)
15/08/06 17:54:32 INFO Executor: Running task 1.0 in stage 10.0 (TID 418)
15/08/06 17:54:32 INFO Executor: Running task 3.0 in stage 10.0 (TID 420)
15/08/06 17:54:32 INFO Executor: Running task 4.0 in stage 10.0 (TID 421)
15/08/06 17:54:32 INFO Executor: Running task 5.0 in stage 10.0 (TID 422)
15/08/06 17:54:32 INFO Executor: Running task 7.0 in stage 10.0 (TID 424)
15/08/06 17:54:32 INFO Executor: Running task 9.0 in stage 10.0 (TID 426)
15/08/06 17:54:32 INFO Executor: Running task 8.0 in stage 10.0 (TID 425)
15/08/06 17:54:32 INFO Executor: Running task 6.0 in stage 10.0 (TID 423)
15/08/06 17:54:32 INFO Executor: Running task 10.0 in stage 10.0 (TID 427)
15/08/06 17:54:32 INFO Executor: Running task 11.0 in stage 10.0 (TID 428)
15/08/06 17:54:32 INFO Executor: Running task 14.0 in stage 10.0 (TID 431)
15/08/06 17:54:32 INFO Executor: Running task 15.0 in stage 10.0 (TID 432)
15/08/06 17:54:32 INFO Executor: Running task 12.0 in stage 10.0 (TID 429)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 13.0 in stage 10.0 (TID 430)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 193
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 144
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 177
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 140
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 138
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 171
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 128
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 0.0 in stage 10.0 (TID 417). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 5.0 in stage 10.0 (TID 422). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 2.0 in stage 10.0 (TID 419). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 167
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 129
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 126
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 154
15/08/06 17:54:32 INFO Executor: Finished task 7.0 in stage 10.0 (TID 424). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 156
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 417) in 63 ms on localhost (1/200)
15/08/06 17:54:32 INFO Executor: Finished task 9.0 in stage 10.0 (TID 426). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 6.0 in stage 10.0 (TID 423). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 16.0 in stage 10.0 (TID 433, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 17.0 in stage 10.0 (TID 434, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 15.0 in stage 10.0 (TID 432). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 13.0 in stage 10.0 (TID 430). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 17.0 in stage 10.0 (TID 434)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 125
15/08/06 17:54:32 INFO Executor: Finished task 8.0 in stage 10.0 (TID 425). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 422) in 65 ms on localhost (2/200)
15/08/06 17:54:32 INFO Executor: Finished task 4.0 in stage 10.0 (TID 421). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 3.0 in stage 10.0 (TID 420). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 18.0 in stage 10.0 (TID 435, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 14.0 in stage 10.0 (TID 431). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 1.0 in stage 10.0 (TID 418). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 419) in 68 ms on localhost (3/200)
15/08/06 17:54:32 INFO Executor: Running task 16.0 in stage 10.0 (TID 433)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 10.0 in stage 10.0 (TID 427). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 19.0 in stage 10.0 (TID 436, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 18.0 in stage 10.0 (TID 435)
15/08/06 17:54:32 INFO Executor: Running task 19.0 in stage 10.0 (TID 436)
15/08/06 17:54:32 INFO Executor: Finished task 11.0 in stage 10.0 (TID 428). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:54:32 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 424) in 70 ms on localhost (4/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 20.0 in stage 10.0 (TID 437, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 20.0 in stage 10.0 (TID 437)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 426) in 71 ms on localhost (5/200)
15/08/06 17:54:32 INFO Executor: Finished task 12.0 in stage 10.0 (TID 429). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 21.0 in stage 10.0 (TID 438, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 21.0 in stage 10.0 (TID 438)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 423) in 79 ms on localhost (6/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 22.0 in stage 10.0 (TID 439, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:32 INFO Executor: Running task 22.0 in stage 10.0 (TID 439)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 15.0 in stage 10.0 (TID 432) in 80 ms on localhost (7/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 23.0 in stage 10.0 (TID 440, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 171
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 121
15/08/06 17:54:32 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 430) in 90 ms on localhost (8/200)
15/08/06 17:54:32 INFO Executor: Running task 23.0 in stage 10.0 (TID 440)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:54:32 INFO TaskSetManager: Starting task 24.0 in stage 10.0 (TID 441, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 17.0 in stage 10.0 (TID 434). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 425) in 94 ms on localhost (9/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 159
15/08/06 17:54:32 INFO TaskSetManager: Starting task 25.0 in stage 10.0 (TID 442, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Running task 24.0 in stage 10.0 (TID 441)
15/08/06 17:54:32 INFO Executor: Running task 25.0 in stage 10.0 (TID 442)
15/08/06 17:54:32 INFO Executor: Finished task 20.0 in stage 10.0 (TID 437). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 16.0 in stage 10.0 (TID 433). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 19.0 in stage 10.0 (TID 436). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 21.0 in stage 10.0 (TID 438). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 421) in 97 ms on localhost (10/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 26.0 in stage 10.0 (TID 443, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 26.0 in stage 10.0 (TID 443)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 420) in 100 ms on localhost (11/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 18.0 in stage 10.0 (TID 435). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 431) in 98 ms on localhost (12/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 27.0 in stage 10.0 (TID 444, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 28.0 in stage 10.0 (TID 445, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 27.0 in stage 10.0 (TID 444)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 28.0 in stage 10.0 (TID 445)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 154
15/08/06 17:54:32 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 418) in 110 ms on localhost (13/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 29.0 in stage 10.0 (TID 446, localhost, ANY, 1526 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 29.0 in stage 10.0 (TID 446)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 427) in 109 ms on localhost (14/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 30.0 in stage 10.0 (TID 447, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 30.0 in stage 10.0 (TID 447)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 161
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 121
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 22.0 in stage 10.0 (TID 439). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 31.0 in stage 10.0 (TID 448, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO Executor: Running task 31.0 in stage 10.0 (TID 448)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 428) in 114 ms on localhost (15/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 25.0 in stage 10.0 (TID 442). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 429) in 115 ms on localhost (16/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 179
15/08/06 17:54:32 INFO TaskSetManager: Starting task 32.0 in stage 10.0 (TID 449, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 23.0 in stage 10.0 (TID 440). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 17.0 in stage 10.0 (TID 434) in 55 ms on localhost (17/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:32 INFO Executor: Running task 32.0 in stage 10.0 (TID 449)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 33.0 in stage 10.0 (TID 450, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 33.0 in stage 10.0 (TID 450)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 151
15/08/06 17:54:32 INFO TaskSetManager: Finished task 20.0 in stage 10.0 (TID 437) in 50 ms on localhost (18/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:54:32 INFO Executor: Finished task 24.0 in stage 10.0 (TID 441). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 196
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 28.0 in stage 10.0 (TID 445). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 27.0 in stage 10.0 (TID 444). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 16.0 in stage 10.0 (TID 433) in 61 ms on localhost (19/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 34.0 in stage 10.0 (TID 451, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:54:32 INFO Executor: Finished task 29.0 in stage 10.0 (TID 446). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 34.0 in stage 10.0 (TID 451)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 35.0 in stage 10.0 (TID 452, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 35.0 in stage 10.0 (TID 452)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 19.0 in stage 10.0 (TID 436) in 61 ms on localhost (20/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 30.0 in stage 10.0 (TID 447). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 36.0 in stage 10.0 (TID 453, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 36.0 in stage 10.0 (TID 453)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 88
15/08/06 17:54:32 INFO TaskSetManager: Finished task 21.0 in stage 10.0 (TID 438) in 56 ms on localhost (21/200)
15/08/06 17:54:32 INFO Executor: Finished task 26.0 in stage 10.0 (TID 443). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 37.0 in stage 10.0 (TID 454, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 37.0 in stage 10.0 (TID 454)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:54:32 INFO TaskSetManager: Finished task 18.0 in stage 10.0 (TID 435) in 68 ms on localhost (22/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 38.0 in stage 10.0 (TID 455, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 38.0 in stage 10.0 (TID 455)
15/08/06 17:54:32 INFO Executor: Finished task 31.0 in stage 10.0 (TID 448). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 22.0 in stage 10.0 (TID 439) in 54 ms on localhost (23/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 165
15/08/06 17:54:32 INFO TaskSetManager: Starting task 39.0 in stage 10.0 (TID 456, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 39.0 in stage 10.0 (TID 456)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 33.0 in stage 10.0 (TID 450). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 25.0 in stage 10.0 (TID 442) in 43 ms on localhost (24/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 40.0 in stage 10.0 (TID 457, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 40.0 in stage 10.0 (TID 457)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 23.0 in stage 10.0 (TID 440) in 49 ms on localhost (25/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 41.0 in stage 10.0 (TID 458, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Running task 41.0 in stage 10.0 (TID 458)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 32.0 in stage 10.0 (TID 449). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 24.0 in stage 10.0 (TID 441) in 49 ms on localhost (26/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 181
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 42.0 in stage 10.0 (TID 459, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:54:32 INFO Executor: Running task 42.0 in stage 10.0 (TID 459)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 28.0 in stage 10.0 (TID 445) in 38 ms on localhost (27/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 27.0 in stage 10.0 (TID 444) in 41 ms on localhost (28/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 43.0 in stage 10.0 (TID 460, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 43.0 in stage 10.0 (TID 460)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:54:32 INFO TaskSetManager: Starting task 44.0 in stage 10.0 (TID 461, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 44.0 in stage 10.0 (TID 461)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 29.0 in stage 10.0 (TID 446) in 41 ms on localhost (29/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 192
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 45.0 in stage 10.0 (TID 462, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 45.0 in stage 10.0 (TID 462)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 178
15/08/06 17:54:32 INFO Executor: Finished task 39.0 in stage 10.0 (TID 456). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 35.0 in stage 10.0 (TID 452). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:32 INFO Executor: Finished task 36.0 in stage 10.0 (TID 453). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 46.0 in stage 10.0 (TID 463, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Running task 46.0 in stage 10.0 (TID 463)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 129
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 34.0 in stage 10.0 (TID 451). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:54:32 INFO Executor: Finished task 38.0 in stage 10.0 (TID 455). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 47.0 in stage 10.0 (TID 464, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Running task 47.0 in stage 10.0 (TID 464)
15/08/06 17:54:32 INFO Executor: Finished task 37.0 in stage 10.0 (TID 454). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 48.0 in stage 10.0 (TID 465, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 41.0 in stage 10.0 (TID 458). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 43.0 in stage 10.0 (TID 460). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 40.0 in stage 10.0 (TID 457). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 44.0 in stage 10.0 (TID 461). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 30.0 in stage 10.0 (TID 447) in 57 ms on localhost (30/200)
15/08/06 17:54:32 INFO Executor: Running task 48.0 in stage 10.0 (TID 465)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 120
15/08/06 17:54:32 INFO TaskSetManager: Starting task 49.0 in stage 10.0 (TID 466, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 45.0 in stage 10.0 (TID 462). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 49.0 in stage 10.0 (TID 466)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 33.0 in stage 10.0 (TID 450) in 53 ms on localhost (31/200)
15/08/06 17:54:32 INFO Executor: Finished task 42.0 in stage 10.0 (TID 459). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 31.0 in stage 10.0 (TID 448) in 62 ms on localhost (32/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 26.0 in stage 10.0 (TID 443) in 77 ms on localhost (33/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 32.0 in stage 10.0 (TID 449) in 59 ms on localhost (34/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 50.0 in stage 10.0 (TID 467, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 50.0 in stage 10.0 (TID 467)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 141
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 121
15/08/06 17:54:32 INFO TaskSetManager: Finished task 39.0 in stage 10.0 (TID 456) in 41 ms on localhost (35/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 35.0 in stage 10.0 (TID 452) in 53 ms on localhost (36/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 51.0 in stage 10.0 (TID 468, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Running task 51.0 in stage 10.0 (TID 468)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 52.0 in stage 10.0 (TID 469, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 47.0 in stage 10.0 (TID 464). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 46.0 in stage 10.0 (TID 463). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 52.0 in stage 10.0 (TID 469)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 36.0 in stage 10.0 (TID 453) in 52 ms on localhost (37/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 53.0 in stage 10.0 (TID 470, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 95
15/08/06 17:54:32 INFO TaskSetManager: Finished task 34.0 in stage 10.0 (TID 451) in 58 ms on localhost (38/200)
15/08/06 17:54:32 INFO Executor: Running task 53.0 in stage 10.0 (TID 470)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 54.0 in stage 10.0 (TID 471, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 54.0 in stage 10.0 (TID 471)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 38.0 in stage 10.0 (TID 455) in 51 ms on localhost (39/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 49.0 in stage 10.0 (TID 466). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 55.0 in stage 10.0 (TID 472, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:54:32 INFO Executor: Running task 55.0 in stage 10.0 (TID 472)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 37.0 in stage 10.0 (TID 454) in 56 ms on localhost (40/200)
15/08/06 17:54:32 INFO Executor: Finished task 48.0 in stage 10.0 (TID 465). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 56.0 in stage 10.0 (TID 473, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 56.0 in stage 10.0 (TID 473)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 41.0 in stage 10.0 (TID 458) in 50 ms on localhost (41/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 57.0 in stage 10.0 (TID 474, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 57.0 in stage 10.0 (TID 474)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 50.0 in stage 10.0 (TID 467). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 121
15/08/06 17:54:32 INFO TaskSetManager: Finished task 43.0 in stage 10.0 (TID 460) in 46 ms on localhost (42/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 40.0 in stage 10.0 (TID 457) in 59 ms on localhost (43/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 137
15/08/06 17:54:32 INFO TaskSetManager: Starting task 58.0 in stage 10.0 (TID 475, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 103
15/08/06 17:54:32 INFO Executor: Running task 58.0 in stage 10.0 (TID 475)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 59.0 in stage 10.0 (TID 476, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 59.0 in stage 10.0 (TID 476)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 51.0 in stage 10.0 (TID 468). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 44.0 in stage 10.0 (TID 461) in 53 ms on localhost (44/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 124
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/06 17:54:32 INFO TaskSetManager: Starting task 60.0 in stage 10.0 (TID 477, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 54.0 in stage 10.0 (TID 471). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 118
15/08/06 17:54:32 INFO Executor: Finished task 55.0 in stage 10.0 (TID 472). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 60.0 in stage 10.0 (TID 477)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 45.0 in stage 10.0 (TID 462) in 54 ms on localhost (45/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 42.0 in stage 10.0 (TID 459) in 63 ms on localhost (46/200)
15/08/06 17:54:32 INFO Executor: Finished task 52.0 in stage 10.0 (TID 469). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 57.0 in stage 10.0 (TID 474). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 61.0 in stage 10.0 (TID 478, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 61.0 in stage 10.0 (TID 478)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 62.0 in stage 10.0 (TID 479, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 62.0 in stage 10.0 (TID 479)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 53.0 in stage 10.0 (TID 470). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 47.0 in stage 10.0 (TID 464) in 51 ms on localhost (47/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 63.0 in stage 10.0 (TID 480, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 63.0 in stage 10.0 (TID 480)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 46.0 in stage 10.0 (TID 463) in 58 ms on localhost (48/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 64.0 in stage 10.0 (TID 481, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 49.0 in stage 10.0 (TID 466) in 42 ms on localhost (49/200)
15/08/06 17:54:32 INFO Executor: Running task 64.0 in stage 10.0 (TID 481)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 65.0 in stage 10.0 (TID 482, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:54:32 INFO Executor: Running task 65.0 in stage 10.0 (TID 482)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 48.0 in stage 10.0 (TID 465) in 49 ms on localhost (50/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 66.0 in stage 10.0 (TID 483, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 66.0 in stage 10.0 (TID 483)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 50.0 in stage 10.0 (TID 467) in 39 ms on localhost (51/200)
15/08/06 17:54:32 INFO Executor: Finished task 59.0 in stage 10.0 (TID 476). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 67.0 in stage 10.0 (TID 484, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 67.0 in stage 10.0 (TID 484)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 112
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 56.0 in stage 10.0 (TID 473). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 51.0 in stage 10.0 (TID 468) in 37 ms on localhost (52/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 203
15/08/06 17:54:32 INFO TaskSetManager: Starting task 68.0 in stage 10.0 (TID 485, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 68.0 in stage 10.0 (TID 485)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 54.0 in stage 10.0 (TID 471) in 36 ms on localhost (53/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 69.0 in stage 10.0 (TID 486, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 60.0 in stage 10.0 (TID 477). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 69.0 in stage 10.0 (TID 486)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:32 INFO Executor: Finished task 58.0 in stage 10.0 (TID 475). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 55.0 in stage 10.0 (TID 472) in 37 ms on localhost (54/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 70.0 in stage 10.0 (TID 487, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 70.0 in stage 10.0 (TID 487)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 52.0 in stage 10.0 (TID 469) in 45 ms on localhost (55/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 71.0 in stage 10.0 (TID 488, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 71.0 in stage 10.0 (TID 488)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:32 INFO Executor: Finished task 62.0 in stage 10.0 (TID 479). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 57.0 in stage 10.0 (TID 474) in 34 ms on localhost (56/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 72.0 in stage 10.0 (TID 489, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 73.0 in stage 10.0 (TID 490, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 73.0 in stage 10.0 (TID 490)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 72.0 in stage 10.0 (TID 489)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 74.0 in stage 10.0 (TID 491, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 74.0 in stage 10.0 (TID 491)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 75.0 in stage 10.0 (TID 492, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 159
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 145
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 75.0 in stage 10.0 (TID 492)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 53.0 in stage 10.0 (TID 470) in 48 ms on localhost (57/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 135
15/08/06 17:54:32 INFO TaskSetManager: Finished task 60.0 in stage 10.0 (TID 477) in 28 ms on localhost (58/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 56.0 in stage 10.0 (TID 473) in 43 ms on localhost (59/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 59.0 in stage 10.0 (TID 476) in 33 ms on localhost (60/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:54:32 INFO Executor: Finished task 65.0 in stage 10.0 (TID 482). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 76.0 in stage 10.0 (TID 493, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 177
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 122
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 61.0 in stage 10.0 (TID 478). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 76.0 in stage 10.0 (TID 493)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:32 INFO Executor: Finished task 64.0 in stage 10.0 (TID 481). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 68.0 in stage 10.0 (TID 485). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:54:32 INFO TaskSetManager: Finished task 58.0 in stage 10.0 (TID 475) in 36 ms on localhost (61/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 66.0 in stage 10.0 (TID 483). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 77.0 in stage 10.0 (TID 494, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 77.0 in stage 10.0 (TID 494)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 62.0 in stage 10.0 (TID 479) in 33 ms on localhost (62/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 63.0 in stage 10.0 (TID 480). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 137
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 148
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 78.0 in stage 10.0 (TID 495, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 138
15/08/06 17:54:32 INFO Executor: Running task 78.0 in stage 10.0 (TID 495)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 79.0 in stage 10.0 (TID 496, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 160
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 184
15/08/06 17:54:32 INFO Executor: Finished task 73.0 in stage 10.0 (TID 490). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 65.0 in stage 10.0 (TID 482) in 33 ms on localhost (63/200)
15/08/06 17:54:32 INFO Executor: Running task 79.0 in stage 10.0 (TID 496)
15/08/06 17:54:32 INFO Executor: Finished task 74.0 in stage 10.0 (TID 491). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 61.0 in stage 10.0 (TID 478) in 40 ms on localhost (64/200)
15/08/06 17:54:32 INFO Executor: Finished task 67.0 in stage 10.0 (TID 484). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 80.0 in stage 10.0 (TID 497, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 80.0 in stage 10.0 (TID 497)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 72.0 in stage 10.0 (TID 489). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 75.0 in stage 10.0 (TID 492). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 71.0 in stage 10.0 (TID 488). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 64.0 in stage 10.0 (TID 481) in 38 ms on localhost (65/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 69.0 in stage 10.0 (TID 486). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 81.0 in stage 10.0 (TID 498, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 81.0 in stage 10.0 (TID 498)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 68.0 in stage 10.0 (TID 485) in 31 ms on localhost (66/200)
15/08/06 17:54:32 INFO Executor: Finished task 70.0 in stage 10.0 (TID 487). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 66.0 in stage 10.0 (TID 483) in 37 ms on localhost (67/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 82.0 in stage 10.0 (TID 499, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 82.0 in stage 10.0 (TID 499)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 83.0 in stage 10.0 (TID 500, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 83.0 in stage 10.0 (TID 500)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 171
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 134
15/08/06 17:54:32 INFO TaskSetManager: Finished task 63.0 in stage 10.0 (TID 480) in 48 ms on localhost (68/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 191
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 124
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 84.0 in stage 10.0 (TID 501, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 84.0 in stage 10.0 (TID 501)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 73.0 in stage 10.0 (TID 490) in 31 ms on localhost (69/200)
15/08/06 17:54:32 INFO Executor: Finished task 79.0 in stage 10.0 (TID 496). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 78.0 in stage 10.0 (TID 495). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 85.0 in stage 10.0 (TID 502, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 76.0 in stage 10.0 (TID 493). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 85.0 in stage 10.0 (TID 502)
15/08/06 17:54:32 INFO Executor: Finished task 77.0 in stage 10.0 (TID 494). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 74.0 in stage 10.0 (TID 491) in 35 ms on localhost (70/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 145
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 155
15/08/06 17:54:32 INFO TaskSetManager: Starting task 86.0 in stage 10.0 (TID 503, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 86.0 in stage 10.0 (TID 503)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 87.0 in stage 10.0 (TID 504, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:54:32 INFO Executor: Finished task 81.0 in stage 10.0 (TID 498). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 80.0 in stage 10.0 (TID 497). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 67.0 in stage 10.0 (TID 484) in 59 ms on localhost (71/200)
15/08/06 17:54:32 INFO Executor: Running task 87.0 in stage 10.0 (TID 504)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:54:32 INFO TaskSetManager: Finished task 72.0 in stage 10.0 (TID 489) in 48 ms on localhost (72/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 88.0 in stage 10.0 (TID 505, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 82.0 in stage 10.0 (TID 499). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 88.0 in stage 10.0 (TID 505)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 75.0 in stage 10.0 (TID 492) in 48 ms on localhost (73/200)
15/08/06 17:54:32 INFO Executor: Finished task 83.0 in stage 10.0 (TID 500). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 89.0 in stage 10.0 (TID 506, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 89.0 in stage 10.0 (TID 506)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 161
15/08/06 17:54:32 INFO TaskSetManager: Finished task 71.0 in stage 10.0 (TID 488) in 53 ms on localhost (74/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 90.0 in stage 10.0 (TID 507, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO Executor: Running task 90.0 in stage 10.0 (TID 507)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 69.0 in stage 10.0 (TID 486) in 61 ms on localhost (75/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:32 INFO Executor: Finished task 84.0 in stage 10.0 (TID 501). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 70.0 in stage 10.0 (TID 487) in 61 ms on localhost (76/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 91.0 in stage 10.0 (TID 508, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 85.0 in stage 10.0 (TID 502). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 91.0 in stage 10.0 (TID 508)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 92.0 in stage 10.0 (TID 509, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 200
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 79.0 in stage 10.0 (TID 496) in 44 ms on localhost (77/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 92.0 in stage 10.0 (TID 509)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 141
15/08/06 17:54:32 INFO TaskSetManager: Starting task 93.0 in stage 10.0 (TID 510, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 93.0 in stage 10.0 (TID 510)
15/08/06 17:54:32 INFO Executor: Finished task 88.0 in stage 10.0 (TID 505). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 87.0 in stage 10.0 (TID 504). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 86.0 in stage 10.0 (TID 503). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 94.0 in stage 10.0 (TID 511, localhost, ANY, 1526 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:54:32 INFO Executor: Running task 94.0 in stage 10.0 (TID 511)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 78.0 in stage 10.0 (TID 495) in 51 ms on localhost (78/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 76.0 in stage 10.0 (TID 493) in 61 ms on localhost (79/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 95.0 in stage 10.0 (TID 512, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 173
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 77.0 in stage 10.0 (TID 494) in 57 ms on localhost (80/200)
15/08/06 17:54:32 INFO Executor: Running task 95.0 in stage 10.0 (TID 512)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 96.0 in stage 10.0 (TID 513, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 96.0 in stage 10.0 (TID 513)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 119
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/06 17:54:32 INFO TaskSetManager: Finished task 81.0 in stage 10.0 (TID 498) in 50 ms on localhost (81/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 97.0 in stage 10.0 (TID 514, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 90.0 in stage 10.0 (TID 507). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 97.0 in stage 10.0 (TID 514)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 80.0 in stage 10.0 (TID 497) in 53 ms on localhost (82/200)
15/08/06 17:54:32 INFO Executor: Finished task 91.0 in stage 10.0 (TID 508). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 98.0 in stage 10.0 (TID 515, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 89.0 in stage 10.0 (TID 506). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 98.0 in stage 10.0 (TID 515)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 82.0 in stage 10.0 (TID 499) in 50 ms on localhost (83/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 99.0 in stage 10.0 (TID 516, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 99.0 in stage 10.0 (TID 516)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:32 INFO TaskSetManager: Starting task 100.0 in stage 10.0 (TID 517, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO Executor: Running task 100.0 in stage 10.0 (TID 517)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 83.0 in stage 10.0 (TID 500) in 51 ms on localhost (84/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:54:32 INFO Executor: Finished task 93.0 in stage 10.0 (TID 510). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 196
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 92.0 in stage 10.0 (TID 509). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 84.0 in stage 10.0 (TID 501) in 48 ms on localhost (85/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 101.0 in stage 10.0 (TID 518, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO Executor: Running task 101.0 in stage 10.0 (TID 518)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 102.0 in stage 10.0 (TID 519, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 96.0 in stage 10.0 (TID 513). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 102.0 in stage 10.0 (TID 519)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 88.0 in stage 10.0 (TID 505) in 36 ms on localhost (86/200)
15/08/06 17:54:32 INFO Executor: Finished task 94.0 in stage 10.0 (TID 511). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 85.0 in stage 10.0 (TID 502) in 51 ms on localhost (87/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 97.0 in stage 10.0 (TID 514). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 103.0 in stage 10.0 (TID 520, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:32 INFO Executor: Running task 103.0 in stage 10.0 (TID 520)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 87.0 in stage 10.0 (TID 504) in 42 ms on localhost (88/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 104.0 in stage 10.0 (TID 521, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 104.0 in stage 10.0 (TID 521)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 86.0 in stage 10.0 (TID 503) in 45 ms on localhost (89/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 105.0 in stage 10.0 (TID 522, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 105.0 in stage 10.0 (TID 522)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 162
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 155
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 142
15/08/06 17:54:32 INFO TaskSetManager: Finished task 90.0 in stage 10.0 (TID 507) in 38 ms on localhost (90/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 106.0 in stage 10.0 (TID 523, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 107.0 in stage 10.0 (TID 524, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 107.0 in stage 10.0 (TID 524)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 108.0 in stage 10.0 (TID 525, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 174
15/08/06 17:54:32 INFO Executor: Running task 108.0 in stage 10.0 (TID 525)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 89.0 in stage 10.0 (TID 506) in 46 ms on localhost (91/200)
15/08/06 17:54:32 INFO Executor: Running task 106.0 in stage 10.0 (TID 523)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 100.0 in stage 10.0 (TID 517). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:54:32 INFO TaskSetManager: Starting task 109.0 in stage 10.0 (TID 526, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 109.0 in stage 10.0 (TID 526)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 99.0 in stage 10.0 (TID 516). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 91.0 in stage 10.0 (TID 508) in 40 ms on localhost (92/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 101.0 in stage 10.0 (TID 518). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 102.0 in stage 10.0 (TID 519). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 92.0 in stage 10.0 (TID 509) in 40 ms on localhost (93/200)
15/08/06 17:54:32 INFO Executor: Finished task 95.0 in stage 10.0 (TID 512). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 93.0 in stage 10.0 (TID 510) in 40 ms on localhost (94/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 110.0 in stage 10.0 (TID 527, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO Executor: Running task 110.0 in stage 10.0 (TID 527)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/06 17:54:32 INFO TaskSetManager: Starting task 111.0 in stage 10.0 (TID 528, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 206
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 129
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 105.0 in stage 10.0 (TID 522). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 111.0 in stage 10.0 (TID 528)
15/08/06 17:54:32 INFO Executor: Finished task 103.0 in stage 10.0 (TID 520). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 112.0 in stage 10.0 (TID 529, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 98.0 in stage 10.0 (TID 515). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 113.0 in stage 10.0 (TID 530, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 113.0 in stage 10.0 (TID 530)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 114.0 in stage 10.0 (TID 531, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 176
15/08/06 17:54:32 INFO TaskSetManager: Finished task 96.0 in stage 10.0 (TID 513) in 41 ms on localhost (95/200)
15/08/06 17:54:32 INFO Executor: Running task 112.0 in stage 10.0 (TID 529)
15/08/06 17:54:32 INFO Executor: Finished task 108.0 in stage 10.0 (TID 525). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 114.0 in stage 10.0 (TID 531)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:32 INFO Executor: Finished task 107.0 in stage 10.0 (TID 524). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 100.0 in stage 10.0 (TID 517) in 34 ms on localhost (96/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:32 INFO TaskSetManager: Finished task 97.0 in stage 10.0 (TID 514) in 42 ms on localhost (97/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 104.0 in stage 10.0 (TID 521). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 106.0 in stage 10.0 (TID 523). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 94.0 in stage 10.0 (TID 511) in 52 ms on localhost (98/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 115.0 in stage 10.0 (TID 532, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 115.0 in stage 10.0 (TID 532)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 99.0 in stage 10.0 (TID 516) in 40 ms on localhost (99/200)
15/08/06 17:54:32 INFO Executor: Finished task 109.0 in stage 10.0 (TID 526). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 116.0 in stage 10.0 (TID 533, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 116.0 in stage 10.0 (TID 533)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 102.0 in stage 10.0 (TID 519) in 36 ms on localhost (100/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 95.0 in stage 10.0 (TID 512) in 55 ms on localhost (101/200)
15/08/06 17:54:32 INFO Executor: Finished task 111.0 in stage 10.0 (TID 528). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 110.0 in stage 10.0 (TID 527). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 149
15/08/06 17:54:32 INFO TaskSetManager: Finished task 101.0 in stage 10.0 (TID 518) in 45 ms on localhost (102/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 128
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 160
15/08/06 17:54:32 INFO TaskSetManager: Starting task 117.0 in stage 10.0 (TID 534, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO Executor: Running task 117.0 in stage 10.0 (TID 534)
15/08/06 17:54:32 INFO Executor: Finished task 114.0 in stage 10.0 (TID 531). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 105.0 in stage 10.0 (TID 522) in 39 ms on localhost (103/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 113.0 in stage 10.0 (TID 530). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 112.0 in stage 10.0 (TID 529). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 118.0 in stage 10.0 (TID 535, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 115.0 in stage 10.0 (TID 532). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 118.0 in stage 10.0 (TID 535)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 119.0 in stage 10.0 (TID 536, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 119.0 in stage 10.0 (TID 536)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 120.0 in stage 10.0 (TID 537, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:54:32 INFO Executor: Running task 120.0 in stage 10.0 (TID 537)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 98.0 in stage 10.0 (TID 515) in 58 ms on localhost (104/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 116.0 in stage 10.0 (TID 533). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 103.0 in stage 10.0 (TID 520) in 47 ms on localhost (105/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 108.0 in stage 10.0 (TID 525) in 41 ms on localhost (106/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 121.0 in stage 10.0 (TID 538, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO Executor: Running task 121.0 in stage 10.0 (TID 538)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 107.0 in stage 10.0 (TID 524) in 44 ms on localhost (107/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/06 17:54:32 INFO TaskSetManager: Starting task 122.0 in stage 10.0 (TID 539, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 122.0 in stage 10.0 (TID 539)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:32 INFO TaskSetManager: Starting task 123.0 in stage 10.0 (TID 540, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 123.0 in stage 10.0 (TID 540)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:54:32 INFO TaskSetManager: Finished task 104.0 in stage 10.0 (TID 521) in 54 ms on localhost (108/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 124.0 in stage 10.0 (TID 541, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 117.0 in stage 10.0 (TID 534). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 124.0 in stage 10.0 (TID 541)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 192
15/08/06 17:54:32 INFO Executor: Finished task 118.0 in stage 10.0 (TID 535). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 106.0 in stage 10.0 (TID 523) in 51 ms on localhost (109/200)
15/08/06 17:54:32 INFO Executor: Finished task 119.0 in stage 10.0 (TID 536). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 125.0 in stage 10.0 (TID 542, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 109.0 in stage 10.0 (TID 526) in 57 ms on localhost (110/200)
15/08/06 17:54:32 INFO Executor: Running task 125.0 in stage 10.0 (TID 542)
15/08/06 17:54:32 INFO Executor: Finished task 120.0 in stage 10.0 (TID 537). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:54:32 INFO TaskSetManager: Finished task 111.0 in stage 10.0 (TID 528) in 51 ms on localhost (111/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 126.0 in stage 10.0 (TID 543, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 126.0 in stage 10.0 (TID 543)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 110.0 in stage 10.0 (TID 527) in 54 ms on localhost (112/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 127.0 in stage 10.0 (TID 544, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 121.0 in stage 10.0 (TID 538). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 127.0 in stage 10.0 (TID 544)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 128.0 in stage 10.0 (TID 545, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 180
15/08/06 17:54:32 INFO Executor: Running task 128.0 in stage 10.0 (TID 545)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 114.0 in stage 10.0 (TID 531) in 48 ms on localhost (113/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 129.0 in stage 10.0 (TID 546, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 130.0 in stage 10.0 (TID 547, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 130.0 in stage 10.0 (TID 547)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 124.0 in stage 10.0 (TID 541). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 129.0 in stage 10.0 (TID 546)
15/08/06 17:54:32 INFO Executor: Finished task 123.0 in stage 10.0 (TID 540). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 113.0 in stage 10.0 (TID 530) in 52 ms on localhost (114/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 175
15/08/06 17:54:32 INFO TaskSetManager: Finished task 112.0 in stage 10.0 (TID 529) in 59 ms on localhost (115/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 131.0 in stage 10.0 (TID 548, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 131.0 in stage 10.0 (TID 548)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 115.0 in stage 10.0 (TID 532) in 49 ms on localhost (116/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 123
15/08/06 17:54:32 INFO TaskSetManager: Starting task 132.0 in stage 10.0 (TID 549, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 122.0 in stage 10.0 (TID 539). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 132.0 in stage 10.0 (TID 549)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 116.0 in stage 10.0 (TID 533) in 48 ms on localhost (117/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 133.0 in stage 10.0 (TID 550, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 125.0 in stage 10.0 (TID 542). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 133.0 in stage 10.0 (TID 550)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:54:32 INFO TaskSetManager: Finished task 117.0 in stage 10.0 (TID 534) in 43 ms on localhost (118/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:54:32 INFO Executor: Finished task 127.0 in stage 10.0 (TID 544). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 134.0 in stage 10.0 (TID 551, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 134.0 in stage 10.0 (TID 551)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 118.0 in stage 10.0 (TID 535) in 43 ms on localhost (119/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 163
15/08/06 17:54:32 INFO Executor: Finished task 130.0 in stage 10.0 (TID 547). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 135.0 in stage 10.0 (TID 552, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 135.0 in stage 10.0 (TID 552)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 126.0 in stage 10.0 (TID 543). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 128.0 in stage 10.0 (TID 545). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 119.0 in stage 10.0 (TID 536) in 44 ms on localhost (120/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 129.0 in stage 10.0 (TID 546). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 136.0 in stage 10.0 (TID 553, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 136.0 in stage 10.0 (TID 553)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 120.0 in stage 10.0 (TID 537) in 47 ms on localhost (121/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 137.0 in stage 10.0 (TID 554, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:32 INFO Executor: Running task 137.0 in stage 10.0 (TID 554)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 121.0 in stage 10.0 (TID 538) in 43 ms on localhost (122/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 108
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 131.0 in stage 10.0 (TID 548). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 138.0 in stage 10.0 (TID 555, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 132.0 in stage 10.0 (TID 549). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 124.0 in stage 10.0 (TID 541) in 41 ms on localhost (123/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 138.0 in stage 10.0 (TID 555)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 133.0 in stage 10.0 (TID 550). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 139.0 in stage 10.0 (TID 556, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 139.0 in stage 10.0 (TID 556)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:54:32 INFO TaskSetManager: Finished task 123.0 in stage 10.0 (TID 540) in 45 ms on localhost (124/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 140.0 in stage 10.0 (TID 557, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 140.0 in stage 10.0 (TID 557)
15/08/06 17:54:32 INFO Executor: Finished task 134.0 in stage 10.0 (TID 551). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 136.0 in stage 10.0 (TID 553). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 122.0 in stage 10.0 (TID 539) in 47 ms on localhost (125/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 135.0 in stage 10.0 (TID 552). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 141.0 in stage 10.0 (TID 558, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO Executor: Running task 141.0 in stage 10.0 (TID 558)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 125.0 in stage 10.0 (TID 542) in 44 ms on localhost (126/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 142.0 in stage 10.0 (TID 559, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 142.0 in stage 10.0 (TID 559)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 127.0 in stage 10.0 (TID 544) in 34 ms on localhost (127/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 143.0 in stage 10.0 (TID 560, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 143.0 in stage 10.0 (TID 560)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:54:32 INFO TaskSetManager: Starting task 144.0 in stage 10.0 (TID 561, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 138.0 in stage 10.0 (TID 555). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 144.0 in stage 10.0 (TID 561)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 130.0 in stage 10.0 (TID 547) in 36 ms on localhost (128/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 126.0 in stage 10.0 (TID 543) in 42 ms on localhost (129/200)
15/08/06 17:54:32 INFO Executor: Finished task 139.0 in stage 10.0 (TID 556). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 137.0 in stage 10.0 (TID 554). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 181
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 145.0 in stage 10.0 (TID 562, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 128.0 in stage 10.0 (TID 545) in 42 ms on localhost (130/200)
15/08/06 17:54:32 INFO Executor: Running task 145.0 in stage 10.0 (TID 562)
15/08/06 17:54:32 INFO Executor: Finished task 140.0 in stage 10.0 (TID 557). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:54:32 INFO TaskSetManager: Finished task 129.0 in stage 10.0 (TID 546) in 42 ms on localhost (131/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 146.0 in stage 10.0 (TID 563, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 146.0 in stage 10.0 (TID 563)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 147.0 in stage 10.0 (TID 564, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 147.0 in stage 10.0 (TID 564)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 131.0 in stage 10.0 (TID 548) in 41 ms on localhost (132/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 142.0 in stage 10.0 (TID 559). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:54:32 INFO Executor: Finished task 141.0 in stage 10.0 (TID 558). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 148.0 in stage 10.0 (TID 565, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 148.0 in stage 10.0 (TID 565)
15/08/06 17:54:32 INFO Executor: Finished task 143.0 in stage 10.0 (TID 560). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 144.0 in stage 10.0 (TID 561). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 132.0 in stage 10.0 (TID 549) in 46 ms on localhost (133/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 130
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 149.0 in stage 10.0 (TID 566, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 149.0 in stage 10.0 (TID 566)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:54:32 INFO TaskSetManager: Finished task 133.0 in stage 10.0 (TID 550) in 46 ms on localhost (134/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:32 INFO Executor: Finished task 145.0 in stage 10.0 (TID 562). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 150.0 in stage 10.0 (TID 567, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 150.0 in stage 10.0 (TID 567)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 134.0 in stage 10.0 (TID 551) in 45 ms on localhost (135/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 146.0 in stage 10.0 (TID 563). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 147.0 in stage 10.0 (TID 564). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 151.0 in stage 10.0 (TID 568, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO Executor: Running task 151.0 in stage 10.0 (TID 568)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 136.0 in stage 10.0 (TID 553) in 44 ms on localhost (136/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 152.0 in stage 10.0 (TID 569, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Running task 152.0 in stage 10.0 (TID 569)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 135.0 in stage 10.0 (TID 552) in 48 ms on localhost (137/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 153.0 in stage 10.0 (TID 570, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:32 INFO Executor: Running task 153.0 in stage 10.0 (TID 570)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 138.0 in stage 10.0 (TID 555) in 44 ms on localhost (138/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 154.0 in stage 10.0 (TID 571, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO Executor: Running task 154.0 in stage 10.0 (TID 571)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 108
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 139.0 in stage 10.0 (TID 556) in 43 ms on localhost (139/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 190
15/08/06 17:54:32 INFO TaskSetManager: Starting task 155.0 in stage 10.0 (TID 572, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 155.0 in stage 10.0 (TID 572)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 137.0 in stage 10.0 (TID 554) in 51 ms on localhost (140/200)
15/08/06 17:54:32 INFO Executor: Finished task 148.0 in stage 10.0 (TID 565). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 150.0 in stage 10.0 (TID 567). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 149.0 in stage 10.0 (TID 566). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 156.0 in stage 10.0 (TID 573, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 151.0 in stage 10.0 (TID 568). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 156.0 in stage 10.0 (TID 573)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:54:32 INFO TaskSetManager: Finished task 140.0 in stage 10.0 (TID 557) in 46 ms on localhost (141/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 157.0 in stage 10.0 (TID 574, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 157.0 in stage 10.0 (TID 574)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/06 17:54:32 INFO Executor: Finished task 152.0 in stage 10.0 (TID 569). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 142.0 in stage 10.0 (TID 559) in 45 ms on localhost (142/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 149
15/08/06 17:54:32 INFO Executor: Finished task 153.0 in stage 10.0 (TID 570). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 141.0 in stage 10.0 (TID 558) in 50 ms on localhost (143/200)
15/08/06 17:54:32 INFO Executor: Finished task 154.0 in stage 10.0 (TID 571). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 158.0 in stage 10.0 (TID 575, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO Executor: Running task 158.0 in stage 10.0 (TID 575)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 155.0 in stage 10.0 (TID 572). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 159.0 in stage 10.0 (TID 576, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 159.0 in stage 10.0 (TID 576)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:32 INFO TaskSetManager: Finished task 143.0 in stage 10.0 (TID 560) in 50 ms on localhost (144/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 160.0 in stage 10.0 (TID 577, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 156.0 in stage 10.0 (TID 573). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:32 INFO Executor: Running task 160.0 in stage 10.0 (TID 577)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 144.0 in stage 10.0 (TID 561) in 57 ms on localhost (145/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 161.0 in stage 10.0 (TID 578, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Running task 161.0 in stage 10.0 (TID 578)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 145.0 in stage 10.0 (TID 562) in 56 ms on localhost (146/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 162.0 in stage 10.0 (TID 579, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 157.0 in stage 10.0 (TID 574). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 162.0 in stage 10.0 (TID 579)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 146.0 in stage 10.0 (TID 563) in 53 ms on localhost (147/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 163.0 in stage 10.0 (TID 580, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 163.0 in stage 10.0 (TID 580)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 147.0 in stage 10.0 (TID 564) in 54 ms on localhost (148/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 164.0 in stage 10.0 (TID 581, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:54:32 INFO Executor: Finished task 159.0 in stage 10.0 (TID 576). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 164.0 in stage 10.0 (TID 581)
15/08/06 17:54:32 INFO Executor: Finished task 158.0 in stage 10.0 (TID 575). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 148.0 in stage 10.0 (TID 565) in 56 ms on localhost (149/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 165.0 in stage 10.0 (TID 582, localhost, ANY, 1531 bytes)
15/08/06 17:54:32 INFO Executor: Running task 165.0 in stage 10.0 (TID 582)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 116
15/08/06 17:54:32 INFO Executor: Finished task 160.0 in stage 10.0 (TID 577). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 150.0 in stage 10.0 (TID 567) in 48 ms on localhost (150/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 136
15/08/06 17:54:32 INFO TaskSetManager: Starting task 166.0 in stage 10.0 (TID 583, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 166.0 in stage 10.0 (TID 583)
15/08/06 17:54:32 INFO Executor: Finished task 161.0 in stage 10.0 (TID 578). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 110
15/08/06 17:54:32 INFO TaskSetManager: Finished task 149.0 in stage 10.0 (TID 566) in 52 ms on localhost (151/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 167.0 in stage 10.0 (TID 584, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:32 INFO Executor: Finished task 162.0 in stage 10.0 (TID 579). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 167.0 in stage 10.0 (TID 584)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 151.0 in stage 10.0 (TID 568) in 50 ms on localhost (152/200)
15/08/06 17:54:32 INFO Executor: Finished task 163.0 in stage 10.0 (TID 580). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 168.0 in stage 10.0 (TID 585, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 168.0 in stage 10.0 (TID 585)
15/08/06 17:54:32 INFO Executor: Finished task 164.0 in stage 10.0 (TID 581). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 152.0 in stage 10.0 (TID 569) in 51 ms on localhost (153/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 169.0 in stage 10.0 (TID 586, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 169.0 in stage 10.0 (TID 586)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 114
15/08/06 17:54:32 INFO TaskSetManager: Finished task 153.0 in stage 10.0 (TID 570) in 52 ms on localhost (154/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:54:32 INFO TaskSetManager: Starting task 170.0 in stage 10.0 (TID 587, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 170.0 in stage 10.0 (TID 587)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 154.0 in stage 10.0 (TID 571) in 52 ms on localhost (155/200)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 171.0 in stage 10.0 (TID 588, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 171.0 in stage 10.0 (TID 588)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 166.0 in stage 10.0 (TID 583). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 155.0 in stage 10.0 (TID 572) in 52 ms on localhost (156/200)
15/08/06 17:54:32 INFO Executor: Finished task 165.0 in stage 10.0 (TID 582). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 156.0 in stage 10.0 (TID 573) in 52 ms on localhost (157/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 172.0 in stage 10.0 (TID 589, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 172.0 in stage 10.0 (TID 589)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 173.0 in stage 10.0 (TID 590, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO Executor: Running task 173.0 in stage 10.0 (TID 590)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:54:32 INFO TaskSetManager: Starting task 174.0 in stage 10.0 (TID 591, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 174.0 in stage 10.0 (TID 591)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 157.0 in stage 10.0 (TID 574) in 53 ms on localhost (158/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 167.0 in stage 10.0 (TID 584). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 168.0 in stage 10.0 (TID 585). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 159.0 in stage 10.0 (TID 576) in 48 ms on localhost (159/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 186
15/08/06 17:54:32 INFO Executor: Finished task 169.0 in stage 10.0 (TID 586). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 175.0 in stage 10.0 (TID 592, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 175.0 in stage 10.0 (TID 592)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 158.0 in stage 10.0 (TID 575) in 51 ms on localhost (160/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 160.0 in stage 10.0 (TID 577) in 47 ms on localhost (161/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 176.0 in stage 10.0 (TID 593, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 177.0 in stage 10.0 (TID 594, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 176.0 in stage 10.0 (TID 593)
15/08/06 17:54:32 INFO Executor: Finished task 170.0 in stage 10.0 (TID 587). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 224
15/08/06 17:54:32 INFO TaskSetManager: Finished task 161.0 in stage 10.0 (TID 578) in 43 ms on localhost (162/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 171.0 in stage 10.0 (TID 588). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 177.0 in stage 10.0 (TID 594)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 178.0 in stage 10.0 (TID 595, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 178.0 in stage 10.0 (TID 595)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 162.0 in stage 10.0 (TID 579) in 43 ms on localhost (163/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:54:32 INFO Executor: Finished task 173.0 in stage 10.0 (TID 590). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:54:32 INFO TaskSetManager: Finished task 163.0 in stage 10.0 (TID 580) in 43 ms on localhost (164/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 174.0 in stage 10.0 (TID 591). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 179.0 in stage 10.0 (TID 596, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 179.0 in stage 10.0 (TID 596)
15/08/06 17:54:32 INFO Executor: Finished task 172.0 in stage 10.0 (TID 589). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 175.0 in stage 10.0 (TID 592). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 164.0 in stage 10.0 (TID 581) in 44 ms on localhost (165/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 180.0 in stage 10.0 (TID 597, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 180.0 in stage 10.0 (TID 597)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 166.0 in stage 10.0 (TID 583) in 40 ms on localhost (166/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 181.0 in stage 10.0 (TID 598, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 197
15/08/06 17:54:32 INFO Executor: Running task 181.0 in stage 10.0 (TID 598)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 182.0 in stage 10.0 (TID 599, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 165.0 in stage 10.0 (TID 582) in 50 ms on localhost (167/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 177.0 in stage 10.0 (TID 594). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 182.0 in stage 10.0 (TID 599)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 167.0 in stage 10.0 (TID 584) in 47 ms on localhost (168/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 176.0 in stage 10.0 (TID 593). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 246
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:32 INFO TaskSetManager: Starting task 183.0 in stage 10.0 (TID 600, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 183.0 in stage 10.0 (TID 600)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 168.0 in stage 10.0 (TID 585) in 47 ms on localhost (169/200)
15/08/06 17:54:32 INFO Executor: Finished task 178.0 in stage 10.0 (TID 595). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 180.0 in stage 10.0 (TID 597). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 184.0 in stage 10.0 (TID 601, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO Executor: Running task 184.0 in stage 10.0 (TID 601)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 169.0 in stage 10.0 (TID 586) in 46 ms on localhost (170/200)
15/08/06 17:54:32 INFO Executor: Finished task 179.0 in stage 10.0 (TID 596). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Starting task 185.0 in stage 10.0 (TID 602, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 185.0 in stage 10.0 (TID 602)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 186.0 in stage 10.0 (TID 603, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Running task 186.0 in stage 10.0 (TID 603)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 187.0 in stage 10.0 (TID 604, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:54:32 INFO Executor: Running task 187.0 in stage 10.0 (TID 604)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 170.0 in stage 10.0 (TID 587) in 47 ms on localhost (171/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 188.0 in stage 10.0 (TID 605, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 188.0 in stage 10.0 (TID 605)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 171.0 in stage 10.0 (TID 588) in 46 ms on localhost (172/200)
15/08/06 17:54:32 INFO Executor: Finished task 181.0 in stage 10.0 (TID 598). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 173.0 in stage 10.0 (TID 590) in 41 ms on localhost (173/200)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Finished task 174.0 in stage 10.0 (TID 591) in 40 ms on localhost (174/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 189.0 in stage 10.0 (TID 606, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Starting task 190.0 in stage 10.0 (TID 607, localhost, ANY, 1528 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:54:32 INFO Executor: Running task 190.0 in stage 10.0 (TID 607)
15/08/06 17:54:32 INFO Executor: Running task 189.0 in stage 10.0 (TID 606)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 172.0 in stage 10.0 (TID 589) in 46 ms on localhost (175/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:54:32 INFO TaskSetManager: Starting task 191.0 in stage 10.0 (TID 608, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 191.0 in stage 10.0 (TID 608)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 192.0 in stage 10.0 (TID 609, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 192.0 in stage 10.0 (TID 609)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 193.0 in stage 10.0 (TID 610, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Running task 193.0 in stage 10.0 (TID 610)
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 194.0 in stage 10.0 (TID 611, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Running task 194.0 in stage 10.0 (TID 611)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO Executor: Finished task 182.0 in stage 10.0 (TID 599). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 195.0 in stage 10.0 (TID 612, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 184.0 in stage 10.0 (TID 601). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 178.0 in stage 10.0 (TID 595) in 38 ms on localhost (176/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO TaskSetManager: Finished task 175.0 in stage 10.0 (TID 592) in 48 ms on localhost (177/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 183.0 in stage 10.0 (TID 600). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Running task 195.0 in stage 10.0 (TID 612)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 176.0 in stage 10.0 (TID 593) in 45 ms on localhost (178/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 177.0 in stage 10.0 (TID 594) in 45 ms on localhost (179/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:32 INFO TaskSetManager: Finished task 180.0 in stage 10.0 (TID 597) in 36 ms on localhost (180/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 130
15/08/06 17:54:32 INFO TaskSetManager: Starting task 196.0 in stage 10.0 (TID 613, localhost, ANY, 1530 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 194
15/08/06 17:54:32 INFO TaskSetManager: Finished task 179.0 in stage 10.0 (TID 596) in 41 ms on localhost (181/200)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO TaskSetManager: Starting task 197.0 in stage 10.0 (TID 614, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO Executor: Finished task 185.0 in stage 10.0 (TID 602). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 181.0 in stage 10.0 (TID 598) in 37 ms on localhost (182/200)
15/08/06 17:54:32 INFO Executor: Running task 196.0 in stage 10.0 (TID 613)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 145
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 197.0 in stage 10.0 (TID 614)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 186.0 in stage 10.0 (TID 603). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:32 INFO Executor: Finished task 187.0 in stage 10.0 (TID 604). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:32 INFO TaskSetManager: Starting task 198.0 in stage 10.0 (TID 615, localhost, ANY, 1527 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 184
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Running task 198.0 in stage 10.0 (TID 615)
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 182.0 in stage 10.0 (TID 599) in 37 ms on localhost (183/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 188.0 in stage 10.0 (TID 605). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Starting task 199.0 in stage 10.0 (TID 616, localhost, ANY, 1529 bytes)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:54:32 INFO Executor: Running task 199.0 in stage 10.0 (TID 616)
15/08/06 17:54:32 INFO Executor: Finished task 192.0 in stage 10.0 (TID 609). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/06 17:54:32 INFO Executor: Finished task 193.0 in stage 10.0 (TID 610). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 190.0 in stage 10.0 (TID 607). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 134
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 INFO TaskSetManager: Finished task 184.0 in stage 10.0 (TID 601) in 32 ms on localhost (184/200)
15/08/06 17:54:32 INFO Executor: Finished task 191.0 in stage 10.0 (TID 608). 1819 bytes result sent to driver
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:32 INFO Executor: Finished task 194.0 in stage 10.0 (TID 611). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:54:32 INFO TaskSetManager: Finished task 187.0 in stage 10.0 (TID 604) in 30 ms on localhost (185/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 189.0 in stage 10.0 (TID 606). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 188.0 in stage 10.0 (TID 605) in 29 ms on localhost (186/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 186.0 in stage 10.0 (TID 603) in 33 ms on localhost (187/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 150
15/08/06 17:54:32 INFO TaskSetManager: Finished task 185.0 in stage 10.0 (TID 602) in 34 ms on localhost (188/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 183.0 in stage 10.0 (TID 600) in 41 ms on localhost (189/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 191.0 in stage 10.0 (TID 608) in 32 ms on localhost (190/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 190.0 in stage 10.0 (TID 607) in 35 ms on localhost (191/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 193.0 in stage 10.0 (TID 610) in 33 ms on localhost (192/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO Executor: Finished task 195.0 in stage 10.0 (TID 612). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:32 INFO TaskSetManager: Finished task 189.0 in stage 10.0 (TID 606) in 39 ms on localhost (193/200)
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:54:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:54:32 INFO TaskSetManager: Finished task 192.0 in stage 10.0 (TID 609) in 37 ms on localhost (194/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 195.0 in stage 10.0 (TID 612) in 36 ms on localhost (195/200)
15/08/06 17:54:32 INFO Executor: Finished task 196.0 in stage 10.0 (TID 613). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 199.0 in stage 10.0 (TID 616). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 198.0 in stage 10.0 (TID 615). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO Executor: Finished task 197.0 in stage 10.0 (TID 614). 1819 bytes result sent to driver
15/08/06 17:54:32 INFO TaskSetManager: Finished task 194.0 in stage 10.0 (TID 611) in 37 ms on localhost (196/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 196.0 in stage 10.0 (TID 613) in 31 ms on localhost (197/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 199.0 in stage 10.0 (TID 616) in 25 ms on localhost (198/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 198.0 in stage 10.0 (TID 615) in 29 ms on localhost (199/200)
15/08/06 17:54:32 INFO TaskSetManager: Finished task 197.0 in stage 10.0 (TID 614) in 32 ms on localhost (200/200)
15/08/06 17:54:32 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/06 17:54:32 INFO DAGScheduler: Stage 10 (mapPartitions at Exchange.scala:100) finished in 0.612 s
15/08/06 17:54:32 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:54:32 INFO DAGScheduler: running: Set()
15/08/06 17:54:32 INFO DAGScheduler: waiting: Set(Stage 11)
15/08/06 17:54:32 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2526bdfb
15/08/06 17:54:32 INFO DAGScheduler: failed: Set()
15/08/06 17:54:32 INFO StatsReportListener: task runtime:(count: 200, mean: 49.180000, stdev: 15.176218, max: 115.000000, min: 25.000000)
15/08/06 17:54:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:32 INFO StatsReportListener: 	25.0 ms	32.0 ms	35.0 ms	41.0 ms	47.0 ms	53.0 ms	62.0 ms	80.0 ms	115.0 ms
15/08/06 17:54:32 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 56.000000, stdev: 0.000000, max: 56.000000, min: 56.000000)
15/08/06 17:54:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:32 INFO StatsReportListener: 	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B
15/08/06 17:54:32 INFO StatsReportListener: task result size:(count: 200, mean: 1819.000000, stdev: 0.000000, max: 1819.000000, min: 1819.000000)
15/08/06 17:54:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:32 INFO StatsReportListener: 	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B
15/08/06 17:54:32 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 41.101457, stdev: 13.081823, max: 80.000000, min: 19.230769)
15/08/06 17:54:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:32 INFO StatsReportListener: 	19 %	25 %	27 %	31 %	39 %	49 %	60 %	68 %	80 %
15/08/06 17:54:32 INFO StatsReportListener: other time pct: (count: 200, mean: 58.898543, stdev: 13.081823, max: 80.769231, min: 20.000000)
15/08/06 17:54:32 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:32 INFO StatsReportListener: 	20 %	32 %	41 %	51 %	61 %	69 %	73 %	76 %	81 %
15/08/06 17:54:32 INFO DAGScheduler: Missing parents for Stage 11: List()
15/08/06 17:54:32 INFO DAGScheduler: Submitting Stage 11 (MapPartitionsRDD[77] at RangePartitioner at Exchange.scala:88), which is now runnable
15/08/06 17:54:32 INFO MemoryStore: ensureFreeSpace(12256) called with curMem=1600703, maxMem=3333968363
15/08/06 17:54:32 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 12.0 KB, free 3.1 GB)
15/08/06 17:54:32 INFO MemoryStore: ensureFreeSpace(6466) called with curMem=1612959, maxMem=3333968363
15/08/06 17:54:32 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.3 KB, free 3.1 GB)
15/08/06 17:54:32 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:37948 (size: 6.3 KB, free: 3.1 GB)
15/08/06 17:54:32 INFO BlockManagerMaster: Updated info of block broadcast_17_piece0
15/08/06 17:54:32 INFO DefaultExecutionContext: Created broadcast 17 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:32 INFO DAGScheduler: Submitting 200 missing tasks from Stage 11 (MapPartitionsRDD[77] at RangePartitioner at Exchange.scala:88)
15/08/06 17:54:32 INFO TaskSchedulerImpl: Adding task set 11.0 with 200 tasks
15/08/06 17:54:32 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 617, localhost, ANY, 1821 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 618, localhost, ANY, 1823 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 619, localhost, ANY, 1824 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 620, localhost, ANY, 1823 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 621, localhost, ANY, 1825 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 622, localhost, ANY, 1824 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 623, localhost, ANY, 1824 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 624, localhost, ANY, 1825 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 625, localhost, ANY, 1824 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 626, localhost, ANY, 1821 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 627, localhost, ANY, 1826 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 11.0 in stage 11.0 (TID 628, localhost, ANY, 1825 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 12.0 in stage 11.0 (TID 629, localhost, ANY, 1825 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 13.0 in stage 11.0 (TID 630, localhost, ANY, 1824 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 14.0 in stage 11.0 (TID 631, localhost, ANY, 1825 bytes)
15/08/06 17:54:32 INFO TaskSetManager: Starting task 15.0 in stage 11.0 (TID 632, localhost, ANY, 1825 bytes)
15/08/06 17:54:32 INFO Executor: Running task 2.0 in stage 11.0 (TID 619)
15/08/06 17:54:32 INFO Executor: Running task 1.0 in stage 11.0 (TID 618)
15/08/06 17:54:32 INFO Executor: Running task 0.0 in stage 11.0 (TID 617)
15/08/06 17:54:32 INFO Executor: Running task 4.0 in stage 11.0 (TID 621)
15/08/06 17:54:32 INFO Executor: Running task 6.0 in stage 11.0 (TID 623)
15/08/06 17:54:32 INFO Executor: Running task 5.0 in stage 11.0 (TID 622)
15/08/06 17:54:32 INFO Executor: Running task 3.0 in stage 11.0 (TID 620)
15/08/06 17:54:32 INFO Executor: Running task 7.0 in stage 11.0 (TID 624)
15/08/06 17:54:32 INFO Executor: Running task 9.0 in stage 11.0 (TID 626)
15/08/06 17:54:32 INFO Executor: Running task 10.0 in stage 11.0 (TID 627)
15/08/06 17:54:32 INFO Executor: Running task 8.0 in stage 11.0 (TID 625)
15/08/06 17:54:32 INFO Executor: Running task 13.0 in stage 11.0 (TID 630)
15/08/06 17:54:32 INFO Executor: Running task 11.0 in stage 11.0 (TID 628)
15/08/06 17:54:32 INFO Executor: Running task 15.0 in stage 11.0 (TID 632)
15/08/06 17:54:32 INFO Executor: Running task 12.0 in stage 11.0 (TID 629)
15/08/06 17:54:32 INFO Executor: Running task 14.0 in stage 11.0 (TID 631)
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 125
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 129
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 144
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Finished task 11.0 in stage 11.0 (TID 628). 2055 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Finished task 9.0 in stage 11.0 (TID 626). 2055 bytes result sent to driver
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 156
15/08/06 17:54:33 INFO Executor: Finished task 7.0 in stage 11.0 (TID 624). 2018 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 16.0 in stage 11.0 (TID 633, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO Executor: Running task 16.0 in stage 11.0 (TID 633)
15/08/06 17:54:33 INFO Executor: Finished task 4.0 in stage 11.0 (TID 621). 2094 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Finished task 11.0 in stage 11.0 (TID 628) in 223 ms on localhost (1/200)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 17.0 in stage 11.0 (TID 634, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO Executor: Running task 17.0 in stage 11.0 (TID 634)
15/08/06 17:54:33 INFO Executor: Finished task 15.0 in stage 11.0 (TID 632). 2112 bytes result sent to driver
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 626) in 231 ms on localhost (2/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 624) in 232 ms on localhost (3/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 193
15/08/06 17:54:33 INFO TaskSetManager: Starting task 18.0 in stage 11.0 (TID 635, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 19.0 in stage 11.0 (TID 636, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO Executor: Running task 18.0 in stage 11.0 (TID 635)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO Executor: Running task 19.0 in stage 11.0 (TID 636)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 621) in 236 ms on localhost (4/200)
15/08/06 17:54:33 INFO Executor: Finished task 6.0 in stage 11.0 (TID 623). 2019 bytes result sent to driver
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:54:33 INFO TaskSetManager: Starting task 20.0 in stage 11.0 (TID 637, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Running task 20.0 in stage 11.0 (TID 637)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Finished task 3.0 in stage 11.0 (TID 620). 2111 bytes result sent to driver
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:54:33 INFO TaskSetManager: Finished task 15.0 in stage 11.0 (TID 632) in 232 ms on localhost (5/200)
15/08/06 17:54:33 INFO Executor: Finished task 13.0 in stage 11.0 (TID 630). 2237 bytes result sent to driver
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:54:33 INFO TaskSetManager: Starting task 21.0 in stage 11.0 (TID 638, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 623) in 250 ms on localhost (6/200)
15/08/06 17:54:33 INFO Executor: Running task 21.0 in stage 11.0 (TID 638)
15/08/06 17:54:33 INFO Executor: Finished task 12.0 in stage 11.0 (TID 629). 2072 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 22.0 in stage 11.0 (TID 639, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO Executor: Finished task 1.0 in stage 11.0 (TID 618). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Running task 22.0 in stage 11.0 (TID 639)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO TaskSetManager: Starting task 23.0 in stage 11.0 (TID 640, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Running task 23.0 in stage 11.0 (TID 640)
15/08/06 17:54:33 INFO Executor: Finished task 8.0 in stage 11.0 (TID 625). 2055 bytes result sent to driver
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/06 17:54:33 INFO Executor: Finished task 0.0 in stage 11.0 (TID 617). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO TaskSetManager: Finished task 13.0 in stage 11.0 (TID 630) in 250 ms on localhost (7/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 620) in 255 ms on localhost (8/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO TaskSetManager: Starting task 24.0 in stage 11.0 (TID 641, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO Executor: Running task 24.0 in stage 11.0 (TID 641)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 25.0 in stage 11.0 (TID 642, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO Executor: Running task 25.0 in stage 11.0 (TID 642)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 12.0 in stage 11.0 (TID 629) in 253 ms on localhost (9/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 618) in 260 ms on localhost (10/200)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 26.0 in stage 11.0 (TID 643, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 27.0 in stage 11.0 (TID 644, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO Executor: Running task 27.0 in stage 11.0 (TID 644)
15/08/06 17:54:33 INFO Executor: Running task 26.0 in stage 11.0 (TID 643)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 625) in 261 ms on localhost (11/200)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 617) in 266 ms on localhost (12/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 126
15/08/06 17:54:33 INFO Executor: Finished task 2.0 in stage 11.0 (TID 619). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Starting task 28.0 in stage 11.0 (TID 645, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:54:33 INFO Executor: Finished task 14.0 in stage 11.0 (TID 631). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Running task 28.0 in stage 11.0 (TID 645)
15/08/06 17:54:33 INFO Executor: Finished task 5.0 in stage 11.0 (TID 622). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 619) in 277 ms on localhost (13/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO TaskSetManager: Starting task 29.0 in stage 11.0 (TID 646, localhost, ANY, 1822 bytes)
15/08/06 17:54:33 INFO Executor: Running task 29.0 in stage 11.0 (TID 646)
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 14.0 in stage 11.0 (TID 631) in 276 ms on localhost (14/200)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 30.0 in stage 11.0 (TID 647, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 30.0 in stage 11.0 (TID 647)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 140
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO Executor: Finished task 10.0 in stage 11.0 (TID 627). 2054 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 622) in 285 ms on localhost (15/200)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 31.0 in stage 11.0 (TID 648, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO Executor: Running task 31.0 in stage 11.0 (TID 648)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 627) in 291 ms on localhost (16/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 154
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 179
15/08/06 17:54:33 INFO Executor: Finished task 22.0 in stage 11.0 (TID 639). 2055 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 32.0 in stage 11.0 (TID 649, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO Executor: Finished task 24.0 in stage 11.0 (TID 641). 2001 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Running task 32.0 in stage 11.0 (TID 649)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 22.0 in stage 11.0 (TID 639) in 176 ms on localhost (17/200)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 33.0 in stage 11.0 (TID 650, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 33.0 in stage 11.0 (TID 650)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 24.0 in stage 11.0 (TID 641) in 186 ms on localhost (18/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 159
15/08/06 17:54:33 INFO Executor: Finished task 18.0 in stage 11.0 (TID 635). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 34.0 in stage 11.0 (TID 651, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO Executor: Running task 34.0 in stage 11.0 (TID 651)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 18.0 in stage 11.0 (TID 635) in 230 ms on localhost (19/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 137
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Finished task 16.0 in stage 11.0 (TID 633). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO TaskSetManager: Starting task 35.0 in stage 11.0 (TID 652, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 35.0 in stage 11.0 (TID 652)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Finished task 16.0 in stage 11.0 (TID 633) in 253 ms on localhost (20/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 121
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO Executor: Finished task 20.0 in stage 11.0 (TID 637). 2055 bytes result sent to driver
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Starting task 36.0 in stage 11.0 (TID 653, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 36.0 in stage 11.0 (TID 653)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 161
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:54:33 INFO TaskSetManager: Finished task 20.0 in stage 11.0 (TID 637) in 252 ms on localhost (21/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Finished task 19.0 in stage 11.0 (TID 636). 2146 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Finished task 23.0 in stage 11.0 (TID 640). 2130 bytes result sent to driver
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:54:33 INFO TaskSetManager: Starting task 37.0 in stage 11.0 (TID 654, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 37.0 in stage 11.0 (TID 654)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 19.0 in stage 11.0 (TID 636) in 260 ms on localhost (22/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Starting task 38.0 in stage 11.0 (TID 655, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:54:33 INFO TaskSetManager: Finished task 23.0 in stage 11.0 (TID 640) in 245 ms on localhost (23/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Running task 38.0 in stage 11.0 (TID 655)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 88
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 153
15/08/06 17:54:33 INFO Executor: Finished task 17.0 in stage 11.0 (TID 634). 2093 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Finished task 31.0 in stage 11.0 (TID 648). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 39.0 in stage 11.0 (TID 656, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 39.0 in stage 11.0 (TID 656)
15/08/06 17:54:33 INFO Executor: Finished task 30.0 in stage 11.0 (TID 647). 2055 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Finished task 17.0 in stage 11.0 (TID 634) in 295 ms on localhost (24/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:54:33 INFO TaskSetManager: Starting task 40.0 in stage 11.0 (TID 657, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Starting task 41.0 in stage 11.0 (TID 658, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 41.0 in stage 11.0 (TID 658)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Finished task 21.0 in stage 11.0 (TID 638). 2019 bytes result sent to driver
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Finished task 31.0 in stage 11.0 (TID 648) in 234 ms on localhost (25/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 121
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 151
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:54:33 INFO TaskSetManager: Finished task 30.0 in stage 11.0 (TID 647) in 242 ms on localhost (26/200)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 42.0 in stage 11.0 (TID 659, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO Executor: Running task 42.0 in stage 11.0 (TID 659)
15/08/06 17:54:33 INFO Executor: Finished task 25.0 in stage 11.0 (TID 642). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Running task 40.0 in stage 11.0 (TID 657)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 21.0 in stage 11.0 (TID 638) in 281 ms on localhost (27/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO Executor: Finished task 29.0 in stage 11.0 (TID 646). 2166 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Finished task 27.0 in stage 11.0 (TID 644). 2019 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 43.0 in stage 11.0 (TID 660, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 25.0 in stage 11.0 (TID 642) in 276 ms on localhost (28/200)
15/08/06 17:54:33 INFO Executor: Finished task 28.0 in stage 11.0 (TID 645). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Running task 43.0 in stage 11.0 (TID 660)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 44.0 in stage 11.0 (TID 661, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 29.0 in stage 11.0 (TID 646) in 261 ms on localhost (29/200)
15/08/06 17:54:33 INFO Executor: Running task 44.0 in stage 11.0 (TID 661)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 45.0 in stage 11.0 (TID 662, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 45.0 in stage 11.0 (TID 662)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 27.0 in stage 11.0 (TID 644) in 291 ms on localhost (30/200)
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO TaskSetManager: Starting task 46.0 in stage 11.0 (TID 663, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO Executor: Running task 46.0 in stage 11.0 (TID 663)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 28.0 in stage 11.0 (TID 645) in 278 ms on localhost (31/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:54:33 INFO Executor: Finished task 26.0 in stage 11.0 (TID 643). 2036 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 47.0 in stage 11.0 (TID 664, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO Executor: Running task 47.0 in stage 11.0 (TID 664)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 26.0 in stage 11.0 (TID 643) in 310 ms on localhost (32/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO BlockManager: Removing broadcast 16
15/08/06 17:54:33 INFO BlockManager: Removing block broadcast_16_piece0
15/08/06 17:54:33 INFO MemoryStore: Block broadcast_16_piece0 of size 4450 dropped from memory (free 3332353388)
15/08/06 17:54:33 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:37948 in memory (size: 4.3 KB, free: 3.1 GB)
15/08/06 17:54:33 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/08/06 17:54:33 INFO BlockManager: Removing block broadcast_16
15/08/06 17:54:33 INFO MemoryStore: Block broadcast_16 of size 8200 dropped from memory (free 3332361588)
15/08/06 17:54:33 INFO ContextCleaner: Cleaned broadcast 16
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 149
15/08/06 17:54:33 INFO Executor: Finished task 33.0 in stage 11.0 (TID 650). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Starting task 48.0 in stage 11.0 (TID 665, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 48.0 in stage 11.0 (TID 665)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 165
15/08/06 17:54:33 INFO TaskSetManager: Finished task 33.0 in stage 11.0 (TID 650) in 270 ms on localhost (33/200)
15/08/06 17:54:33 INFO Executor: Finished task 35.0 in stage 11.0 (TID 652). 2001 bytes result sent to driver
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO Executor: Finished task 32.0 in stage 11.0 (TID 649). 2094 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 49.0 in stage 11.0 (TID 666, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO Executor: Running task 49.0 in stage 11.0 (TID 666)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 35.0 in stage 11.0 (TID 652) in 236 ms on localhost (34/200)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 50.0 in stage 11.0 (TID 667, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 50.0 in stage 11.0 (TID 667)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 32.0 in stage 11.0 (TID 649) in 289 ms on localhost (35/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:54:33 INFO Executor: Finished task 34.0 in stage 11.0 (TID 651). 2093 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 51.0 in stage 11.0 (TID 668, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Running task 51.0 in stage 11.0 (TID 668)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:54:33 INFO TaskSetManager: Finished task 34.0 in stage 11.0 (TID 651) in 261 ms on localhost (36/200)
15/08/06 17:54:33 INFO Executor: Finished task 37.0 in stage 11.0 (TID 654). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO TaskSetManager: Starting task 52.0 in stage 11.0 (TID 669, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 37.0 in stage 11.0 (TID 654) in 235 ms on localhost (37/200)
15/08/06 17:54:33 INFO Executor: Running task 52.0 in stage 11.0 (TID 669)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Finished task 36.0 in stage 11.0 (TID 653). 2112 bytes result sent to driver
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO TaskSetManager: Starting task 53.0 in stage 11.0 (TID 670, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Running task 53.0 in stage 11.0 (TID 670)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Finished task 36.0 in stage 11.0 (TID 653) in 265 ms on localhost (38/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:54:33 INFO Executor: Finished task 47.0 in stage 11.0 (TID 664). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 54.0 in stage 11.0 (TID 671, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:54:33 INFO Executor: Running task 54.0 in stage 11.0 (TID 671)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO TaskSetManager: Finished task 47.0 in stage 11.0 (TID 664) in 193 ms on localhost (39/200)
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 120
15/08/06 17:54:33 INFO Executor: Finished task 41.0 in stage 11.0 (TID 658). 2094 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 55.0 in stage 11.0 (TID 672, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 55.0 in stage 11.0 (TID 672)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 41.0 in stage 11.0 (TID 658) in 242 ms on localhost (40/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO Executor: Finished task 42.0 in stage 11.0 (TID 659). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:54:33 INFO TaskSetManager: Starting task 56.0 in stage 11.0 (TID 673, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 56.0 in stage 11.0 (TID 673)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 42.0 in stage 11.0 (TID 659) in 242 ms on localhost (41/200)
15/08/06 17:54:33 INFO Executor: Finished task 45.0 in stage 11.0 (TID 662). 2001 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Finished task 39.0 in stage 11.0 (TID 656). 2019 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 57.0 in stage 11.0 (TID 674, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 45.0 in stage 11.0 (TID 662) in 225 ms on localhost (42/200)
15/08/06 17:54:33 INFO Executor: Running task 57.0 in stage 11.0 (TID 674)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 58.0 in stage 11.0 (TID 675, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 58.0 in stage 11.0 (TID 675)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Finished task 39.0 in stage 11.0 (TID 656) in 254 ms on localhost (43/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO Executor: Finished task 40.0 in stage 11.0 (TID 657). 2093 bytes result sent to driver
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Finished task 44.0 in stage 11.0 (TID 661). 2148 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 59.0 in stage 11.0 (TID 676, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 59.0 in stage 11.0 (TID 676)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 40.0 in stage 11.0 (TID 657) in 256 ms on localhost (44/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Starting task 60.0 in stage 11.0 (TID 677, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 60.0 in stage 11.0 (TID 677)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 44.0 in stage 11.0 (TID 661) in 243 ms on localhost (45/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Finished task 38.0 in stage 11.0 (TID 655). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 61.0 in stage 11.0 (TID 678, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO Executor: Finished task 43.0 in stage 11.0 (TID 660). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Finished task 38.0 in stage 11.0 (TID 655) in 287 ms on localhost (46/200)
15/08/06 17:54:33 INFO Executor: Running task 61.0 in stage 11.0 (TID 678)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 62.0 in stage 11.0 (TID 679, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO Executor: Running task 62.0 in stage 11.0 (TID 679)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 43.0 in stage 11.0 (TID 660) in 257 ms on localhost (47/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 141
15/08/06 17:54:33 INFO Executor: Finished task 46.0 in stage 11.0 (TID 663). 2055 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 63.0 in stage 11.0 (TID 680, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 63.0 in stage 11.0 (TID 680)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 46.0 in stage 11.0 (TID 663) in 282 ms on localhost (48/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 95
15/08/06 17:54:33 INFO Executor: Finished task 49.0 in stage 11.0 (TID 666). 2019 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 64.0 in stage 11.0 (TID 681, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO Executor: Running task 64.0 in stage 11.0 (TID 681)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 49.0 in stage 11.0 (TID 666) in 141 ms on localhost (49/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Finished task 48.0 in stage 11.0 (TID 665). 2147 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 65.0 in stage 11.0 (TID 682, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 65.0 in stage 11.0 (TID 682)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 48.0 in stage 11.0 (TID 665) in 159 ms on localhost (50/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO Executor: Finished task 50.0 in stage 11.0 (TID 667). 2055 bytes result sent to driver
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO TaskSetManager: Starting task 66.0 in stage 11.0 (TID 683, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 66.0 in stage 11.0 (TID 683)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 50.0 in stage 11.0 (TID 667) in 160 ms on localhost (51/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO Executor: Finished task 51.0 in stage 11.0 (TID 668). 2072 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 67.0 in stage 11.0 (TID 684, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO Executor: Running task 67.0 in stage 11.0 (TID 684)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 51.0 in stage 11.0 (TID 668) in 161 ms on localhost (52/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 124
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO Executor: Finished task 52.0 in stage 11.0 (TID 669). 2019 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 68.0 in stage 11.0 (TID 685, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 68.0 in stage 11.0 (TID 685)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 52.0 in stage 11.0 (TID 669) in 161 ms on localhost (53/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 137
15/08/06 17:54:33 INFO Executor: Finished task 54.0 in stage 11.0 (TID 671). 2093 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 69.0 in stage 11.0 (TID 686, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO Executor: Running task 69.0 in stage 11.0 (TID 686)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Finished task 54.0 in stage 11.0 (TID 671) in 164 ms on localhost (54/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Finished task 59.0 in stage 11.0 (TID 676). 2073 bytes result sent to driver
15/08/06 17:54:33 INFO Executor: Finished task 53.0 in stage 11.0 (TID 670). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Starting task 70.0 in stage 11.0 (TID 687, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 70.0 in stage 11.0 (TID 687)
15/08/06 17:54:33 INFO TaskSetManager: Starting task 71.0 in stage 11.0 (TID 688, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 71.0 in stage 11.0 (TID 688)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO TaskSetManager: Finished task 59.0 in stage 11.0 (TID 676) in 150 ms on localhost (55/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 133
15/08/06 17:54:33 INFO TaskSetManager: Finished task 53.0 in stage 11.0 (TID 670) in 179 ms on localhost (56/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Finished task 56.0 in stage 11.0 (TID 673). 2019 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 72.0 in stage 11.0 (TID 689, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 72.0 in stage 11.0 (TID 689)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 56.0 in stage 11.0 (TID 673) in 173 ms on localhost (57/200)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 203
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:54:33 INFO Executor: Finished task 60.0 in stage 11.0 (TID 677). 2111 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 73.0 in stage 11.0 (TID 690, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 73.0 in stage 11.0 (TID 690)
15/08/06 17:54:33 INFO Executor: Finished task 61.0 in stage 11.0 (TID 678). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Finished task 60.0 in stage 11.0 (TID 677) in 174 ms on localhost (58/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO TaskSetManager: Starting task 74.0 in stage 11.0 (TID 691, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:54:33 INFO TaskSetManager: Finished task 61.0 in stage 11.0 (TID 678) in 174 ms on localhost (59/200)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 112
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO Executor: Running task 74.0 in stage 11.0 (TID 691)
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 103
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:33 INFO Executor: Finished task 55.0 in stage 11.0 (TID 672). 2001 bytes result sent to driver
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:33 INFO TaskSetManager: Starting task 75.0 in stage 11.0 (TID 692, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 75.0 in stage 11.0 (TID 692)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 55.0 in stage 11.0 (TID 672) in 197 ms on localhost (60/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO Executor: Finished task 58.0 in stage 11.0 (TID 675). 2037 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 76.0 in stage 11.0 (TID 693, localhost, ANY, 1827 bytes)
15/08/06 17:54:33 INFO Executor: Running task 76.0 in stage 11.0 (TID 693)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 58.0 in stage 11.0 (TID 675) in 194 ms on localhost (61/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO Executor: Finished task 62.0 in stage 11.0 (TID 679). 2111 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 77.0 in stage 11.0 (TID 694, localhost, ANY, 1826 bytes)
15/08/06 17:54:33 INFO Executor: Running task 77.0 in stage 11.0 (TID 694)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 62.0 in stage 11.0 (TID 679) in 196 ms on localhost (62/200)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:33 INFO Executor: Finished task 57.0 in stage 11.0 (TID 674). 2112 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 78.0 in stage 11.0 (TID 695, localhost, ANY, 1824 bytes)
15/08/06 17:54:33 INFO Executor: Running task 78.0 in stage 11.0 (TID 695)
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO TaskSetManager: Finished task 57.0 in stage 11.0 (TID 674) in 218 ms on localhost (63/200)
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:54:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:54:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 139
15/08/06 17:54:33 INFO Executor: Finished task 66.0 in stage 11.0 (TID 683). 2220 bytes result sent to driver
15/08/06 17:54:33 INFO TaskSetManager: Starting task 79.0 in stage 11.0 (TID 696, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO TaskSetManager: Finished task 66.0 in stage 11.0 (TID 683) in 131 ms on localhost (64/200)
15/08/06 17:54:33 INFO Executor: Running task 79.0 in stage 11.0 (TID 696)
15/08/06 17:54:33 INFO Executor: Finished task 63.0 in stage 11.0 (TID 680). 2019 bytes result sent to driver
15/08/06 17:54:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:33 INFO TaskSetManager: Starting task 80.0 in stage 11.0 (TID 697, localhost, ANY, 1825 bytes)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:33 INFO TaskSetManager: Finished task 63.0 in stage 11.0 (TID 680) in 181 ms on localhost (65/200)
15/08/06 17:54:33 INFO Executor: Running task 80.0 in stage 11.0 (TID 697)
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:54:34 INFO Executor: Finished task 64.0 in stage 11.0 (TID 681). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 81.0 in stage 11.0 (TID 698, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 81.0 in stage 11.0 (TID 698)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 64.0 in stage 11.0 (TID 681) in 179 ms on localhost (66/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Finished task 65.0 in stage 11.0 (TID 682). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 82.0 in stage 11.0 (TID 699, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 82.0 in stage 11.0 (TID 699)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 65.0 in stage 11.0 (TID 682) in 189 ms on localhost (67/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/06 17:54:34 INFO Executor: Finished task 67.0 in stage 11.0 (TID 684). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 83.0 in stage 11.0 (TID 700, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO Executor: Running task 83.0 in stage 11.0 (TID 700)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Finished task 67.0 in stage 11.0 (TID 684) in 192 ms on localhost (68/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Finished task 71.0 in stage 11.0 (TID 688). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Starting task 84.0 in stage 11.0 (TID 701, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 122
15/08/06 17:54:34 INFO TaskSetManager: Finished task 71.0 in stage 11.0 (TID 688) in 162 ms on localhost (69/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO Executor: Running task 84.0 in stage 11.0 (TID 701)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO Executor: Finished task 68.0 in stage 11.0 (TID 685). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 85.0 in stage 11.0 (TID 702, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 85.0 in stage 11.0 (TID 702)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:54:34 INFO TaskSetManager: Finished task 68.0 in stage 11.0 (TID 685) in 209 ms on localhost (70/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Finished task 70.0 in stage 11.0 (TID 687). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Finished task 75.0 in stage 11.0 (TID 692). 2131 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 86.0 in stage 11.0 (TID 703, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 86.0 in stage 11.0 (TID 703)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Finished task 70.0 in stage 11.0 (TID 687) in 174 ms on localhost (71/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 87.0 in stage 11.0 (TID 704, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 87.0 in stage 11.0 (TID 704)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO TaskSetManager: Finished task 75.0 in stage 11.0 (TID 692) in 140 ms on localhost (72/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Finished task 69.0 in stage 11.0 (TID 686). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Starting task 88.0 in stage 11.0 (TID 705, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 88.0 in stage 11.0 (TID 705)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 69.0 in stage 11.0 (TID 686) in 183 ms on localhost (73/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Finished task 74.0 in stage 11.0 (TID 691). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 89.0 in stage 11.0 (TID 706, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 89.0 in stage 11.0 (TID 706)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 74.0 in stage 11.0 (TID 691) in 161 ms on localhost (74/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:34 INFO Executor: Finished task 73.0 in stage 11.0 (TID 690). 2055 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 90.0 in stage 11.0 (TID 707, localhost, ANY, 1823 bytes)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Running task 90.0 in stage 11.0 (TID 707)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 73.0 in stage 11.0 (TID 690) in 169 ms on localhost (75/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 134
15/08/06 17:54:34 INFO Executor: Finished task 72.0 in stage 11.0 (TID 689). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Starting task 91.0 in stage 11.0 (TID 708, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 91.0 in stage 11.0 (TID 708)
15/08/06 17:54:34 INFO Executor: Finished task 76.0 in stage 11.0 (TID 693). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Finished task 72.0 in stage 11.0 (TID 689) in 183 ms on localhost (76/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Starting task 92.0 in stage 11.0 (TID 709, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 92.0 in stage 11.0 (TID 709)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 76.0 in stage 11.0 (TID 693) in 159 ms on localhost (77/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 191
15/08/06 17:54:34 INFO Executor: Finished task 78.0 in stage 11.0 (TID 695). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO TaskSetManager: Starting task 93.0 in stage 11.0 (TID 710, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 78.0 in stage 11.0 (TID 695) in 157 ms on localhost (78/200)
15/08/06 17:54:34 INFO Executor: Running task 93.0 in stage 11.0 (TID 710)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 155
15/08/06 17:54:34 INFO Executor: Finished task 81.0 in stage 11.0 (TID 698). 2055 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Starting task 94.0 in stage 11.0 (TID 711, localhost, ANY, 1823 bytes)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Finished task 81.0 in stage 11.0 (TID 698) in 134 ms on localhost (79/200)
15/08/06 17:54:34 INFO Executor: Running task 94.0 in stage 11.0 (TID 711)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/06 17:54:34 INFO Executor: Finished task 77.0 in stage 11.0 (TID 694). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO TaskSetManager: Starting task 95.0 in stage 11.0 (TID 712, localhost, ANY, 1827 bytes)
15/08/06 17:54:34 INFO Executor: Running task 95.0 in stage 11.0 (TID 712)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Finished task 77.0 in stage 11.0 (TID 694) in 187 ms on localhost (80/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 145
15/08/06 17:54:34 INFO Executor: Finished task 80.0 in stage 11.0 (TID 697). 2073 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 96.0 in stage 11.0 (TID 713, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 96.0 in stage 11.0 (TID 713)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 80.0 in stage 11.0 (TID 697) in 166 ms on localhost (81/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO Executor: Finished task 79.0 in stage 11.0 (TID 696). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 97.0 in stage 11.0 (TID 714, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 97.0 in stage 11.0 (TID 714)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 79.0 in stage 11.0 (TID 696) in 178 ms on localhost (82/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 163
15/08/06 17:54:34 INFO Executor: Finished task 83.0 in stage 11.0 (TID 700). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 98.0 in stage 11.0 (TID 715, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO Executor: Running task 98.0 in stage 11.0 (TID 715)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Finished task 83.0 in stage 11.0 (TID 700) in 173 ms on localhost (83/200)
15/08/06 17:54:34 INFO Executor: Finished task 82.0 in stage 11.0 (TID 699). 2130 bytes result sent to driver
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO TaskSetManager: Starting task 99.0 in stage 11.0 (TID 716, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 99.0 in stage 11.0 (TID 716)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Finished task 82.0 in stage 11.0 (TID 699) in 197 ms on localhost (84/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 158
15/08/06 17:54:34 INFO Executor: Finished task 84.0 in stage 11.0 (TID 701). 2002 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 100.0 in stage 11.0 (TID 717, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 100.0 in stage 11.0 (TID 717)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Finished task 84.0 in stage 11.0 (TID 701) in 176 ms on localhost (85/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/06 17:54:34 INFO Executor: Finished task 86.0 in stage 11.0 (TID 703). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 87.0 in stage 11.0 (TID 704). 2113 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 85.0 in stage 11.0 (TID 702). 2113 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 101.0 in stage 11.0 (TID 718, localhost, ANY, 1823 bytes)
15/08/06 17:54:34 INFO Executor: Running task 101.0 in stage 11.0 (TID 718)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 102.0 in stage 11.0 (TID 719, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Starting task 103.0 in stage 11.0 (TID 720, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 103.0 in stage 11.0 (TID 720)
15/08/06 17:54:34 INFO Executor: Running task 102.0 in stage 11.0 (TID 719)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 85.0 in stage 11.0 (TID 702) in 174 ms on localhost (86/200)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 87.0 in stage 11.0 (TID 704) in 170 ms on localhost (87/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 86.0 in stage 11.0 (TID 703) in 173 ms on localhost (88/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 200
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Finished task 88.0 in stage 11.0 (TID 705). 2095 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 182
15/08/06 17:54:34 INFO TaskSetManager: Starting task 104.0 in stage 11.0 (TID 721, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 104.0 in stage 11.0 (TID 721)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO TaskSetManager: Finished task 88.0 in stage 11.0 (TID 705) in 187 ms on localhost (89/200)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Finished task 89.0 in stage 11.0 (TID 706). 2095 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 105.0 in stage 11.0 (TID 722, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 89.0 in stage 11.0 (TID 706) in 180 ms on localhost (90/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:54:34 INFO Executor: Running task 105.0 in stage 11.0 (TID 722)
15/08/06 17:54:34 INFO Executor: Finished task 94.0 in stage 11.0 (TID 711). 2095 bytes result sent to driver
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Starting task 106.0 in stage 11.0 (TID 723, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 106.0 in stage 11.0 (TID 723)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Finished task 94.0 in stage 11.0 (TID 711) in 138 ms on localhost (91/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 119
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO Executor: Finished task 91.0 in stage 11.0 (TID 708). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 107.0 in stage 11.0 (TID 724, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 107.0 in stage 11.0 (TID 724)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 91.0 in stage 11.0 (TID 708) in 181 ms on localhost (92/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 173
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Finished task 90.0 in stage 11.0 (TID 707). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Starting task 108.0 in stage 11.0 (TID 725, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 108.0 in stage 11.0 (TID 725)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 90.0 in stage 11.0 (TID 707) in 246 ms on localhost (93/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Finished task 97.0 in stage 11.0 (TID 714). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO Executor: Finished task 95.0 in stage 11.0 (TID 712). 2131 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 109.0 in stage 11.0 (TID 726, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Running task 109.0 in stage 11.0 (TID 726)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 97.0 in stage 11.0 (TID 714) in 199 ms on localhost (94/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 108
15/08/06 17:54:34 INFO TaskSetManager: Starting task 110.0 in stage 11.0 (TID 727, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 110.0 in stage 11.0 (TID 727)
15/08/06 17:54:34 INFO Executor: Finished task 93.0 in stage 11.0 (TID 710). 2055 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Finished task 95.0 in stage 11.0 (TID 712) in 215 ms on localhost (95/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Starting task 111.0 in stage 11.0 (TID 728, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 111.0 in stage 11.0 (TID 728)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 93.0 in stage 11.0 (TID 710) in 238 ms on localhost (96/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:34 INFO Executor: Finished task 92.0 in stage 11.0 (TID 709). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 112.0 in stage 11.0 (TID 729, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 112.0 in stage 11.0 (TID 729)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO TaskSetManager: Finished task 92.0 in stage 11.0 (TID 709) in 267 ms on localhost (97/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 114
15/08/06 17:54:34 INFO Executor: Finished task 96.0 in stage 11.0 (TID 713). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 113.0 in stage 11.0 (TID 730, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 113.0 in stage 11.0 (TID 730)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 96.0 in stage 11.0 (TID 713) in 229 ms on localhost (98/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 129
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Finished task 102.0 in stage 11.0 (TID 719). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Starting task 114.0 in stage 11.0 (TID 731, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 114.0 in stage 11.0 (TID 731)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 102.0 in stage 11.0 (TID 719) in 201 ms on localhost (99/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 162
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Finished task 99.0 in stage 11.0 (TID 716). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 98.0 in stage 11.0 (TID 715). 2095 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 115.0 in stage 11.0 (TID 732, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 115.0 in stage 11.0 (TID 732)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 99.0 in stage 11.0 (TID 716) in 232 ms on localhost (100/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO TaskSetManager: Starting task 116.0 in stage 11.0 (TID 733, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO TaskSetManager: Finished task 98.0 in stage 11.0 (TID 715) in 239 ms on localhost (101/200)
15/08/06 17:54:34 INFO Executor: Running task 116.0 in stage 11.0 (TID 733)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Finished task 103.0 in stage 11.0 (TID 720). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 155
15/08/06 17:54:34 INFO TaskSetManager: Starting task 117.0 in stage 11.0 (TID 734, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Running task 117.0 in stage 11.0 (TID 734)
15/08/06 17:54:34 INFO Executor: Finished task 100.0 in stage 11.0 (TID 717). 2147 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Finished task 103.0 in stage 11.0 (TID 720) in 218 ms on localhost (102/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 118.0 in stage 11.0 (TID 735, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Running task 118.0 in stage 11.0 (TID 735)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO TaskSetManager: Finished task 100.0 in stage 11.0 (TID 717) in 225 ms on localhost (103/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/06 17:54:34 INFO Executor: Finished task 105.0 in stage 11.0 (TID 722). 2094 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 119.0 in stage 11.0 (TID 736, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 119.0 in stage 11.0 (TID 736)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 105.0 in stage 11.0 (TID 722) in 200 ms on localhost (104/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 174
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:54:34 INFO Executor: Finished task 101.0 in stage 11.0 (TID 718). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO TaskSetManager: Starting task 120.0 in stage 11.0 (TID 737, localhost, ANY, 1823 bytes)
15/08/06 17:54:34 INFO Executor: Running task 120.0 in stage 11.0 (TID 737)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO TaskSetManager: Finished task 101.0 in stage 11.0 (TID 718) in 235 ms on localhost (105/200)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Finished task 104.0 in stage 11.0 (TID 721). 2073 bytes result sent to driver
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Starting task 121.0 in stage 11.0 (TID 738, localhost, ANY, 1823 bytes)
15/08/06 17:54:34 INFO Executor: Running task 121.0 in stage 11.0 (TID 738)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 104.0 in stage 11.0 (TID 721) in 218 ms on localhost (106/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Finished task 109.0 in stage 11.0 (TID 726). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:54:34 INFO TaskSetManager: Starting task 122.0 in stage 11.0 (TID 739, localhost, ANY, 1823 bytes)
15/08/06 17:54:34 INFO Executor: Finished task 108.0 in stage 11.0 (TID 725). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Running task 122.0 in stage 11.0 (TID 739)
15/08/06 17:54:34 INFO Executor: Finished task 106.0 in stage 11.0 (TID 723). 2167 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:34 INFO TaskSetManager: Finished task 109.0 in stage 11.0 (TID 726) in 141 ms on localhost (107/200)
15/08/06 17:54:34 INFO Executor: Finished task 110.0 in stage 11.0 (TID 727). 2149 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 123.0 in stage 11.0 (TID 740, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 123.0 in stage 11.0 (TID 740)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 108.0 in stage 11.0 (TID 725) in 156 ms on localhost (108/200)
15/08/06 17:54:34 INFO Executor: Finished task 107.0 in stage 11.0 (TID 724). 2113 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 124.0 in stage 11.0 (TID 741, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO TaskSetManager: Starting task 125.0 in stage 11.0 (TID 742, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 124.0 in stage 11.0 (TID 741)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Running task 125.0 in stage 11.0 (TID 742)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 126.0 in stage 11.0 (TID 743, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 126.0 in stage 11.0 (TID 743)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 110.0 in stage 11.0 (TID 727) in 146 ms on localhost (109/200)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 106.0 in stage 11.0 (TID 723) in 235 ms on localhost (110/200)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 107.0 in stage 11.0 (TID 724) in 229 ms on localhost (111/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 125
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 149
15/08/06 17:54:34 INFO Executor: Finished task 111.0 in stage 11.0 (TID 728). 2002 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 127.0 in stage 11.0 (TID 744, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 127.0 in stage 11.0 (TID 744)
15/08/06 17:54:34 INFO Executor: Finished task 112.0 in stage 11.0 (TID 729). 2055 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Finished task 111.0 in stage 11.0 (TID 728) in 190 ms on localhost (112/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 128.0 in stage 11.0 (TID 745, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 128.0 in stage 11.0 (TID 745)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 112.0 in stage 11.0 (TID 729) in 182 ms on localhost (113/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:54:34 INFO Executor: Finished task 113.0 in stage 11.0 (TID 730). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 129.0 in stage 11.0 (TID 746, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 129.0 in stage 11.0 (TID 746)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 113.0 in stage 11.0 (TID 730) in 202 ms on localhost (114/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 175
15/08/06 17:54:34 INFO Executor: Finished task 122.0 in stage 11.0 (TID 739). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 130.0 in stage 11.0 (TID 747, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 130.0 in stage 11.0 (TID 747)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Finished task 122.0 in stage 11.0 (TID 739) in 133 ms on localhost (115/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 135
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO Executor: Finished task 116.0 in stage 11.0 (TID 733). 2056 bytes result sent to driver
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Starting task 131.0 in stage 11.0 (TID 748, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:54:34 INFO TaskSetManager: Finished task 116.0 in stage 11.0 (TID 733) in 176 ms on localhost (116/200)
15/08/06 17:54:34 INFO Executor: Finished task 114.0 in stage 11.0 (TID 731). 2095 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 132.0 in stage 11.0 (TID 749, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 131.0 in stage 11.0 (TID 748)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Running task 132.0 in stage 11.0 (TID 749)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 114.0 in stage 11.0 (TID 731) in 192 ms on localhost (117/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Finished task 119.0 in stage 11.0 (TID 736). 2095 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 133.0 in stage 11.0 (TID 750, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 119.0 in stage 11.0 (TID 736) in 173 ms on localhost (118/200)
15/08/06 17:54:34 INFO Executor: Running task 133.0 in stage 11.0 (TID 750)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:34 INFO Executor: Finished task 118.0 in stage 11.0 (TID 735). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:54:34 INFO TaskSetManager: Starting task 134.0 in stage 11.0 (TID 751, localhost, ANY, 1827 bytes)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 118.0 in stage 11.0 (TID 735) in 187 ms on localhost (119/200)
15/08/06 17:54:34 INFO Executor: Running task 134.0 in stage 11.0 (TID 751)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 180
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Finished task 123.0 in stage 11.0 (TID 740). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 135.0 in stage 11.0 (TID 752, localhost, ANY, 1827 bytes)
15/08/06 17:54:34 INFO Executor: Running task 135.0 in stage 11.0 (TID 752)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO TaskSetManager: Finished task 123.0 in stage 11.0 (TID 740) in 162 ms on localhost (120/200)
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 128
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 169
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 189
15/08/06 17:54:34 INFO Executor: Finished task 126.0 in stage 11.0 (TID 743). 2094 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 115.0 in stage 11.0 (TID 732). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 120.0 in stage 11.0 (TID 737). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 117.0 in stage 11.0 (TID 734). 2113 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 136.0 in stage 11.0 (TID 753, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 136.0 in stage 11.0 (TID 753)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 126.0 in stage 11.0 (TID 743) in 172 ms on localhost (121/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 137.0 in stage 11.0 (TID 754, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 115.0 in stage 11.0 (TID 732) in 218 ms on localhost (122/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 138.0 in stage 11.0 (TID 755, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 137.0 in stage 11.0 (TID 754)
15/08/06 17:54:34 INFO Executor: Running task 138.0 in stage 11.0 (TID 755)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 120.0 in stage 11.0 (TID 737) in 197 ms on localhost (123/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Starting task 139.0 in stage 11.0 (TID 756, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Running task 139.0 in stage 11.0 (TID 756)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 117.0 in stage 11.0 (TID 734) in 216 ms on localhost (124/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 148
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 123
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Finished task 125.0 in stage 11.0 (TID 742). 2002 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 124.0 in stage 11.0 (TID 741). 2130 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:54:34 INFO TaskSetManager: Starting task 140.0 in stage 11.0 (TID 757, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 140.0 in stage 11.0 (TID 757)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 125.0 in stage 11.0 (TID 742) in 183 ms on localhost (125/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 141.0 in stage 11.0 (TID 758, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 141.0 in stage 11.0 (TID 758)
15/08/06 17:54:34 INFO Executor: Finished task 121.0 in stage 11.0 (TID 738). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO TaskSetManager: Finished task 124.0 in stage 11.0 (TID 741) in 186 ms on localhost (126/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO TaskSetManager: Starting task 142.0 in stage 11.0 (TID 759, localhost, ANY, 1823 bytes)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 121.0 in stage 11.0 (TID 738) in 204 ms on localhost (127/200)
15/08/06 17:54:34 INFO Executor: Running task 142.0 in stage 11.0 (TID 759)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:54:34 INFO Executor: Finished task 127.0 in stage 11.0 (TID 744). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 143.0 in stage 11.0 (TID 760, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 143.0 in stage 11.0 (TID 760)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 127.0 in stage 11.0 (TID 744) in 160 ms on localhost (128/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Finished task 128.0 in stage 11.0 (TID 745). 2149 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 144.0 in stage 11.0 (TID 761, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 144.0 in stage 11.0 (TID 761)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 128.0 in stage 11.0 (TID 745) in 165 ms on localhost (129/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:54:34 INFO Executor: Finished task 129.0 in stage 11.0 (TID 746). 2095 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 145.0 in stage 11.0 (TID 762, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 145.0 in stage 11.0 (TID 762)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 129.0 in stage 11.0 (TID 746) in 158 ms on localhost (130/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/06 17:54:34 INFO Executor: Finished task 131.0 in stage 11.0 (TID 748). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 133.0 in stage 11.0 (TID 750). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 146.0 in stage 11.0 (TID 763, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 131.0 in stage 11.0 (TID 748) in 143 ms on localhost (131/200)
15/08/06 17:54:34 INFO Executor: Running task 146.0 in stage 11.0 (TID 763)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 147.0 in stage 11.0 (TID 764, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 147.0 in stage 11.0 (TID 764)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 133.0 in stage 11.0 (TID 750) in 137 ms on localhost (132/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO Executor: Finished task 130.0 in stage 11.0 (TID 747). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:54:34 INFO TaskSetManager: Starting task 148.0 in stage 11.0 (TID 765, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 148.0 in stage 11.0 (TID 765)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 130.0 in stage 11.0 (TID 747) in 171 ms on localhost (133/200)
15/08/06 17:54:34 INFO Executor: Finished task 136.0 in stage 11.0 (TID 753). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 149.0 in stage 11.0 (TID 766, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 149.0 in stage 11.0 (TID 766)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 136.0 in stage 11.0 (TID 753) in 128 ms on localhost (134/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 141
15/08/06 17:54:34 INFO Executor: Finished task 134.0 in stage 11.0 (TID 751). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 135.0 in stage 11.0 (TID 752). 2074 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 150.0 in stage 11.0 (TID 767, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 150.0 in stage 11.0 (TID 767)
15/08/06 17:54:34 INFO Executor: Finished task 139.0 in stage 11.0 (TID 756). 2002 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Finished task 134.0 in stage 11.0 (TID 751) in 172 ms on localhost (135/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 151.0 in stage 11.0 (TID 768, localhost, ANY, 1823 bytes)
15/08/06 17:54:34 INFO Executor: Running task 151.0 in stage 11.0 (TID 768)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 135.0 in stage 11.0 (TID 752) in 163 ms on localhost (136/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Starting task 152.0 in stage 11.0 (TID 769, localhost, ANY, 1827 bytes)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Running task 152.0 in stage 11.0 (TID 769)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:54:34 INFO TaskSetManager: Finished task 139.0 in stage 11.0 (TID 756) in 146 ms on localhost (137/200)
15/08/06 17:54:34 INFO Executor: Finished task 132.0 in stage 11.0 (TID 749). 2002 bytes result sent to driver
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Starting task 153.0 in stage 11.0 (TID 770, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 153.0 in stage 11.0 (TID 770)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 132.0 in stage 11.0 (TID 749) in 189 ms on localhost (138/200)
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 113
15/08/06 17:54:34 INFO Executor: Finished task 138.0 in stage 11.0 (TID 755). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 154.0 in stage 11.0 (TID 771, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO Executor: Running task 154.0 in stage 11.0 (TID 771)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 138.0 in stage 11.0 (TID 755) in 156 ms on localhost (139/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO Executor: Finished task 142.0 in stage 11.0 (TID 759). 2038 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 155.0 in stage 11.0 (TID 772, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 155.0 in stage 11.0 (TID 772)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 142.0 in stage 11.0 (TID 759) in 159 ms on localhost (140/200)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 111
15/08/06 17:54:34 INFO Executor: Finished task 141.0 in stage 11.0 (TID 758). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 156.0 in stage 11.0 (TID 773, localhost, ANY, 1827 bytes)
15/08/06 17:54:34 INFO Executor: Running task 156.0 in stage 11.0 (TID 773)
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:54:34 INFO TaskSetManager: Finished task 141.0 in stage 11.0 (TID 758) in 171 ms on localhost (141/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Finished task 140.0 in stage 11.0 (TID 757). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 157.0 in stage 11.0 (TID 774, localhost, ANY, 1827 bytes)
15/08/06 17:54:34 INFO Executor: Running task 157.0 in stage 11.0 (TID 774)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:54:34 INFO TaskSetManager: Finished task 140.0 in stage 11.0 (TID 757) in 185 ms on localhost (142/200)
15/08/06 17:54:34 INFO Executor: Finished task 137.0 in stage 11.0 (TID 754). 2113 bytes result sent to driver
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO TaskSetManager: Starting task 158.0 in stage 11.0 (TID 775, localhost, ANY, 1824 bytes)
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:34 INFO Executor: Running task 158.0 in stage 11.0 (TID 775)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 137.0 in stage 11.0 (TID 754) in 207 ms on localhost (143/200)
15/08/06 17:54:34 INFO Executor: Finished task 144.0 in stage 11.0 (TID 761). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 159.0 in stage 11.0 (TID 776, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 159.0 in stage 11.0 (TID 776)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 144.0 in stage 11.0 (TID 761) in 169 ms on localhost (144/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 103
15/08/06 17:54:34 INFO Executor: Finished task 143.0 in stage 11.0 (TID 760). 2056 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 145.0 in stage 11.0 (TID 762). 2095 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 160.0 in stage 11.0 (TID 777, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO Executor: Running task 160.0 in stage 11.0 (TID 777)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 143.0 in stage 11.0 (TID 760) in 198 ms on localhost (145/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 161.0 in stage 11.0 (TID 778, localhost, ANY, 1827 bytes)
15/08/06 17:54:34 INFO Executor: Running task 161.0 in stage 11.0 (TID 778)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 145.0 in stage 11.0 (TID 762) in 170 ms on localhost (146/200)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:54:34 INFO Executor: Finished task 146.0 in stage 11.0 (TID 763). 2113 bytes result sent to driver
15/08/06 17:54:34 INFO Executor: Finished task 147.0 in stage 11.0 (TID 764). 2020 bytes result sent to driver
15/08/06 17:54:34 INFO TaskSetManager: Starting task 162.0 in stage 11.0 (TID 779, localhost, ANY, 1826 bytes)
15/08/06 17:54:34 INFO Executor: Running task 162.0 in stage 11.0 (TID 779)
15/08/06 17:54:34 INFO TaskSetManager: Finished task 146.0 in stage 11.0 (TID 763) in 152 ms on localhost (147/200)
15/08/06 17:54:34 INFO TaskSetManager: Starting task 163.0 in stage 11.0 (TID 780, localhost, ANY, 1825 bytes)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO Executor: Running task 163.0 in stage 11.0 (TID 780)
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:34 INFO TaskSetManager: Finished task 147.0 in stage 11.0 (TID 764) in 155 ms on localhost (148/200)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 113
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO Executor: Finished task 152.0 in stage 11.0 (TID 769). 1913 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 164.0 in stage 11.0 (TID 781, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 164.0 in stage 11.0 (TID 781)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 152.0 in stage 11.0 (TID 769) in 213 ms on localhost (149/200)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO Executor: Finished task 148.0 in stage 11.0 (TID 765). 2056 bytes result sent to driver
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO TaskSetManager: Starting task 165.0 in stage 11.0 (TID 782, localhost, ANY, 1827 bytes)
15/08/06 17:54:35 INFO Executor: Running task 165.0 in stage 11.0 (TID 782)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 108
15/08/06 17:54:35 INFO TaskSetManager: Finished task 148.0 in stage 11.0 (TID 765) in 248 ms on localhost (150/200)
15/08/06 17:54:35 INFO Executor: Finished task 149.0 in stage 11.0 (TID 766). 2038 bytes result sent to driver
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO TaskSetManager: Starting task 166.0 in stage 11.0 (TID 783, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 166.0 in stage 11.0 (TID 783)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 149.0 in stage 11.0 (TID 766) in 250 ms on localhost (151/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 163
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO Executor: Finished task 153.0 in stage 11.0 (TID 770). 2056 bytes result sent to driver
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO TaskSetManager: Starting task 167.0 in stage 11.0 (TID 784, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 167.0 in stage 11.0 (TID 784)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO Executor: Finished task 150.0 in stage 11.0 (TID 767). 2038 bytes result sent to driver
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO TaskSetManager: Finished task 153.0 in stage 11.0 (TID 770) in 228 ms on localhost (152/200)
15/08/06 17:54:35 INFO Executor: Finished task 154.0 in stage 11.0 (TID 771). 2130 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 168.0 in stage 11.0 (TID 785, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Finished task 150.0 in stage 11.0 (TID 767) in 238 ms on localhost (153/200)
15/08/06 17:54:35 INFO Executor: Running task 168.0 in stage 11.0 (TID 785)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 169.0 in stage 11.0 (TID 786, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 169.0 in stage 11.0 (TID 786)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Finished task 154.0 in stage 11.0 (TID 771) in 225 ms on localhost (154/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 190
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 149
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO Executor: Finished task 151.0 in stage 11.0 (TID 768). 2148 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 170.0 in stage 11.0 (TID 787, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 170.0 in stage 11.0 (TID 787)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Finished task 151.0 in stage 11.0 (TID 768) in 245 ms on localhost (155/200)
15/08/06 17:54:35 INFO Executor: Finished task 155.0 in stage 11.0 (TID 772). 2074 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 171.0 in stage 11.0 (TID 788, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 171.0 in stage 11.0 (TID 788)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 172
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO Executor: Finished task 159.0 in stage 11.0 (TID 776). 2056 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Finished task 155.0 in stage 11.0 (TID 772) in 222 ms on localhost (156/200)
15/08/06 17:54:35 INFO Executor: Finished task 156.0 in stage 11.0 (TID 773). 2056 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 172.0 in stage 11.0 (TID 789, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 172.0 in stage 11.0 (TID 789)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Starting task 173.0 in stage 11.0 (TID 790, localhost, ANY, 1823 bytes)
15/08/06 17:54:35 INFO Executor: Running task 173.0 in stage 11.0 (TID 790)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:54:35 INFO TaskSetManager: Finished task 159.0 in stage 11.0 (TID 776) in 190 ms on localhost (157/200)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 156.0 in stage 11.0 (TID 773) in 218 ms on localhost (158/200)
15/08/06 17:54:35 INFO Executor: Finished task 157.0 in stage 11.0 (TID 774). 2113 bytes result sent to driver
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Starting task 174.0 in stage 11.0 (TID 791, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 157.0 in stage 11.0 (TID 774) in 209 ms on localhost (159/200)
15/08/06 17:54:35 INFO Executor: Running task 174.0 in stage 11.0 (TID 791)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 116
15/08/06 17:54:35 INFO Executor: Finished task 158.0 in stage 11.0 (TID 775). 2074 bytes result sent to driver
15/08/06 17:54:35 INFO Executor: Finished task 161.0 in stage 11.0 (TID 778). 2038 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 175.0 in stage 11.0 (TID 792, localhost, ANY, 1823 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 158.0 in stage 11.0 (TID 775) in 216 ms on localhost (160/200)
15/08/06 17:54:35 INFO Executor: Running task 175.0 in stage 11.0 (TID 792)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO TaskSetManager: Starting task 176.0 in stage 11.0 (TID 793, localhost, ANY, 1824 bytes)
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO Executor: Running task 176.0 in stage 11.0 (TID 793)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Finished task 161.0 in stage 11.0 (TID 778) in 192 ms on localhost (161/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/06 17:54:35 INFO Executor: Finished task 160.0 in stage 11.0 (TID 777). 2020 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 177.0 in stage 11.0 (TID 794, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 177.0 in stage 11.0 (TID 794)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 160.0 in stage 11.0 (TID 777) in 206 ms on localhost (162/200)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 110
15/08/06 17:54:35 INFO Executor: Finished task 163.0 in stage 11.0 (TID 780). 1913 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 178.0 in stage 11.0 (TID 795, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 178.0 in stage 11.0 (TID 795)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Finished task 163.0 in stage 11.0 (TID 780) in 207 ms on localhost (163/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 136
15/08/06 17:54:35 INFO Executor: Finished task 162.0 in stage 11.0 (TID 779). 2094 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 179.0 in stage 11.0 (TID 796, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 179.0 in stage 11.0 (TID 796)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 162.0 in stage 11.0 (TID 779) in 221 ms on localhost (164/200)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 155
15/08/06 17:54:35 INFO Executor: Finished task 164.0 in stage 11.0 (TID 781). 2111 bytes result sent to driver
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Starting task 180.0 in stage 11.0 (TID 797, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 180.0 in stage 11.0 (TID 797)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 164.0 in stage 11.0 (TID 781) in 155 ms on localhost (165/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 142
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 INFO Executor: Finished task 165.0 in stage 11.0 (TID 782). 2094 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 181.0 in stage 11.0 (TID 798, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 165.0 in stage 11.0 (TID 782) in 151 ms on localhost (166/200)
15/08/06 17:54:35 INFO Executor: Running task 181.0 in stage 11.0 (TID 798)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 114
15/08/06 17:54:35 INFO Executor: Finished task 169.0 in stage 11.0 (TID 786). 2020 bytes result sent to driver
15/08/06 17:54:35 INFO Executor: Finished task 167.0 in stage 11.0 (TID 784). 2094 bytes result sent to driver
15/08/06 17:54:35 INFO Executor: Finished task 166.0 in stage 11.0 (TID 783). 2073 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 182.0 in stage 11.0 (TID 799, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 182.0 in stage 11.0 (TID 799)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 169.0 in stage 11.0 (TID 786) in 156 ms on localhost (167/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO TaskSetManager: Starting task 183.0 in stage 11.0 (TID 800, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO Executor: Running task 183.0 in stage 11.0 (TID 800)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:54:35 INFO Executor: Finished task 168.0 in stage 11.0 (TID 785). 2002 bytes result sent to driver
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Finished task 166.0 in stage 11.0 (TID 783) in 166 ms on localhost (168/200)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO TaskSetManager: Starting task 184.0 in stage 11.0 (TID 801, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Finished task 167.0 in stage 11.0 (TID 784) in 163 ms on localhost (169/200)
15/08/06 17:54:35 INFO Executor: Running task 184.0 in stage 11.0 (TID 801)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 185.0 in stage 11.0 (TID 802, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 185.0 in stage 11.0 (TID 802)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO Executor: Finished task 173.0 in stage 11.0 (TID 790). 1913 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Finished task 168.0 in stage 11.0 (TID 785) in 163 ms on localhost (170/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Starting task 186.0 in stage 11.0 (TID 803, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 186.0 in stage 11.0 (TID 803)
15/08/06 17:54:35 INFO Executor: Finished task 170.0 in stage 11.0 (TID 787). 2131 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Finished task 173.0 in stage 11.0 (TID 790) in 148 ms on localhost (171/200)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:35 INFO TaskSetManager: Starting task 187.0 in stage 11.0 (TID 804, localhost, ANY, 1823 bytes)
15/08/06 17:54:35 INFO Executor: Running task 187.0 in stage 11.0 (TID 804)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO TaskSetManager: Finished task 170.0 in stage 11.0 (TID 787) in 158 ms on localhost (172/200)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO Executor: Finished task 174.0 in stage 11.0 (TID 791). 2130 bytes result sent to driver
15/08/06 17:54:35 INFO Executor: Finished task 171.0 in stage 11.0 (TID 788). 2131 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 188.0 in stage 11.0 (TID 805, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 188.0 in stage 11.0 (TID 805)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Finished task 174.0 in stage 11.0 (TID 791) in 159 ms on localhost (173/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 197
15/08/06 17:54:35 INFO TaskSetManager: Starting task 189.0 in stage 11.0 (TID 806, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 189.0 in stage 11.0 (TID 806)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 171.0 in stage 11.0 (TID 788) in 177 ms on localhost (174/200)
15/08/06 17:54:35 INFO Executor: Finished task 176.0 in stage 11.0 (TID 793). 2095 bytes result sent to driver
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Starting task 190.0 in stage 11.0 (TID 807, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Running task 190.0 in stage 11.0 (TID 807)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Finished task 176.0 in stage 11.0 (TID 793) in 148 ms on localhost (175/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 186
15/08/06 17:54:35 INFO Executor: Finished task 177.0 in stage 11.0 (TID 794). 2038 bytes result sent to driver
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 129
15/08/06 17:54:35 INFO TaskSetManager: Starting task 191.0 in stage 11.0 (TID 808, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO Executor: Finished task 172.0 in stage 11.0 (TID 789). 2095 bytes result sent to driver
15/08/06 17:54:35 INFO Executor: Running task 191.0 in stage 11.0 (TID 808)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 177.0 in stage 11.0 (TID 794) in 141 ms on localhost (176/200)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO TaskSetManager: Starting task 192.0 in stage 11.0 (TID 809, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 192.0 in stage 11.0 (TID 809)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO TaskSetManager: Finished task 172.0 in stage 11.0 (TID 789) in 181 ms on localhost (177/200)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:54:35 INFO Executor: Finished task 175.0 in stage 11.0 (TID 792). 2113 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 193.0 in stage 11.0 (TID 810, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 193.0 in stage 11.0 (TID 810)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Finished task 175.0 in stage 11.0 (TID 792) in 171 ms on localhost (178/200)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 246
15/08/06 17:54:35 INFO Executor: Finished task 178.0 in stage 11.0 (TID 795). 2185 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 194.0 in stage 11.0 (TID 811, localhost, ANY, 1825 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 178.0 in stage 11.0 (TID 795) in 159 ms on localhost (179/200)
15/08/06 17:54:35 INFO Executor: Running task 194.0 in stage 11.0 (TID 811)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:35 INFO Executor: Finished task 179.0 in stage 11.0 (TID 796). 2020 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 195.0 in stage 11.0 (TID 812, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 195.0 in stage 11.0 (TID 812)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 179.0 in stage 11.0 (TID 796) in 162 ms on localhost (180/200)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:54:35 INFO Executor: Finished task 180.0 in stage 11.0 (TID 797). 2056 bytes result sent to driver
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO TaskSetManager: Starting task 196.0 in stage 11.0 (TID 813, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO Executor: Running task 196.0 in stage 11.0 (TID 813)
15/08/06 17:54:35 INFO Executor: Finished task 181.0 in stage 11.0 (TID 798). 2038 bytes result sent to driver
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Finished task 180.0 in stage 11.0 (TID 797) in 163 ms on localhost (181/200)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 197.0 in stage 11.0 (TID 814, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 197.0 in stage 11.0 (TID 814)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 181.0 in stage 11.0 (TID 798) in 166 ms on localhost (182/200)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:54:35 INFO Executor: Finished task 183.0 in stage 11.0 (TID 800). 2113 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 198.0 in stage 11.0 (TID 815, localhost, ANY, 1824 bytes)
15/08/06 17:54:35 INFO Executor: Running task 198.0 in stage 11.0 (TID 815)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO TaskSetManager: Finished task 183.0 in stage 11.0 (TID 800) in 157 ms on localhost (183/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 130
15/08/06 17:54:35 INFO Executor: Finished task 185.0 in stage 11.0 (TID 802). 2038 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Starting task 199.0 in stage 11.0 (TID 816, localhost, ANY, 1826 bytes)
15/08/06 17:54:35 INFO Executor: Running task 199.0 in stage 11.0 (TID 816)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 185.0 in stage 11.0 (TID 802) in 170 ms on localhost (184/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO Executor: Finished task 191.0 in stage 11.0 (TID 808). 2112 bytes result sent to driver
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO TaskSetManager: Finished task 191.0 in stage 11.0 (TID 808) in 144 ms on localhost (185/200)
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO Executor: Finished task 184.0 in stage 11.0 (TID 801). 2020 bytes result sent to driver
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:54:35 INFO TaskSetManager: Finished task 184.0 in stage 11.0 (TID 801) in 188 ms on localhost (186/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO Executor: Finished task 192.0 in stage 11.0 (TID 809). 2020 bytes result sent to driver
15/08/06 17:54:35 INFO Executor: Finished task 190.0 in stage 11.0 (TID 807). 2038 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Finished task 192.0 in stage 11.0 (TID 809) in 155 ms on localhost (187/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Finished task 190.0 in stage 11.0 (TID 807) in 162 ms on localhost (188/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO Executor: Finished task 189.0 in stage 11.0 (TID 806). 2055 bytes result sent to driver
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/06 17:54:35 INFO TaskSetManager: Finished task 189.0 in stage 11.0 (TID 806) in 167 ms on localhost (189/200)
15/08/06 17:54:35 INFO Executor: Finished task 182.0 in stage 11.0 (TID 799). 2056 bytes result sent to driver
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO TaskSetManager: Finished task 182.0 in stage 11.0 (TID 799) in 201 ms on localhost (190/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 194
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO Executor: Finished task 187.0 in stage 11.0 (TID 804). 2074 bytes result sent to driver
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:54:35 INFO TaskSetManager: Finished task 187.0 in stage 11.0 (TID 804) in 197 ms on localhost (191/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:35 INFO Executor: Finished task 188.0 in stage 11.0 (TID 805). 2002 bytes result sent to driver
15/08/06 17:54:35 INFO Executor: Finished task 186.0 in stage 11.0 (TID 803). 2074 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Finished task 188.0 in stage 11.0 (TID 805) in 191 ms on localhost (192/200)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 186.0 in stage 11.0 (TID 803) in 205 ms on localhost (193/200)
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:54:35 INFO Executor: Finished task 193.0 in stage 11.0 (TID 810). 2037 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Finished task 193.0 in stage 11.0 (TID 810) in 163 ms on localhost (194/200)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 150
15/08/06 17:54:35 INFO Executor: Finished task 195.0 in stage 11.0 (TID 812). 2074 bytes result sent to driver
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO TaskSetManager: Finished task 195.0 in stage 11.0 (TID 812) in 155 ms on localhost (195/200)
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/06 17:54:35 INFO Executor: Finished task 194.0 in stage 11.0 (TID 811). 2054 bytes result sent to driver
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:35 INFO TaskSetManager: Finished task 194.0 in stage 11.0 (TID 811) in 174 ms on localhost (196/200)
15/08/06 17:54:35 INFO Executor: Finished task 197.0 in stage 11.0 (TID 814). 2056 bytes result sent to driver
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO TaskSetManager: Finished task 197.0 in stage 11.0 (TID 814) in 113 ms on localhost (197/200)
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:54:35 INFO Executor: Finished task 196.0 in stage 11.0 (TID 813). 2113 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Finished task 196.0 in stage 11.0 (TID 813) in 135 ms on localhost (198/200)
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:54:35 INFO Executor: Finished task 198.0 in stage 11.0 (TID 815). 2095 bytes result sent to driver
15/08/06 17:54:35 INFO Executor: Finished task 199.0 in stage 11.0 (TID 816). 2131 bytes result sent to driver
15/08/06 17:54:35 INFO TaskSetManager: Finished task 198.0 in stage 11.0 (TID 815) in 125 ms on localhost (199/200)
15/08/06 17:54:35 INFO TaskSetManager: Finished task 199.0 in stage 11.0 (TID 816) in 109 ms on localhost (200/200)
15/08/06 17:54:35 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/08/06 17:54:35 INFO DAGScheduler: Stage 11 (RangePartitioner at Exchange.scala:88) finished in 2.524 s
15/08/06 17:54:35 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@71c95903
15/08/06 17:54:35 INFO DAGScheduler: Job 7 finished: RangePartitioner at Exchange.scala:88, took 3.503599 s
15/08/06 17:54:35 INFO StatsReportListener: task runtime:(count: 200, mean: 197.955000, stdev: 43.217161, max: 310.000000, min: 109.000000)
15/08/06 17:54:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:35 INFO StatsReportListener: 	109.0 ms	140.0 ms	150.0 ms	163.0 ms	189.0 ms	232.0 ms	261.0 ms	278.0 ms	310.0 ms
15/08/06 17:54:35 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.265000, stdev: 0.595630, max: 4.000000, min: 0.000000)
15/08/06 17:54:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:35 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	4.0 ms
15/08/06 17:54:35 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:54:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:35 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:54:35 INFO StatsReportListener: task result size:(count: 200, mean: 2066.055000, stdev: 46.469151, max: 2237.000000, min: 1913.000000)
15/08/06 17:54:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:35 INFO StatsReportListener: 	1913.0 B	2002.0 B	2019.0 B	2037.0 B	2.0 KB	2.0 KB	2.1 KB	2.1 KB	2.2 KB
15/08/06 17:54:35 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 96.175885, stdev: 1.973453, max: 99.173554, min: 88.345865)
15/08/06 17:54:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:35 INFO StatsReportListener: 	88 %	92 %	93 %	95 %	97 %	98 %	98 %	98 %	99 %
15/08/06 17:54:35 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.143873, stdev: 0.321556, max: 2.222222, min: 0.000000)
15/08/06 17:54:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:35 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 2 %
15/08/06 17:54:35 INFO StatsReportListener: other time pct: (count: 200, mean: 3.680242, stdev: 1.983142, max: 11.654135, min: 0.826446)
15/08/06 17:54:35 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:35 INFO StatsReportListener: 	 1 %	 1 %	 2 %	 2 %	 3 %	 5 %	 7 %	 8 %	12 %
15/08/06 17:54:35 INFO DefaultExecutionContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/08/06 17:54:35 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 203 bytes
15/08/06 17:54:35 INFO DAGScheduler: Registering RDD 75 (mapPartitions at Exchange.scala:77)
15/08/06 17:54:35 INFO DAGScheduler: Got job 8 (runJob at InsertIntoHiveTable.scala:93) with 200 output partitions (allowLocal=false)
15/08/06 17:54:35 INFO DAGScheduler: Final stage: Stage 14(runJob at InsertIntoHiveTable.scala:93)
15/08/06 17:54:35 INFO DAGScheduler: Parents of final stage: List(Stage 13)
15/08/06 17:54:35 INFO DAGScheduler: Missing parents: List(Stage 13)
15/08/06 17:54:35 INFO DAGScheduler: Submitting Stage 13 (MapPartitionsRDD[75] at mapPartitions at Exchange.scala:77), which has no missing parents
15/08/06 17:54:35 INFO MemoryStore: ensureFreeSpace(16352) called with curMem=1606775, maxMem=3333968363
15/08/06 17:54:35 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 16.0 KB, free 3.1 GB)
15/08/06 17:54:35 INFO MemoryStore: ensureFreeSpace(9186) called with curMem=1623127, maxMem=3333968363
15/08/06 17:54:35 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 9.0 KB, free 3.1 GB)
15/08/06 17:54:35 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:37948 (size: 9.0 KB, free: 3.1 GB)
15/08/06 17:54:35 INFO BlockManagerMaster: Updated info of block broadcast_18_piece0
15/08/06 17:54:35 INFO DefaultExecutionContext: Created broadcast 18 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:35 INFO DAGScheduler: Submitting 200 missing tasks from Stage 13 (MapPartitionsRDD[75] at mapPartitions at Exchange.scala:77)
15/08/06 17:54:35 INFO TaskSchedulerImpl: Adding task set 13.0 with 200 tasks
15/08/06 17:54:35 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 817, localhost, ANY, 1810 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 818, localhost, ANY, 1812 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 819, localhost, ANY, 1813 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 820, localhost, ANY, 1812 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 821, localhost, ANY, 1814 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 822, localhost, ANY, 1813 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 823, localhost, ANY, 1813 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 824, localhost, ANY, 1814 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 825, localhost, ANY, 1813 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 826, localhost, ANY, 1810 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 827, localhost, ANY, 1815 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 828, localhost, ANY, 1814 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 829, localhost, ANY, 1814 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 830, localhost, ANY, 1813 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 14.0 in stage 13.0 (TID 831, localhost, ANY, 1814 bytes)
15/08/06 17:54:35 INFO TaskSetManager: Starting task 15.0 in stage 13.0 (TID 832, localhost, ANY, 1814 bytes)
15/08/06 17:54:35 INFO Executor: Running task 1.0 in stage 13.0 (TID 818)
15/08/06 17:54:35 INFO Executor: Running task 0.0 in stage 13.0 (TID 817)
15/08/06 17:54:35 INFO Executor: Running task 2.0 in stage 13.0 (TID 819)
15/08/06 17:54:35 INFO Executor: Running task 4.0 in stage 13.0 (TID 821)
15/08/06 17:54:35 INFO Executor: Running task 3.0 in stage 13.0 (TID 820)
15/08/06 17:54:35 INFO Executor: Running task 5.0 in stage 13.0 (TID 822)
15/08/06 17:54:35 INFO Executor: Running task 6.0 in stage 13.0 (TID 823)
15/08/06 17:54:35 INFO Executor: Running task 7.0 in stage 13.0 (TID 824)
15/08/06 17:54:35 INFO Executor: Running task 10.0 in stage 13.0 (TID 827)
15/08/06 17:54:35 INFO Executor: Running task 8.0 in stage 13.0 (TID 825)
15/08/06 17:54:35 INFO Executor: Running task 9.0 in stage 13.0 (TID 826)
15/08/06 17:54:35 INFO Executor: Running task 11.0 in stage 13.0 (TID 828)
15/08/06 17:54:35 INFO Executor: Running task 12.0 in stage 13.0 (TID 829)
15/08/06 17:54:35 INFO Executor: Running task 13.0 in stage 13.0 (TID 830)
15/08/06 17:54:35 INFO Executor: Running task 15.0 in stage 13.0 (TID 832)
15/08/06 17:54:35 INFO Executor: Running task 14.0 in stage 13.0 (TID 831)
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 144
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 140
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 171
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 177
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 182
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 193
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:54:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:54:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/06 17:54:36 INFO Executor: Finished task 1.0 in stage 13.0 (TID 818). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 16.0 in stage 13.0 (TID 833, localhost, ANY, 1815 bytes)
15/08/06 17:54:36 INFO Executor: Running task 16.0 in stage 13.0 (TID 833)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 818) in 574 ms on localhost (1/200)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:54:36 INFO Executor: Finished task 9.0 in stage 13.0 (TID 826). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 17.0 in stage 13.0 (TID 834, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 17.0 in stage 13.0 (TID 834)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 826) in 704 ms on localhost (2/200)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO Executor: Finished task 13.0 in stage 13.0 (TID 830). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 18.0 in stage 13.0 (TID 835, localhost, ANY, 1815 bytes)
15/08/06 17:54:36 INFO Executor: Running task 18.0 in stage 13.0 (TID 835)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 830) in 744 ms on localhost (3/200)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:54:36 INFO Executor: Finished task 4.0 in stage 13.0 (TID 821). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO Executor: Finished task 14.0 in stage 13.0 (TID 831). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 19.0 in stage 13.0 (TID 836, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 19.0 in stage 13.0 (TID 836)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 821) in 837 ms on localhost (4/200)
15/08/06 17:54:36 INFO TaskSetManager: Starting task 20.0 in stage 13.0 (TID 837, localhost, ANY, 1814 bytes)
15/08/06 17:54:36 INFO Executor: Running task 20.0 in stage 13.0 (TID 837)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 14.0 in stage 13.0 (TID 831) in 835 ms on localhost (5/200)
15/08/06 17:54:36 INFO Executor: Finished task 3.0 in stage 13.0 (TID 820). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO Executor: Finished task 10.0 in stage 13.0 (TID 827). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 21.0 in stage 13.0 (TID 838, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 21.0 in stage 13.0 (TID 838)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 820) in 844 ms on localhost (6/200)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO TaskSetManager: Starting task 22.0 in stage 13.0 (TID 839, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 22.0 in stage 13.0 (TID 839)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 827) in 843 ms on localhost (7/200)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO Executor: Finished task 7.0 in stage 13.0 (TID 824). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 23.0 in stage 13.0 (TID 840, localhost, ANY, 1814 bytes)
15/08/06 17:54:36 INFO Executor: Running task 23.0 in stage 13.0 (TID 840)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 824) in 848 ms on localhost (8/200)
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO Executor: Finished task 5.0 in stage 13.0 (TID 822). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 24.0 in stage 13.0 (TID 841, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 24.0 in stage 13.0 (TID 841)
15/08/06 17:54:36 INFO Executor: Finished task 0.0 in stage 13.0 (TID 817). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 822) in 853 ms on localhost (9/200)
15/08/06 17:54:36 INFO TaskSetManager: Starting task 25.0 in stage 13.0 (TID 842, localhost, ANY, 1816 bytes)
15/08/06 17:54:36 INFO Executor: Running task 25.0 in stage 13.0 (TID 842)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 817) in 857 ms on localhost (10/200)
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO Executor: Finished task 15.0 in stage 13.0 (TID 832). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO Executor: Finished task 8.0 in stage 13.0 (TID 825). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO Executor: Finished task 2.0 in stage 13.0 (TID 819). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 26.0 in stage 13.0 (TID 843, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 26.0 in stage 13.0 (TID 843)
15/08/06 17:54:36 INFO Executor: Finished task 6.0 in stage 13.0 (TID 823). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Finished task 15.0 in stage 13.0 (TID 832) in 859 ms on localhost (11/200)
15/08/06 17:54:36 INFO TaskSetManager: Starting task 27.0 in stage 13.0 (TID 844, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 27.0 in stage 13.0 (TID 844)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 825) in 863 ms on localhost (12/200)
15/08/06 17:54:36 INFO TaskSetManager: Starting task 28.0 in stage 13.0 (TID 845, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 28.0 in stage 13.0 (TID 845)
15/08/06 17:54:36 INFO Executor: Finished task 16.0 in stage 13.0 (TID 833). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 819) in 867 ms on localhost (13/200)
15/08/06 17:54:36 INFO TaskSetManager: Starting task 29.0 in stage 13.0 (TID 846, localhost, ANY, 1811 bytes)
15/08/06 17:54:36 INFO Executor: Running task 29.0 in stage 13.0 (TID 846)
15/08/06 17:54:36 INFO Executor: Finished task 12.0 in stage 13.0 (TID 829). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO Executor: Finished task 11.0 in stage 13.0 (TID 828). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 823) in 868 ms on localhost (14/200)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:36 INFO TaskSetManager: Starting task 30.0 in stage 13.0 (TID 847, localhost, ANY, 1814 bytes)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO TaskSetManager: Finished task 16.0 in stage 13.0 (TID 833) in 313 ms on localhost (15/200)
15/08/06 17:54:36 INFO Executor: Running task 30.0 in stage 13.0 (TID 847)
15/08/06 17:54:36 INFO TaskSetManager: Starting task 31.0 in stage 13.0 (TID 848, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 31.0 in stage 13.0 (TID 848)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 829) in 872 ms on localhost (16/200)
15/08/06 17:54:36 INFO TaskSetManager: Starting task 32.0 in stage 13.0 (TID 849, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 INFO Executor: Running task 32.0 in stage 13.0 (TID 849)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 828) in 875 ms on localhost (17/200)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO Executor: Finished task 17.0 in stage 13.0 (TID 834). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 33.0 in stage 13.0 (TID 850, localhost, ANY, 1814 bytes)
15/08/06 17:54:36 INFO Executor: Running task 33.0 in stage 13.0 (TID 850)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 17.0 in stage 13.0 (TID 834) in 283 ms on localhost (18/200)
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 179
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO Executor: Finished task 18.0 in stage 13.0 (TID 835). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 INFO TaskSetManager: Starting task 34.0 in stage 13.0 (TID 851, localhost, ANY, 1813 bytes)
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO Executor: Running task 34.0 in stage 13.0 (TID 851)
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 INFO TaskSetManager: Finished task 18.0 in stage 13.0 (TID 835) in 293 ms on localhost (19/200)
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 151
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 88
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 165
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 149
15/08/06 17:54:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:54:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:54:36 INFO Executor: Finished task 19.0 in stage 13.0 (TID 836). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 35.0 in stage 13.0 (TID 852, localhost, ANY, 1814 bytes)
15/08/06 17:54:36 INFO Executor: Running task 35.0 in stage 13.0 (TID 852)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 19.0 in stage 13.0 (TID 836) in 364 ms on localhost (20/200)
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:36 INFO Executor: Finished task 24.0 in stage 13.0 (TID 841). 2182 bytes result sent to driver
15/08/06 17:54:36 INFO TaskSetManager: Starting task 36.0 in stage 13.0 (TID 853, localhost, ANY, 1815 bytes)
15/08/06 17:54:36 INFO Executor: Running task 36.0 in stage 13.0 (TID 853)
15/08/06 17:54:36 INFO TaskSetManager: Finished task 24.0 in stage 13.0 (TID 841) in 500 ms on localhost (21/200)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:54:37 INFO Executor: Finished task 25.0 in stage 13.0 (TID 842). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 37.0 in stage 13.0 (TID 854, localhost, ANY, 1814 bytes)
15/08/06 17:54:37 INFO Executor: Running task 37.0 in stage 13.0 (TID 854)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 25.0 in stage 13.0 (TID 842) in 712 ms on localhost (22/200)
15/08/06 17:54:37 INFO Executor: Finished task 21.0 in stage 13.0 (TID 838). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:37 INFO Executor: Finished task 20.0 in stage 13.0 (TID 837). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 38.0 in stage 13.0 (TID 855, localhost, ANY, 1814 bytes)
15/08/06 17:54:37 INFO Executor: Running task 38.0 in stage 13.0 (TID 855)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 21.0 in stage 13.0 (TID 838) in 756 ms on localhost (23/200)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO TaskSetManager: Starting task 39.0 in stage 13.0 (TID 856, localhost, ANY, 1815 bytes)
15/08/06 17:54:37 INFO Executor: Running task 39.0 in stage 13.0 (TID 856)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 20.0 in stage 13.0 (TID 837) in 775 ms on localhost (24/200)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO Executor: Finished task 28.0 in stage 13.0 (TID 845). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 40.0 in stage 13.0 (TID 857, localhost, ANY, 1815 bytes)
15/08/06 17:54:37 INFO Executor: Running task 40.0 in stage 13.0 (TID 857)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 28.0 in stage 13.0 (TID 845) in 787 ms on localhost (25/200)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:37 INFO Executor: Finished task 30.0 in stage 13.0 (TID 847). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO Executor: Finished task 27.0 in stage 13.0 (TID 844). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO Executor: Finished task 22.0 in stage 13.0 (TID 839). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO Executor: Finished task 23.0 in stage 13.0 (TID 840). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO Executor: Finished task 29.0 in stage 13.0 (TID 846). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO TaskSetManager: Starting task 41.0 in stage 13.0 (TID 858, localhost, ANY, 1815 bytes)
15/08/06 17:54:37 INFO Executor: Running task 41.0 in stage 13.0 (TID 858)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 30.0 in stage 13.0 (TID 847) in 804 ms on localhost (26/200)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 27.0 in stage 13.0 (TID 844) in 813 ms on localhost (27/200)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO TaskSetManager: Starting task 42.0 in stage 13.0 (TID 859, localhost, ANY, 1816 bytes)
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO Executor: Running task 42.0 in stage 13.0 (TID 859)
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:54:37 INFO TaskSetManager: Starting task 43.0 in stage 13.0 (TID 860, localhost, ANY, 1813 bytes)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 22.0 in stage 13.0 (TID 839) in 842 ms on localhost (28/200)
15/08/06 17:54:37 INFO Executor: Running task 43.0 in stage 13.0 (TID 860)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO TaskSetManager: Finished task 23.0 in stage 13.0 (TID 840) in 840 ms on localhost (29/200)
15/08/06 17:54:37 INFO Executor: Finished task 31.0 in stage 13.0 (TID 848). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:37 INFO TaskSetManager: Starting task 44.0 in stage 13.0 (TID 861, localhost, ANY, 1814 bytes)
15/08/06 17:54:37 INFO Executor: Running task 44.0 in stage 13.0 (TID 861)
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO TaskSetManager: Starting task 45.0 in stage 13.0 (TID 862, localhost, ANY, 1814 bytes)
15/08/06 17:54:37 INFO Executor: Running task 45.0 in stage 13.0 (TID 862)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 29.0 in stage 13.0 (TID 846) in 835 ms on localhost (30/200)
15/08/06 17:54:37 INFO Executor: Finished task 32.0 in stage 13.0 (TID 849). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:37 INFO Executor: Finished task 26.0 in stage 13.0 (TID 843). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:37 INFO Executor: Finished task 34.0 in stage 13.0 (TID 851). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO TaskSetManager: Starting task 46.0 in stage 13.0 (TID 863, localhost, ANY, 1816 bytes)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO Executor: Running task 46.0 in stage 13.0 (TID 863)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 31.0 in stage 13.0 (TID 848) in 839 ms on localhost (31/200)
15/08/06 17:54:37 INFO Executor: Finished task 33.0 in stage 13.0 (TID 850). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 47.0 in stage 13.0 (TID 864, localhost, ANY, 1816 bytes)
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO Executor: Running task 47.0 in stage 13.0 (TID 864)
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:54:37 INFO TaskSetManager: Finished task 32.0 in stage 13.0 (TID 849) in 841 ms on localhost (32/200)
15/08/06 17:54:37 INFO TaskSetManager: Starting task 48.0 in stage 13.0 (TID 865, localhost, ANY, 1814 bytes)
15/08/06 17:54:37 INFO Executor: Running task 48.0 in stage 13.0 (TID 865)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO TaskSetManager: Finished task 26.0 in stage 13.0 (TID 843) in 861 ms on localhost (33/200)
15/08/06 17:54:37 INFO TaskSetManager: Starting task 49.0 in stage 13.0 (TID 866, localhost, ANY, 1816 bytes)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO Executor: Running task 49.0 in stage 13.0 (TID 866)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 34.0 in stage 13.0 (TID 851) in 686 ms on localhost (34/200)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:37 INFO TaskSetManager: Starting task 50.0 in stage 13.0 (TID 867, localhost, ANY, 1814 bytes)
15/08/06 17:54:37 INFO Executor: Running task 50.0 in stage 13.0 (TID 867)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO TaskSetManager: Finished task 33.0 in stage 13.0 (TID 850) in 744 ms on localhost (35/200)
15/08/06 17:54:37 INFO Executor: Finished task 36.0 in stage 13.0 (TID 853). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 51.0 in stage 13.0 (TID 868, localhost, ANY, 1816 bytes)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 36.0 in stage 13.0 (TID 853) in 411 ms on localhost (36/200)
15/08/06 17:54:37 INFO Executor: Running task 51.0 in stage 13.0 (TID 868)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO Executor: Finished task 35.0 in stage 13.0 (TID 852). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 52.0 in stage 13.0 (TID 869, localhost, ANY, 1815 bytes)
15/08/06 17:54:37 INFO Executor: Running task 52.0 in stage 13.0 (TID 869)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 35.0 in stage 13.0 (TID 852) in 568 ms on localhost (37/200)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 120
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 INFO Executor: Finished task 37.0 in stage 13.0 (TID 854). 2182 bytes result sent to driver
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO TaskSetManager: Starting task 53.0 in stage 13.0 (TID 870, localhost, ANY, 1816 bytes)
15/08/06 17:54:37 INFO Executor: Running task 53.0 in stage 13.0 (TID 870)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 37.0 in stage 13.0 (TID 854) in 279 ms on localhost (38/200)
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:54:37 INFO Executor: Finished task 39.0 in stage 13.0 (TID 856). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 54.0 in stage 13.0 (TID 871, localhost, ANY, 1815 bytes)
15/08/06 17:54:37 INFO Executor: Finished task 38.0 in stage 13.0 (TID 855). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO Executor: Running task 54.0 in stage 13.0 (TID 871)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 39.0 in stage 13.0 (TID 856) in 313 ms on localhost (39/200)
15/08/06 17:54:37 INFO TaskSetManager: Starting task 55.0 in stage 13.0 (TID 872, localhost, ANY, 1815 bytes)
15/08/06 17:54:37 INFO Executor: Running task 55.0 in stage 13.0 (TID 872)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 38.0 in stage 13.0 (TID 855) in 333 ms on localhost (40/200)
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 95
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 124
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:54:37 INFO Executor: Finished task 42.0 in stage 13.0 (TID 859). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 56.0 in stage 13.0 (TID 873, localhost, ANY, 1814 bytes)
15/08/06 17:54:37 INFO Executor: Running task 56.0 in stage 13.0 (TID 873)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 42.0 in stage 13.0 (TID 859) in 322 ms on localhost (41/200)
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO Executor: Finished task 43.0 in stage 13.0 (TID 860). 2182 bytes result sent to driver
15/08/06 17:54:37 INFO TaskSetManager: Starting task 57.0 in stage 13.0 (TID 874, localhost, ANY, 1814 bytes)
15/08/06 17:54:37 INFO Executor: Running task 57.0 in stage 13.0 (TID 874)
15/08/06 17:54:37 INFO TaskSetManager: Finished task 43.0 in stage 13.0 (TID 860) in 341 ms on localhost (42/200)
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:37 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:37 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:38 INFO Executor: Finished task 50.0 in stage 13.0 (TID 867). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 58.0 in stage 13.0 (TID 875, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 58.0 in stage 13.0 (TID 875)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 50.0 in stage 13.0 (TID 867) in 723 ms on localhost (43/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO Executor: Finished task 41.0 in stage 13.0 (TID 858). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Finished task 45.0 in stage 13.0 (TID 862). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 59.0 in stage 13.0 (TID 876, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 59.0 in stage 13.0 (TID 876)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 41.0 in stage 13.0 (TID 858) in 817 ms on localhost (44/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Starting task 60.0 in stage 13.0 (TID 877, localhost, ANY, 1814 bytes)
15/08/06 17:54:38 INFO Executor: Finished task 48.0 in stage 13.0 (TID 865). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Running task 60.0 in stage 13.0 (TID 877)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 45.0 in stage 13.0 (TID 862) in 803 ms on localhost (45/200)
15/08/06 17:54:38 INFO Executor: Finished task 40.0 in stage 13.0 (TID 857). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 61.0 in stage 13.0 (TID 878, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 61.0 in stage 13.0 (TID 878)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Finished task 48.0 in stage 13.0 (TID 865) in 795 ms on localhost (46/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO Executor: Finished task 46.0 in stage 13.0 (TID 863). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Finished task 51.0 in stage 13.0 (TID 868). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 62.0 in stage 13.0 (TID 879, localhost, ANY, 1814 bytes)
15/08/06 17:54:38 INFO Executor: Finished task 47.0 in stage 13.0 (TID 864). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Running task 62.0 in stage 13.0 (TID 879)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 40.0 in stage 13.0 (TID 857) in 875 ms on localhost (47/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Starting task 63.0 in stage 13.0 (TID 880, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 63.0 in stage 13.0 (TID 880)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 46.0 in stage 13.0 (TID 863) in 833 ms on localhost (48/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO Executor: Finished task 44.0 in stage 13.0 (TID 861). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 64.0 in stage 13.0 (TID 881, localhost, ANY, 1816 bytes)
15/08/06 17:54:38 INFO Executor: Running task 64.0 in stage 13.0 (TID 881)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 51.0 in stage 13.0 (TID 868) in 797 ms on localhost (49/200)
15/08/06 17:54:38 INFO Executor: Finished task 52.0 in stage 13.0 (TID 869). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Starting task 65.0 in stage 13.0 (TID 882, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 47.0 in stage 13.0 (TID 864) in 846 ms on localhost (50/200)
15/08/06 17:54:38 INFO Executor: Running task 65.0 in stage 13.0 (TID 882)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 66.0 in stage 13.0 (TID 883, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 66.0 in stage 13.0 (TID 883)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 44.0 in stage 13.0 (TID 861) in 876 ms on localhost (51/200)
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 INFO Executor: Finished task 49.0 in stage 13.0 (TID 866). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 67.0 in stage 13.0 (TID 884, localhost, ANY, 1816 bytes)
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO TaskSetManager: Finished task 52.0 in stage 13.0 (TID 869) in 808 ms on localhost (52/200)
15/08/06 17:54:38 INFO Executor: Running task 67.0 in stage 13.0 (TID 884)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 68.0 in stage 13.0 (TID 885, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO Executor: Running task 68.0 in stage 13.0 (TID 885)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Finished task 49.0 in stage 13.0 (TID 866) in 848 ms on localhost (53/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 112
15/08/06 17:54:38 INFO Executor: Finished task 54.0 in stage 13.0 (TID 871). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 69.0 in stage 13.0 (TID 886, localhost, ANY, 1816 bytes)
15/08/06 17:54:38 INFO Executor: Running task 69.0 in stage 13.0 (TID 886)
15/08/06 17:54:38 INFO Executor: Finished task 55.0 in stage 13.0 (TID 872). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Finished task 53.0 in stage 13.0 (TID 870). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO Executor: Finished task 57.0 in stage 13.0 (TID 874). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Finished task 54.0 in stage 13.0 (TID 871) in 751 ms on localhost (54/200)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 70.0 in stage 13.0 (TID 887, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 70.0 in stage 13.0 (TID 887)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 55.0 in stage 13.0 (TID 872) in 757 ms on localhost (55/200)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 71.0 in stage 13.0 (TID 888, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 71.0 in stage 13.0 (TID 888)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 53.0 in stage 13.0 (TID 870) in 830 ms on localhost (56/200)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 72.0 in stage 13.0 (TID 889, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 72.0 in stage 13.0 (TID 889)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 57.0 in stage 13.0 (TID 874) in 655 ms on localhost (57/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO Executor: Finished task 56.0 in stage 13.0 (TID 873). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 73.0 in stage 13.0 (TID 890, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 56.0 in stage 13.0 (TID 873) in 691 ms on localhost (58/200)
15/08/06 17:54:38 INFO Executor: Running task 73.0 in stage 13.0 (TID 890)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 203
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 139
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 145
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 122
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 159
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 135
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 137
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 160
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/06 17:54:38 INFO Executor: Finished task 58.0 in stage 13.0 (TID 875). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO TaskSetManager: Starting task 74.0 in stage 13.0 (TID 891, localhost, ANY, 1813 bytes)
15/08/06 17:54:38 INFO Executor: Running task 74.0 in stage 13.0 (TID 891)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 58.0 in stage 13.0 (TID 875) in 427 ms on localhost (59/200)
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:54:38 INFO Executor: Finished task 60.0 in stage 13.0 (TID 877). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 75.0 in stage 13.0 (TID 892, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 75.0 in stage 13.0 (TID 892)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 60.0 in stage 13.0 (TID 877) in 639 ms on localhost (60/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO Executor: Finished task 63.0 in stage 13.0 (TID 880). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Finished task 59.0 in stage 13.0 (TID 876). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Finished task 62.0 in stage 13.0 (TID 879). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 76.0 in stage 13.0 (TID 893, localhost, ANY, 1816 bytes)
15/08/06 17:54:38 INFO Executor: Running task 76.0 in stage 13.0 (TID 893)
15/08/06 17:54:38 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:38 INFO TaskSetManager: Finished task 63.0 in stage 13.0 (TID 880) in 690 ms on localhost (61/200)
15/08/06 17:54:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:38 INFO Executor: Finished task 61.0 in stage 13.0 (TID 878). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:38 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:54:38 INFO TaskSetManager: Starting task 77.0 in stage 13.0 (TID 894, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 77.0 in stage 13.0 (TID 894)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 59.0 in stage 13.0 (TID 876) in 762 ms on localhost (62/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Starting task 78.0 in stage 13.0 (TID 895, localhost, ANY, 1813 bytes)
15/08/06 17:54:38 INFO Executor: Running task 78.0 in stage 13.0 (TID 895)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 62.0 in stage 13.0 (TID 879) in 726 ms on localhost (63/200)
15/08/06 17:54:38 INFO Executor: Finished task 66.0 in stage 13.0 (TID 883). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Starting task 79.0 in stage 13.0 (TID 896, localhost, ANY, 1814 bytes)
15/08/06 17:54:38 INFO Executor: Running task 79.0 in stage 13.0 (TID 896)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 61.0 in stage 13.0 (TID 878) in 765 ms on localhost (64/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:38 INFO TaskSetManager: Starting task 80.0 in stage 13.0 (TID 897, localhost, ANY, 1814 bytes)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 66.0 in stage 13.0 (TID 883) in 710 ms on localhost (65/200)
15/08/06 17:54:38 INFO Executor: Running task 80.0 in stage 13.0 (TID 897)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO Executor: Finished task 65.0 in stage 13.0 (TID 882). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 81.0 in stage 13.0 (TID 898, localhost, ANY, 1814 bytes)
15/08/06 17:54:38 INFO Executor: Running task 81.0 in stage 13.0 (TID 898)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 65.0 in stage 13.0 (TID 882) in 746 ms on localhost (66/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO Executor: Finished task 72.0 in stage 13.0 (TID 889). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Finished task 73.0 in stage 13.0 (TID 890). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Starting task 82.0 in stage 13.0 (TID 899, localhost, ANY, 1813 bytes)
15/08/06 17:54:38 INFO Executor: Running task 82.0 in stage 13.0 (TID 899)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 72.0 in stage 13.0 (TID 889) in 643 ms on localhost (67/200)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 83.0 in stage 13.0 (TID 900, localhost, ANY, 1814 bytes)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 73.0 in stage 13.0 (TID 890) in 637 ms on localhost (68/200)
15/08/06 17:54:38 INFO Executor: Finished task 68.0 in stage 13.0 (TID 885). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Finished task 64.0 in stage 13.0 (TID 881). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Running task 83.0 in stage 13.0 (TID 900)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 84.0 in stage 13.0 (TID 901, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Finished task 70.0 in stage 13.0 (TID 887). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Finished task 71.0 in stage 13.0 (TID 888). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Finished task 68.0 in stage 13.0 (TID 885) in 758 ms on localhost (69/200)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 85.0 in stage 13.0 (TID 902, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 85.0 in stage 13.0 (TID 902)
15/08/06 17:54:38 INFO Executor: Finished task 67.0 in stage 13.0 (TID 884). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO Executor: Running task 84.0 in stage 13.0 (TID 901)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 86.0 in stage 13.0 (TID 903, localhost, ANY, 1815 bytes)
15/08/06 17:54:38 INFO Executor: Running task 86.0 in stage 13.0 (TID 903)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 64.0 in stage 13.0 (TID 881) in 792 ms on localhost (70/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO Executor: Finished task 74.0 in stage 13.0 (TID 891). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Finished task 70.0 in stage 13.0 (TID 887) in 662 ms on localhost (71/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO TaskSetManager: Starting task 87.0 in stage 13.0 (TID 904, localhost, ANY, 1813 bytes)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:38 INFO Executor: Running task 87.0 in stage 13.0 (TID 904)
15/08/06 17:54:38 INFO Executor: Finished task 69.0 in stage 13.0 (TID 886). 2182 bytes result sent to driver
15/08/06 17:54:38 INFO TaskSetManager: Finished task 71.0 in stage 13.0 (TID 888) in 664 ms on localhost (72/200)
15/08/06 17:54:38 INFO TaskSetManager: Starting task 88.0 in stage 13.0 (TID 905, localhost, ANY, 1813 bytes)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 67.0 in stage 13.0 (TID 884) in 773 ms on localhost (73/200)
15/08/06 17:54:38 INFO Executor: Running task 88.0 in stage 13.0 (TID 905)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:38 INFO TaskSetManager: Starting task 89.0 in stage 13.0 (TID 906, localhost, ANY, 1814 bytes)
15/08/06 17:54:38 INFO Executor: Running task 89.0 in stage 13.0 (TID 906)
15/08/06 17:54:38 INFO TaskSetManager: Finished task 74.0 in stage 13.0 (TID 891) in 492 ms on localhost (74/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:38 INFO TaskSetManager: Finished task 69.0 in stage 13.0 (TID 886) in 765 ms on localhost (75/200)
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Starting task 90.0 in stage 13.0 (TID 907, localhost, ANY, 1812 bytes)
15/08/06 17:54:39 INFO Executor: Running task 90.0 in stage 13.0 (TID 907)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO Executor: Finished task 75.0 in stage 13.0 (TID 892). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO TaskSetManager: Starting task 91.0 in stage 13.0 (TID 908, localhost, ANY, 1815 bytes)
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 134
15/08/06 17:54:39 INFO Executor: Running task 91.0 in stage 13.0 (TID 908)
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Finished task 75.0 in stage 13.0 (TID 892) in 286 ms on localhost (76/200)
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 191
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 173
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 206
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 161
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 200
15/08/06 17:54:39 INFO Executor: Finished task 76.0 in stage 13.0 (TID 893). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO TaskSetManager: Starting task 92.0 in stage 13.0 (TID 909, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 92.0 in stage 13.0 (TID 909)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 76.0 in stage 13.0 (TID 893) in 351 ms on localhost (77/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 119
15/08/06 17:54:39 INFO Executor: Finished task 77.0 in stage 13.0 (TID 894). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 93.0 in stage 13.0 (TID 910, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 93.0 in stage 13.0 (TID 910)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 77.0 in stage 13.0 (TID 894) in 437 ms on localhost (78/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO Executor: Finished task 80.0 in stage 13.0 (TID 897). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 94.0 in stage 13.0 (TID 911, localhost, ANY, 1812 bytes)
15/08/06 17:54:39 INFO Executor: Running task 94.0 in stage 13.0 (TID 911)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 80.0 in stage 13.0 (TID 897) in 441 ms on localhost (79/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:39 INFO Executor: Finished task 79.0 in stage 13.0 (TID 896). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 95.0 in stage 13.0 (TID 912, localhost, ANY, 1816 bytes)
15/08/06 17:54:39 INFO Executor: Running task 95.0 in stage 13.0 (TID 912)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 79.0 in stage 13.0 (TID 896) in 516 ms on localhost (80/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:54:39 INFO Executor: Finished task 81.0 in stage 13.0 (TID 898). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO Executor: Finished task 78.0 in stage 13.0 (TID 895). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 96.0 in stage 13.0 (TID 913, localhost, ANY, 1815 bytes)
15/08/06 17:54:39 INFO Executor: Running task 96.0 in stage 13.0 (TID 913)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 81.0 in stage 13.0 (TID 898) in 661 ms on localhost (81/200)
15/08/06 17:54:39 INFO Executor: Finished task 89.0 in stage 13.0 (TID 906). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 97.0 in stage 13.0 (TID 914, localhost, ANY, 1815 bytes)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Finished task 78.0 in stage 13.0 (TID 895) in 731 ms on localhost (82/200)
15/08/06 17:54:39 INFO Executor: Running task 97.0 in stage 13.0 (TID 914)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO Executor: Finished task 90.0 in stage 13.0 (TID 907). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 98.0 in stage 13.0 (TID 915, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 98.0 in stage 13.0 (TID 915)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 89.0 in stage 13.0 (TID 906) in 631 ms on localhost (83/200)
15/08/06 17:54:39 INFO Executor: Finished task 87.0 in stage 13.0 (TID 904). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Starting task 99.0 in stage 13.0 (TID 916, localhost, ANY, 1815 bytes)
15/08/06 17:54:39 INFO Executor: Finished task 85.0 in stage 13.0 (TID 902). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO Executor: Running task 99.0 in stage 13.0 (TID 916)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 90.0 in stage 13.0 (TID 907) in 639 ms on localhost (84/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO Executor: Finished task 82.0 in stage 13.0 (TID 899). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 100.0 in stage 13.0 (TID 917, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 87.0 in stage 13.0 (TID 904) in 677 ms on localhost (85/200)
15/08/06 17:54:39 INFO Executor: Running task 100.0 in stage 13.0 (TID 917)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 85.0 in stage 13.0 (TID 902) in 689 ms on localhost (86/200)
15/08/06 17:54:39 INFO Executor: Finished task 84.0 in stage 13.0 (TID 901). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 101.0 in stage 13.0 (TID 918, localhost, ANY, 1812 bytes)
15/08/06 17:54:39 INFO Executor: Running task 101.0 in stage 13.0 (TID 918)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Starting task 102.0 in stage 13.0 (TID 919, localhost, ANY, 1813 bytes)
15/08/06 17:54:39 INFO Executor: Running task 102.0 in stage 13.0 (TID 919)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 82.0 in stage 13.0 (TID 899) in 712 ms on localhost (87/200)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 84.0 in stage 13.0 (TID 901) in 706 ms on localhost (88/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO Executor: Finished task 83.0 in stage 13.0 (TID 900). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO Executor: Finished task 86.0 in stage 13.0 (TID 903). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 103.0 in stage 13.0 (TID 920, localhost, ANY, 1813 bytes)
15/08/06 17:54:39 INFO Executor: Running task 103.0 in stage 13.0 (TID 920)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 83.0 in stage 13.0 (TID 900) in 719 ms on localhost (89/200)
15/08/06 17:54:39 INFO TaskSetManager: Starting task 104.0 in stage 13.0 (TID 921, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 104.0 in stage 13.0 (TID 921)
15/08/06 17:54:39 INFO TaskSetManager: Starting task 105.0 in stage 13.0 (TID 922, localhost, ANY, 1815 bytes)
15/08/06 17:54:39 INFO Executor: Running task 105.0 in stage 13.0 (TID 922)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Finished task 86.0 in stage 13.0 (TID 903) in 721 ms on localhost (90/200)
15/08/06 17:54:39 INFO Executor: Finished task 88.0 in stage 13.0 (TID 905). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO Executor: Finished task 91.0 in stage 13.0 (TID 908). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 106.0 in stage 13.0 (TID 923, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 106.0 in stage 13.0 (TID 923)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Finished task 88.0 in stage 13.0 (TID 905) in 718 ms on localhost (91/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Starting task 107.0 in stage 13.0 (TID 924, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 107.0 in stage 13.0 (TID 924)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 91.0 in stage 13.0 (TID 908) in 670 ms on localhost (92/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 114
15/08/06 17:54:39 INFO Executor: Finished task 93.0 in stage 13.0 (TID 910). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 108.0 in stage 13.0 (TID 925, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 INFO TaskSetManager: Finished task 93.0 in stage 13.0 (TID 910) in 454 ms on localhost (93/200)
15/08/06 17:54:39 INFO Executor: Running task 108.0 in stage 13.0 (TID 925)
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 162
15/08/06 17:54:39 INFO Executor: Finished task 92.0 in stage 13.0 (TID 909). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:39 INFO TaskSetManager: Starting task 109.0 in stage 13.0 (TID 926, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 92.0 in stage 13.0 (TID 909) in 561 ms on localhost (94/200)
15/08/06 17:54:39 INFO Executor: Finished task 95.0 in stage 13.0 (TID 912). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 110.0 in stage 13.0 (TID 927, localhost, ANY, 1813 bytes)
15/08/06 17:54:39 INFO Executor: Running task 110.0 in stage 13.0 (TID 927)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 95.0 in stage 13.0 (TID 912) in 374 ms on localhost (95/200)
15/08/06 17:54:39 INFO Executor: Running task 109.0 in stage 13.0 (TID 926)
15/08/06 17:54:39 INFO Executor: Finished task 94.0 in stage 13.0 (TID 911). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 111.0 in stage 13.0 (TID 928, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 111.0 in stage 13.0 (TID 928)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO TaskSetManager: Finished task 94.0 in stage 13.0 (TID 911) in 427 ms on localhost (96/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 206
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 129
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 174
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 124
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 172
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 167
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 178
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 147
15/08/06 17:54:39 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:54:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:39 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:39 INFO Executor: Finished task 97.0 in stage 13.0 (TID 914). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 112.0 in stage 13.0 (TID 929, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 112.0 in stage 13.0 (TID 929)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 97.0 in stage 13.0 (TID 914) in 366 ms on localhost (97/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO Executor: Finished task 96.0 in stage 13.0 (TID 913). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 113.0 in stage 13.0 (TID 930, localhost, ANY, 1815 bytes)
15/08/06 17:54:39 INFO TaskSetManager: Finished task 96.0 in stage 13.0 (TID 913) in 393 ms on localhost (98/200)
15/08/06 17:54:39 INFO Executor: Running task 113.0 in stage 13.0 (TID 930)
15/08/06 17:54:39 INFO Executor: Finished task 99.0 in stage 13.0 (TID 916). 2182 bytes result sent to driver
15/08/06 17:54:39 INFO TaskSetManager: Starting task 114.0 in stage 13.0 (TID 931, localhost, ANY, 1814 bytes)
15/08/06 17:54:39 INFO Executor: Running task 114.0 in stage 13.0 (TID 931)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:39 INFO TaskSetManager: Finished task 99.0 in stage 13.0 (TID 916) in 363 ms on localhost (99/200)
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO Executor: Finished task 100.0 in stage 13.0 (TID 917). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 115.0 in stage 13.0 (TID 932, localhost, ANY, 1814 bytes)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 100.0 in stage 13.0 (TID 917) in 388 ms on localhost (100/200)
15/08/06 17:54:40 INFO Executor: Running task 115.0 in stage 13.0 (TID 932)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 160
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 149
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:54:40 INFO Executor: Finished task 98.0 in stage 13.0 (TID 915). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 116.0 in stage 13.0 (TID 933, localhost, ANY, 1815 bytes)
15/08/06 17:54:40 INFO Executor: Running task 116.0 in stage 13.0 (TID 933)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 98.0 in stage 13.0 (TID 915) in 693 ms on localhost (101/200)
15/08/06 17:54:40 INFO Executor: Finished task 105.0 in stage 13.0 (TID 922). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO TaskSetManager: Starting task 117.0 in stage 13.0 (TID 934, localhost, ANY, 1813 bytes)
15/08/06 17:54:40 INFO Executor: Running task 117.0 in stage 13.0 (TID 934)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 105.0 in stage 13.0 (TID 922) in 626 ms on localhost (102/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO Executor: Finished task 104.0 in stage 13.0 (TID 921). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 118.0 in stage 13.0 (TID 935, localhost, ANY, 1815 bytes)
15/08/06 17:54:40 INFO Executor: Running task 118.0 in stage 13.0 (TID 935)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 104.0 in stage 13.0 (TID 921) in 678 ms on localhost (103/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:54:40 INFO Executor: Finished task 106.0 in stage 13.0 (TID 923). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 119.0 in stage 13.0 (TID 936, localhost, ANY, 1813 bytes)
15/08/06 17:54:40 INFO Executor: Running task 119.0 in stage 13.0 (TID 936)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 106.0 in stage 13.0 (TID 923) in 737 ms on localhost (104/200)
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO Executor: Finished task 102.0 in stage 13.0 (TID 919). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO TaskSetManager: Starting task 120.0 in stage 13.0 (TID 937, localhost, ANY, 1812 bytes)
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO TaskSetManager: Finished task 102.0 in stage 13.0 (TID 919) in 783 ms on localhost (105/200)
15/08/06 17:54:40 INFO Executor: Running task 120.0 in stage 13.0 (TID 937)
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/06 17:54:40 INFO Executor: Finished task 101.0 in stage 13.0 (TID 918). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 121.0 in stage 13.0 (TID 938, localhost, ANY, 1812 bytes)
15/08/06 17:54:40 INFO Executor: Running task 121.0 in stage 13.0 (TID 938)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO TaskSetManager: Finished task 101.0 in stage 13.0 (TID 918) in 797 ms on localhost (106/200)
15/08/06 17:54:40 INFO Executor: Finished task 103.0 in stage 13.0 (TID 920). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 122.0 in stage 13.0 (TID 939, localhost, ANY, 1812 bytes)
15/08/06 17:54:40 INFO Executor: Running task 122.0 in stage 13.0 (TID 939)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 103.0 in stage 13.0 (TID 920) in 785 ms on localhost (107/200)
15/08/06 17:54:40 INFO Executor: Finished task 107.0 in stage 13.0 (TID 924). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO TaskSetManager: Starting task 123.0 in stage 13.0 (TID 940, localhost, ANY, 1814 bytes)
15/08/06 17:54:40 INFO Executor: Running task 123.0 in stage 13.0 (TID 940)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 107.0 in stage 13.0 (TID 924) in 772 ms on localhost (108/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO Executor: Finished task 110.0 in stage 13.0 (TID 927). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO TaskSetManager: Starting task 124.0 in stage 13.0 (TID 941, localhost, ANY, 1813 bytes)
15/08/06 17:54:40 INFO Executor: Running task 124.0 in stage 13.0 (TID 941)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 110.0 in stage 13.0 (TID 927) in 728 ms on localhost (109/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:40 INFO Executor: Finished task 111.0 in stage 13.0 (TID 928). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO Executor: Finished task 108.0 in stage 13.0 (TID 925). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:40 INFO Executor: Finished task 109.0 in stage 13.0 (TID 926). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 125.0 in stage 13.0 (TID 942, localhost, ANY, 1814 bytes)
15/08/06 17:54:40 INFO Executor: Running task 125.0 in stage 13.0 (TID 942)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 111.0 in stage 13.0 (TID 928) in 741 ms on localhost (110/200)
15/08/06 17:54:40 INFO TaskSetManager: Starting task 126.0 in stage 13.0 (TID 943, localhost, ANY, 1813 bytes)
15/08/06 17:54:40 INFO Executor: Running task 126.0 in stage 13.0 (TID 943)
15/08/06 17:54:40 INFO TaskSetManager: Starting task 127.0 in stage 13.0 (TID 944, localhost, ANY, 1815 bytes)
15/08/06 17:54:40 INFO Executor: Running task 127.0 in stage 13.0 (TID 944)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO TaskSetManager: Finished task 108.0 in stage 13.0 (TID 925) in 778 ms on localhost (111/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO TaskSetManager: Finished task 109.0 in stage 13.0 (TID 926) in 761 ms on localhost (112/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO Executor: Finished task 113.0 in stage 13.0 (TID 930). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 128.0 in stage 13.0 (TID 945, localhost, ANY, 1814 bytes)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 113.0 in stage 13.0 (TID 930) in 567 ms on localhost (113/200)
15/08/06 17:54:40 INFO Executor: Running task 128.0 in stage 13.0 (TID 945)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO Executor: Finished task 112.0 in stage 13.0 (TID 929). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO Executor: Finished task 115.0 in stage 13.0 (TID 932). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 129.0 in stage 13.0 (TID 946, localhost, ANY, 1815 bytes)
15/08/06 17:54:40 INFO Executor: Running task 129.0 in stage 13.0 (TID 946)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 112.0 in stage 13.0 (TID 929) in 598 ms on localhost (114/200)
15/08/06 17:54:40 INFO TaskSetManager: Starting task 130.0 in stage 13.0 (TID 947, localhost, ANY, 1814 bytes)
15/08/06 17:54:40 INFO Executor: Running task 130.0 in stage 13.0 (TID 947)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 115.0 in stage 13.0 (TID 932) in 542 ms on localhost (115/200)
15/08/06 17:54:40 INFO Executor: Finished task 114.0 in stage 13.0 (TID 931). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:40 INFO TaskSetManager: Starting task 131.0 in stage 13.0 (TID 948, localhost, ANY, 1815 bytes)
15/08/06 17:54:40 INFO Executor: Running task 131.0 in stage 13.0 (TID 948)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 114.0 in stage 13.0 (TID 931) in 589 ms on localhost (116/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO Executor: Finished task 116.0 in stage 13.0 (TID 933). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 132.0 in stage 13.0 (TID 949, localhost, ANY, 1814 bytes)
15/08/06 17:54:40 INFO Executor: Running task 132.0 in stage 13.0 (TID 949)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 116.0 in stage 13.0 (TID 933) in 283 ms on localhost (117/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO Executor: Finished task 117.0 in stage 13.0 (TID 934). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 133.0 in stage 13.0 (TID 950, localhost, ANY, 1814 bytes)
15/08/06 17:54:40 INFO Executor: Running task 133.0 in stage 13.0 (TID 950)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 117.0 in stage 13.0 (TID 934) in 315 ms on localhost (118/200)
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 175
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 148
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 180
15/08/06 17:54:40 INFO Executor: Finished task 118.0 in stage 13.0 (TID 935). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 134.0 in stage 13.0 (TID 951, localhost, ANY, 1816 bytes)
15/08/06 17:54:40 INFO Executor: Running task 134.0 in stage 13.0 (TID 951)
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO TaskSetManager: Finished task 118.0 in stage 13.0 (TID 935) in 343 ms on localhost (119/200)
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 163
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 142
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 125
15/08/06 17:54:40 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:54:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:40 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 139
15/08/06 17:54:40 INFO Executor: Finished task 119.0 in stage 13.0 (TID 936). 2182 bytes result sent to driver
15/08/06 17:54:40 INFO TaskSetManager: Starting task 135.0 in stage 13.0 (TID 952, localhost, ANY, 1816 bytes)
15/08/06 17:54:40 INFO Executor: Running task 135.0 in stage 13.0 (TID 952)
15/08/06 17:54:40 INFO TaskSetManager: Finished task 119.0 in stage 13.0 (TID 936) in 476 ms on localhost (120/200)
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:41 INFO Executor: Finished task 122.0 in stage 13.0 (TID 939). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 136.0 in stage 13.0 (TID 953, localhost, ANY, 1814 bytes)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 122.0 in stage 13.0 (TID 939) in 719 ms on localhost (121/200)
15/08/06 17:54:41 INFO Executor: Running task 136.0 in stage 13.0 (TID 953)
15/08/06 17:54:41 INFO Executor: Finished task 124.0 in stage 13.0 (TID 941). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO Executor: Finished task 120.0 in stage 13.0 (TID 937). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO Executor: Finished task 121.0 in stage 13.0 (TID 938). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 137.0 in stage 13.0 (TID 954, localhost, ANY, 1814 bytes)
15/08/06 17:54:41 INFO Executor: Running task 137.0 in stage 13.0 (TID 954)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 124.0 in stage 13.0 (TID 941) in 730 ms on localhost (122/200)
15/08/06 17:54:41 INFO Executor: Finished task 123.0 in stage 13.0 (TID 940). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO Executor: Finished task 128.0 in stage 13.0 (TID 945). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 138.0 in stage 13.0 (TID 955, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 138.0 in stage 13.0 (TID 955)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 120.0 in stage 13.0 (TID 937) in 781 ms on localhost (123/200)
15/08/06 17:54:41 INFO Executor: Finished task 126.0 in stage 13.0 (TID 943). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 139.0 in stage 13.0 (TID 956, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 139.0 in stage 13.0 (TID 956)
15/08/06 17:54:41 INFO Executor: Finished task 127.0 in stage 13.0 (TID 944). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Finished task 121.0 in stage 13.0 (TID 938) in 776 ms on localhost (124/200)
15/08/06 17:54:41 INFO Executor: Finished task 125.0 in stage 13.0 (TID 942). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO TaskSetManager: Starting task 140.0 in stage 13.0 (TID 957, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 140.0 in stage 13.0 (TID 957)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 123.0 in stage 13.0 (TID 940) in 778 ms on localhost (125/200)
15/08/06 17:54:41 INFO TaskSetManager: Starting task 141.0 in stage 13.0 (TID 958, localhost, ANY, 1813 bytes)
15/08/06 17:54:41 INFO Executor: Running task 141.0 in stage 13.0 (TID 958)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:41 INFO TaskSetManager: Finished task 128.0 in stage 13.0 (TID 945) in 701 ms on localhost (126/200)
15/08/06 17:54:41 INFO TaskSetManager: Starting task 142.0 in stage 13.0 (TID 959, localhost, ANY, 1812 bytes)
15/08/06 17:54:41 INFO Executor: Running task 142.0 in stage 13.0 (TID 959)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 126.0 in stage 13.0 (TID 943) in 739 ms on localhost (127/200)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO Executor: Finished task 130.0 in stage 13.0 (TID 947). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 143.0 in stage 13.0 (TID 960, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 143.0 in stage 13.0 (TID 960)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 127.0 in stage 13.0 (TID 944) in 744 ms on localhost (128/200)
15/08/06 17:54:41 INFO TaskSetManager: Starting task 144.0 in stage 13.0 (TID 961, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 144.0 in stage 13.0 (TID 961)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 125.0 in stage 13.0 (TID 942) in 756 ms on localhost (129/200)
15/08/06 17:54:41 INFO TaskSetManager: Starting task 145.0 in stage 13.0 (TID 962, localhost, ANY, 1814 bytes)
15/08/06 17:54:41 INFO Executor: Running task 145.0 in stage 13.0 (TID 962)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO TaskSetManager: Finished task 130.0 in stage 13.0 (TID 947) in 697 ms on localhost (130/200)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO Executor: Finished task 129.0 in stage 13.0 (TID 946). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 146.0 in stage 13.0 (TID 963, localhost, ANY, 1814 bytes)
15/08/06 17:54:41 INFO Executor: Running task 146.0 in stage 13.0 (TID 963)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:41 INFO Executor: Finished task 133.0 in stage 13.0 (TID 950). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO Executor: Finished task 131.0 in stage 13.0 (TID 948). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO Executor: Finished task 132.0 in stage 13.0 (TID 949). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Finished task 129.0 in stage 13.0 (TID 946) in 708 ms on localhost (131/200)
15/08/06 17:54:41 INFO TaskSetManager: Starting task 147.0 in stage 13.0 (TID 964, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 147.0 in stage 13.0 (TID 964)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 133.0 in stage 13.0 (TID 950) in 658 ms on localhost (132/200)
15/08/06 17:54:41 INFO TaskSetManager: Starting task 148.0 in stage 13.0 (TID 965, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 148.0 in stage 13.0 (TID 965)
15/08/06 17:54:41 INFO TaskSetManager: Starting task 149.0 in stage 13.0 (TID 966, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 149.0 in stage 13.0 (TID 966)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 132.0 in stage 13.0 (TID 949) in 698 ms on localhost (133/200)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 131.0 in stage 13.0 (TID 948) in 716 ms on localhost (134/200)
15/08/06 17:54:41 INFO Executor: Finished task 134.0 in stage 13.0 (TID 951). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 150.0 in stage 13.0 (TID 967, localhost, ANY, 1815 bytes)
15/08/06 17:54:41 INFO Executor: Running task 150.0 in stage 13.0 (TID 967)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:41 INFO TaskSetManager: Finished task 134.0 in stage 13.0 (TID 951) in 595 ms on localhost (135/200)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:41 INFO Executor: Finished task 135.0 in stage 13.0 (TID 952). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:54:41 INFO TaskSetManager: Starting task 151.0 in stage 13.0 (TID 968, localhost, ANY, 1812 bytes)
15/08/06 17:54:41 INFO Executor: Running task 151.0 in stage 13.0 (TID 968)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO TaskSetManager: Finished task 135.0 in stage 13.0 (TID 952) in 396 ms on localhost (136/200)
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 130
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 141
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 111
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 190
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 125
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 126
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 169
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:54:41 INFO Executor: Finished task 136.0 in stage 13.0 (TID 953). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 152.0 in stage 13.0 (TID 969, localhost, ANY, 1816 bytes)
15/08/06 17:54:41 INFO Executor: Running task 152.0 in stage 13.0 (TID 969)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 136.0 in stage 13.0 (TID 953) in 443 ms on localhost (137/200)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO Executor: Finished task 137.0 in stage 13.0 (TID 954). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 153.0 in stage 13.0 (TID 970, localhost, ANY, 1814 bytes)
15/08/06 17:54:41 INFO Executor: Running task 153.0 in stage 13.0 (TID 970)
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO TaskSetManager: Finished task 137.0 in stage 13.0 (TID 954) in 532 ms on localhost (138/200)
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:54:41 INFO Executor: Finished task 152.0 in stage 13.0 (TID 969). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:41 INFO TaskSetManager: Starting task 154.0 in stage 13.0 (TID 971, localhost, ANY, 1813 bytes)
15/08/06 17:54:41 INFO Executor: Running task 154.0 in stage 13.0 (TID 971)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 152.0 in stage 13.0 (TID 969) in 129 ms on localhost (139/200)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:54:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:54:41 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:54:41 INFO Executor: Finished task 139.0 in stage 13.0 (TID 956). 2182 bytes result sent to driver
15/08/06 17:54:41 INFO TaskSetManager: Starting task 155.0 in stage 13.0 (TID 972, localhost, ANY, 1814 bytes)
15/08/06 17:54:41 INFO Executor: Running task 155.0 in stage 13.0 (TID 972)
15/08/06 17:54:41 INFO TaskSetManager: Finished task 139.0 in stage 13.0 (TID 956) in 690 ms on localhost (140/200)
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO Executor: Finished task 145.0 in stage 13.0 (TID 962). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 156.0 in stage 13.0 (TID 973, localhost, ANY, 1816 bytes)
15/08/06 17:54:42 INFO Executor: Running task 156.0 in stage 13.0 (TID 973)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 145.0 in stage 13.0 (TID 962) in 749 ms on localhost (141/200)
15/08/06 17:54:42 INFO Executor: Finished task 151.0 in stage 13.0 (TID 968). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 157.0 in stage 13.0 (TID 974, localhost, ANY, 1816 bytes)
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 INFO Executor: Running task 157.0 in stage 13.0 (TID 974)
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO TaskSetManager: Finished task 151.0 in stage 13.0 (TID 968) in 717 ms on localhost (142/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO Executor: Finished task 143.0 in stage 13.0 (TID 960). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 158.0 in stage 13.0 (TID 975, localhost, ANY, 1813 bytes)
15/08/06 17:54:42 INFO Executor: Running task 158.0 in stage 13.0 (TID 975)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Finished task 143.0 in stage 13.0 (TID 960) in 772 ms on localhost (143/200)
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 149
15/08/06 17:54:42 INFO Executor: Finished task 149.0 in stage 13.0 (TID 966). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 159.0 in stage 13.0 (TID 976, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 159.0 in stage 13.0 (TID 976)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 149.0 in stage 13.0 (TID 966) in 788 ms on localhost (144/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO Executor: Finished task 140.0 in stage 13.0 (TID 957). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 160.0 in stage 13.0 (TID 977, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 160.0 in stage 13.0 (TID 977)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 140.0 in stage 13.0 (TID 957) in 848 ms on localhost (145/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO Executor: Finished task 138.0 in stage 13.0 (TID 955). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 161.0 in stage 13.0 (TID 978, localhost, ANY, 1816 bytes)
15/08/06 17:54:42 INFO Executor: Running task 161.0 in stage 13.0 (TID 978)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 138.0 in stage 13.0 (TID 955) in 884 ms on localhost (146/200)
15/08/06 17:54:42 INFO Executor: Finished task 141.0 in stage 13.0 (TID 958). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 162.0 in stage 13.0 (TID 979, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 162.0 in stage 13.0 (TID 979)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 141.0 in stage 13.0 (TID 958) in 857 ms on localhost (147/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO Executor: Finished task 146.0 in stage 13.0 (TID 963). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Starting task 163.0 in stage 13.0 (TID 980, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 163.0 in stage 13.0 (TID 980)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 146.0 in stage 13.0 (TID 963) in 835 ms on localhost (148/200)
15/08/06 17:54:42 INFO Executor: Finished task 147.0 in stage 13.0 (TID 964). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 164.0 in stage 13.0 (TID 981, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 164.0 in stage 13.0 (TID 981)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 147.0 in stage 13.0 (TID 964) in 831 ms on localhost (149/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO Executor: Finished task 150.0 in stage 13.0 (TID 967). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 165.0 in stage 13.0 (TID 982, localhost, ANY, 1816 bytes)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO TaskSetManager: Finished task 150.0 in stage 13.0 (TID 967) in 820 ms on localhost (150/200)
15/08/06 17:54:42 INFO Executor: Finished task 144.0 in stage 13.0 (TID 961). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 166.0 in stage 13.0 (TID 983, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Finished task 144.0 in stage 13.0 (TID 961) in 856 ms on localhost (151/200)
15/08/06 17:54:42 INFO Executor: Running task 166.0 in stage 13.0 (TID 983)
15/08/06 17:54:42 INFO Executor: Finished task 142.0 in stage 13.0 (TID 959). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 167.0 in stage 13.0 (TID 984, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 142.0 in stage 13.0 (TID 959) in 872 ms on localhost (152/200)
15/08/06 17:54:42 INFO Executor: Running task 167.0 in stage 13.0 (TID 984)
15/08/06 17:54:42 INFO Executor: Running task 165.0 in stage 13.0 (TID 982)
15/08/06 17:54:42 INFO Executor: Finished task 148.0 in stage 13.0 (TID 965). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 INFO TaskSetManager: Starting task 168.0 in stage 13.0 (TID 985, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 168.0 in stage 13.0 (TID 985)
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO Executor: Finished task 153.0 in stage 13.0 (TID 970). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Finished task 148.0 in stage 13.0 (TID 965) in 840 ms on localhost (153/200)
15/08/06 17:54:42 INFO TaskSetManager: Starting task 169.0 in stage 13.0 (TID 986, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 169.0 in stage 13.0 (TID 986)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 153.0 in stage 13.0 (TID 970) in 398 ms on localhost (154/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO Executor: Finished task 154.0 in stage 13.0 (TID 971). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 170.0 in stage 13.0 (TID 987, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 170.0 in stage 13.0 (TID 987)
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO TaskSetManager: Finished task 154.0 in stage 13.0 (TID 971) in 399 ms on localhost (155/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/06 17:54:42 INFO Executor: Finished task 155.0 in stage 13.0 (TID 972). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 171.0 in stage 13.0 (TID 988, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 171.0 in stage 13.0 (TID 988)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 155.0 in stage 13.0 (TID 972) in 273 ms on localhost (156/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 136
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 110
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO Executor: Finished task 163.0 in stage 13.0 (TID 980). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 INFO TaskSetManager: Starting task 172.0 in stage 13.0 (TID 989, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 172.0 in stage 13.0 (TID 989)
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO TaskSetManager: Finished task 163.0 in stage 13.0 (TID 980) in 155 ms on localhost (157/200)
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 114
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 116
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 186
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 155
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:54:42 INFO Executor: Finished task 157.0 in stage 13.0 (TID 974). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 173.0 in stage 13.0 (TID 990, localhost, ANY, 1812 bytes)
15/08/06 17:54:42 INFO Executor: Running task 173.0 in stage 13.0 (TID 990)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 157.0 in stage 13.0 (TID 974) in 332 ms on localhost (158/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO Executor: Finished task 158.0 in stage 13.0 (TID 975). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 174.0 in stage 13.0 (TID 991, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 174.0 in stage 13.0 (TID 991)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 158.0 in stage 13.0 (TID 975) in 337 ms on localhost (159/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO Executor: Finished task 156.0 in stage 13.0 (TID 973). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 175.0 in stage 13.0 (TID 992, localhost, ANY, 1812 bytes)
15/08/06 17:54:42 INFO Executor: Running task 175.0 in stage 13.0 (TID 992)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 156.0 in stage 13.0 (TID 973) in 362 ms on localhost (160/200)
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 129
15/08/06 17:54:42 INFO Executor: Finished task 159.0 in stage 13.0 (TID 976). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 176.0 in stage 13.0 (TID 993, localhost, ANY, 1813 bytes)
15/08/06 17:54:42 INFO Executor: Running task 176.0 in stage 13.0 (TID 993)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 159.0 in stage 13.0 (TID 976) in 330 ms on localhost (161/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:54:42 INFO Executor: Finished task 173.0 in stage 13.0 (TID 990). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO TaskSetManager: Starting task 177.0 in stage 13.0 (TID 994, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:54:42 INFO Executor: Running task 177.0 in stage 13.0 (TID 994)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 173.0 in stage 13.0 (TID 990) in 126 ms on localhost (162/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/06 17:54:42 INFO Executor: Finished task 162.0 in stage 13.0 (TID 979). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 178.0 in stage 13.0 (TID 995, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 178.0 in stage 13.0 (TID 995)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 162.0 in stage 13.0 (TID 979) in 399 ms on localhost (163/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 197
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:54:42 INFO Executor: Finished task 164.0 in stage 13.0 (TID 981). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO TaskSetManager: Starting task 179.0 in stage 13.0 (TID 996, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 179.0 in stage 13.0 (TID 996)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 164.0 in stage 13.0 (TID 981) in 608 ms on localhost (164/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 246
15/08/06 17:54:42 INFO Executor: Finished task 160.0 in stage 13.0 (TID 977). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 180.0 in stage 13.0 (TID 997, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 180.0 in stage 13.0 (TID 997)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 160.0 in stage 13.0 (TID 977) in 692 ms on localhost (165/200)
15/08/06 17:54:42 INFO Executor: Finished task 168.0 in stage 13.0 (TID 985). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO Executor: Finished task 172.0 in stage 13.0 (TID 989). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Starting task 181.0 in stage 13.0 (TID 998, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 181.0 in stage 13.0 (TID 998)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 168.0 in stage 13.0 (TID 985) in 668 ms on localhost (166/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Starting task 182.0 in stage 13.0 (TID 999, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Finished task 169.0 in stage 13.0 (TID 986). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO Executor: Running task 182.0 in stage 13.0 (TID 999)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 172.0 in stage 13.0 (TID 989) in 545 ms on localhost (167/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO Executor: Finished task 166.0 in stage 13.0 (TID 983). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 183.0 in stage 13.0 (TID 1000, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 169.0 in stage 13.0 (TID 986) in 693 ms on localhost (168/200)
15/08/06 17:54:42 INFO Executor: Running task 183.0 in stage 13.0 (TID 1000)
15/08/06 17:54:42 INFO Executor: Finished task 165.0 in stage 13.0 (TID 982). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Starting task 184.0 in stage 13.0 (TID 1001, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 184.0 in stage 13.0 (TID 1001)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 166.0 in stage 13.0 (TID 983) in 711 ms on localhost (169/200)
15/08/06 17:54:42 INFO TaskSetManager: Starting task 185.0 in stage 13.0 (TID 1002, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Finished task 161.0 in stage 13.0 (TID 978). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO Executor: Running task 185.0 in stage 13.0 (TID 1002)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 165.0 in stage 13.0 (TID 982) in 716 ms on localhost (170/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Starting task 186.0 in stage 13.0 (TID 1003, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 186.0 in stage 13.0 (TID 1003)
15/08/06 17:54:42 INFO Executor: Finished task 167.0 in stage 13.0 (TID 984). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Finished task 161.0 in stage 13.0 (TID 978) in 737 ms on localhost (171/200)
15/08/06 17:54:42 INFO Executor: Finished task 170.0 in stage 13.0 (TID 987). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO Executor: Finished task 171.0 in stage 13.0 (TID 988). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Starting task 187.0 in stage 13.0 (TID 1004, localhost, ANY, 1812 bytes)
15/08/06 17:54:42 INFO Executor: Running task 187.0 in stage 13.0 (TID 1004)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 167.0 in stage 13.0 (TID 984) in 723 ms on localhost (172/200)
15/08/06 17:54:42 INFO TaskSetManager: Starting task 188.0 in stage 13.0 (TID 1005, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO TaskSetManager: Finished task 170.0 in stage 13.0 (TID 987) in 717 ms on localhost (173/200)
15/08/06 17:54:42 INFO Executor: Running task 188.0 in stage 13.0 (TID 1005)
15/08/06 17:54:42 INFO TaskSetManager: Starting task 189.0 in stage 13.0 (TID 1006, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 189.0 in stage 13.0 (TID 1006)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO TaskSetManager: Finished task 171.0 in stage 13.0 (TID 988) in 665 ms on localhost (174/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO Executor: Finished task 174.0 in stage 13.0 (TID 991). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO Executor: Finished task 175.0 in stage 13.0 (TID 992). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 190.0 in stage 13.0 (TID 1007, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO TaskSetManager: Starting task 191.0 in stage 13.0 (TID 1008, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO Executor: Running task 191.0 in stage 13.0 (TID 1008)
15/08/06 17:54:42 INFO Executor: Running task 190.0 in stage 13.0 (TID 1007)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 174.0 in stage 13.0 (TID 991) in 531 ms on localhost (175/200)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 175.0 in stage 13.0 (TID 992) in 525 ms on localhost (176/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 INFO Executor: Finished task 176.0 in stage 13.0 (TID 993). 2182 bytes result sent to driver
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO TaskSetManager: Starting task 192.0 in stage 13.0 (TID 1009, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 192.0 in stage 13.0 (TID 1009)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 176.0 in stage 13.0 (TID 993) in 513 ms on localhost (177/200)
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:54:42 INFO Executor: Finished task 177.0 in stage 13.0 (TID 994). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 193.0 in stage 13.0 (TID 1010, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 193.0 in stage 13.0 (TID 1010)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 177.0 in stage 13.0 (TID 994) in 479 ms on localhost (178/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO Executor: Finished task 178.0 in stage 13.0 (TID 995). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO TaskSetManager: Starting task 194.0 in stage 13.0 (TID 1011, localhost, ANY, 1814 bytes)
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 194
15/08/06 17:54:42 INFO Executor: Running task 194.0 in stage 13.0 (TID 1011)
15/08/06 17:54:42 INFO TaskSetManager: Finished task 178.0 in stage 13.0 (TID 995) in 462 ms on localhost (179/200)
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 130
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:54:42 INFO Executor: Finished task 179.0 in stage 13.0 (TID 996). 2182 bytes result sent to driver
15/08/06 17:54:42 INFO TaskSetManager: Starting task 195.0 in stage 13.0 (TID 1012, localhost, ANY, 1815 bytes)
15/08/06 17:54:42 INFO Executor: Running task 195.0 in stage 13.0 (TID 1012)
15/08/06 17:54:42 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:42 INFO TaskSetManager: Finished task 179.0 in stage 13.0 (TID 996) in 251 ms on localhost (180/200)
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:54:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:42 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 184
15/08/06 17:54:43 INFO Executor: Finished task 182.0 in stage 13.0 (TID 999). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Starting task 196.0 in stage 13.0 (TID 1013, localhost, ANY, 1815 bytes)
15/08/06 17:54:43 INFO Executor: Running task 196.0 in stage 13.0 (TID 1013)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 182.0 in stage 13.0 (TID 999) in 262 ms on localhost (181/200)
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:54:43 INFO Executor: Finished task 183.0 in stage 13.0 (TID 1000). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO Executor: Finished task 180.0 in stage 13.0 (TID 997). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Starting task 197.0 in stage 13.0 (TID 1014, localhost, ANY, 1815 bytes)
15/08/06 17:54:43 INFO Executor: Running task 197.0 in stage 13.0 (TID 1014)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 183.0 in stage 13.0 (TID 1000) in 378 ms on localhost (182/200)
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO TaskSetManager: Starting task 198.0 in stage 13.0 (TID 1015, localhost, ANY, 1813 bytes)
15/08/06 17:54:43 INFO Executor: Running task 198.0 in stage 13.0 (TID 1015)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 180.0 in stage 13.0 (TID 997) in 431 ms on localhost (183/200)
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO Executor: Finished task 186.0 in stage 13.0 (TID 1003). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO Executor: Finished task 184.0 in stage 13.0 (TID 1001). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Starting task 199.0 in stage 13.0 (TID 1016, localhost, ANY, 1815 bytes)
15/08/06 17:54:43 INFO Executor: Running task 199.0 in stage 13.0 (TID 1016)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 186.0 in stage 13.0 (TID 1003) in 440 ms on localhost (184/200)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 184.0 in stage 13.0 (TID 1001) in 452 ms on localhost (185/200)
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO Executor: Finished task 185.0 in stage 13.0 (TID 1002). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 185.0 in stage 13.0 (TID 1002) in 456 ms on localhost (186/200)
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:54:43 INFO Executor: Finished task 188.0 in stage 13.0 (TID 1005). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO TaskSetManager: Finished task 188.0 in stage 13.0 (TID 1005) in 447 ms on localhost (187/200)
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:54:43 INFO Executor: Finished task 181.0 in stage 13.0 (TID 998). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 181.0 in stage 13.0 (TID 998) in 529 ms on localhost (188/200)
15/08/06 17:54:43 INFO Executor: Finished task 187.0 in stage 13.0 (TID 1004). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 187.0 in stage 13.0 (TID 1004) in 484 ms on localhost (189/200)
15/08/06 17:54:43 INFO Executor: Finished task 189.0 in stage 13.0 (TID 1006). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 189.0 in stage 13.0 (TID 1006) in 476 ms on localhost (190/200)
15/08/06 17:54:43 INFO Executor: Finished task 190.0 in stage 13.0 (TID 1007). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 190.0 in stage 13.0 (TID 1007) in 474 ms on localhost (191/200)
15/08/06 17:54:43 INFO Executor: Finished task 191.0 in stage 13.0 (TID 1008). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 191.0 in stage 13.0 (TID 1008) in 482 ms on localhost (192/200)
15/08/06 17:54:43 INFO Executor: Finished task 192.0 in stage 13.0 (TID 1009). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 192.0 in stage 13.0 (TID 1009) in 465 ms on localhost (193/200)
15/08/06 17:54:43 INFO Executor: Finished task 193.0 in stage 13.0 (TID 1010). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 193.0 in stage 13.0 (TID 1010) in 441 ms on localhost (194/200)
15/08/06 17:54:43 INFO Executor: Finished task 194.0 in stage 13.0 (TID 1011). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO Executor: Finished task 195.0 in stage 13.0 (TID 1012). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 194.0 in stage 13.0 (TID 1011) in 426 ms on localhost (195/200)
15/08/06 17:54:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:54:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:54:43 INFO TaskSetManager: Finished task 195.0 in stage 13.0 (TID 1012) in 419 ms on localhost (196/200)
15/08/06 17:54:43 INFO Executor: Finished task 196.0 in stage 13.0 (TID 1013). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 196.0 in stage 13.0 (TID 1013) in 329 ms on localhost (197/200)
15/08/06 17:54:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:54:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:54:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:54:43 INFO Executor: Finished task 197.0 in stage 13.0 (TID 1014). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 197.0 in stage 13.0 (TID 1014) in 241 ms on localhost (198/200)
15/08/06 17:54:43 INFO Executor: Finished task 198.0 in stage 13.0 (TID 1015). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 198.0 in stage 13.0 (TID 1015) in 240 ms on localhost (199/200)
15/08/06 17:54:43 INFO Executor: Finished task 199.0 in stage 13.0 (TID 1016). 2182 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 199.0 in stage 13.0 (TID 1016) in 224 ms on localhost (200/200)
15/08/06 17:54:43 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/08/06 17:54:43 INFO DAGScheduler: Stage 13 (mapPartitions at Exchange.scala:77) finished in 7.852 s
15/08/06 17:54:43 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:54:43 INFO DAGScheduler: running: Set()
15/08/06 17:54:43 INFO DAGScheduler: waiting: Set(Stage 14)
15/08/06 17:54:43 INFO DAGScheduler: failed: Set()
15/08/06 17:54:43 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5ae38113
15/08/06 17:54:43 INFO StatsReportListener: task runtime:(count: 200, mean: 621.375000, stdev: 194.829167, max: 884.000000, min: 126.000000)
15/08/06 17:54:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:43 INFO StatsReportListener: 	126.0 ms	283.0 ms	332.0 ms	452.0 ms	692.0 ms	778.0 ms	844.0 ms	861.0 ms	884.0 ms
15/08/06 17:54:43 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 3352.795000, stdev: 425.781685, max: 3815.000000, min: 0.000000)
15/08/06 17:54:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:43 INFO StatsReportListener: 	0.0 B	3.2 KB	3.2 KB	3.3 KB	3.3 KB	3.4 KB	3.5 KB	3.5 KB	3.7 KB
15/08/06 17:54:43 INFO DAGScheduler: Missing parents for Stage 14: List()
15/08/06 17:54:43 INFO DAGScheduler: Submitting Stage 14 (MapPartitionsRDD[80] at mapPartitions at basicOperators.scala:207), which is now runnable
15/08/06 17:54:43 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.405000, stdev: 0.894972, max: 5.000000, min: 0.000000)
15/08/06 17:54:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:43 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	5.0 ms
15/08/06 17:54:43 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:54:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:43 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:54:43 INFO StatsReportListener: task result size:(count: 200, mean: 2182.000000, stdev: 0.000000, max: 2182.000000, min: 2182.000000)
15/08/06 17:54:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:43 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/06 17:54:43 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 96.915446, stdev: 2.296719, max: 99.533256, min: 84.920635)
15/08/06 17:54:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:43 INFO StatsReportListener: 	85 %	93 %	95 %	96 %	98 %	99 %	99 %	99 %	100 %
15/08/06 17:54:43 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.068547, stdev: 0.148517, max: 0.793651, min: 0.000000)
15/08/06 17:54:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:43 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/06 17:54:43 INFO StatsReportListener: other time pct: (count: 200, mean: 3.016007, stdev: 2.266287, max: 14.285714, min: 0.466744)
15/08/06 17:54:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:43 INFO StatsReportListener: 	 0 %	 1 %	 1 %	 1 %	 2 %	 4 %	 6 %	 7 %	14 %
15/08/06 17:54:43 INFO MemoryStore: ensureFreeSpace(151624) called with curMem=1632313, maxMem=3333968363
15/08/06 17:54:43 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 148.1 KB, free 3.1 GB)
15/08/06 17:54:43 INFO MemoryStore: ensureFreeSpace(66865) called with curMem=1783937, maxMem=3333968363
15/08/06 17:54:43 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 65.3 KB, free 3.1 GB)
15/08/06 17:54:43 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:37948 (size: 65.3 KB, free: 3.1 GB)
15/08/06 17:54:43 INFO BlockManagerMaster: Updated info of block broadcast_19_piece0
15/08/06 17:54:43 INFO DefaultExecutionContext: Created broadcast 19 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:43 INFO DAGScheduler: Submitting 200 missing tasks from Stage 14 (MapPartitionsRDD[80] at mapPartitions at basicOperators.scala:207)
15/08/06 17:54:43 INFO TaskSchedulerImpl: Adding task set 14.0 with 200 tasks
15/08/06 17:54:43 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 1017, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 1018, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 1019, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 1020, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 1021, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 1022, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 1023, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 7.0 in stage 14.0 (TID 1024, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 8.0 in stage 14.0 (TID 1025, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 9.0 in stage 14.0 (TID 1026, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 10.0 in stage 14.0 (TID 1027, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 11.0 in stage 14.0 (TID 1028, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 12.0 in stage 14.0 (TID 1029, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 13.0 in stage 14.0 (TID 1030, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 14.0 in stage 14.0 (TID 1031, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 15.0 in stage 14.0 (TID 1032, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 0.0 in stage 14.0 (TID 1017)
15/08/06 17:54:43 INFO Executor: Running task 1.0 in stage 14.0 (TID 1018)
15/08/06 17:54:43 INFO Executor: Running task 4.0 in stage 14.0 (TID 1021)
15/08/06 17:54:43 INFO Executor: Running task 2.0 in stage 14.0 (TID 1019)
15/08/06 17:54:43 INFO Executor: Running task 3.0 in stage 14.0 (TID 1020)
15/08/06 17:54:43 INFO Executor: Running task 6.0 in stage 14.0 (TID 1023)
15/08/06 17:54:43 INFO Executor: Running task 15.0 in stage 14.0 (TID 1032)
15/08/06 17:54:43 INFO Executor: Running task 8.0 in stage 14.0 (TID 1025)
15/08/06 17:54:43 INFO Executor: Running task 5.0 in stage 14.0 (TID 1022)
15/08/06 17:54:43 INFO Executor: Running task 10.0 in stage 14.0 (TID 1027)
15/08/06 17:54:43 INFO Executor: Running task 14.0 in stage 14.0 (TID 1031)
15/08/06 17:54:43 INFO Executor: Running task 9.0 in stage 14.0 (TID 1026)
15/08/06 17:54:43 INFO Executor: Running task 11.0 in stage 14.0 (TID 1028)
15/08/06 17:54:43 INFO Executor: Running task 12.0 in stage 14.0 (TID 1029)
15/08/06 17:54:43 INFO Executor: Running task 13.0 in stage 14.0 (TID 1030)
15/08/06 17:54:43 INFO Executor: Running task 7.0 in stage 14.0 (TID 1024)
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@382da75c
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000012_1029/part-00012
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@693120b9
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000007_1024/part-00007
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@405bd3a5
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000015_1032/part-00015
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1b9d7bca
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000014_1031/part-00014
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2a18f0
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000006_1023/part-00006
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4020328c
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000009_1026/part-00009
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@333aa779
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000003_1020/part-00003
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1371c235
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000001_1018/part-00001
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@11852f8c
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000000_1017/part-00000
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3ef49950
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@60286567
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2cf1ee7f
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000008_1025/part-00008
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3f60eae6
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000011_1028/part-00011
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e98e13e
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@26ebcd8d
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000005_1022/part-00005
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4303f7f1
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000013_1030/part-00013
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@398b1196
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3fbcf9e8
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@41d81ff0
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000010_1027/part-00010
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d78db8
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7eb2338f
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000002_1019/part-00002
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@20b5ad6c
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64c1a50
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@266ffc3e
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2195e29
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4f74d6d2
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6686d576
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@29b91276
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d9ab5a7
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000004_1021/part-00004
15/08/06 17:54:43 INFO CodecConfig: Compression set to false
15/08/06 17:54:43 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15e33cd0
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:43 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:43 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d4c75ed
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000007_1024' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000007
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000007_1024: Committed
15/08/06 17:54:43 INFO Executor: Finished task 7.0 in stage 14.0 (TID 1024). 781 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Starting task 16.0 in stage 14.0 (TID 1033, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 16.0 in stage 14.0 (TID 1033)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 7.0 in stage 14.0 (TID 1024) in 381 ms on localhost (1/200)
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000006_1023' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000006
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000006_1023: Committed
15/08/06 17:54:43 INFO Executor: Finished task 6.0 in stage 14.0 (TID 1023). 781 bytes result sent to driver
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000009_1026' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000009
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000009_1026: Committed
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000001_1018' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000001
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000001_1018: Committed
15/08/06 17:54:43 INFO TaskSetManager: Starting task 17.0 in stage 14.0 (TID 1034, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000003_1020' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000003
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000003_1020: Committed
15/08/06 17:54:43 INFO Executor: Running task 17.0 in stage 14.0 (TID 1034)
15/08/06 17:54:43 INFO Executor: Finished task 9.0 in stage 14.0 (TID 1026). 781 bytes result sent to driver
15/08/06 17:54:43 INFO Executor: Finished task 1.0 in stage 14.0 (TID 1018). 781 bytes result sent to driver
15/08/06 17:54:43 INFO Executor: Finished task 3.0 in stage 14.0 (TID 1020). 781 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 6.0 in stage 14.0 (TID 1023) in 385 ms on localhost (2/200)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 18.0 in stage 14.0 (TID 1035, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 18.0 in stage 14.0 (TID 1035)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 9.0 in stage 14.0 (TID 1026) in 387 ms on localhost (3/200)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 19.0 in stage 14.0 (TID 1036, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 20.0 in stage 14.0 (TID 1037, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 19.0 in stage 14.0 (TID 1036)
15/08/06 17:54:43 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6102562e
15/08/06 17:54:43 INFO Executor: Running task 20.0 in stage 14.0 (TID 1037)
15/08/06 17:54:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:43 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 1018) in 390 ms on localhost (4/200)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 1020) in 391 ms on localhost (5/200)
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000011_1028' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000011
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000011_1028: Committed
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000008_1025' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000008
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000008_1025: Committed
15/08/06 17:54:43 INFO Executor: Finished task 11.0 in stage 14.0 (TID 1028). 781 bytes result sent to driver
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000002_1019' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000002
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000002_1019: Committed
15/08/06 17:54:43 INFO TaskSetManager: Starting task 21.0 in stage 14.0 (TID 1038, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 21.0 in stage 14.0 (TID 1038)
15/08/06 17:54:43 INFO Executor: Finished task 2.0 in stage 14.0 (TID 1019). 781 bytes result sent to driver
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000010_1027' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000010
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000010_1027: Committed
15/08/06 17:54:43 INFO TaskSetManager: Finished task 11.0 in stage 14.0 (TID 1028) in 404 ms on localhost (6/200)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 22.0 in stage 14.0 (TID 1039, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 22.0 in stage 14.0 (TID 1039)
15/08/06 17:54:43 INFO Executor: Finished task 10.0 in stage 14.0 (TID 1027). 781 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 1019) in 406 ms on localhost (7/200)
15/08/06 17:54:43 INFO TaskSetManager: Starting task 23.0 in stage 14.0 (TID 1040, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 23.0 in stage 14.0 (TID 1040)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 10.0 in stage 14.0 (TID 1027) in 406 ms on localhost (8/200)
15/08/06 17:54:43 INFO Executor: Finished task 8.0 in stage 14.0 (TID 1025). 781 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Starting task 24.0 in stage 14.0 (TID 1041, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 24.0 in stage 14.0 (TID 1041)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 8.0 in stage 14.0 (TID 1025) in 409 ms on localhost (9/200)
15/08/06 17:54:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000004_1021' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000004
15/08/06 17:54:43 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000004_1021: Committed
15/08/06 17:54:43 INFO Executor: Finished task 4.0 in stage 14.0 (TID 1021). 781 bytes result sent to driver
15/08/06 17:54:43 INFO TaskSetManager: Starting task 25.0 in stage 14.0 (TID 1042, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:43 INFO Executor: Running task 25.0 in stage 14.0 (TID 1042)
15/08/06 17:54:43 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 1021) in 412 ms on localhost (10/200)
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10284932
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000017_1034/part-00017
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@286e5003
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000020_1037/part-00020
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53f596dc
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000019_1036/part-00019
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@20837094
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000022_1039/part-00022
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@15541539
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@716b19b5
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000021_1038/part-00021
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000018_1035/part-00018
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e64f43f
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2f2479fc
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000025_1042/part-00025
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e7f08ad
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000016_1033/part-00016
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6612c652
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000024_1041/part-00024
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a402120
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3373d1c9
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12a9ba9f
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@796f295e
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7105189a
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2749871c
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@11c19607
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a508b33
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000017_1034' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000017
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000017_1034: Committed
15/08/06 17:54:44 INFO Executor: Finished task 17.0 in stage 14.0 (TID 1034). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 26.0 in stage 14.0 (TID 1043, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 26.0 in stage 14.0 (TID 1043)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 17.0 in stage 14.0 (TID 1034) in 178 ms on localhost (11/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000022_1039' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000022
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000022_1039: Committed
15/08/06 17:54:44 INFO Executor: Finished task 22.0 in stage 14.0 (TID 1039). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 27.0 in stage 14.0 (TID 1044, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 27.0 in stage 14.0 (TID 1044)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 22.0 in stage 14.0 (TID 1039) in 161 ms on localhost (12/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000016_1033' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000016
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000016_1033: Committed
15/08/06 17:54:44 INFO Executor: Finished task 16.0 in stage 14.0 (TID 1033). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 28.0 in stage 14.0 (TID 1045, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 28.0 in stage 14.0 (TID 1045)
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4af83c4f
15/08/06 17:54:44 INFO TaskSetManager: Finished task 16.0 in stage 14.0 (TID 1033) in 189 ms on localhost (13/200)
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000023_1040/part-00023
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000021_1038' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000021
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000021_1038: Committed
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000025_1042' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000025
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000025_1042: Committed
15/08/06 17:54:44 INFO Executor: Finished task 21.0 in stage 14.0 (TID 1038). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 29.0 in stage 14.0 (TID 1046, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 29.0 in stage 14.0 (TID 1046)
15/08/06 17:54:44 INFO Executor: Finished task 25.0 in stage 14.0 (TID 1042). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Finished task 21.0 in stage 14.0 (TID 1038) in 169 ms on localhost (14/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 30.0 in stage 14.0 (TID 1047, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 30.0 in stage 14.0 (TID 1047)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 25.0 in stage 14.0 (TID 1042) in 163 ms on localhost (15/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000018_1035' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000018
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000018_1035: Committed
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000024_1041' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000024
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000024_1041: Committed
15/08/06 17:54:44 INFO Executor: Finished task 18.0 in stage 14.0 (TID 1035). 781 bytes result sent to driver
15/08/06 17:54:44 INFO Executor: Finished task 24.0 in stage 14.0 (TID 1041). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 31.0 in stage 14.0 (TID 1048, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 31.0 in stage 14.0 (TID 1048)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 18.0 in stage 14.0 (TID 1035) in 195 ms on localhost (16/200)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 24.0 in stage 14.0 (TID 1041) in 174 ms on localhost (17/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 32.0 in stage 14.0 (TID 1049, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 32.0 in stage 14.0 (TID 1049)
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@552664ce
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000023_1040' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000023
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000023_1040: Committed
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO Executor: Finished task 23.0 in stage 14.0 (TID 1040). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 33.0 in stage 14.0 (TID 1050, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 33.0 in stage 14.0 (TID 1050)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO TaskSetManager: Finished task 23.0 in stage 14.0 (TID 1040) in 201 ms on localhost (18/200)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000015_1032' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000015
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000015_1032: Committed
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000005_1022' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000005
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000005_1022: Committed
15/08/06 17:54:44 INFO Executor: Finished task 15.0 in stage 14.0 (TID 1032). 781 bytes result sent to driver
15/08/06 17:54:44 INFO Executor: Finished task 5.0 in stage 14.0 (TID 1022). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 34.0 in stage 14.0 (TID 1051, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 34.0 in stage 14.0 (TID 1051)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 35.0 in stage 14.0 (TID 1052, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 35.0 in stage 14.0 (TID 1052)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 15.0 in stage 14.0 (TID 1032) in 791 ms on localhost (19/200)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 5.0 in stage 14.0 (TID 1022) in 794 ms on localhost (20/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000013_1030' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000013
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000013_1030: Committed
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000014_1031' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000014
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000014_1031: Committed
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000012_1029' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000012
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000012_1029: Committed
15/08/06 17:54:44 INFO Executor: Finished task 13.0 in stage 14.0 (TID 1030). 781 bytes result sent to driver
15/08/06 17:54:44 INFO Executor: Finished task 14.0 in stage 14.0 (TID 1031). 781 bytes result sent to driver
15/08/06 17:54:44 INFO Executor: Finished task 12.0 in stage 14.0 (TID 1029). 781 bytes result sent to driver
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000000_1017' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000000
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000000_1017: Committed
15/08/06 17:54:44 INFO TaskSetManager: Starting task 36.0 in stage 14.0 (TID 1053, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 36.0 in stage 14.0 (TID 1053)
15/08/06 17:54:44 INFO Executor: Finished task 0.0 in stage 14.0 (TID 1017). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Finished task 13.0 in stage 14.0 (TID 1030) in 799 ms on localhost (21/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 37.0 in stage 14.0 (TID 1054, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 14.0 in stage 14.0 (TID 1031) in 800 ms on localhost (22/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 38.0 in stage 14.0 (TID 1055, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 37.0 in stage 14.0 (TID 1054)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 12.0 in stage 14.0 (TID 1029) in 801 ms on localhost (23/200)
15/08/06 17:54:44 INFO Executor: Running task 38.0 in stage 14.0 (TID 1055)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 39.0 in stage 14.0 (TID 1056, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 39.0 in stage 14.0 (TID 1056)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 1017) in 804 ms on localhost (24/200)
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10a9c47e
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000026_1043/part-00026
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57a5cc0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000030_1047/part-00030
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4eb2bb3d
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000028_1045/part-00028
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2376ebae
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000027_1044/part-00027
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@546afeb2
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a54c329
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000031_1048/part-00031
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d8dedf
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5119c01
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000029_1046/part-00029
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1da63a80
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@fd18e04
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16350b0d
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000032_1049/part-00032
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7c3a2586
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000026_1043' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000026
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000026_1043: Committed
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO Executor: Finished task 26.0 in stage 14.0 (TID 1043). 781 bytes result sent to driver
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@57f60804
15/08/06 17:54:44 INFO TaskSetManager: Starting task 40.0 in stage 14.0 (TID 1057, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO Executor: Running task 40.0 in stage 14.0 (TID 1057)
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO TaskSetManager: Finished task 26.0 in stage 14.0 (TID 1043) in 312 ms on localhost (25/200)
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000030_1047' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000030
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000030_1047: Committed
15/08/06 17:54:44 INFO Executor: Finished task 30.0 in stage 14.0 (TID 1047). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 41.0 in stage 14.0 (TID 1058, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 41.0 in stage 14.0 (TID 1058)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 30.0 in stage 14.0 (TID 1047) in 305 ms on localhost (26/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000028_1045' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000028
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000028_1045: Committed
15/08/06 17:54:44 INFO Executor: Finished task 28.0 in stage 14.0 (TID 1045). 781 bytes result sent to driver
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@54789237
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000033_1050/part-00033
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO TaskSetManager: Starting task 42.0 in stage 14.0 (TID 1059, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 42.0 in stage 14.0 (TID 1059)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 28.0 in stage 14.0 (TID 1045) in 317 ms on localhost (27/200)
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ca3e85e
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000029_1046' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000029
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000029_1046: Committed
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2f5aed05
15/08/06 17:54:44 INFO Executor: Finished task 29.0 in stage 14.0 (TID 1046). 781 bytes result sent to driver
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO TaskSetManager: Starting task 43.0 in stage 14.0 (TID 1060, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 43.0 in stage 14.0 (TID 1060)
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO TaskSetManager: Finished task 29.0 in stage 14.0 (TID 1046) in 327 ms on localhost (28/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000032_1049' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000032
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000032_1049: Committed
15/08/06 17:54:44 INFO Executor: Finished task 32.0 in stage 14.0 (TID 1049). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 44.0 in stage 14.0 (TID 1061, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 44.0 in stage 14.0 (TID 1061)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 32.0 in stage 14.0 (TID 1049) in 326 ms on localhost (29/200)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000033_1050' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000033
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000033_1050: Committed
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5569a330
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000034_1051/part-00034
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48095e72
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000035_1052/part-00035
15/08/06 17:54:44 INFO Executor: Finished task 33.0 in stage 14.0 (TID 1050). 781 bytes result sent to driver
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO TaskSetManager: Starting task 45.0 in stage 14.0 (TID 1062, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO Executor: Running task 45.0 in stage 14.0 (TID 1062)
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35efcdad
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000037_1054/part-00037
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO TaskSetManager: Finished task 33.0 in stage 14.0 (TID 1050) in 314 ms on localhost (30/200)
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b7ee77a
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000038_1055/part-00038
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@788f214e
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000039_1056/part-00039
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76ccad3f
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77824007
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@28ec21c4
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000036_1053/part-00036
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@269cdfd9
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2a489579
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34d509f
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@48e7f86a
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000019_1036' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000019
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000020_1037' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000020
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000020_1037: Committed
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000019_1036: Committed
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000034_1051' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000034
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000034_1051: Committed
15/08/06 17:54:44 INFO Executor: Finished task 19.0 in stage 14.0 (TID 1036). 781 bytes result sent to driver
15/08/06 17:54:44 INFO Executor: Finished task 20.0 in stage 14.0 (TID 1037). 781 bytes result sent to driver
15/08/06 17:54:44 INFO Executor: Finished task 34.0 in stage 14.0 (TID 1051). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 46.0 in stage 14.0 (TID 1063, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 46.0 in stage 14.0 (TID 1063)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 19.0 in stage 14.0 (TID 1036) in 589 ms on localhost (31/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 47.0 in stage 14.0 (TID 1064, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 47.0 in stage 14.0 (TID 1064)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 20.0 in stage 14.0 (TID 1037) in 591 ms on localhost (32/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 48.0 in stage 14.0 (TID 1065, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 48.0 in stage 14.0 (TID 1065)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 34.0 in stage 14.0 (TID 1051) in 189 ms on localhost (33/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000036_1053' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000036
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000036_1053: Committed
15/08/06 17:54:44 INFO Executor: Finished task 36.0 in stage 14.0 (TID 1053). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 49.0 in stage 14.0 (TID 1066, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 49.0 in stage 14.0 (TID 1066)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000039_1056' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000039
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000039_1056: Committed
15/08/06 17:54:44 INFO TaskSetManager: Finished task 36.0 in stage 14.0 (TID 1053) in 186 ms on localhost (34/200)
15/08/06 17:54:44 INFO Executor: Finished task 39.0 in stage 14.0 (TID 1056). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 50.0 in stage 14.0 (TID 1067, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 50.0 in stage 14.0 (TID 1067)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 39.0 in stage 14.0 (TID 1056) in 186 ms on localhost (35/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000038_1055' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000038
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000038_1055: Committed
15/08/06 17:54:44 INFO Executor: Finished task 38.0 in stage 14.0 (TID 1055). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 51.0 in stage 14.0 (TID 1068, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 51.0 in stage 14.0 (TID 1068)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 38.0 in stage 14.0 (TID 1055) in 190 ms on localhost (36/200)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@29ecc6cf
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000040_1057/part-00040
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f7efdb0
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2a414e61
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000041_1058/part-00041
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7659c3ca
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000044_1061/part-00044
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@801aa0a
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000043_1060/part-00043
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d0281c4
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2f95aafa
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000042_1059/part-00042
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1686e62
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@47e0f2dc
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000045_1062/part-00045
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000040_1057' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000040
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000040_1057: Committed
15/08/06 17:54:44 INFO Executor: Finished task 40.0 in stage 14.0 (TID 1057). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 52.0 in stage 14.0 (TID 1069, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 52.0 in stage 14.0 (TID 1069)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 40.0 in stage 14.0 (TID 1057) in 184 ms on localhost (37/200)
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f7e227b
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@40d920aa
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79ac4e22
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000041_1058' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000041
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000041_1058: Committed
15/08/06 17:54:44 INFO Executor: Finished task 41.0 in stage 14.0 (TID 1058). 781 bytes result sent to driver
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000044_1061' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000044
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000044_1061: Committed
15/08/06 17:54:44 INFO TaskSetManager: Starting task 53.0 in stage 14.0 (TID 1070, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 53.0 in stage 14.0 (TID 1070)
15/08/06 17:54:44 INFO Executor: Finished task 44.0 in stage 14.0 (TID 1061). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Finished task 41.0 in stage 14.0 (TID 1058) in 208 ms on localhost (38/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 54.0 in stage 14.0 (TID 1071, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 54.0 in stage 14.0 (TID 1071)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 44.0 in stage 14.0 (TID 1061) in 176 ms on localhost (39/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000043_1060' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000043
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000043_1060: Committed
15/08/06 17:54:44 INFO Executor: Finished task 43.0 in stage 14.0 (TID 1060). 781 bytes result sent to driver
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000045_1062' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000045
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000045_1062: Committed
15/08/06 17:54:44 INFO TaskSetManager: Starting task 55.0 in stage 14.0 (TID 1072, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 55.0 in stage 14.0 (TID 1072)
15/08/06 17:54:44 INFO Executor: Finished task 45.0 in stage 14.0 (TID 1062). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Finished task 43.0 in stage 14.0 (TID 1060) in 196 ms on localhost (40/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 56.0 in stage 14.0 (TID 1073, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 56.0 in stage 14.0 (TID 1073)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 45.0 in stage 14.0 (TID 1062) in 175 ms on localhost (41/200)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000042_1059' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000042
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000042_1059: Committed
15/08/06 17:54:44 INFO Executor: Finished task 42.0 in stage 14.0 (TID 1059). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 57.0 in stage 14.0 (TID 1074, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 57.0 in stage 14.0 (TID 1074)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 42.0 in stage 14.0 (TID 1059) in 217 ms on localhost (42/200)
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17b8e9ed
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000048_1065/part-00048
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1950bb6e
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000046_1063/part-00046
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c5796a0
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000047_1064/part-00047
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6eff15a6
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000050_1067/part-00050
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7db4683c
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b4706f
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f817a17
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a572d5c
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000049_1066/part-00049
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@549267fc
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23029e1e
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000051_1068/part-00051
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f076670
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4fc91375
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000046_1063' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000046
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000046_1063: Committed
15/08/06 17:54:44 INFO Executor: Finished task 46.0 in stage 14.0 (TID 1063). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 58.0 in stage 14.0 (TID 1075, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 58.0 in stage 14.0 (TID 1075)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 46.0 in stage 14.0 (TID 1063) in 293 ms on localhost (43/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000048_1065' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000048
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000048_1065: Committed
15/08/06 17:54:44 INFO Executor: Finished task 48.0 in stage 14.0 (TID 1065). 781 bytes result sent to driver
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000050_1067' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000050
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000050_1067: Committed
15/08/06 17:54:44 INFO TaskSetManager: Starting task 59.0 in stage 14.0 (TID 1076, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Finished task 50.0 in stage 14.0 (TID 1067). 781 bytes result sent to driver
15/08/06 17:54:44 INFO Executor: Running task 59.0 in stage 14.0 (TID 1076)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 48.0 in stage 14.0 (TID 1065) in 295 ms on localhost (44/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 60.0 in stage 14.0 (TID 1077, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 60.0 in stage 14.0 (TID 1077)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 50.0 in stage 14.0 (TID 1067) in 289 ms on localhost (45/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000051_1068' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000051
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000051_1068: Committed
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000049_1066' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000049
15/08/06 17:54:44 INFO Executor: Finished task 51.0 in stage 14.0 (TID 1068). 781 bytes result sent to driver
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000049_1066: Committed
15/08/06 17:54:44 INFO TaskSetManager: Starting task 61.0 in stage 14.0 (TID 1078, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 61.0 in stage 14.0 (TID 1078)
15/08/06 17:54:44 INFO Executor: Finished task 49.0 in stage 14.0 (TID 1066). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Finished task 51.0 in stage 14.0 (TID 1068) in 290 ms on localhost (46/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 62.0 in stage 14.0 (TID 1079, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 62.0 in stage 14.0 (TID 1079)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 49.0 in stage 14.0 (TID 1066) in 300 ms on localhost (47/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000027_1044' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000027
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000027_1044: Committed
15/08/06 17:54:44 INFO Executor: Finished task 27.0 in stage 14.0 (TID 1044). 781 bytes result sent to driver
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000031_1048' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000031
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000031_1048: Committed
15/08/06 17:54:44 INFO TaskSetManager: Starting task 63.0 in stage 14.0 (TID 1080, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 63.0 in stage 14.0 (TID 1080)
15/08/06 17:54:44 INFO Executor: Finished task 31.0 in stage 14.0 (TID 1048). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Finished task 27.0 in stage 14.0 (TID 1044) in 728 ms on localhost (48/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 64.0 in stage 14.0 (TID 1081, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 64.0 in stage 14.0 (TID 1081)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 31.0 in stage 14.0 (TID 1048) in 715 ms on localhost (49/200)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35237f74
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000052_1069/part-00052
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@ca150e5
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4000f7f9
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000054_1071/part-00054
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000052_1069' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000052
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000052_1069: Committed
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2cddf764
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000053_1070/part-00053
15/08/06 17:54:44 INFO Executor: Finished task 52.0 in stage 14.0 (TID 1069). 781 bytes result sent to driver
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO TaskSetManager: Starting task 65.0 in stage 14.0 (TID 1082, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 65.0 in stage 14.0 (TID 1082)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 52.0 in stage 14.0 (TID 1069) in 281 ms on localhost (50/200)
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@29928eb6
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000056_1073/part-00056
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48dbf596
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000055_1072/part-00055
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@620c19ed
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1447b6bc
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@238c0d9c
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@70c23d7d
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000057_1074/part-00057
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f84970b
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000035_1052' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000035
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000035_1052: Committed
15/08/06 17:54:44 INFO Executor: Finished task 35.0 in stage 14.0 (TID 1052). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 66.0 in stage 14.0 (TID 1083, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 66.0 in stage 14.0 (TID 1083)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 35.0 in stage 14.0 (TID 1052) in 579 ms on localhost (51/200)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000054_1071' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000054
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000054_1071: Committed
15/08/06 17:54:44 INFO Executor: Finished task 54.0 in stage 14.0 (TID 1071). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 67.0 in stage 14.0 (TID 1084, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000037_1054' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000037
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000037_1054: Committed
15/08/06 17:54:44 INFO TaskSetManager: Finished task 54.0 in stage 14.0 (TID 1071) in 297 ms on localhost (52/200)
15/08/06 17:54:44 INFO Executor: Running task 67.0 in stage 14.0 (TID 1084)
15/08/06 17:54:44 INFO Executor: Finished task 37.0 in stage 14.0 (TID 1054). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 68.0 in stage 14.0 (TID 1085, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 68.0 in stage 14.0 (TID 1085)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 37.0 in stage 14.0 (TID 1054) in 587 ms on localhost (53/200)
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000056_1073' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000056
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000056_1073: Committed
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000055_1072' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000055
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000055_1072: Committed
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1c60f915
15/08/06 17:54:44 INFO Executor: Finished task 56.0 in stage 14.0 (TID 1073). 781 bytes result sent to driver
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO Executor: Finished task 55.0 in stage 14.0 (TID 1072). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 69.0 in stage 14.0 (TID 1086, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 69.0 in stage 14.0 (TID 1086)
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO TaskSetManager: Finished task 56.0 in stage 14.0 (TID 1073) in 298 ms on localhost (54/200)
15/08/06 17:54:44 INFO TaskSetManager: Starting task 70.0 in stage 14.0 (TID 1087, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 70.0 in stage 14.0 (TID 1087)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 55.0 in stage 14.0 (TID 1072) in 301 ms on localhost (55/200)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@18242136
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000059_1076/part-00059
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@fda6b82
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000061_1078/part-00061
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2f6acc4
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000062_1079/part-00062
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5998d7e2
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2295818
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000063_1080/part-00063
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@583e58c1
15/08/06 17:54:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000057_1074' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000057
15/08/06 17:54:44 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000057_1074: Committed
15/08/06 17:54:44 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@510f6e89
15/08/06 17:54:44 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000058_1075/part-00058
15/08/06 17:54:44 INFO CodecConfig: Compression set to false
15/08/06 17:54:44 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:44 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:44 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:44 INFO Executor: Finished task 57.0 in stage 14.0 (TID 1074). 781 bytes result sent to driver
15/08/06 17:54:44 INFO TaskSetManager: Starting task 71.0 in stage 14.0 (TID 1088, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:44 INFO Executor: Running task 71.0 in stage 14.0 (TID 1088)
15/08/06 17:54:44 INFO TaskSetManager: Finished task 57.0 in stage 14.0 (TID 1074) in 342 ms on localhost (56/200)
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@27957ac
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000061_1078' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000061
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000061_1078: Committed
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Finished task 61.0 in stage 14.0 (TID 1078). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 72.0 in stage 14.0 (TID 1089, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 72.0 in stage 14.0 (TID 1089)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 61.0 in stage 14.0 (TID 1078) in 179 ms on localhost (57/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f15ed5f
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f612146
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3ebdf58b
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000060_1077/part-00060
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@50a4d2a2
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000066_1083/part-00066
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32857fcc
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000062_1079' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000062
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000062_1079: Committed
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Finished task 62.0 in stage 14.0 (TID 1079). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 73.0 in stage 14.0 (TID 1090, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 73.0 in stage 14.0 (TID 1090)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 62.0 in stage 14.0 (TID 1079) in 210 ms on localhost (58/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5aaeb328
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000064_1081/part-00064
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@374359b
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000065_1082/part-00065
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5478f4a4
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@71f3ebca
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000059_1076' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000059
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@44fd5538
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000058_1075' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000058
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000058_1075: Committed
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Finished task 58.0 in stage 14.0 (TID 1075). 781 bytes result sent to driver
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000059_1076: Committed
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO TaskSetManager: Starting task 74.0 in stage 14.0 (TID 1091, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Finished task 59.0 in stage 14.0 (TID 1076). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Running task 74.0 in stage 14.0 (TID 1091)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 58.0 in stage 14.0 (TID 1075) in 266 ms on localhost (59/200)
15/08/06 17:54:45 INFO TaskSetManager: Starting task 75.0 in stage 14.0 (TID 1092, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 59.0 in stage 14.0 (TID 1076) in 263 ms on localhost (60/200)
15/08/06 17:54:45 INFO Executor: Running task 75.0 in stage 14.0 (TID 1092)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000060_1077' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000060
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000060_1077: Committed
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO Executor: Finished task 60.0 in stage 14.0 (TID 1077). 781 bytes result sent to driver
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000063_1080' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000063
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000063_1080: Committed
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000066_1083' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000066
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000066_1083: Committed
15/08/06 17:54:45 INFO TaskSetManager: Starting task 76.0 in stage 14.0 (TID 1093, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Finished task 63.0 in stage 14.0 (TID 1080). 781 bytes result sent to driver
15/08/06 17:54:45 INFO Executor: Finished task 66.0 in stage 14.0 (TID 1083). 781 bytes result sent to driver
15/08/06 17:54:45 INFO Executor: Running task 76.0 in stage 14.0 (TID 1093)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 60.0 in stage 14.0 (TID 1077) in 273 ms on localhost (61/200)
15/08/06 17:54:45 INFO TaskSetManager: Starting task 77.0 in stage 14.0 (TID 1094, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 77.0 in stage 14.0 (TID 1094)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 63.0 in stage 14.0 (TID 1080) in 257 ms on localhost (62/200)
15/08/06 17:54:45 INFO TaskSetManager: Starting task 78.0 in stage 14.0 (TID 1095, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 66.0 in stage 14.0 (TID 1083) in 185 ms on localhost (63/200)
15/08/06 17:54:45 INFO Executor: Running task 78.0 in stage 14.0 (TID 1095)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@162b59a8
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000067_1084/part-00067
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a750c06
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000068_1085/part-00068
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000064_1081' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000064
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000064_1081: Committed
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ccd7099
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6d4be11b
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO Executor: Finished task 64.0 in stage 14.0 (TID 1081). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO TaskSetManager: Starting task 79.0 in stage 14.0 (TID 1096, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO Executor: Running task 79.0 in stage 14.0 (TID 1096)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 64.0 in stage 14.0 (TID 1081) in 412 ms on localhost (64/200)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000065_1082' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000065
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000065_1082: Committed
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO Executor: Finished task 65.0 in stage 14.0 (TID 1082). 781 bytes result sent to driver
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000047_1064' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000047
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000047_1064: Committed
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO Executor: Finished task 47.0 in stage 14.0 (TID 1064). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO TaskSetManager: Starting task 80.0 in stage 14.0 (TID 1097, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 80.0 in stage 14.0 (TID 1097)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 65.0 in stage 14.0 (TID 1082) in 379 ms on localhost (65/200)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO TaskSetManager: Starting task 81.0 in stage 14.0 (TID 1098, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 81.0 in stage 14.0 (TID 1098)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 47.0 in stage 14.0 (TID 1064) in 736 ms on localhost (66/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2b4568b6
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000070_1087/part-00070
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000067_1084' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000067
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000067_1084: Committed
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000068_1085' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000068
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000068_1085: Committed
15/08/06 17:54:45 INFO Executor: Finished task 67.0 in stage 14.0 (TID 1084). 781 bytes result sent to driver
15/08/06 17:54:45 INFO Executor: Finished task 68.0 in stage 14.0 (TID 1085). 781 bytes result sent to driver
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@290da81f
15/08/06 17:54:45 INFO TaskSetManager: Starting task 82.0 in stage 14.0 (TID 1099, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000072_1089/part-00072
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO TaskSetManager: Finished task 67.0 in stage 14.0 (TID 1084) in 356 ms on localhost (67/200)
15/08/06 17:54:45 INFO Executor: Running task 82.0 in stage 14.0 (TID 1099)
15/08/06 17:54:45 INFO TaskSetManager: Starting task 83.0 in stage 14.0 (TID 1100, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 83.0 in stage 14.0 (TID 1100)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 68.0 in stage 14.0 (TID 1085) in 354 ms on localhost (68/200)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5746f98f
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@430e68ee
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000071_1088/part-00071
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b43c158
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66d5fe70
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000070_1087' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000070
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000070_1087: Committed
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Finished task 70.0 in stage 14.0 (TID 1087). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 84.0 in stage 14.0 (TID 1101, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 84.0 in stage 14.0 (TID 1101)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO TaskSetManager: Finished task 70.0 in stage 14.0 (TID 1087) in 372 ms on localhost (69/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@99c2ecf
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000073_1090/part-00073
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000072_1089' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000072
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000072_1089: Committed
15/08/06 17:54:45 INFO Executor: Finished task 72.0 in stage 14.0 (TID 1089). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 85.0 in stage 14.0 (TID 1102, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 85.0 in stage 14.0 (TID 1102)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 72.0 in stage 14.0 (TID 1089) in 313 ms on localhost (70/200)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000053_1070' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000053
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000053_1070: Committed
15/08/06 17:54:45 INFO Executor: Finished task 53.0 in stage 14.0 (TID 1070). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 86.0 in stage 14.0 (TID 1103, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 86.0 in stage 14.0 (TID 1103)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 53.0 in stage 14.0 (TID 1070) in 693 ms on localhost (71/200)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1179e1fb
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@416dc62f
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000069_1086/part-00069
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5581e4a
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000077_1094/part-00077
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@36d8f743
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000074_1091/part-00074
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000071_1088' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000071
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000071_1088: Committed
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO Executor: Finished task 71.0 in stage 14.0 (TID 1088). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO TaskSetManager: Starting task 87.0 in stage 14.0 (TID 1104, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO Executor: Running task 87.0 in stage 14.0 (TID 1104)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 71.0 in stage 14.0 (TID 1088) in 350 ms on localhost (72/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24af8fc9
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000078_1095/part-00078
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c89a221
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64727568
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000073_1090' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000073
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000073_1090: Committed
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@69e9c8af
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO Executor: Finished task 73.0 in stage 14.0 (TID 1090). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO TaskSetManager: Starting task 88.0 in stage 14.0 (TID 1105, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 88.0 in stage 14.0 (TID 1105)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 73.0 in stage 14.0 (TID 1090) in 319 ms on localhost (73/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@38bd336e
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000077_1094' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000077
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000077_1094: Committed
15/08/06 17:54:45 INFO Executor: Finished task 77.0 in stage 14.0 (TID 1094). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 89.0 in stage 14.0 (TID 1106, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 89.0 in stage 14.0 (TID 1106)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 77.0 in stage 14.0 (TID 1094) in 283 ms on localhost (74/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51a559e3
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000080_1097/part-00080
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000074_1091' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000074
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000074_1091: Committed
15/08/06 17:54:45 INFO Executor: Finished task 74.0 in stage 14.0 (TID 1091). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 90.0 in stage 14.0 (TID 1107, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000078_1095' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000078
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000078_1095: Committed
15/08/06 17:54:45 INFO Executor: Running task 90.0 in stage 14.0 (TID 1107)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 74.0 in stage 14.0 (TID 1091) in 305 ms on localhost (75/200)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO Executor: Finished task 78.0 in stage 14.0 (TID 1095). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 91.0 in stage 14.0 (TID 1108, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 91.0 in stage 14.0 (TID 1108)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 78.0 in stage 14.0 (TID 1095) in 292 ms on localhost (76/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7c4cb1d3
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@493d4153
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000076_1093/part-00076
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5509f08c
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000075_1092/part-00075
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@346fd2c
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000082_1099/part-00082
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ded2648
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3c6c5051
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@201cb2e4
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000080_1097' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000080
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000080_1097: Committed
15/08/06 17:54:45 INFO Executor: Finished task 80.0 in stage 14.0 (TID 1097). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 92.0 in stage 14.0 (TID 1109, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 92.0 in stage 14.0 (TID 1109)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 80.0 in stage 14.0 (TID 1097) in 170 ms on localhost (77/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6fab2e3f
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000079_1096/part-00079
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@13c1c132
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1572c3c0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@21917d40
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000084_1101/part-00084
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000086_1103/part-00086
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000075_1092' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000075
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000075_1092: Committed
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@152bf998
15/08/06 17:54:45 INFO Executor: Finished task 75.0 in stage 14.0 (TID 1092). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000085_1102/part-00085
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO TaskSetManager: Starting task 93.0 in stage 14.0 (TID 1110, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000082_1099' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000082
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000082_1099: Committed
15/08/06 17:54:45 INFO Executor: Running task 93.0 in stage 14.0 (TID 1110)
15/08/06 17:54:45 INFO Executor: Finished task 82.0 in stage 14.0 (TID 1099). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Finished task 75.0 in stage 14.0 (TID 1092) in 366 ms on localhost (78/200)
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO TaskSetManager: Starting task 94.0 in stage 14.0 (TID 1111, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO Executor: Running task 94.0 in stage 14.0 (TID 1111)
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO TaskSetManager: Finished task 82.0 in stage 14.0 (TID 1099) in 166 ms on localhost (79/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@390512f0
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000083_1100/part-00083
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b984566
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d0630d7
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1add1022
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000079_1096' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000079
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000079_1096: Committed
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000081_1098/part-00081
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7fb87bfb
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO Executor: Finished task 79.0 in stage 14.0 (TID 1096). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 95.0 in stage 14.0 (TID 1112, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 95.0 in stage 14.0 (TID 1112)
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d4b8e6d
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO TaskSetManager: Finished task 79.0 in stage 14.0 (TID 1096) in 217 ms on localhost (80/200)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2f26ac7c
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@74e654ce
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000088_1105/part-00088
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000083_1100' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000083
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000083_1100: Committed
15/08/06 17:54:45 INFO Executor: Finished task 83.0 in stage 14.0 (TID 1100). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 96.0 in stage 14.0 (TID 1113, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 96.0 in stage 14.0 (TID 1113)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 83.0 in stage 14.0 (TID 1100) in 219 ms on localhost (81/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14476c14
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000081_1098' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000081
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000081_1098: Committed
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Finished task 81.0 in stage 14.0 (TID 1098). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO TaskSetManager: Starting task 97.0 in stage 14.0 (TID 1114, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 97.0 in stage 14.0 (TID 1114)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 81.0 in stage 14.0 (TID 1098) in 251 ms on localhost (82/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f07383f
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000089_1106/part-00089
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@289e12b9
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000087_1104/part-00087
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d9b008d
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000091_1108/part-00091
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000088_1105' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000088
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000088_1105: Committed
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2de64023
15/08/06 17:54:45 INFO Executor: Finished task 88.0 in stage 14.0 (TID 1105). 781 bytes result sent to driver
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO TaskSetManager: Starting task 98.0 in stage 14.0 (TID 1115, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Running task 98.0 in stage 14.0 (TID 1115)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 88.0 in stage 14.0 (TID 1105) in 177 ms on localhost (83/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@632cb255
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79cd3109
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@730b96d
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000090_1107/part-00090
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000089_1106' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000089
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000089_1106: Committed
15/08/06 17:54:45 INFO Executor: Finished task 89.0 in stage 14.0 (TID 1106). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 99.0 in stage 14.0 (TID 1116, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 99.0 in stage 14.0 (TID 1116)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 89.0 in stage 14.0 (TID 1106) in 354 ms on localhost (84/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@270af01d
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000091_1108' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000091
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000091_1108: Committed
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO Executor: Finished task 91.0 in stage 14.0 (TID 1108). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 100.0 in stage 14.0 (TID 1117, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 100.0 in stage 14.0 (TID 1117)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 91.0 in stage 14.0 (TID 1108) in 349 ms on localhost (85/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7bcdedab
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000092_1109/part-00092
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c775143
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000095_1112/part-00095
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000090_1107' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000090
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000090_1107: Committed
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6599e7e9
15/08/06 17:54:45 INFO Executor: Finished task 90.0 in stage 14.0 (TID 1107). 781 bytes result sent to driver
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO TaskSetManager: Starting task 101.0 in stage 14.0 (TID 1118, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Running task 101.0 in stage 14.0 (TID 1118)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 90.0 in stage 14.0 (TID 1107) in 375 ms on localhost (86/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ad38adc
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2fd6cae9
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000094_1111/part-00094
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000092_1109' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000092
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000092_1109: Committed
15/08/06 17:54:45 INFO Executor: Finished task 92.0 in stage 14.0 (TID 1109). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 102.0 in stage 14.0 (TID 1119, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 102.0 in stage 14.0 (TID 1119)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 92.0 in stage 14.0 (TID 1109) in 353 ms on localhost (87/200)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000069_1086' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000069
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000069_1086: Committed
15/08/06 17:54:45 INFO Executor: Finished task 69.0 in stage 14.0 (TID 1086). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 103.0 in stage 14.0 (TID 1120, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 103.0 in stage 14.0 (TID 1120)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 69.0 in stage 14.0 (TID 1086) in 847 ms on localhost (88/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b1a284e
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@669bef85
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000093_1110/part-00093
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@27d87c53
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000096_1113/part-00096
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@53c50195
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fdd6944
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000097_1114/part-00097
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000094_1111' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000094
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000094_1111: Committed
15/08/06 17:54:45 INFO Executor: Finished task 94.0 in stage 14.0 (TID 1111). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@eacfb5e
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO TaskSetManager: Starting task 104.0 in stage 14.0 (TID 1121, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 104.0 in stage 14.0 (TID 1121)
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO TaskSetManager: Finished task 94.0 in stage 14.0 (TID 1111) in 376 ms on localhost (89/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@65e4b317
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000098_1115/part-00098
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ab247f
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000093_1110' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000093
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000093_1110: Committed
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Finished task 93.0 in stage 14.0 (TID 1110). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 105.0 in stage 14.0 (TID 1122, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 105.0 in stage 14.0 (TID 1122)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000076_1093' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000076
15/08/06 17:54:45 INFO TaskSetManager: Finished task 93.0 in stage 14.0 (TID 1110) in 393 ms on localhost (90/200)
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000076_1093: Committed
15/08/06 17:54:45 INFO Executor: Finished task 76.0 in stage 14.0 (TID 1093). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 106.0 in stage 14.0 (TID 1123, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 106.0 in stage 14.0 (TID 1123)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@114cebeb
15/08/06 17:54:45 INFO TaskSetManager: Finished task 76.0 in stage 14.0 (TID 1093) in 751 ms on localhost (91/200)
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000096_1113' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000096
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000096_1113: Committed
15/08/06 17:54:45 INFO Executor: Finished task 96.0 in stage 14.0 (TID 1113). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 107.0 in stage 14.0 (TID 1124, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 107.0 in stage 14.0 (TID 1124)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 96.0 in stage 14.0 (TID 1113) in 345 ms on localhost (92/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@346e348a
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000100_1117/part-00100
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000097_1114' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000097
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000097_1114: Committed
15/08/06 17:54:45 INFO Executor: Finished task 97.0 in stage 14.0 (TID 1114). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 108.0 in stage 14.0 (TID 1125, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 108.0 in stage 14.0 (TID 1125)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000098_1115' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000098
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000098_1115: Committed
15/08/06 17:54:45 INFO TaskSetManager: Finished task 97.0 in stage 14.0 (TID 1114) in 353 ms on localhost (93/200)
15/08/06 17:54:45 INFO Executor: Finished task 98.0 in stage 14.0 (TID 1115). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 109.0 in stage 14.0 (TID 1126, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 109.0 in stage 14.0 (TID 1126)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 98.0 in stage 14.0 (TID 1115) in 333 ms on localhost (94/200)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@18e2184c
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@158d3da0
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000099_1116/part-00099
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b518cd1
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000102_1119/part-00102
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000086_1103' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000086
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000086_1103: Committed
15/08/06 17:54:45 INFO Executor: Finished task 86.0 in stage 14.0 (TID 1103). 781 bytes result sent to driver
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5cec570b
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000101_1118/part-00101
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000084_1101' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000084
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000084_1101: Committed
15/08/06 17:54:45 INFO TaskSetManager: Starting task 110.0 in stage 14.0 (TID 1127, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO Executor: Running task 110.0 in stage 14.0 (TID 1127)
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO Executor: Finished task 84.0 in stage 14.0 (TID 1101). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Finished task 86.0 in stage 14.0 (TID 1103) in 568 ms on localhost (95/200)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000085_1102' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000085
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000085_1102: Committed
15/08/06 17:54:45 INFO TaskSetManager: Starting task 111.0 in stage 14.0 (TID 1128, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 111.0 in stage 14.0 (TID 1128)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 84.0 in stage 14.0 (TID 1101) in 583 ms on localhost (96/200)
15/08/06 17:54:45 INFO Executor: Finished task 85.0 in stage 14.0 (TID 1102). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO TaskSetManager: Starting task 112.0 in stage 14.0 (TID 1129, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 112.0 in stage 14.0 (TID 1129)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO TaskSetManager: Finished task 85.0 in stage 14.0 (TID 1102) in 577 ms on localhost (97/200)
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64c837c5
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f0ebab7
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000100_1117' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000100
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000100_1117: Committed
15/08/06 17:54:45 INFO Executor: Finished task 100.0 in stage 14.0 (TID 1117). 781 bytes result sent to driver
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3bcf3250
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO TaskSetManager: Starting task 113.0 in stage 14.0 (TID 1130, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO Executor: Running task 113.0 in stage 14.0 (TID 1130)
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8c631c3
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000103_1120/part-00103
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO TaskSetManager: Finished task 100.0 in stage 14.0 (TID 1117) in 168 ms on localhost (98/200)
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2738ccda
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000099_1116' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000099
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000099_1116: Committed
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO Executor: Finished task 99.0 in stage 14.0 (TID 1116). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 114.0 in stage 14.0 (TID 1131, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 114.0 in stage 14.0 (TID 1131)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 99.0 in stage 14.0 (TID 1116) in 205 ms on localhost (99/200)
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000103_1120' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000103
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000103_1120: Committed
15/08/06 17:54:45 INFO Executor: Finished task 103.0 in stage 14.0 (TID 1120). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 115.0 in stage 14.0 (TID 1132, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO Executor: Running task 115.0 in stage 14.0 (TID 1132)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 103.0 in stage 14.0 (TID 1120) in 169 ms on localhost (100/200)
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@15ece3dd
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000106_1123/part-00106
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f1d6a8b
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@29af1c7
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000109_1126/part-00109
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4701980e
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000108_1125/part-00108
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:45 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:45 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000106_1123' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000106
15/08/06 17:54:45 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000106_1123: Committed
15/08/06 17:54:45 INFO Executor: Finished task 106.0 in stage 14.0 (TID 1123). 781 bytes result sent to driver
15/08/06 17:54:45 INFO TaskSetManager: Starting task 116.0 in stage 14.0 (TID 1133, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6cd8a66e
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:45 INFO Executor: Running task 116.0 in stage 14.0 (TID 1133)
15/08/06 17:54:45 INFO TaskSetManager: Finished task 106.0 in stage 14.0 (TID 1123) in 148 ms on localhost (101/200)
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1beedab1
15/08/06 17:54:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:45 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@d64ea8b
15/08/06 17:54:45 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000104_1121/part-00104
15/08/06 17:54:45 INFO CodecConfig: Compression set to false
15/08/06 17:54:45 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b19530f
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000111_1128/part-00111
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b06d6c3
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000105_1122/part-00105
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@44447529
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000109_1126' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000109
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000109_1126: Committed
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@55c2c4f1
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO Executor: Finished task 109.0 in stage 14.0 (TID 1126). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@684c4913
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4bf93196
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5835f803
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000112_1129/part-00112
15/08/06 17:54:46 INFO TaskSetManager: Starting task 117.0 in stage 14.0 (TID 1134, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO TaskSetManager: Finished task 109.0 in stage 14.0 (TID 1126) in 153 ms on localhost (102/200)
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000110_1127/part-00110
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000108_1125' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000108
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000108_1125: Committed
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO Executor: Running task 117.0 in stage 14.0 (TID 1134)
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO Executor: Finished task 108.0 in stage 14.0 (TID 1125). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO TaskSetManager: Starting task 118.0 in stage 14.0 (TID 1135, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@405b2af4
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000107_1124/part-00107
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO Executor: Running task 118.0 in stage 14.0 (TID 1135)
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO TaskSetManager: Finished task 108.0 in stage 14.0 (TID 1125) in 160 ms on localhost (103/200)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@47ff4cec
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000111_1128' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000111
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000111_1128: Committed
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@63796e05
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO Executor: Finished task 111.0 in stage 14.0 (TID 1128). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 119.0 in stage 14.0 (TID 1136, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 119.0 in stage 14.0 (TID 1136)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@33ddcc30
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO TaskSetManager: Finished task 111.0 in stage 14.0 (TID 1128) in 149 ms on localhost (104/200)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000104_1121' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000104
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000104_1121: Committed
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6021d430
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000114_1131/part-00114
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO Executor: Finished task 104.0 in stage 14.0 (TID 1121). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 120.0 in stage 14.0 (TID 1137, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 120.0 in stage 14.0 (TID 1137)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 104.0 in stage 14.0 (TID 1121) in 220 ms on localhost (105/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3102f772
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000095_1112' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000095
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000095_1112: Committed
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000087_1104' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000087
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000087_1104: Committed
15/08/06 17:54:46 INFO Executor: Finished task 95.0 in stage 14.0 (TID 1112). 781 bytes result sent to driver
15/08/06 17:54:46 INFO Executor: Finished task 87.0 in stage 14.0 (TID 1104). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 121.0 in stage 14.0 (TID 1138, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 121.0 in stage 14.0 (TID 1138)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000107_1124' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000107
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000107_1124: Committed
15/08/06 17:54:46 INFO TaskSetManager: Finished task 95.0 in stage 14.0 (TID 1112) in 723 ms on localhost (106/200)
15/08/06 17:54:46 INFO Executor: Finished task 107.0 in stage 14.0 (TID 1124). 781 bytes result sent to driver
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000112_1129' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000112
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000112_1129: Committed
15/08/06 17:54:46 INFO TaskSetManager: Starting task 122.0 in stage 14.0 (TID 1139, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 122.0 in stage 14.0 (TID 1139)
15/08/06 17:54:46 INFO Executor: Finished task 112.0 in stage 14.0 (TID 1129). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Finished task 87.0 in stage 14.0 (TID 1104) in 854 ms on localhost (107/200)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 123.0 in stage 14.0 (TID 1140, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 123.0 in stage 14.0 (TID 1140)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 107.0 in stage 14.0 (TID 1124) in 345 ms on localhost (108/200)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 124.0 in stage 14.0 (TID 1141, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 124.0 in stage 14.0 (TID 1141)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000114_1131' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000114
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000114_1131: Committed
15/08/06 17:54:46 INFO TaskSetManager: Finished task 112.0 in stage 14.0 (TID 1129) in 300 ms on localhost (109/200)
15/08/06 17:54:46 INFO Executor: Finished task 114.0 in stage 14.0 (TID 1131). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 125.0 in stage 14.0 (TID 1142, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 125.0 in stage 14.0 (TID 1142)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO TaskSetManager: Finished task 114.0 in stage 14.0 (TID 1131) in 262 ms on localhost (110/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42359b90
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000113_1130/part-00113
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5495bc1e
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@46ac4ad0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000115_1132/part-00115
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@524c7d3d
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000113_1130' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000113
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000113_1130: Committed
15/08/06 17:54:46 INFO Executor: Finished task 113.0 in stage 14.0 (TID 1130). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 126.0 in stage 14.0 (TID 1143, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 126.0 in stage 14.0 (TID 1143)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 113.0 in stage 14.0 (TID 1130) in 341 ms on localhost (111/200)
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62376c37
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000116_1133/part-00116
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000115_1132' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000115
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000115_1132: Committed
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2865c976
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000117_1134/part-00117
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO Executor: Finished task 115.0 in stage 14.0 (TID 1132). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 127.0 in stage 14.0 (TID 1144, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 115.0 in stage 14.0 (TID 1132) in 311 ms on localhost (112/200)
15/08/06 17:54:46 INFO Executor: Running task 127.0 in stage 14.0 (TID 1144)
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@146155bf
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000118_1135/part-00118
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@59ef2d41
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b90776e
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000119_1136/part-00119
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3743c840
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@22cbb261
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6a525416
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22dea18e
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000120_1137/part-00120
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000116_1133' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000116
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000116_1133: Committed
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000117_1134' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000117
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000117_1134: Committed
15/08/06 17:54:46 INFO Executor: Finished task 116.0 in stage 14.0 (TID 1133). 781 bytes result sent to driver
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a95eb59
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000121_1138/part-00121
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO TaskSetManager: Starting task 128.0 in stage 14.0 (TID 1145, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000119_1136' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000119
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000119_1136: Committed
15/08/06 17:54:46 INFO Executor: Running task 128.0 in stage 14.0 (TID 1145)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5868b03e
15/08/06 17:54:46 INFO Executor: Finished task 117.0 in stage 14.0 (TID 1134). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000123_1140/part-00123
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO TaskSetManager: Starting task 129.0 in stage 14.0 (TID 1146, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Finished task 119.0 in stage 14.0 (TID 1136). 781 bytes result sent to driver
15/08/06 17:54:46 INFO Executor: Running task 129.0 in stage 14.0 (TID 1146)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 130.0 in stage 14.0 (TID 1147, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 116.0 in stage 14.0 (TID 1133) in 316 ms on localhost (113/200)
15/08/06 17:54:46 INFO Executor: Running task 130.0 in stage 14.0 (TID 1147)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 117.0 in stage 14.0 (TID 1134) in 291 ms on localhost (114/200)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 119.0 in stage 14.0 (TID 1136) in 270 ms on localhost (115/200)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000118_1135' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000118
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000118_1135: Committed
15/08/06 17:54:46 INFO Executor: Finished task 118.0 in stage 14.0 (TID 1135). 781 bytes result sent to driver
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@496afa3c
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000124_1141/part-00124
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO TaskSetManager: Starting task 131.0 in stage 14.0 (TID 1148, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO Executor: Running task 131.0 in stage 14.0 (TID 1148)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3bc3de07
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@28e70a67
15/08/06 17:54:46 INFO TaskSetManager: Finished task 118.0 in stage 14.0 (TID 1135) in 292 ms on localhost (116/200)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000122_1139/part-00122
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6a85993f
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@443b8d19
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5544d9ac
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6f33ff2c
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000102_1119' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000102
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000102_1119: Committed
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000101_1118' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000101
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000101_1118: Committed
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3012be5b
15/08/06 17:54:46 INFO Executor: Finished task 102.0 in stage 14.0 (TID 1119). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000125_1142/part-00125
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO Executor: Finished task 101.0 in stage 14.0 (TID 1118). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO TaskSetManager: Starting task 132.0 in stage 14.0 (TID 1149, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO Executor: Running task 132.0 in stage 14.0 (TID 1149)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 102.0 in stage 14.0 (TID 1119) in 562 ms on localhost (117/200)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 133.0 in stage 14.0 (TID 1150, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 133.0 in stage 14.0 (TID 1150)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 101.0 in stage 14.0 (TID 1118) in 582 ms on localhost (118/200)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000120_1137' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000120
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000120_1137: Committed
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO Executor: Finished task 120.0 in stage 14.0 (TID 1137). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 134.0 in stage 14.0 (TID 1151, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 134.0 in stage 14.0 (TID 1151)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 120.0 in stage 14.0 (TID 1137) in 302 ms on localhost (119/200)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000123_1140' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000123
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000123_1140: Committed
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35b7b77b
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO Executor: Finished task 123.0 in stage 14.0 (TID 1140). 781 bytes result sent to driver
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5939ed30
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000126_1143/part-00126
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000121_1138' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000121
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000121_1138: Committed
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO TaskSetManager: Starting task 135.0 in stage 14.0 (TID 1152, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO Executor: Running task 135.0 in stage 14.0 (TID 1152)
15/08/06 17:54:46 INFO Executor: Finished task 121.0 in stage 14.0 (TID 1138). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Finished task 123.0 in stage 14.0 (TID 1140) in 165 ms on localhost (120/200)
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO TaskSetManager: Starting task 136.0 in stage 14.0 (TID 1153, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000122_1139' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000122
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000122_1139: Committed
15/08/06 17:54:46 INFO TaskSetManager: Finished task 121.0 in stage 14.0 (TID 1138) in 170 ms on localhost (121/200)
15/08/06 17:54:46 INFO Executor: Running task 136.0 in stage 14.0 (TID 1153)
15/08/06 17:54:46 INFO Executor: Finished task 122.0 in stage 14.0 (TID 1139). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 137.0 in stage 14.0 (TID 1154, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 137.0 in stage 14.0 (TID 1154)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 122.0 in stage 14.0 (TID 1139) in 170 ms on localhost (122/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30005187
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000125_1142' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000125
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000125_1142: Committed
15/08/06 17:54:46 INFO Executor: Finished task 125.0 in stage 14.0 (TID 1142). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 138.0 in stage 14.0 (TID 1155, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 138.0 in stage 14.0 (TID 1155)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 125.0 in stage 14.0 (TID 1142) in 185 ms on localhost (123/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000126_1143' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000126
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000126_1143: Committed
15/08/06 17:54:46 INFO Executor: Finished task 126.0 in stage 14.0 (TID 1143). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 139.0 in stage 14.0 (TID 1156, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 139.0 in stage 14.0 (TID 1156)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 126.0 in stage 14.0 (TID 1143) in 154 ms on localhost (124/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@47e78ad3
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000127_1144/part-00127
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b605baf
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@65eec324
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000130_1147/part-00130
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7b7c4f66
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000131_1148/part-00131
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ddecc3b
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000129_1146/part-00129
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@233d897
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000128_1145/part-00128
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@104499ee
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000127_1144' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000127
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000127_1144: Committed
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6a8ca27a
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO Executor: Finished task 127.0 in stage 14.0 (TID 1144). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO TaskSetManager: Starting task 140.0 in stage 14.0 (TID 1157, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO Executor: Running task 140.0 in stage 14.0 (TID 1157)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 127.0 in stage 14.0 (TID 1144) in 177 ms on localhost (125/200)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000105_1122' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000105
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000105_1122: Committed
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32d1d76c
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO Executor: Finished task 105.0 in stage 14.0 (TID 1122). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO TaskSetManager: Starting task 141.0 in stage 14.0 (TID 1158, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 141.0 in stage 14.0 (TID 1158)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@305e2149
15/08/06 17:54:46 INFO TaskSetManager: Finished task 105.0 in stage 14.0 (TID 1122) in 602 ms on localhost (126/200)
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@788262ab
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000133_1150/part-00133
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000129_1146' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000129
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000129_1146: Committed
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO Executor: Finished task 129.0 in stage 14.0 (TID 1146). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 142.0 in stage 14.0 (TID 1159, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 142.0 in stage 14.0 (TID 1159)
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53d33ef5
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000128_1145' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000128
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000132_1149/part-00132
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000128_1145: Committed
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO TaskSetManager: Finished task 129.0 in stage 14.0 (TID 1146) in 154 ms on localhost (127/200)
15/08/06 17:54:46 INFO Executor: Finished task 128.0 in stage 14.0 (TID 1145). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 143.0 in stage 14.0 (TID 1160, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 143.0 in stage 14.0 (TID 1160)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 128.0 in stage 14.0 (TID 1145) in 258 ms on localhost (128/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@26d6cc7e
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000135_1152/part-00135
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@10fae92e
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@58ad5050
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78b9e54
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000134_1151/part-00134
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6680cdaa
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000137_1154/part-00137
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34827401
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000133_1150' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000133
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000133_1150: Committed
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000110_1127' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000110
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000110_1127: Committed
15/08/06 17:54:46 INFO Executor: Finished task 133.0 in stage 14.0 (TID 1150). 781 bytes result sent to driver
15/08/06 17:54:46 INFO Executor: Finished task 110.0 in stage 14.0 (TID 1127). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 144.0 in stage 14.0 (TID 1161, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO Executor: Running task 144.0 in stage 14.0 (TID 1161)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO TaskSetManager: Finished task 133.0 in stage 14.0 (TID 1150) in 251 ms on localhost (129/200)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 145.0 in stage 14.0 (TID 1162, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 145.0 in stage 14.0 (TID 1162)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000132_1149' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000132
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000132_1149: Committed
15/08/06 17:54:46 INFO TaskSetManager: Finished task 110.0 in stage 14.0 (TID 1127) in 706 ms on localhost (130/200)
15/08/06 17:54:46 INFO Executor: Finished task 132.0 in stage 14.0 (TID 1149). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 146.0 in stage 14.0 (TID 1163, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 146.0 in stage 14.0 (TID 1163)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 132.0 in stage 14.0 (TID 1149) in 259 ms on localhost (131/200)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f99aa8b
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5bd5d80c
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5991c348
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000136_1153/part-00136
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000135_1152' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000135
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000135_1152: Committed
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5037dd45
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000138_1155/part-00138
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO Executor: Finished task 135.0 in stage 14.0 (TID 1152). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO TaskSetManager: Starting task 147.0 in stage 14.0 (TID 1164, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 147.0 in stage 14.0 (TID 1164)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 135.0 in stage 14.0 (TID 1152) in 257 ms on localhost (132/200)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2571aa9
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000134_1151' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000134
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000134_1151: Committed
15/08/06 17:54:46 INFO Executor: Finished task 134.0 in stage 14.0 (TID 1151). 781 bytes result sent to driver
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d1f75af
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000139_1156/part-00139
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO TaskSetManager: Starting task 148.0 in stage 14.0 (TID 1165, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO Executor: Running task 148.0 in stage 14.0 (TID 1165)
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000137_1154' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000137
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000137_1154: Committed
15/08/06 17:54:46 INFO TaskSetManager: Finished task 134.0 in stage 14.0 (TID 1151) in 277 ms on localhost (133/200)
15/08/06 17:54:46 INFO Executor: Finished task 137.0 in stage 14.0 (TID 1154). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 149.0 in stage 14.0 (TID 1166, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 149.0 in stage 14.0 (TID 1166)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 137.0 in stage 14.0 (TID 1154) in 265 ms on localhost (134/200)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b11337
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@18b9d4f2
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000136_1153' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000136
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000136_1153: Committed
15/08/06 17:54:46 INFO Executor: Finished task 136.0 in stage 14.0 (TID 1153). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 150.0 in stage 14.0 (TID 1167, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 150.0 in stage 14.0 (TID 1167)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 136.0 in stage 14.0 (TID 1153) in 288 ms on localhost (135/200)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000138_1155' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000138
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000138_1155: Committed
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000139_1156' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000139
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000139_1156: Committed
15/08/06 17:54:46 INFO Executor: Finished task 138.0 in stage 14.0 (TID 1155). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO Executor: Finished task 139.0 in stage 14.0 (TID 1156). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO TaskSetManager: Starting task 151.0 in stage 14.0 (TID 1168, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 151.0 in stage 14.0 (TID 1168)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 138.0 in stage 14.0 (TID 1155) in 276 ms on localhost (136/200)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 152.0 in stage 14.0 (TID 1169, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 152.0 in stage 14.0 (TID 1169)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 139.0 in stage 14.0 (TID 1156) in 256 ms on localhost (137/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@33bff9e8
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000140_1157/part-00140
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1b66ae5d
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000141_1158/part-00141
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@58c6c18a
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000143_1160/part-00143
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24f7d9d4
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000142_1159/part-00142
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@50c2f334
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4430e814
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4fbd25d3
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12d41c53
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000141_1158' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000141
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000141_1158: Committed
15/08/06 17:54:46 INFO Executor: Finished task 141.0 in stage 14.0 (TID 1158). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 153.0 in stage 14.0 (TID 1170, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 153.0 in stage 14.0 (TID 1170)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 141.0 in stage 14.0 (TID 1158) in 265 ms on localhost (138/200)
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4dfe248c
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000145_1162/part-00145
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16078a97
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000144_1161/part-00144
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000143_1160' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000143
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000143_1160: Committed
15/08/06 17:54:46 INFO Executor: Finished task 143.0 in stage 14.0 (TID 1160). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 154.0 in stage 14.0 (TID 1171, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 154.0 in stage 14.0 (TID 1171)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000142_1159' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000142
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000142_1159: Committed
15/08/06 17:54:46 INFO TaskSetManager: Finished task 143.0 in stage 14.0 (TID 1160) in 158 ms on localhost (139/200)
15/08/06 17:54:46 INFO Executor: Finished task 142.0 in stage 14.0 (TID 1159). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 155.0 in stage 14.0 (TID 1172, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 155.0 in stage 14.0 (TID 1172)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 142.0 in stage 14.0 (TID 1159) in 263 ms on localhost (140/200)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1bfbac6c
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4483d37f
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000146_1163/part-00146
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3111e2c5
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34115ba2
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000147_1164/part-00147
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e1c3f2
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000148_1165/part-00148
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ef7eca3
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000149_1166/part-00149
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d9a3600
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@774f492e
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000145_1162' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000145
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000145_1162: Committed
15/08/06 17:54:46 INFO Executor: Finished task 145.0 in stage 14.0 (TID 1162). 781 bytes result sent to driver
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000144_1161' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000144
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000144_1161: Committed
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO TaskSetManager: Starting task 156.0 in stage 14.0 (TID 1173, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO Executor: Running task 156.0 in stage 14.0 (TID 1173)
15/08/06 17:54:46 INFO Executor: Finished task 144.0 in stage 14.0 (TID 1161). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Finished task 145.0 in stage 14.0 (TID 1162) in 165 ms on localhost (141/200)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 157.0 in stage 14.0 (TID 1174, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77173f2d
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO Executor: Running task 157.0 in stage 14.0 (TID 1174)
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO TaskSetManager: Finished task 144.0 in stage 14.0 (TID 1161) in 168 ms on localhost (142/200)
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000124_1141' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000124
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000124_1141: Committed
15/08/06 17:54:46 INFO Executor: Finished task 124.0 in stage 14.0 (TID 1141). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 158.0 in stage 14.0 (TID 1175, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 158.0 in stage 14.0 (TID 1175)
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@23f62167
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@740b03a2
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000151_1168/part-00151
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO TaskSetManager: Finished task 124.0 in stage 14.0 (TID 1141) in 574 ms on localhost (143/200)
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b30774a
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000150_1167/part-00150
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000146_1163' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000146
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000146_1163: Committed
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO Executor: Finished task 146.0 in stage 14.0 (TID 1163). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@662dd161
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO TaskSetManager: Starting task 159.0 in stage 14.0 (TID 1176, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 159.0 in stage 14.0 (TID 1176)
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000147_1164' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000147
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000147_1164: Committed
15/08/06 17:54:46 INFO TaskSetManager: Finished task 146.0 in stage 14.0 (TID 1163) in 183 ms on localhost (144/200)
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000148_1165' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000148
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000148_1165: Committed
15/08/06 17:54:46 INFO Executor: Finished task 147.0 in stage 14.0 (TID 1164). 781 bytes result sent to driver
15/08/06 17:54:46 INFO Executor: Finished task 148.0 in stage 14.0 (TID 1165). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 160.0 in stage 14.0 (TID 1177, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 160.0 in stage 14.0 (TID 1177)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 161.0 in stage 14.0 (TID 1178, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 161.0 in stage 14.0 (TID 1178)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 147.0 in stage 14.0 (TID 1164) in 170 ms on localhost (145/200)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 148.0 in stage 14.0 (TID 1165) in 163 ms on localhost (146/200)
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@f9508a3
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000152_1169/part-00152
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000149_1166' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000149
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000149_1166: Committed
15/08/06 17:54:46 INFO Executor: Finished task 149.0 in stage 14.0 (TID 1166). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 162.0 in stage 14.0 (TID 1179, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 162.0 in stage 14.0 (TID 1179)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 149.0 in stage 14.0 (TID 1166) in 169 ms on localhost (147/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1967c690
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78d33d94
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000151_1168' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000151
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000151_1168: Committed
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO Executor: Finished task 151.0 in stage 14.0 (TID 1168). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 163.0 in stage 14.0 (TID 1180, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 163.0 in stage 14.0 (TID 1180)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 151.0 in stage 14.0 (TID 1168) in 152 ms on localhost (148/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000152_1169' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000152
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000152_1169: Committed
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO Executor: Finished task 152.0 in stage 14.0 (TID 1169). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Starting task 164.0 in stage 14.0 (TID 1181, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 164.0 in stage 14.0 (TID 1181)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 152.0 in stage 14.0 (TID 1169) in 270 ms on localhost (149/200)
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000131_1148' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000131
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000131_1148: Committed
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO Executor: Finished task 131.0 in stage 14.0 (TID 1148). 781 bytes result sent to driver
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000130_1147' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000130
15/08/06 17:54:46 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000130_1147: Committed
15/08/06 17:54:46 INFO TaskSetManager: Starting task 165.0 in stage 14.0 (TID 1182, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 165.0 in stage 14.0 (TID 1182)
15/08/06 17:54:46 INFO Executor: Finished task 130.0 in stage 14.0 (TID 1147). 781 bytes result sent to driver
15/08/06 17:54:46 INFO TaskSetManager: Finished task 131.0 in stage 14.0 (TID 1148) in 631 ms on localhost (150/200)
15/08/06 17:54:46 INFO TaskSetManager: Starting task 166.0 in stage 14.0 (TID 1183, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:46 INFO Executor: Running task 166.0 in stage 14.0 (TID 1183)
15/08/06 17:54:46 INFO TaskSetManager: Finished task 130.0 in stage 14.0 (TID 1147) in 639 ms on localhost (151/200)
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@95fc507
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000153_1170/part-00153
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@198f92bb
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000155_1172/part-00155
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6da100a5
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17109912
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1558eac0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000154_1171/part-00154
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6cdd3098
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000156_1173/part-00156
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d271fc0
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000158_1175/part-00158
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@43959dd2
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@13ac68a8
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000157_1174/part-00157
15/08/06 17:54:46 INFO CodecConfig: Compression set to false
15/08/06 17:54:46 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:46 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:46 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@140b108b
15/08/06 17:54:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:46 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a8c6096
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76192577
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000161_1178/part-00161
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ceae433
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000160_1177/part-00160
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@41ca0966
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22e0ecf0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000159_1176/part-00159
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000156_1173' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000156
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000156_1173: Committed
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@73d613f0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000162_1179/part-00162
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000154_1171' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000154
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000154_1171: Committed
15/08/06 17:54:47 INFO Executor: Finished task 156.0 in stage 14.0 (TID 1173). 781 bytes result sent to driver
15/08/06 17:54:47 INFO Executor: Finished task 154.0 in stage 14.0 (TID 1171). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 167.0 in stage 14.0 (TID 1184, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 167.0 in stage 14.0 (TID 1184)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 156.0 in stage 14.0 (TID 1173) in 257 ms on localhost (152/200)
15/08/06 17:54:47 INFO TaskSetManager: Starting task 168.0 in stage 14.0 (TID 1185, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 168.0 in stage 14.0 (TID 1185)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 154.0 in stage 14.0 (TID 1171) in 297 ms on localhost (153/200)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15bc4af6
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2bbe37c4
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@44778b0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@43c87d08
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@adc4616
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000163_1180/part-00163
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000157_1174' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000157
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000157_1174: Committed
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000158_1175' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000158
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000158_1175: Committed
15/08/06 17:54:47 INFO Executor: Finished task 157.0 in stage 14.0 (TID 1174). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 169.0 in stage 14.0 (TID 1186, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Finished task 158.0 in stage 14.0 (TID 1175). 781 bytes result sent to driver
15/08/06 17:54:47 INFO Executor: Running task 169.0 in stage 14.0 (TID 1186)
15/08/06 17:54:47 INFO TaskSetManager: Starting task 170.0 in stage 14.0 (TID 1187, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 170.0 in stage 14.0 (TID 1187)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 157.0 in stage 14.0 (TID 1174) in 280 ms on localhost (154/200)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 158.0 in stage 14.0 (TID 1175) in 274 ms on localhost (155/200)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77f81476
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@163d3890
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000164_1181/part-00164
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000159_1176' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000159
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000159_1176: Committed
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@456e474
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000166_1183/part-00166
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO Executor: Finished task 159.0 in stage 14.0 (TID 1176). 781 bytes result sent to driver
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO TaskSetManager: Starting task 171.0 in stage 14.0 (TID 1188, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 171.0 in stage 14.0 (TID 1188)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c4a4b39
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000165_1182/part-00165
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO TaskSetManager: Finished task 159.0 in stage 14.0 (TID 1176) in 277 ms on localhost (156/200)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66aab7df
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@183c0ca5
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000163_1180' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000163
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000163_1180: Committed
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6edad1e8
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO Executor: Finished task 163.0 in stage 14.0 (TID 1180). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 172.0 in stage 14.0 (TID 1189, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 172.0 in stage 14.0 (TID 1189)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 163.0 in stage 14.0 (TID 1180) in 267 ms on localhost (157/200)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000164_1181' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000164
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000164_1181: Committed
15/08/06 17:54:47 INFO Executor: Finished task 164.0 in stage 14.0 (TID 1181). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 173.0 in stage 14.0 (TID 1190, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 173.0 in stage 14.0 (TID 1190)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 164.0 in stage 14.0 (TID 1181) in 275 ms on localhost (158/200)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000166_1183' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000166
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000166_1183: Committed
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO Executor: Finished task 166.0 in stage 14.0 (TID 1183). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 174.0 in stage 14.0 (TID 1191, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 174.0 in stage 14.0 (TID 1191)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 166.0 in stage 14.0 (TID 1183) in 159 ms on localhost (159/200)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000140_1157' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000140
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000140_1157: Committed
15/08/06 17:54:47 INFO Executor: Finished task 140.0 in stage 14.0 (TID 1157). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 175.0 in stage 14.0 (TID 1192, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 175.0 in stage 14.0 (TID 1192)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 140.0 in stage 14.0 (TID 1157) in 668 ms on localhost (160/200)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c8bb9f2
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000167_1184/part-00167
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4f67bc16
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000168_1185/part-00168
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7fcf3511
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3741b0f
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000167_1184' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000167
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000167_1184: Committed
15/08/06 17:54:47 INFO Executor: Finished task 167.0 in stage 14.0 (TID 1184). 781 bytes result sent to driver
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c5e234c
15/08/06 17:54:47 INFO TaskSetManager: Starting task 176.0 in stage 14.0 (TID 1193, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000171_1188/part-00171
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO Executor: Running task 176.0 in stage 14.0 (TID 1193)
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO TaskSetManager: Finished task 167.0 in stage 14.0 (TID 1184) in 155 ms on localhost (161/200)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000168_1185' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000168
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000168_1185: Committed
15/08/06 17:54:47 INFO Executor: Finished task 168.0 in stage 14.0 (TID 1185). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 177.0 in stage 14.0 (TID 1194, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 177.0 in stage 14.0 (TID 1194)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 168.0 in stage 14.0 (TID 1185) in 161 ms on localhost (162/200)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6cb2998a
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35867309
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000172_1189/part-00172
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e700b9b
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000171_1188' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000171
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000171_1188: Committed
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO Executor: Finished task 171.0 in stage 14.0 (TID 1188). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 178.0 in stage 14.0 (TID 1195, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 178.0 in stage 14.0 (TID 1195)
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1578fdc3
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000170_1187/part-00170
15/08/06 17:54:47 INFO TaskSetManager: Finished task 171.0 in stage 14.0 (TID 1188) in 151 ms on localhost (163/200)
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42a9e11d
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@45f04d27
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000169_1186/part-00169
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000173_1190/part-00173
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000150_1167' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000150
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000150_1167: Committed
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e4adc1b
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000174_1191/part-00174
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO Executor: Finished task 150.0 in stage 14.0 (TID 1167). 781 bytes result sent to driver
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO TaskSetManager: Starting task 179.0 in stage 14.0 (TID 1196, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1e7f876c
15/08/06 17:54:47 INFO Executor: Running task 179.0 in stage 14.0 (TID 1196)
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO TaskSetManager: Finished task 150.0 in stage 14.0 (TID 1167) in 576 ms on localhost (164/200)
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2f1abc3e
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@28d77049
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000175_1192/part-00175
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000172_1189' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000172
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000172_1189: Committed
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@37f712b2
15/08/06 17:54:47 INFO Executor: Finished task 172.0 in stage 14.0 (TID 1189). 781 bytes result sent to driver
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO TaskSetManager: Starting task 180.0 in stage 14.0 (TID 1197, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 180.0 in stage 14.0 (TID 1197)
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO TaskSetManager: Finished task 172.0 in stage 14.0 (TID 1189) in 159 ms on localhost (165/200)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@789be909
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000170_1187' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000170
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000170_1187: Committed
15/08/06 17:54:47 INFO Executor: Finished task 170.0 in stage 14.0 (TID 1187). 781 bytes result sent to driver
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO TaskSetManager: Starting task 181.0 in stage 14.0 (TID 1198, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:47 INFO Executor: Running task 181.0 in stage 14.0 (TID 1198)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 170.0 in stage 14.0 (TID 1187) in 208 ms on localhost (166/200)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e4d4b1b
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000174_1191' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000174
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000174_1191: Committed
15/08/06 17:54:47 INFO Executor: Finished task 174.0 in stage 14.0 (TID 1191). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 182.0 in stage 14.0 (TID 1199, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 182.0 in stage 14.0 (TID 1199)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 174.0 in stage 14.0 (TID 1191) in 285 ms on localhost (167/200)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000153_1170' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000153
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000153_1170: Committed
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000155_1172' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000155
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000155_1172: Committed
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO Executor: Finished task 155.0 in stage 14.0 (TID 1172). 781 bytes result sent to driver
15/08/06 17:54:47 INFO Executor: Finished task 153.0 in stage 14.0 (TID 1170). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 183.0 in stage 14.0 (TID 1200, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 155.0 in stage 14.0 (TID 1172) in 680 ms on localhost (168/200)
15/08/06 17:54:47 INFO Executor: Running task 183.0 in stage 14.0 (TID 1200)
15/08/06 17:54:47 INFO TaskSetManager: Starting task 184.0 in stage 14.0 (TID 1201, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 184.0 in stage 14.0 (TID 1201)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 153.0 in stage 14.0 (TID 1170) in 697 ms on localhost (169/200)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000175_1192' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000175
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000175_1192: Committed
15/08/06 17:54:47 INFO Executor: Finished task 175.0 in stage 14.0 (TID 1192). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 185.0 in stage 14.0 (TID 1202, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 185.0 in stage 14.0 (TID 1202)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 175.0 in stage 14.0 (TID 1192) in 301 ms on localhost (170/200)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000160_1177' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000160
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000160_1177: Committed
15/08/06 17:54:47 INFO Executor: Finished task 160.0 in stage 14.0 (TID 1177). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 186.0 in stage 14.0 (TID 1203, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000162_1179' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000162
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000162_1179: Committed
15/08/06 17:54:47 INFO Executor: Running task 186.0 in stage 14.0 (TID 1203)
15/08/06 17:54:47 INFO Executor: Finished task 162.0 in stage 14.0 (TID 1179). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Finished task 160.0 in stage 14.0 (TID 1177) in 670 ms on localhost (171/200)
15/08/06 17:54:47 INFO TaskSetManager: Starting task 187.0 in stage 14.0 (TID 1204, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 187.0 in stage 14.0 (TID 1204)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000161_1178' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000161
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000161_1178: Committed
15/08/06 17:54:47 INFO TaskSetManager: Finished task 162.0 in stage 14.0 (TID 1179) in 662 ms on localhost (172/200)
15/08/06 17:54:47 INFO Executor: Finished task 161.0 in stage 14.0 (TID 1178). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 188.0 in stage 14.0 (TID 1205, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 188.0 in stage 14.0 (TID 1205)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 161.0 in stage 14.0 (TID 1178) in 672 ms on localhost (173/200)
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@30f4a75
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000176_1193/part-00176
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a99e178
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000180_1197/part-00180
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@63146e74
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4dadc4c9
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000177_1194/part-00177
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@70dd0f97
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5cb66aa8
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000178_1195/part-00178
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000165_1182' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000165
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000165_1182: Committed
15/08/06 17:54:47 INFO Executor: Finished task 165.0 in stage 14.0 (TID 1182). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 189.0 in stage 14.0 (TID 1206, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 189.0 in stage 14.0 (TID 1206)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 165.0 in stage 14.0 (TID 1182) in 552 ms on localhost (174/200)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64dc8e12
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bf37379
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c2233e3
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17d4c0cb
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000181_1198/part-00181
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000182_1199/part-00182
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57a3026f
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000179_1196/part-00179
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000180_1197' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000180
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000180_1197: Committed
15/08/06 17:54:47 INFO Executor: Finished task 180.0 in stage 14.0 (TID 1197). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 190.0 in stage 14.0 (TID 1207, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 190.0 in stage 14.0 (TID 1207)
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7701c10b
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000183_1200/part-00183
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@d3c9f38
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000185_1202/part-00185
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO TaskSetManager: Finished task 180.0 in stage 14.0 (TID 1197) in 279 ms on localhost (175/200)
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@685e4092
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000177_1194' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000177
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000177_1194: Committed
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e166c6
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO Executor: Finished task 177.0 in stage 14.0 (TID 1194). 781 bytes result sent to driver
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3c015110
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO TaskSetManager: Starting task 191.0 in stage 14.0 (TID 1208, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 191.0 in stage 14.0 (TID 1208)
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000178_1195' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000178
15/08/06 17:54:47 INFO TaskSetManager: Finished task 177.0 in stage 14.0 (TID 1194) in 352 ms on localhost (176/200)
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000178_1195: Committed
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@27ffbee1
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO Executor: Finished task 178.0 in stage 14.0 (TID 1195). 781 bytes result sent to driver
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1de1b328
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO TaskSetManager: Starting task 192.0 in stage 14.0 (TID 1209, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 192.0 in stage 14.0 (TID 1209)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO TaskSetManager: Finished task 178.0 in stage 14.0 (TID 1195) in 330 ms on localhost (177/200)
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@44ee4855
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000184_1201/part-00184
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000181_1198' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000181
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000181_1198: Committed
15/08/06 17:54:47 INFO Executor: Finished task 181.0 in stage 14.0 (TID 1198). 781 bytes result sent to driver
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000185_1202' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000185
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000185_1202: Committed
15/08/06 17:54:47 INFO TaskSetManager: Starting task 193.0 in stage 14.0 (TID 1210, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000179_1196' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000179
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000179_1196: Committed
15/08/06 17:54:47 INFO Executor: Finished task 185.0 in stage 14.0 (TID 1202). 781 bytes result sent to driver
15/08/06 17:54:47 INFO Executor: Running task 193.0 in stage 14.0 (TID 1210)
15/08/06 17:54:47 INFO Executor: Finished task 179.0 in stage 14.0 (TID 1196). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Finished task 181.0 in stage 14.0 (TID 1198) in 303 ms on localhost (178/200)
15/08/06 17:54:47 INFO TaskSetManager: Starting task 194.0 in stage 14.0 (TID 1211, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000182_1199' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000182
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000182_1199: Committed
15/08/06 17:54:47 INFO Executor: Running task 194.0 in stage 14.0 (TID 1211)
15/08/06 17:54:47 INFO Executor: Finished task 182.0 in stage 14.0 (TID 1199). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Finished task 185.0 in stage 14.0 (TID 1202) in 145 ms on localhost (179/200)
15/08/06 17:54:47 INFO TaskSetManager: Starting task 195.0 in stage 14.0 (TID 1212, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000183_1200' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000183
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000183_1200: Committed
15/08/06 17:54:47 INFO Executor: Running task 195.0 in stage 14.0 (TID 1212)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 179.0 in stage 14.0 (TID 1196) in 332 ms on localhost (180/200)
15/08/06 17:54:47 INFO Executor: Finished task 183.0 in stage 14.0 (TID 1200). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 196.0 in stage 14.0 (TID 1213, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 196.0 in stage 14.0 (TID 1213)
15/08/06 17:54:47 INFO TaskSetManager: Starting task 197.0 in stage 14.0 (TID 1214, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 197.0 in stage 14.0 (TID 1214)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 182.0 in stage 14.0 (TID 1199) in 168 ms on localhost (181/200)
15/08/06 17:54:47 INFO TaskSetManager: Finished task 183.0 in stage 14.0 (TID 1200) in 154 ms on localhost (182/200)
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f5c1723
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5bab0fcf
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b9b0e0d
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000186_1203/part-00186
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@40b7960d
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000188_1205/part-00188
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000187_1204/part-00187
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000184_1201' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000184
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000184_1201: Committed
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO Executor: Finished task 184.0 in stage 14.0 (TID 1201). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 198.0 in stage 14.0 (TID 1215, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO Executor: Running task 198.0 in stage 14.0 (TID 1215)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO TaskSetManager: Finished task 184.0 in stage 14.0 (TID 1201) in 186 ms on localhost (183/200)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6dcb7f85
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2635a095
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2532acac
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000188_1205' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000188
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000188_1205: Committed
15/08/06 17:54:47 INFO Executor: Finished task 188.0 in stage 14.0 (TID 1205). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Starting task 199.0 in stage 14.0 (TID 1216, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO Executor: Running task 199.0 in stage 14.0 (TID 1216)
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO TaskSetManager: Finished task 188.0 in stage 14.0 (TID 1205) in 164 ms on localhost (184/200)
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4c7ad4c
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000189_1206/part-00189
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2f9bc2f4
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:54:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000173_1190' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000173
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000173_1190: Committed
15/08/06 17:54:47 INFO Executor: Finished task 173.0 in stage 14.0 (TID 1190). 781 bytes result sent to driver
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000169_1186' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000169
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000169_1186: Committed
15/08/06 17:54:47 INFO Executor: Finished task 169.0 in stage 14.0 (TID 1186). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Finished task 173.0 in stage 14.0 (TID 1190) in 557 ms on localhost (185/200)
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@657ce094
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000192_1209/part-00192
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO TaskSetManager: Finished task 169.0 in stage 14.0 (TID 1186) in 617 ms on localhost (186/200)
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000189_1206' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000189
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000189_1206: Committed
15/08/06 17:54:47 INFO Executor: Finished task 189.0 in stage 14.0 (TID 1206). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Finished task 189.0 in stage 14.0 (TID 1206) in 167 ms on localhost (187/200)
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3be00b7
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000190_1207/part-00190
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1b8025c0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000196_1213/part-00196
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a2e6e5a
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000193_1210/part-00193
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e4f1560
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a049268
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000197_1214/part-00197
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@71b8d77
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000191_1208/part-00191
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@f95c5f7
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000195_1212/part-00195
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@31f7e31f
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@326b747
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62ca97ee
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000194_1211/part-00194
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7eac48c2
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7af04474
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5aa51380
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2227e8c1
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42c9a0e7
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000192_1209' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000192
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000192_1209: Committed
15/08/06 17:54:47 INFO Executor: Finished task 192.0 in stage 14.0 (TID 1209). 781 bytes result sent to driver
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b7ab07
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000198_1215/part-00198
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO TaskSetManager: Finished task 192.0 in stage 14.0 (TID 1209) in 386 ms on localhost (188/200)
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000176_1193' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000176
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000176_1193: Committed
15/08/06 17:54:47 INFO Executor: Finished task 176.0 in stage 14.0 (TID 1193). 781 bytes result sent to driver
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000193_1210' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000193
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000193_1210: Committed
15/08/06 17:54:47 INFO TaskSetManager: Finished task 176.0 in stage 14.0 (TID 1193) in 759 ms on localhost (189/200)
15/08/06 17:54:47 INFO Executor: Finished task 193.0 in stage 14.0 (TID 1210). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Finished task 193.0 in stage 14.0 (TID 1210) in 381 ms on localhost (190/200)
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000195_1212' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000195
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000195_1212: Committed
15/08/06 17:54:47 INFO Executor: Finished task 195.0 in stage 14.0 (TID 1212). 781 bytes result sent to driver
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@454d7b7f
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000194_1211' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000194
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000194_1211: Committed
15/08/06 17:54:47 INFO TaskSetManager: Finished task 195.0 in stage 14.0 (TID 1212) in 385 ms on localhost (191/200)
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO Executor: Finished task 194.0 in stage 14.0 (TID 1211). 781 bytes result sent to driver
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO TaskSetManager: Finished task 194.0 in stage 14.0 (TID 1211) in 389 ms on localhost (192/200)
15/08/06 17:54:47 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2dd2a09e
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/_temporary/attempt_201508061754_0014_m_000199_1216/part-00199
15/08/06 17:54:47 INFO CodecConfig: Compression set to false
15/08/06 17:54:47 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:54:47 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:54:47 INFO ParquetOutputFormat: Validation is off
15/08/06 17:54:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:54:47 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@54d002e5
15/08/06 17:54:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
15/08/06 17:54:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000199_1216' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000199
15/08/06 17:54:47 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000199_1216: Committed
15/08/06 17:54:47 INFO Executor: Finished task 199.0 in stage 14.0 (TID 1216). 781 bytes result sent to driver
15/08/06 17:54:47 INFO TaskSetManager: Finished task 199.0 in stage 14.0 (TID 1216) in 366 ms on localhost (193/200)
15/08/06 17:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000186_1203' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000186
15/08/06 17:54:48 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000186_1203: Committed
15/08/06 17:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000187_1204' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000187
15/08/06 17:54:48 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000187_1204: Committed
15/08/06 17:54:48 INFO Executor: Finished task 187.0 in stage 14.0 (TID 1204). 781 bytes result sent to driver
15/08/06 17:54:48 INFO Executor: Finished task 186.0 in stage 14.0 (TID 1203). 781 bytes result sent to driver
15/08/06 17:54:48 INFO TaskSetManager: Finished task 187.0 in stage 14.0 (TID 1204) in 569 ms on localhost (194/200)
15/08/06 17:54:48 INFO TaskSetManager: Finished task 186.0 in stage 14.0 (TID 1203) in 571 ms on localhost (195/200)
15/08/06 17:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000190_1207' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000190
15/08/06 17:54:48 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000190_1207: Committed
15/08/06 17:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000197_1214' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000197
15/08/06 17:54:48 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000197_1214: Committed
15/08/06 17:54:48 INFO Executor: Finished task 190.0 in stage 14.0 (TID 1207). 781 bytes result sent to driver
15/08/06 17:54:48 INFO Executor: Finished task 197.0 in stage 14.0 (TID 1214). 781 bytes result sent to driver
15/08/06 17:54:48 INFO TaskSetManager: Finished task 190.0 in stage 14.0 (TID 1207) in 820 ms on localhost (196/200)
15/08/06 17:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000191_1208' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000191
15/08/06 17:54:48 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000191_1208: Committed
15/08/06 17:54:48 INFO Executor: Finished task 191.0 in stage 14.0 (TID 1208). 781 bytes result sent to driver
15/08/06 17:54:48 INFO TaskSetManager: Finished task 197.0 in stage 14.0 (TID 1214) in 778 ms on localhost (197/200)
15/08/06 17:54:48 INFO TaskSetManager: Finished task 191.0 in stage 14.0 (TID 1208) in 805 ms on localhost (198/200)
15/08/06 17:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000196_1213' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000196
15/08/06 17:54:48 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000196_1213: Committed
15/08/06 17:54:48 INFO Executor: Finished task 196.0 in stage 14.0 (TID 1213). 781 bytes result sent to driver
15/08/06 17:54:48 INFO TaskSetManager: Finished task 196.0 in stage 14.0 (TID 1213) in 782 ms on localhost (199/200)
15/08/06 17:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508061754_0014_m_000198_1215' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_temporary/0/task_201508061754_0014_m_000198
15/08/06 17:54:48 INFO SparkHiveWriterContainer: attempt_201508061754_0014_m_000198_1215: Committed
15/08/06 17:54:48 INFO Executor: Finished task 198.0 in stage 14.0 (TID 1215). 781 bytes result sent to driver
15/08/06 17:54:48 INFO TaskSetManager: Finished task 198.0 in stage 14.0 (TID 1215) in 770 ms on localhost (200/200)
15/08/06 17:54:48 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/08/06 17:54:48 INFO DAGScheduler: Stage 14 (runJob at InsertIntoHiveTable.scala:93) finished in 4.816 s
15/08/06 17:54:48 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@21c1cb81
15/08/06 17:54:48 INFO DAGScheduler: Job 8 finished: runJob at InsertIntoHiveTable.scala:93, took 12.791862 s
15/08/06 17:54:48 INFO StatsReportListener: task runtime:(count: 200, mean: 359.270000, stdev: 195.723088, max: 854.000000, min: 145.000000)
15/08/06 17:54:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:48 INFO StatsReportListener: 	145.0 ms	158.0 ms	165.0 ms	195.0 ms	300.0 ms	406.0 ms	706.0 ms	791.0 ms	854.0 ms
15/08/06 17:54:48 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.335000, stdev: 0.642476, max: 4.000000, min: 0.000000)
15/08/06 17:54:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:48 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	4.0 ms
15/08/06 17:54:48 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:54:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:48 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:54:48 INFO StatsReportListener: task result size:(count: 200, mean: 781.000000, stdev: 0.000000, max: 781.000000, min: 781.000000)
15/08/06 17:54:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:48 INFO StatsReportListener: 	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B
15/08/06 17:54:48 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 98.068610, stdev: 4.351039, max: 99.872123, min: 60.740741)
15/08/06 17:54:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:48 INFO StatsReportListener: 	61 %	97 %	97 %	98 %	99 %	99 %	99 %	100 %	100 %
15/08/06 17:54:48 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.113772, stdev: 0.242865, max: 1.639344, min: 0.000000)
15/08/06 17:54:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:48 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 2 %
15/08/06 17:54:48 INFO StatsReportListener: other time pct: (count: 200, mean: 1.817619, stdev: 4.348538, max: 39.259259, min: 0.127877)
15/08/06 17:54:48 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:48 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 1 %	 2 %	 2 %	 3 %	39 %
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/_SUCCESS;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/_SUCCESS;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00000;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00000;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00001;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00001;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00002;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00002;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00003;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00003;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00004;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00004;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00005;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00005;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00006;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00006;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00007;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00007;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00008;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00008;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00009;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00009;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00010;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00010;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00011;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00011;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00012;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00012;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00013;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00013;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00014;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00014;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00015;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00015;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00016;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00016;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00017;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00017;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00018;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00018;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00019;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00019;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00020;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00020;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00021;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00021;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00022;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00022;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00023;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00023;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00024;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00024;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00025;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00025;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00026;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00026;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00027;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00027;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00028;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00028;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00029;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00029;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00030;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00030;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00031;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00031;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00032;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00032;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00033;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00033;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00034;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00034;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00035;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00035;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00036;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00036;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00037;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00037;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00038;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00038;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00039;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00039;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00040;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00040;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00041;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00041;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00042;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00042;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00043;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00043;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00044;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00044;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00045;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00045;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00046;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00046;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00047;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00047;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00048;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00048;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00049;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00049;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00050;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00050;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00051;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00051;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00052;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00052;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00053;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00053;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00054;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00054;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00055;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00055;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00056;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00056;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00057;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00057;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00058;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00058;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00059;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00059;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00060;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00060;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00061;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00061;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00062;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00062;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00063;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00063;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00064;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00064;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00065;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00065;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00066;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00066;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00067;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00067;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00068;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00068;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00069;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00069;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00070;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00070;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00071;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00071;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00072;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00072;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00073;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00073;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00074;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00074;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00075;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00075;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00076;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00076;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00077;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00077;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00078;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00078;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00079;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00079;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00080;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00080;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00081;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00081;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00082;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00082;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00083;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00083;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00084;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00084;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00085;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00085;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00086;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00086;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00087;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00087;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00088;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00088;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00089;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00089;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00090;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00090;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00091;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00091;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00092;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00092;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00093;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00093;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00094;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00094;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00095;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00095;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00096;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00096;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00097;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00097;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00098;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00098;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00099;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00099;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00100;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00100;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00101;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00101;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00102;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00102;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00103;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00103;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00104;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00104;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00105;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00105;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00106;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00106;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00107;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00107;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00108;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00108;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00109;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00109;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00110;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00110;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00111;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00111;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00112;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00112;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00113;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00113;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00114;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00114;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00115;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00115;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00116;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00116;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00117;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00117;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00118;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00118;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00119;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00119;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00120;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00120;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00121;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00121;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00122;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00122;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00123;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00123;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00124;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00124;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00125;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00125;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00126;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00126;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00127;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00127;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00128;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00128;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00129;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00129;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00130;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00130;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00131;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00131;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00132;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00132;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00133;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00133;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00134;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00134;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00135;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00135;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00136;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00136;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00137;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00137;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00138;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00138;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00139;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00139;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00140;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00140;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00141;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00141;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00142;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00142;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00143;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00143;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00144;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00144;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00145;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00145;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00146;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00146;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00147;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00147;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00148;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00148;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00149;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00149;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00150;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00150;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00151;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00151;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00152;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00152;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00153;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00153;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00154;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00154;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00155;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00155;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00156;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00156;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00157;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00157;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00158;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00158;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00159;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00159;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00160;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00160;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00161;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00161;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00162;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00162;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00163;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00163;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00164;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00164;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00165;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00165;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00166;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00166;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00167;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00167;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00168;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00168;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00169;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00169;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00170;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00170;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00171;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00171;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00172;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00172;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00173;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00173;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00174;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00174;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00175;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00175;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00176;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00176;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00177;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00177;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00178;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00178;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00179;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00179;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00180;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00180;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00181;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00181;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00182;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00182;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00183;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00183;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00184;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00184;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00185;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00185;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00186;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00186;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00187;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00187;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00188;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00188;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00189;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00189;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00190;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00190;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00191;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00191;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00192;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00192;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00193;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00193;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00194;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00194;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00195;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00195;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00196;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00196;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00197;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00197;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00198;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00198;Status:true
15/08/06 17:54:50 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-54-30_986_8247449668434750189-1/-ext-10000/part-00199;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00199;Status:true
15/08/06 17:54:50 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:54:50 INFO DAGScheduler: Got job 9 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:54:50 INFO DAGScheduler: Final stage: Stage 15(collect at SparkPlan.scala:84)
15/08/06 17:54:50 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:54:50 INFO DAGScheduler: Missing parents: List()
15/08/06 17:54:50 INFO DAGScheduler: Submitting Stage 15 (MappedRDD[82] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:54:50 INFO MemoryStore: ensureFreeSpace(3256) called with curMem=1850802, maxMem=3333968363
15/08/06 17:54:50 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 3.2 KB, free 3.1 GB)
15/08/06 17:54:50 INFO MemoryStore: ensureFreeSpace(1958) called with curMem=1854058, maxMem=3333968363
15/08/06 17:54:50 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 1958.0 B, free 3.1 GB)
15/08/06 17:54:50 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:37948 (size: 1958.0 B, free: 3.1 GB)
15/08/06 17:54:50 INFO BlockManagerMaster: Updated info of block broadcast_20_piece0
15/08/06 17:54:50 INFO DefaultExecutionContext: Created broadcast 20 from broadcast at DAGScheduler.scala:838
15/08/06 17:54:50 INFO DAGScheduler: Submitting 1 missing tasks from Stage 15 (MappedRDD[82] at map at SparkPlan.scala:84)
15/08/06 17:54:50 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
15/08/06 17:54:50 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 1217, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:54:50 INFO Executor: Running task 0.0 in stage 15.0 (TID 1217)
15/08/06 17:54:50 INFO Executor: Finished task 0.0 in stage 15.0 (TID 1217). 618 bytes result sent to driver
15/08/06 17:54:50 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 1217) in 7 ms on localhost (1/1)
15/08/06 17:54:50 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/08/06 17:54:50 INFO DAGScheduler: Stage 15 (collect at SparkPlan.scala:84) finished in 0.007 s
15/08/06 17:54:50 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3755b539
15/08/06 17:54:50 INFO DAGScheduler: Job 9 finished: collect at SparkPlan.scala:84, took 0.018215 s
Time taken: 19.897 seconds
15/08/06 17:54:50 INFO CliDriver: Time taken: 19.897 seconds
15/08/06 17:54:50 INFO StatsReportListener: task runtime:(count: 1, mean: 7.000000, stdev: 0.000000, max: 7.000000, min: 7.000000)
15/08/06 17:54:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:50 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:50 INFO StatsReportListener: 	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms
15/08/06 17:54:50 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883690507 end=1438883690507 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:54:50 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:54:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:50 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:54:50 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 14.285714, stdev: 0.000000, max: 14.285714, min: 14.285714)
15/08/06 17:54:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:50 INFO StatsReportListener: 	14 %	14 %	14 %	14 %	14 %	14 %	14 %	14 %	14 %
15/08/06 17:54:50 INFO StatsReportListener: other time pct: (count: 1, mean: 85.714286, stdev: 0.000000, max: 85.714286, min: 85.714286)
15/08/06 17:54:50 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:54:50 INFO StatsReportListener: 	86 %	86 %	86 %	86 %	86 %	86 %	86 %	86 %	86 %
15/08/06 17:54:50 INFO SparkUI: Stopped Spark web UI at http://sandbox.hortonworks.com:4040
15/08/06 17:54:50 INFO DAGScheduler: Stopping DAGScheduler
15/08/06 17:54:51 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
15/08/06 17:54:51 INFO MemoryStore: MemoryStore cleared
15/08/06 17:54:51 INFO BlockManager: BlockManager stopped
15/08/06 17:54:51 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/06 17:54:51 INFO SparkContext: Successfully stopped SparkContext
