-- number of partitions when shuffling data for aggregates and joins
--set spark.sql.shuffle.partitions=1024;

DROP TABLE q11_important_stock_par_spark;
DROP TABLE q11_part_tmp_par_spark;

-- create the target table
create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet;
create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet;

-- the query
insert into table q11_part_tmp_par_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_par_spark
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par_spark) sum_tmp
        join q11_part_tmp_par_spark
where part_value > total_value * 0.0001
order by value desc;

Spark assembly has been built with Hive, including Datanucleus jars on classpath
15/08/06 17:44:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/06 17:44:34 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/06 17:44:34 INFO metastore: Connected to metastore.
15/08/06 17:44:35 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/06 17:44:35 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/06 17:44:35 INFO SecurityManager: Changing view acls to: hive
15/08/06 17:44:35 INFO SecurityManager: Changing modify acls to: hive
15/08/06 17:44:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/06 17:44:35 INFO Slf4jLogger: Slf4jLogger started
15/08/06 17:44:35 INFO Remoting: Starting remoting
15/08/06 17:44:36 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@sandbox.hortonworks.com:39227]
15/08/06 17:44:36 INFO Utils: Successfully started service 'sparkDriver' on port 39227.
15/08/06 17:44:36 INFO SparkEnv: Registering MapOutputTracker
15/08/06 17:44:36 INFO SparkEnv: Registering BlockManagerMaster
15/08/06 17:44:36 INFO DiskBlockManager: Created local directory at /tmp/spark-b1dbfdbd-e704-4ca4-8ad3-a15a34b5e3c0/spark-73a51a4c-8375-42d0-8237-38060fd3d752
15/08/06 17:44:36 INFO MemoryStore: MemoryStore started with capacity 3.1 GB
15/08/06 17:44:36 INFO HttpFileServer: HTTP File server directory is /tmp/spark-655f19ee-1aaf-4398-b0c1-9c47f21086c3/spark-7e2aa9b9-fa06-4d0e-8bf5-515639be3609
15/08/06 17:44:36 INFO HttpServer: Starting HTTP Server
15/08/06 17:44:36 INFO Utils: Successfully started service 'HTTP file server' on port 41301.
15/08/06 17:44:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/06 17:44:36 INFO SparkUI: Started SparkUI at http://sandbox.hortonworks.com:4040
15/08/06 17:44:36 INFO Executor: Starting executor ID <driver> on host localhost
15/08/06 17:44:36 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@sandbox.hortonworks.com:39227/user/HeartbeatReceiver
15/08/06 17:44:36 INFO NettyBlockTransferService: Server created on 42907
15/08/06 17:44:36 INFO BlockManagerMaster: Trying to register BlockManager
15/08/06 17:44:36 INFO BlockManagerMasterActor: Registering block manager localhost:42907 with 3.1 GB RAM, BlockManagerId(<driver>, localhost, 42907)
15/08/06 17:44:36 INFO BlockManagerMaster: Registered BlockManager
SET spark.sql.hive.version=0.13.1
15/08/06 17:44:37 INFO ParseDriver: Parsing command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:44:38 INFO ParseDriver: Parse Completed
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO ParseDriver: Parsing command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:44:39 INFO ParseDriver: Parse Completed
15/08/06 17:44:39 INFO PerfLogger: </PERFLOG method=parse start=1438883079443 end=1438883079444 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO Driver: Semantic Analysis Completed
15/08/06 17:44:39 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438883079444 end=1438883079623 duration=179 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:44:39 INFO PerfLogger: </PERFLOG method=compile start=1438883079409 end=1438883079640 duration=231 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO Driver: Starting command: DROP TABLE q11_important_stock_par_spark
15/08/06 17:44:39 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438883079400 end=1438883079666 duration=266 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: </PERFLOG method=runTasks start=1438883079666 end=1438883079864 duration=198 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438883079640 end=1438883079864 duration=224 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:44:39 INFO Driver: OK
15/08/06 17:44:39 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883079865 end=1438883079865 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO PerfLogger: </PERFLOG method=Driver.run start=1438883079400 end=1438883079865 duration=465 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:39 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:44:40 INFO DAGScheduler: Got job 0 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:44:40 INFO DAGScheduler: Final stage: Stage 0(collect at SparkPlan.scala:84)
15/08/06 17:44:40 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:44:40 INFO DAGScheduler: Missing parents: List()
15/08/06 17:44:40 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[2] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:44:40 INFO MemoryStore: ensureFreeSpace(1896) called with curMem=0, maxMem=3333968363
15/08/06 17:44:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1896.0 B, free 3.1 GB)
15/08/06 17:44:40 INFO MemoryStore: ensureFreeSpace(1208) called with curMem=1896, maxMem=3333968363
15/08/06 17:44:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1208.0 B, free 3.1 GB)
15/08/06 17:44:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:42907 (size: 1208.0 B, free: 3.1 GB)
15/08/06 17:44:40 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/08/06 17:44:40 INFO DefaultExecutionContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:40 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[2] at map at SparkPlan.scala:84)
15/08/06 17:44:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/08/06 17:44:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:44:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/06 17:44:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 618 bytes result sent to driver
15/08/06 17:44:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 103 ms on localhost (1/1)
15/08/06 17:44:40 INFO DAGScheduler: Stage 0 (collect at SparkPlan.scala:84) finished in 0.133 s
15/08/06 17:44:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/06 17:44:40 INFO DAGScheduler: Job 0 finished: collect at SparkPlan.scala:84, took 0.618182 s
Time taken: 3.024 seconds
15/08/06 17:44:40 INFO CliDriver: Time taken: 3.024 seconds
15/08/06 17:44:40 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5f47f7e6
15/08/06 17:44:40 INFO StatsReportListener: task runtime:(count: 1, mean: 103.000000, stdev: 0.000000, max: 103.000000, min: 103.000000)
15/08/06 17:44:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:40 INFO StatsReportListener: 	103.0 ms	103.0 ms	103.0 ms	103.0 ms	103.0 ms	103.0 ms	103.0 ms	103.0 ms	103.0 ms
15/08/06 17:44:40 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:44:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:40 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:44:40 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 13.592233, stdev: 0.000000, max: 13.592233, min: 13.592233)
15/08/06 17:44:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:40 INFO StatsReportListener: 	14 %	14 %	14 %	14 %	14 %	14 %	14 %	14 %	14 %
15/08/06 17:44:40 INFO StatsReportListener: other time pct: (count: 1, mean: 86.407767, stdev: 0.000000, max: 86.407767, min: 86.407767)
15/08/06 17:44:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:40 INFO StatsReportListener: 	86 %	86 %	86 %	86 %	86 %	86 %	86 %	86 %	86 %
15/08/06 17:44:40 INFO ParseDriver: Parsing command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:44:40 INFO ParseDriver: Parse Completed
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO ParseDriver: Parsing command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:44:40 INFO ParseDriver: Parse Completed
15/08/06 17:44:40 INFO PerfLogger: </PERFLOG method=parse start=1438883080754 end=1438883080759 duration=5 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO Driver: Semantic Analysis Completed
15/08/06 17:44:40 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438883080759 end=1438883080781 duration=22 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:44:40 INFO PerfLogger: </PERFLOG method=compile start=1438883080753 end=1438883080782 duration=29 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO Driver: Starting command: DROP TABLE q11_part_tmp_par_spark
15/08/06 17:44:40 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438883080753 end=1438883080783 duration=30 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: </PERFLOG method=runTasks start=1438883080783 end=1438883080867 duration=84 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438883080782 end=1438883080868 duration=86 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:44:40 INFO Driver: OK
15/08/06 17:44:40 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883080868 end=1438883080868 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO PerfLogger: </PERFLOG method=Driver.run start=1438883080753 end=1438883080868 duration=115 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:40 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:44:40 INFO DAGScheduler: Got job 1 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:44:40 INFO DAGScheduler: Final stage: Stage 1(collect at SparkPlan.scala:84)
15/08/06 17:44:40 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:44:40 INFO DAGScheduler: Missing parents: List()
15/08/06 17:44:40 INFO DAGScheduler: Submitting Stage 1 (MappedRDD[5] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:44:40 INFO MemoryStore: ensureFreeSpace(1896) called with curMem=3104, maxMem=3333968363
15/08/06 17:44:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 1896.0 B, free 3.1 GB)
15/08/06 17:44:40 INFO MemoryStore: ensureFreeSpace(1207) called with curMem=5000, maxMem=3333968363
15/08/06 17:44:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1207.0 B, free 3.1 GB)
15/08/06 17:44:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:42907 (size: 1207.0 B, free: 3.1 GB)
15/08/06 17:44:40 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/08/06 17:44:40 INFO DefaultExecutionContext: Created broadcast 1 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:40 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1 (MappedRDD[5] at map at SparkPlan.scala:84)
15/08/06 17:44:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/08/06 17:44:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:44:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/08/06 17:44:40 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 618 bytes result sent to driver
15/08/06 17:44:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 27 ms on localhost (1/1)
15/08/06 17:44:40 INFO DAGScheduler: Stage 1 (collect at SparkPlan.scala:84) finished in 0.036 s
15/08/06 17:44:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/06 17:44:40 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@b043e5
15/08/06 17:44:40 INFO DAGScheduler: Job 1 finished: collect at SparkPlan.scala:84, took 0.067607 s
Time taken: 0.347 seconds
15/08/06 17:44:40 INFO CliDriver: Time taken: 0.347 seconds
15/08/06 17:44:40 INFO StatsReportListener: task runtime:(count: 1, mean: 27.000000, stdev: 0.000000, max: 27.000000, min: 27.000000)
15/08/06 17:44:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:40 INFO StatsReportListener: 	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms
15/08/06 17:44:40 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:44:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:40 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:44:40 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 11.111111, stdev: 0.000000, max: 11.111111, min: 11.111111)
15/08/06 17:44:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:40 INFO StatsReportListener: 	11 %	11 %	11 %	11 %	11 %	11 %	11 %	11 %	11 %
15/08/06 17:44:40 INFO StatsReportListener: other time pct: (count: 1, mean: 88.888889, stdev: 0.000000, max: 88.888889, min: 88.888889)
15/08/06 17:44:40 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:40 INFO StatsReportListener: 	89 %	89 %	89 %	89 %	89 %	89 %	89 %	89 %	89 %
15/08/06 17:44:41 INFO ParseDriver: Parsing command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:44:41 INFO ParseDriver: Parse Completed
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO ParseDriver: Parsing command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:44:41 INFO ParseDriver: Parse Completed
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=parse start=1438883081151 end=1438883081152 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO SemanticAnalyzer: Starting Semantic Analysis
15/08/06 17:44:41 INFO SemanticAnalyzer: Creating table q11_important_stock_par_spark position=13
15/08/06 17:44:41 INFO Driver: Semantic Analysis Completed
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438883081152 end=1438883081201 duration=49 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=compile start=1438883081151 end=1438883081201 duration=50 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO Driver: Starting command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438883081150 end=1438883081203 duration=53 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=runTasks start=1438883081203 end=1438883081257 duration=54 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438883081201 end=1438883081257 duration=56 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:44:41 INFO Driver: OK
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883081257 end=1438883081257 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=Driver.run start=1438883081150 end=1438883081257 duration=107 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:44:41 INFO DAGScheduler: Got job 2 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:44:41 INFO DAGScheduler: Final stage: Stage 2(collect at SparkPlan.scala:84)
15/08/06 17:44:41 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:44:41 INFO DAGScheduler: Missing parents: List()
15/08/06 17:44:41 INFO DAGScheduler: Submitting Stage 2 (MappedRDD[8] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:44:41 INFO MemoryStore: ensureFreeSpace(2560) called with curMem=6207, maxMem=3333968363
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 3.1 GB)
15/08/06 17:44:41 INFO MemoryStore: ensureFreeSpace(1562) called with curMem=8767, maxMem=3333968363
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1562.0 B, free 3.1 GB)
15/08/06 17:44:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:42907 (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:44:41 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/08/06 17:44:41 INFO DefaultExecutionContext: Created broadcast 2 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:41 INFO DAGScheduler: Submitting 1 missing tasks from Stage 2 (MappedRDD[8] at map at SparkPlan.scala:84)
15/08/06 17:44:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/06 17:44:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:44:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
15/08/06 17:44:41 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 618 bytes result sent to driver
15/08/06 17:44:41 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 33 ms on localhost (1/1)
15/08/06 17:44:41 INFO DAGScheduler: Stage 2 (collect at SparkPlan.scala:84) finished in 0.043 s
15/08/06 17:44:41 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/06 17:44:41 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@f32147
15/08/06 17:44:41 INFO DAGScheduler: Job 2 finished: collect at SparkPlan.scala:84, took 0.074092 s
15/08/06 17:44:41 INFO StatsReportListener: task runtime:(count: 1, mean: 33.000000, stdev: 0.000000, max: 33.000000, min: 33.000000)
15/08/06 17:44:41 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:41 INFO StatsReportListener: 	33.0 ms	33.0 ms	33.0 ms	33.0 ms	33.0 ms	33.0 ms	33.0 ms	33.0 ms	33.0 ms
15/08/06 17:44:41 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:44:41 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:41 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:44:41 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 21.212121, stdev: 0.000000, max: 21.212121, min: 21.212121)
15/08/06 17:44:41 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:41 INFO StatsReportListener: 	21 %	21 %	21 %	21 %	21 %	21 %	21 %	21 %	21 %
15/08/06 17:44:41 INFO StatsReportListener: other time pct: (count: 1, mean: 78.787879, stdev: 0.000000, max: 78.787879, min: 78.787879)
15/08/06 17:44:41 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:41 INFO StatsReportListener: 	79 %	79 %	79 %	79 %	79 %	79 %	79 %	79 %	79 %
Time taken: 0.415 seconds
15/08/06 17:44:41 INFO CliDriver: Time taken: 0.415 seconds
15/08/06 17:44:41 INFO ParseDriver: Parsing command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:44:41 INFO ParseDriver: Parse Completed
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO ParseDriver: Parsing command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:44:41 INFO ParseDriver: Parse Completed
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=parse start=1438883081497 end=1438883081499 duration=2 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO SemanticAnalyzer: Starting Semantic Analysis
15/08/06 17:44:41 INFO SemanticAnalyzer: Creating table q11_part_tmp_par_spark position=13
15/08/06 17:44:41 INFO Driver: Semantic Analysis Completed
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1438883081499 end=1438883081509 duration=10 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=compile start=1438883081497 end=1438883081510 duration=13 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO Driver: Starting command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1438883081496 end=1438883081511 duration=15 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=runTasks start=1438883081511 end=1438883081546 duration=35 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=Driver.execute start=1438883081510 end=1438883081546 duration=36 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/06 17:44:41 INFO Driver: OK
15/08/06 17:44:41 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883081546 end=1438883081547 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO PerfLogger: </PERFLOG method=Driver.run start=1438883081496 end=1438883081547 duration=51 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:44:41 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:44:41 INFO DAGScheduler: Got job 3 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:44:41 INFO DAGScheduler: Final stage: Stage 3(collect at SparkPlan.scala:84)
15/08/06 17:44:41 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:44:41 INFO DAGScheduler: Missing parents: List()
15/08/06 17:44:41 INFO DAGScheduler: Submitting Stage 3 (MappedRDD[11] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:44:41 INFO MemoryStore: ensureFreeSpace(2560) called with curMem=10329, maxMem=3333968363
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 3.1 GB)
15/08/06 17:44:41 INFO MemoryStore: ensureFreeSpace(1562) called with curMem=12889, maxMem=3333968363
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1562.0 B, free 3.1 GB)
15/08/06 17:44:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:42907 (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:44:41 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/08/06 17:44:41 INFO DefaultExecutionContext: Created broadcast 3 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:41 INFO DAGScheduler: Submitting 1 missing tasks from Stage 3 (MappedRDD[11] at map at SparkPlan.scala:84)
15/08/06 17:44:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
15/08/06 17:44:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:44:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
15/08/06 17:44:41 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 618 bytes result sent to driver
15/08/06 17:44:41 INFO DAGScheduler: Stage 3 (collect at SparkPlan.scala:84) finished in 0.049 s
15/08/06 17:44:41 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 39 ms on localhost (1/1)
15/08/06 17:44:41 INFO DAGScheduler: Job 3 finished: collect at SparkPlan.scala:84, took 0.078758 s
15/08/06 17:44:41 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5956139c
15/08/06 17:44:41 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
Time taken: 0.276 seconds
15/08/06 17:44:41 INFO CliDriver: Time taken: 0.276 seconds
15/08/06 17:44:41 INFO StatsReportListener: task runtime:(count: 1, mean: 39.000000, stdev: 0.000000, max: 39.000000, min: 39.000000)
15/08/06 17:44:41 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:41 INFO StatsReportListener: 	39.0 ms	39.0 ms	39.0 ms	39.0 ms	39.0 ms	39.0 ms	39.0 ms	39.0 ms	39.0 ms
15/08/06 17:44:41 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:44:41 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:41 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:44:41 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 12.820513, stdev: 0.000000, max: 12.820513, min: 12.820513)
15/08/06 17:44:41 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:41 INFO StatsReportListener: 	13 %	13 %	13 %	13 %	13 %	13 %	13 %	13 %	13 %
15/08/06 17:44:41 INFO StatsReportListener: other time pct: (count: 1, mean: 87.179487, stdev: 0.000000, max: 87.179487, min: 87.179487)
15/08/06 17:44:41 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:41 INFO StatsReportListener: 	87 %	87 %	87 %	87 %	87 %	87 %	87 %	87 %	87 %
15/08/06 17:44:41 INFO ParseDriver: Parsing command: insert into table q11_part_tmp_par_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey
15/08/06 17:44:41 INFO ParseDriver: Parse Completed
15/08/06 17:44:41 INFO BlockManager: Removing broadcast 3
15/08/06 17:44:41 INFO BlockManager: Removing block broadcast_3_piece0
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_3_piece0 of size 1562 dropped from memory (free 3333955474)
15/08/06 17:44:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:42907 in memory (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:44:41 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/08/06 17:44:41 INFO BlockManager: Removing block broadcast_3
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_3 of size 2560 dropped from memory (free 3333958034)
15/08/06 17:44:41 INFO ContextCleaner: Cleaned broadcast 3
15/08/06 17:44:41 INFO BlockManager: Removing broadcast 2
15/08/06 17:44:41 INFO BlockManager: Removing block broadcast_2
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_2 of size 2560 dropped from memory (free 3333960594)
15/08/06 17:44:41 INFO BlockManager: Removing block broadcast_2_piece0
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_2_piece0 of size 1562 dropped from memory (free 3333962156)
15/08/06 17:44:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:42907 in memory (size: 1562.0 B, free: 3.1 GB)
15/08/06 17:44:41 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/08/06 17:44:41 INFO ContextCleaner: Cleaned broadcast 2
15/08/06 17:44:41 INFO BlockManager: Removing broadcast 1
15/08/06 17:44:41 INFO BlockManager: Removing block broadcast_1_piece0
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_1_piece0 of size 1207 dropped from memory (free 3333963363)
15/08/06 17:44:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:42907 in memory (size: 1207.0 B, free: 3.1 GB)
15/08/06 17:44:41 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/08/06 17:44:41 INFO BlockManager: Removing block broadcast_1
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_1 of size 1896 dropped from memory (free 3333965259)
15/08/06 17:44:41 INFO ContextCleaner: Cleaned broadcast 1
15/08/06 17:44:41 INFO BlockManager: Removing broadcast 0
15/08/06 17:44:41 INFO BlockManager: Removing block broadcast_0
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_0 of size 1896 dropped from memory (free 3333967155)
15/08/06 17:44:41 INFO BlockManager: Removing block broadcast_0_piece0
15/08/06 17:44:41 INFO MemoryStore: Block broadcast_0_piece0 of size 1208 dropped from memory (free 3333968363)
15/08/06 17:44:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:42907 in memory (size: 1208.0 B, free: 3.1 GB)
15/08/06 17:44:41 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/08/06 17:44:41 INFO ContextCleaner: Cleaned broadcast 0
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/06 17:44:43 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(n_nationkey#31, n_name#32, n_regionkey#33, n_comment#34)
15/08/06 17:44:43 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(s_suppkey#35, s_name#36, s_address#37, s_nationkey#38, s_phone#39, s_acctbal#40, s_comment#41)
15/08/06 17:44:43 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#42, ps_suppkey#43, ps_availqty#44, ps_supplycost#45, ps_comment#46)
15/08/06 17:44:43 INFO MemoryStore: ensureFreeSpace(281594) called with curMem=0, maxMem=3333968363
15/08/06 17:44:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 275.0 KB, free 3.1 GB)
15/08/06 17:44:43 INFO MemoryStore: ensureFreeSpace(281634) called with curMem=281594, maxMem=3333968363
15/08/06 17:44:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 275.0 KB, free 3.1 GB)
15/08/06 17:44:43 INFO MemoryStore: ensureFreeSpace(31895) called with curMem=563228, maxMem=3333968363
15/08/06 17:44:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:44:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:42907 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:44:43 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/08/06 17:44:43 INFO DefaultExecutionContext: Created broadcast 4 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:44:43 INFO MemoryStore: ensureFreeSpace(31949) called with curMem=595123, maxMem=3333968363
15/08/06 17:44:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 31.2 KB, free 3.1 GB)
15/08/06 17:44:43 INFO FileInputFormat: Total input paths to process : 1
15/08/06 17:44:43 INFO ParquetInputFormat: Total input paths to process : 1
15/08/06 17:44:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:42907 (size: 31.2 KB, free: 3.1 GB)
15/08/06 17:44:43 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0
15/08/06 17:44:43 INFO DefaultExecutionContext: Created broadcast 5 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:44:43 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:44:43 INFO ParquetFileReader: reading another 1 footers
15/08/06 17:44:43 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:44:43 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0; isDirectory=false; length=1493206; replication=1; blocksize=134217728; modification_time=1438802778074; access_time=1438881494768; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 25 ms
15/08/06 17:44:43 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/06 17:44:43 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/06 17:44:43 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:44:43 INFO DefaultExecutionContext: Starting job: collect at BroadcastHashJoin.scala:53
15/08/06 17:44:43 INFO DAGScheduler: Got job 4 (collect at BroadcastHashJoin.scala:53) with 1 output partitions (allowLocal=false)
15/08/06 17:44:43 INFO DAGScheduler: Final stage: Stage 4(collect at BroadcastHashJoin.scala:53)
15/08/06 17:44:43 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:44:43 INFO DAGScheduler: Missing parents: List()
15/08/06 17:44:43 INFO DAGScheduler: Submitting Stage 4 (MappedRDD[28] at map at BroadcastHashJoin.scala:53), which has no missing parents
15/08/06 17:44:43 INFO MemoryStore: ensureFreeSpace(2488) called with curMem=627072, maxMem=3333968363
15/08/06 17:44:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.4 KB, free 3.1 GB)
15/08/06 17:44:43 INFO MemoryStore: ensureFreeSpace(1467) called with curMem=629560, maxMem=3333968363
15/08/06 17:44:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1467.0 B, free 3.1 GB)
15/08/06 17:44:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:42907 (size: 1467.0 B, free: 3.1 GB)
15/08/06 17:44:43 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/08/06 17:44:43 INFO DefaultExecutionContext: Created broadcast 6 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:43 INFO DAGScheduler: Submitting 1 missing tasks from Stage 4 (MappedRDD[28] at map at BroadcastHashJoin.scala:53)
15/08/06 17:44:43 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15/08/06 17:44:43 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, ANY, 1586 bytes)
15/08/06 17:44:43 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
15/08/06 17:44:43 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0 start: 0 end: 1493206 length: 1493206 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 10000 records.
15/08/06 17:44:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:43 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 10000
15/08/06 17:44:44 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 144335 bytes result sent to driver
15/08/06 17:44:44 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 461 ms on localhost (1/1)
15/08/06 17:44:44 INFO DAGScheduler: Stage 4 (collect at BroadcastHashJoin.scala:53) finished in 0.667 s
15/08/06 17:44:44 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/06 17:44:44 INFO DAGScheduler: Job 4 finished: collect at BroadcastHashJoin.scala:53, took 0.704471 s
15/08/06 17:44:44 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@112e6b5d
15/08/06 17:44:44 INFO StatsReportListener: task runtime:(count: 1, mean: 461.000000, stdev: 0.000000, max: 461.000000, min: 461.000000)
15/08/06 17:44:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:44 INFO StatsReportListener: 	461.0 ms	461.0 ms	461.0 ms	461.0 ms	461.0 ms	461.0 ms	461.0 ms	461.0 ms	461.0 ms
15/08/06 17:44:44 INFO StatsReportListener: task result size:(count: 1, mean: 144335.000000, stdev: 0.000000, max: 144335.000000, min: 144335.000000)
15/08/06 17:44:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:44 INFO StatsReportListener: 	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB
15/08/06 17:44:44 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 72.017354, stdev: 0.000000, max: 72.017354, min: 72.017354)
15/08/06 17:44:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:44 INFO StatsReportListener: 	72 %	72 %	72 %	72 %	72 %	72 %	72 %	72 %	72 %
15/08/06 17:44:44 INFO StatsReportListener: other time pct: (count: 1, mean: 27.982646, stdev: 0.000000, max: 27.982646, min: 27.982646)
15/08/06 17:44:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:44 INFO StatsReportListener: 	28 %	28 %	28 %	28 %	28 %	28 %	28 %	28 %	28 %
15/08/06 17:44:44 INFO MemoryStore: ensureFreeSpace(65648) called with curMem=631027, maxMem=3333968363
15/08/06 17:44:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 64.1 KB, free 3.1 GB)
15/08/06 17:44:44 INFO MemoryStore: ensureFreeSpace(55237) called with curMem=696675, maxMem=3333968363
15/08/06 17:44:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 53.9 KB, free 3.1 GB)
15/08/06 17:44:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:42907 (size: 53.9 KB, free: 3.1 GB)
15/08/06 17:44:44 INFO BlockManagerMaster: Updated info of block broadcast_7_piece0
15/08/06 17:44:44 INFO DefaultExecutionContext: Created broadcast 7 from broadcast at BroadcastHashJoin.scala:55
15/08/06 17:44:44 INFO MemoryStore: ensureFreeSpace(281194) called with curMem=751912, maxMem=3333968363
15/08/06 17:44:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 274.6 KB, free 3.1 GB)
15/08/06 17:44:44 INFO MemoryStore: ensureFreeSpace(31858) called with curMem=1033106, maxMem=3333968363
15/08/06 17:44:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:44:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:42907 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:44:44 INFO BlockManagerMaster: Updated info of block broadcast_8_piece0
15/08/06 17:44:44 INFO DefaultExecutionContext: Created broadcast 8 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:44:44 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/06 17:44:44 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/06 17:44:44 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/06 17:44:44 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/06 17:44:44 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/06 17:44:44 INFO DefaultExecutionContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/08/06 17:44:44 INFO FileInputFormat: Total input paths to process : 1
15/08/06 17:44:44 INFO ParquetInputFormat: Total input paths to process : 1
15/08/06 17:44:44 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:44:44 INFO ParquetFileReader: reading another 1 footers
15/08/06 17:44:44 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:44:44 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0; isDirectory=false; length=3216; replication=1; blocksize=134217728; modification_time=1438802774354; access_time=1438881494331; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 15 ms
15/08/06 17:44:44 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:44:44 INFO FileInputFormat: Total input paths to process : 10
15/08/06 17:44:44 INFO ParquetInputFormat: Total input paths to process : 10
15/08/06 17:44:44 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:44:44 INFO ParquetFileReader: reading another 10 footers
15/08/06 17:44:44 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:44:44 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0; isDirectory=false; length=11478369; replication=1; blocksize=134217728; modification_time=1438802779757; access_time=1438881494861; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0; isDirectory=false; length=11437027; replication=1; blocksize=134217728; modification_time=1438802780428; access_time=1438881496406; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0; isDirectory=false; length=11437121; replication=1; blocksize=134217728; modification_time=1438802779669; access_time=1438881496411; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0; isDirectory=false; length=11437895; replication=1; blocksize=134217728; modification_time=1438802779902; access_time=1438881496416; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0; isDirectory=false; length=11436112; replication=1; blocksize=134217728; modification_time=1438802779829; access_time=1438881496416; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0; isDirectory=false; length=11361987; replication=1; blocksize=134217728; modification_time=1438802780059; access_time=1438881496419; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0; isDirectory=false; length=11361789; replication=1; blocksize=134217728; modification_time=1438802780860; access_time=1438881496423; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0; isDirectory=false; length=11362156; replication=1; blocksize=134217728; modification_time=1438802781041; access_time=1438881496427; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0; isDirectory=false; length=11362379; replication=1; blocksize=134217728; modification_time=1438802780967; access_time=1438881496428; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0; isDirectory=false; length=11361401; replication=1; blocksize=134217728; modification_time=1438802781090; access_time=1438881496428; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 33 ms
15/08/06 17:44:44 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:44:45 INFO DAGScheduler: Registering RDD 30 (mapPartitions at Exchange.scala:64)
15/08/06 17:44:45 INFO DAGScheduler: Registering RDD 39 (mapPartitions at Exchange.scala:64)
15/08/06 17:44:45 INFO DAGScheduler: Registering RDD 45 (mapPartitions at Exchange.scala:64)
15/08/06 17:44:45 INFO DAGScheduler: Got job 5 (runJob at InsertIntoHiveTable.scala:93) with 200 output partitions (allowLocal=false)
15/08/06 17:44:45 INFO DAGScheduler: Final stage: Stage 8(runJob at InsertIntoHiveTable.scala:93)
15/08/06 17:44:45 INFO DAGScheduler: Parents of final stage: List(Stage 7)
15/08/06 17:44:45 INFO DAGScheduler: Missing parents: List(Stage 7)
15/08/06 17:44:45 INFO DAGScheduler: Submitting Stage 5 (MapPartitionsRDD[30] at mapPartitions at Exchange.scala:64), which has no missing parents
15/08/06 17:44:45 INFO MemoryStore: ensureFreeSpace(7264) called with curMem=1064964, maxMem=3333968363
15/08/06 17:44:45 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.1 KB, free 3.1 GB)
15/08/06 17:44:45 INFO MemoryStore: ensureFreeSpace(4177) called with curMem=1072228, maxMem=3333968363
15/08/06 17:44:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 3.1 GB)
15/08/06 17:44:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:42907 (size: 4.1 KB, free: 3.1 GB)
15/08/06 17:44:45 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/08/06 17:44:45 INFO DefaultExecutionContext: Created broadcast 9 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:45 INFO DAGScheduler: Submitting 10 missing tasks from Stage 5 (MapPartitionsRDD[30] at mapPartitions at Exchange.scala:64)
15/08/06 17:44:45 INFO TaskSchedulerImpl: Adding task set 5.0 with 10 tasks
15/08/06 17:44:45 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, ANY, 1581 bytes)
15/08/06 17:44:45 INFO DAGScheduler: Submitting Stage 6 (MapPartitionsRDD[39] at mapPartitions at Exchange.scala:64), which has no missing parents
15/08/06 17:44:45 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, ANY, 1585 bytes)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 7, localhost, ANY, 1584 bytes)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 8, localhost, ANY, 1584 bytes)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 9, localhost, ANY, 1584 bytes)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 10, localhost, ANY, 1584 bytes)
15/08/06 17:44:45 INFO MemoryStore: ensureFreeSpace(10096) called with curMem=1076405, maxMem=3333968363
15/08/06 17:44:45 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.9 KB, free 3.1 GB)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 11, localhost, ANY, 1583 bytes)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 12, localhost, ANY, 1582 bytes)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 13, localhost, ANY, 1583 bytes)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 14, localhost, ANY, 1582 bytes)
15/08/06 17:44:45 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
15/08/06 17:44:45 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
15/08/06 17:44:45 INFO Executor: Running task 2.0 in stage 5.0 (TID 7)
15/08/06 17:44:45 INFO MemoryStore: ensureFreeSpace(5665) called with curMem=1086501, maxMem=3333968363
15/08/06 17:44:45 INFO Executor: Running task 4.0 in stage 5.0 (TID 9)
15/08/06 17:44:45 INFO Executor: Running task 5.0 in stage 5.0 (TID 10)
15/08/06 17:44:45 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.5 KB, free 3.1 GB)
15/08/06 17:44:45 INFO Executor: Running task 3.0 in stage 5.0 (TID 8)
15/08/06 17:44:45 INFO Executor: Running task 6.0 in stage 5.0 (TID 11)
15/08/06 17:44:45 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:42907 (size: 5.5 KB, free: 3.1 GB)
15/08/06 17:44:45 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/08/06 17:44:45 INFO DefaultExecutionContext: Created broadcast 10 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:45 INFO Executor: Running task 7.0 in stage 5.0 (TID 12)
15/08/06 17:44:45 INFO Executor: Running task 8.0 in stage 5.0 (TID 13)
15/08/06 17:44:45 INFO DAGScheduler: Submitting 1 missing tasks from Stage 6 (MapPartitionsRDD[39] at mapPartitions at Exchange.scala:64)
15/08/06 17:44:45 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/08/06 17:44:45 INFO Executor: Running task 9.0 in stage 5.0 (TID 14)
15/08/06 17:44:45 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 15, localhost, ANY, 1552 bytes)
15/08/06 17:44:45 INFO Executor: Running task 0.0 in stage 6.0 (TID 15)
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 0 end: 11361789 length: 11361789 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 0 end: 11362379 length: 11362379 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 0 end: 11437895 length: 11437895 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 0 end: 11436112 length: 11436112 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 0 end: 11362156 length: 11362156 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 0 end: 11437121 length: 11437121 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 0 end: 11437027 length: 11437027 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 0 end: 11361401 length: 11361401 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 0 end: 11361987 length: 11361987 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 0 end: 11478369 length: 11478369 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0 start: 0 end: 3216 length: 3216 hosts: [] requestedSchema: message root {
  optional int32 n_nationkey;
  optional binary n_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}},{"name":"n_regionkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80265 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79589 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80206 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79671 records.
15/08/06 17:44:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79667 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80615 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79706 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79826 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80260 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80195 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 25 records.
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 25
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 79667
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 80195
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 79589
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 80260
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 79826
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 79671
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 80206
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 80615
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 80265
15/08/06 17:44:45 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 79706
15/08/06 17:44:46 INFO Executor: Finished task 0.0 in stage 6.0 (TID 15). 2019 bytes result sent to driver
15/08/06 17:44:46 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 15) in 975 ms on localhost (1/1)
15/08/06 17:44:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/06 17:44:46 INFO DAGScheduler: Stage 6 (mapPartitions at Exchange.scala:64) finished in 0.990 s
15/08/06 17:44:46 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@34fccead
15/08/06 17:44:46 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:44:46 INFO DAGScheduler: running: Set(Stage 5)
15/08/06 17:44:46 INFO DAGScheduler: waiting: Set(Stage 7, Stage 8)
15/08/06 17:44:46 INFO StatsReportListener: task runtime:(count: 1, mean: 975.000000, stdev: 0.000000, max: 975.000000, min: 975.000000)
15/08/06 17:44:46 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:46 INFO StatsReportListener: 	975.0 ms	975.0 ms	975.0 ms	975.0 ms	975.0 ms	975.0 ms	975.0 ms	975.0 ms	975.0 ms
15/08/06 17:44:46 INFO DAGScheduler: failed: Set()
15/08/06 17:44:46 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 13231.000000, stdev: 0.000000, max: 13231.000000, min: 13231.000000)
15/08/06 17:44:46 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:46 INFO StatsReportListener: 	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB
15/08/06 17:44:46 INFO StatsReportListener: task result size:(count: 1, mean: 2019.000000, stdev: 0.000000, max: 2019.000000, min: 2019.000000)
15/08/06 17:44:46 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:46 INFO StatsReportListener: 	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B
15/08/06 17:44:46 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 96.820513, stdev: 0.000000, max: 96.820513, min: 96.820513)
15/08/06 17:44:46 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:46 INFO StatsReportListener: 	97 %	97 %	97 %	97 %	97 %	97 %	97 %	97 %	97 %
15/08/06 17:44:46 INFO StatsReportListener: other time pct: (count: 1, mean: 3.179487, stdev: 0.000000, max: 3.179487, min: 3.179487)
15/08/06 17:44:46 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:46 INFO StatsReportListener: 	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %
15/08/06 17:44:46 INFO DAGScheduler: Missing parents for Stage 7: List(Stage 5)
15/08/06 17:44:46 INFO DAGScheduler: Missing parents for Stage 8: List(Stage 7)
15/08/06 17:44:47 INFO Executor: Finished task 5.0 in stage 5.0 (TID 10). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 10) in 2390 ms on localhost (1/10)
15/08/06 17:44:47 INFO Executor: Finished task 2.0 in stage 5.0 (TID 7). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 7) in 2409 ms on localhost (2/10)
15/08/06 17:44:47 INFO Executor: Finished task 7.0 in stage 5.0 (TID 12). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 12) in 2413 ms on localhost (3/10)
15/08/06 17:44:47 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO Executor: Finished task 3.0 in stage 5.0 (TID 8). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO Executor: Finished task 9.0 in stage 5.0 (TID 14). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2432 ms on localhost (4/10)
15/08/06 17:44:47 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 8) in 2431 ms on localhost (5/10)
15/08/06 17:44:47 INFO Executor: Finished task 8.0 in stage 5.0 (TID 13). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO Executor: Finished task 6.0 in stage 5.0 (TID 11). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 2435 ms on localhost (6/10)
15/08/06 17:44:47 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 14) in 2427 ms on localhost (7/10)
15/08/06 17:44:47 INFO Executor: Finished task 4.0 in stage 5.0 (TID 9). 2019 bytes result sent to driver
15/08/06 17:44:47 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 13) in 2433 ms on localhost (8/10)
15/08/06 17:44:47 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 11) in 2438 ms on localhost (9/10)
15/08/06 17:44:47 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 9) in 2442 ms on localhost (10/10)
15/08/06 17:44:47 INFO DAGScheduler: Stage 5 (mapPartitions at Exchange.scala:64) finished in 2.450 s
15/08/06 17:44:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/06 17:44:47 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:44:47 INFO DAGScheduler: running: Set()
15/08/06 17:44:47 INFO DAGScheduler: waiting: Set(Stage 7, Stage 8)
15/08/06 17:44:47 INFO DAGScheduler: failed: Set()
15/08/06 17:44:47 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@287facf6
15/08/06 17:44:47 INFO StatsReportListener: task runtime:(count: 10, mean: 2425.000000, stdev: 15.283979, max: 2442.000000, min: 2390.000000)
15/08/06 17:44:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:47 INFO StatsReportListener: 	2.4 s	2.4 s	2.4 s	2.4 s	2.4 s	2.4 s	2.4 s	2.4 s	2.4 s
15/08/06 17:44:47 INFO StatsReportListener: shuffle bytes written:(count: 10, mean: 7582991.200000, stdev: 26756.903116, max: 7616204.000000, min: 7549451.000000)
15/08/06 17:44:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:47 INFO StatsReportListener: 	7.2 MB	7.2 MB	7.2 MB	7.2 MB	7.3 MB	7.3 MB	7.3 MB	7.3 MB	7.3 MB
15/08/06 17:44:47 INFO StatsReportListener: task result size:(count: 10, mean: 2019.000000, stdev: 0.000000, max: 2019.000000, min: 2019.000000)
15/08/06 17:44:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:47 INFO StatsReportListener: 	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B
15/08/06 17:44:47 INFO StatsReportListener: executor (non-fetch) time pct: (count: 10, mean: 98.478144, stdev: 0.208702, max: 98.771499, min: 98.132005)
15/08/06 17:44:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:47 INFO StatsReportListener: 	98 %	98 %	98 %	98 %	99 %	99 %	99 %	99 %	99 %
15/08/06 17:44:47 INFO StatsReportListener: other time pct: (count: 10, mean: 1.521856, stdev: 0.208702, max: 1.867995, min: 1.228501)
15/08/06 17:44:47 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:47 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 2 %	 2 %	 2 %	 2 %	 2 %
15/08/06 17:44:47 INFO DAGScheduler: Missing parents for Stage 7: List()
15/08/06 17:44:47 INFO DAGScheduler: Missing parents for Stage 8: List(Stage 7)
15/08/06 17:44:47 INFO DAGScheduler: Submitting Stage 7 (MapPartitionsRDD[45] at mapPartitions at Exchange.scala:64), which is now runnable
15/08/06 17:44:47 INFO MemoryStore: ensureFreeSpace(13544) called with curMem=1092166, maxMem=3333968363
15/08/06 17:44:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.2 KB, free 3.1 GB)
15/08/06 17:44:47 INFO MemoryStore: ensureFreeSpace(7369) called with curMem=1105710, maxMem=3333968363
15/08/06 17:44:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.2 KB, free 3.1 GB)
15/08/06 17:44:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:42907 (size: 7.2 KB, free: 3.1 GB)
15/08/06 17:44:47 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/08/06 17:44:47 INFO DefaultExecutionContext: Created broadcast 11 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:47 INFO DAGScheduler: Submitting 200 missing tasks from Stage 7 (MapPartitionsRDD[45] at mapPartitions at Exchange.scala:64)
15/08/06 17:44:47 INFO TaskSchedulerImpl: Adding task set 7.0 with 200 tasks
15/08/06 17:44:47 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 16, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 17, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 18, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 19, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 20, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 21, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 22, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 23, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 24, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 25, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 26, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 27, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 28, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 29, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 30, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 31, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:47 INFO Executor: Running task 1.0 in stage 7.0 (TID 17)
15/08/06 17:44:47 INFO Executor: Running task 0.0 in stage 7.0 (TID 16)
15/08/06 17:44:47 INFO Executor: Running task 10.0 in stage 7.0 (TID 26)
15/08/06 17:44:47 INFO Executor: Running task 9.0 in stage 7.0 (TID 25)
15/08/06 17:44:47 INFO Executor: Running task 8.0 in stage 7.0 (TID 24)
15/08/06 17:44:47 INFO Executor: Running task 6.0 in stage 7.0 (TID 22)
15/08/06 17:44:47 INFO Executor: Running task 4.0 in stage 7.0 (TID 20)
15/08/06 17:44:47 INFO Executor: Running task 7.0 in stage 7.0 (TID 23)
15/08/06 17:44:47 INFO Executor: Running task 5.0 in stage 7.0 (TID 21)
15/08/06 17:44:47 INFO Executor: Running task 3.0 in stage 7.0 (TID 19)
15/08/06 17:44:47 INFO Executor: Running task 2.0 in stage 7.0 (TID 18)
15/08/06 17:44:47 INFO Executor: Running task 11.0 in stage 7.0 (TID 27)
15/08/06 17:44:47 INFO Executor: Running task 14.0 in stage 7.0 (TID 30)
15/08/06 17:44:47 INFO Executor: Running task 12.0 in stage 7.0 (TID 28)
15/08/06 17:44:47 INFO Executor: Running task 13.0 in stage 7.0 (TID 29)
15/08/06 17:44:47 INFO Executor: Running task 15.0 in stage 7.0 (TID 31)
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:44:48 INFO Executor: Finished task 15.0 in stage 7.0 (TID 31). 1124 bytes result sent to driver
15/08/06 17:44:48 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 32, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:48 INFO Executor: Running task 16.0 in stage 7.0 (TID 32)
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:48 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 31) in 866 ms on localhost (1/200)
15/08/06 17:44:48 INFO Executor: Finished task 12.0 in stage 7.0 (TID 28). 1124 bytes result sent to driver
15/08/06 17:44:48 INFO Executor: Finished task 7.0 in stage 7.0 (TID 23). 1124 bytes result sent to driver
15/08/06 17:44:48 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 33, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:48 INFO Executor: Running task 17.0 in stage 7.0 (TID 33)
15/08/06 17:44:48 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 28) in 884 ms on localhost (2/200)
15/08/06 17:44:48 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 34, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:48 INFO Executor: Running task 18.0 in stage 7.0 (TID 34)
15/08/06 17:44:48 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 23) in 890 ms on localhost (3/200)
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO BlockManager: Removing broadcast 10
15/08/06 17:44:49 INFO BlockManager: Removing block broadcast_10
15/08/06 17:44:49 INFO MemoryStore: Block broadcast_10 of size 10096 dropped from memory (free 3332865380)
15/08/06 17:44:49 INFO BlockManager: Removing block broadcast_10_piece0
15/08/06 17:44:49 INFO MemoryStore: Block broadcast_10_piece0 of size 5665 dropped from memory (free 3332871045)
15/08/06 17:44:49 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:42907 in memory (size: 5.5 KB, free: 3.1 GB)
15/08/06 17:44:49 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/08/06 17:44:49 INFO ContextCleaner: Cleaned broadcast 10
15/08/06 17:44:49 INFO BlockManager: Removing broadcast 9
15/08/06 17:44:49 INFO BlockManager: Removing block broadcast_9
15/08/06 17:44:49 INFO MemoryStore: Block broadcast_9 of size 7264 dropped from memory (free 3332878309)
15/08/06 17:44:49 INFO BlockManager: Removing block broadcast_9_piece0
15/08/06 17:44:49 INFO MemoryStore: Block broadcast_9_piece0 of size 4177 dropped from memory (free 3332882486)
15/08/06 17:44:49 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:42907 in memory (size: 4.1 KB, free: 3.1 GB)
15/08/06 17:44:49 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/08/06 17:44:49 INFO ContextCleaner: Cleaned broadcast 9
15/08/06 17:44:49 INFO BlockManager: Removing broadcast 6
15/08/06 17:44:49 INFO BlockManager: Removing block broadcast_6_piece0
15/08/06 17:44:49 INFO MemoryStore: Block broadcast_6_piece0 of size 1467 dropped from memory (free 3332883953)
15/08/06 17:44:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:42907 in memory (size: 1467.0 B, free: 3.1 GB)
15/08/06 17:44:49 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/08/06 17:44:49 INFO BlockManager: Removing block broadcast_6
15/08/06 17:44:49 INFO MemoryStore: Block broadcast_6 of size 2488 dropped from memory (free 3332886441)
15/08/06 17:44:49 INFO ContextCleaner: Cleaned broadcast 6
15/08/06 17:44:49 INFO BlockManager: Removing broadcast 4
15/08/06 17:44:49 INFO BlockManager: Removing block broadcast_4
15/08/06 17:44:49 INFO MemoryStore: Block broadcast_4 of size 281594 dropped from memory (free 3333168035)
15/08/06 17:44:49 INFO BlockManager: Removing block broadcast_4_piece0
15/08/06 17:44:49 INFO MemoryStore: Block broadcast_4_piece0 of size 31895 dropped from memory (free 3333199930)
15/08/06 17:44:49 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:42907 in memory (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:44:49 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/08/06 17:44:49 INFO ContextCleaner: Cleaned broadcast 4
15/08/06 17:44:49 INFO Executor: Finished task 6.0 in stage 7.0 (TID 22). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 35, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 19.0 in stage 7.0 (TID 35)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 22) in 1796 ms on localhost (4/200)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO Executor: Finished task 8.0 in stage 7.0 (TID 24). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 36, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 20.0 in stage 7.0 (TID 36)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 24) in 1840 ms on localhost (5/200)
15/08/06 17:44:49 INFO Executor: Finished task 0.0 in stage 7.0 (TID 16). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 37, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 21.0 in stage 7.0 (TID 37)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 16) in 1852 ms on localhost (6/200)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO Executor: Finished task 2.0 in stage 7.0 (TID 18). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 18) in 1894 ms on localhost (7/200)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 38, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 22.0 in stage 7.0 (TID 38)
15/08/06 17:44:49 INFO Executor: Finished task 4.0 in stage 7.0 (TID 20). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 39, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Finished task 21.0 in stage 7.0 (TID 37). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO Executor: Running task 23.0 in stage 7.0 (TID 39)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 40, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Finished task 13.0 in stage 7.0 (TID 29). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO Executor: Running task 24.0 in stage 7.0 (TID 40)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 37) in 58 ms on localhost (8/200)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 20) in 1909 ms on localhost (9/200)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 41, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 25.0 in stage 7.0 (TID 41)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 29) in 1905 ms on localhost (10/200)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO Executor: Finished task 1.0 in stage 7.0 (TID 17). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO Executor: Finished task 14.0 in stage 7.0 (TID 30). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 42, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO Executor: Running task 26.0 in stage 7.0 (TID 42)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 17) in 1922 ms on localhost (11/200)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 43, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 27.0 in stage 7.0 (TID 43)
15/08/06 17:44:49 INFO Executor: Finished task 11.0 in stage 7.0 (TID 27). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 30) in 1917 ms on localhost (12/200)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 44, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO Executor: Running task 28.0 in stage 7.0 (TID 44)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 27) in 1922 ms on localhost (13/200)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:49 INFO Executor: Finished task 9.0 in stage 7.0 (TID 25). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 45, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 25) in 1971 ms on localhost (14/200)
15/08/06 17:44:49 INFO Executor: Running task 29.0 in stage 7.0 (TID 45)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO Executor: Finished task 3.0 in stage 7.0 (TID 19). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO Executor: Finished task 16.0 in stage 7.0 (TID 32). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 46, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 30.0 in stage 7.0 (TID 46)
15/08/06 17:44:49 INFO Executor: Finished task 10.0 in stage 7.0 (TID 26). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 19) in 1996 ms on localhost (15/200)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 47, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Finished task 18.0 in stage 7.0 (TID 34). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 32) in 1148 ms on localhost (16/200)
15/08/06 17:44:49 INFO Executor: Finished task 17.0 in stage 7.0 (TID 33). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO Executor: Running task 31.0 in stage 7.0 (TID 47)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 48, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 32.0 in stage 7.0 (TID 48)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 49, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 33.0 in stage 7.0 (TID 49)
15/08/06 17:44:49 INFO Executor: Finished task 5.0 in stage 7.0 (TID 21). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 26) in 2004 ms on localhost (17/200)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 34) in 1133 ms on localhost (18/200)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 33) in 1145 ms on localhost (19/200)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 50, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 34.0 in stage 7.0 (TID 50)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 51, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 35.0 in stage 7.0 (TID 51)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 21) in 2031 ms on localhost (20/200)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO Executor: Finished task 31.0 in stage 7.0 (TID 47). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 52, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Finished task 32.0 in stage 7.0 (TID 48). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO Executor: Running task 36.0 in stage 7.0 (TID 52)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 47) in 96 ms on localhost (21/200)
15/08/06 17:44:49 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 53, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO Executor: Running task 37.0 in stage 7.0 (TID 53)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 48) in 97 ms on localhost (22/200)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO Executor: Finished task 19.0 in stage 7.0 (TID 35). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 54, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:49 INFO Executor: Running task 38.0 in stage 7.0 (TID 54)
15/08/06 17:44:49 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 35) in 313 ms on localhost (23/200)
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:49 INFO Executor: Finished task 23.0 in stage 7.0 (TID 39). 1124 bytes result sent to driver
15/08/06 17:44:49 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 55, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 39) in 388 ms on localhost (24/200)
15/08/06 17:44:50 INFO Executor: Running task 39.0 in stage 7.0 (TID 55)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 22.0 in stage 7.0 (TID 38). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 56, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 40.0 in stage 7.0 (TID 56)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 38) in 568 ms on localhost (25/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 20.0 in stage 7.0 (TID 36). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 57, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 41.0 in stage 7.0 (TID 57)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 36) in 655 ms on localhost (26/200)
15/08/06 17:44:50 INFO Executor: Finished task 27.0 in stage 7.0 (TID 43). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 58, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 42.0 in stage 7.0 (TID 58)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 43) in 582 ms on localhost (27/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 24.0 in stage 7.0 (TID 40). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 59, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 43.0 in stage 7.0 (TID 59)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 40) in 613 ms on localhost (28/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 25.0 in stage 7.0 (TID 41). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 60, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 44.0 in stage 7.0 (TID 60)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 41) in 617 ms on localhost (29/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 28.0 in stage 7.0 (TID 44). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 61, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 45.0 in stage 7.0 (TID 61)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 44) in 613 ms on localhost (30/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO Executor: Finished task 43.0 in stage 7.0 (TID 59). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 62, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 46.0 in stage 7.0 (TID 62)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 59) in 46 ms on localhost (31/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO Executor: Finished task 26.0 in stage 7.0 (TID 42). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 63, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 47.0 in stage 7.0 (TID 63)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 42) in 683 ms on localhost (32/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO Executor: Finished task 46.0 in stage 7.0 (TID 62). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 64, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 48.0 in stage 7.0 (TID 64)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 62) in 57 ms on localhost (33/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO Executor: Finished task 34.0 in stage 7.0 (TID 50). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 65, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 49.0 in stage 7.0 (TID 65)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 50) in 613 ms on localhost (34/200)
15/08/06 17:44:50 INFO Executor: Finished task 30.0 in stage 7.0 (TID 46). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 66, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 50.0 in stage 7.0 (TID 66)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 46) in 651 ms on localhost (35/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 29.0 in stage 7.0 (TID 45). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 67, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 51.0 in stage 7.0 (TID 67)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 45) in 689 ms on localhost (36/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 33.0 in stage 7.0 (TID 49). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 68, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 52.0 in stage 7.0 (TID 68)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 49) in 677 ms on localhost (37/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO Executor: Finished task 50.0 in stage 7.0 (TID 66). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 69, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 53.0 in stage 7.0 (TID 69)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 66) in 48 ms on localhost (38/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO Executor: Finished task 37.0 in stage 7.0 (TID 53). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 70, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 54.0 in stage 7.0 (TID 70)
15/08/06 17:44:50 INFO Executor: Finished task 36.0 in stage 7.0 (TID 52). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 53) in 615 ms on localhost (39/200)
15/08/06 17:44:50 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 71, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 55.0 in stage 7.0 (TID 71)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 52) in 643 ms on localhost (40/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 35.0 in stage 7.0 (TID 51). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO Executor: Finished task 38.0 in stage 7.0 (TID 54). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 72, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 56.0 in stage 7.0 (TID 72)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 51) in 710 ms on localhost (41/200)
15/08/06 17:44:50 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 73, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 57.0 in stage 7.0 (TID 73)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 54) in 638 ms on localhost (42/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 52.0 in stage 7.0 (TID 68). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 74, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 58.0 in stage 7.0 (TID 74)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 68) in 103 ms on localhost (43/200)
15/08/06 17:44:50 INFO Executor: Finished task 56.0 in stage 7.0 (TID 72). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 75, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO Executor: Running task 59.0 in stage 7.0 (TID 75)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 72) in 52 ms on localhost (44/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 39.0 in stage 7.0 (TID 55). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 76, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 60.0 in stage 7.0 (TID 76)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 55) in 521 ms on localhost (45/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 59.0 in stage 7.0 (TID 75). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 77, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 61.0 in stage 7.0 (TID 77)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 75) in 50 ms on localhost (46/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 40.0 in stage 7.0 (TID 56). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 78, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 62.0 in stage 7.0 (TID 78)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 56) in 402 ms on localhost (47/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO Executor: Finished task 41.0 in stage 7.0 (TID 57). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO Executor: Finished task 42.0 in stage 7.0 (TID 58). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 79, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 63.0 in stage 7.0 (TID 79)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 57) in 403 ms on localhost (48/200)
15/08/06 17:44:50 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 80, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 64.0 in stage 7.0 (TID 80)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 58) in 398 ms on localhost (49/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 45.0 in stage 7.0 (TID 61). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 81, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 61) in 424 ms on localhost (50/200)
15/08/06 17:44:50 INFO Executor: Running task 65.0 in stage 7.0 (TID 81)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 44.0 in stage 7.0 (TID 60). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 82, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 66.0 in stage 7.0 (TID 82)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 60) in 480 ms on localhost (51/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO Executor: Finished task 47.0 in stage 7.0 (TID 63). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 83, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 67.0 in stage 7.0 (TID 83)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 63) in 546 ms on localhost (52/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:50 INFO Executor: Finished task 48.0 in stage 7.0 (TID 64). 1124 bytes result sent to driver
15/08/06 17:44:50 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 84, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:50 INFO Executor: Running task 68.0 in stage 7.0 (TID 84)
15/08/06 17:44:50 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 64) in 549 ms on localhost (53/200)
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 51.0 in stage 7.0 (TID 67). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 85, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 69.0 in stage 7.0 (TID 85)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 67) in 722 ms on localhost (54/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 49.0 in stage 7.0 (TID 65). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 86, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO Executor: Running task 70.0 in stage 7.0 (TID 86)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 65) in 751 ms on localhost (55/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 69.0 in stage 7.0 (TID 85). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 87, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 71.0 in stage 7.0 (TID 87)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 85) in 49 ms on localhost (56/200)
15/08/06 17:44:51 INFO Executor: Finished task 70.0 in stage 7.0 (TID 86). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 88, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 86) in 45 ms on localhost (57/200)
15/08/06 17:44:51 INFO Executor: Running task 72.0 in stage 7.0 (TID 88)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 55.0 in stage 7.0 (TID 71). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO Executor: Finished task 54.0 in stage 7.0 (TID 70). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO Executor: Finished task 57.0 in stage 7.0 (TID 73). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 89, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 73.0 in stage 7.0 (TID 89)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 71) in 720 ms on localhost (58/200)
15/08/06 17:44:51 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 90, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 74.0 in stage 7.0 (TID 90)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 70) in 748 ms on localhost (59/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 91, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 75.0 in stage 7.0 (TID 91)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 73) in 716 ms on localhost (60/200)
15/08/06 17:44:51 INFO Executor: Finished task 53.0 in stage 7.0 (TID 69). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 92, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 76.0 in stage 7.0 (TID 92)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 69) in 770 ms on localhost (61/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 60.0 in stage 7.0 (TID 76). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO Executor: Finished task 58.0 in stage 7.0 (TID 74). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 93, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 77.0 in stage 7.0 (TID 93)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 76) in 714 ms on localhost (62/200)
15/08/06 17:44:51 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 94, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 78.0 in stage 7.0 (TID 94)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 74) in 736 ms on localhost (63/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 61.0 in stage 7.0 (TID 77). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 95, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 79.0 in stage 7.0 (TID 95)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 77) in 743 ms on localhost (64/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 62.0 in stage 7.0 (TID 78). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 96, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 80.0 in stage 7.0 (TID 96)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 78) in 735 ms on localhost (65/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 64.0 in stage 7.0 (TID 80). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 97, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 81.0 in stage 7.0 (TID 97)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 80) in 724 ms on localhost (66/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:51 INFO Executor: Finished task 63.0 in stage 7.0 (TID 79). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 98, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 82.0 in stage 7.0 (TID 98)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 79) in 753 ms on localhost (67/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 65.0 in stage 7.0 (TID 81). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 99, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 83.0 in stage 7.0 (TID 99)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 81) in 701 ms on localhost (68/200)
15/08/06 17:44:51 INFO Executor: Finished task 66.0 in stage 7.0 (TID 82). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 100, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 82) in 663 ms on localhost (69/200)
15/08/06 17:44:51 INFO Executor: Running task 84.0 in stage 7.0 (TID 100)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 68.0 in stage 7.0 (TID 84). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 101, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Finished task 67.0 in stage 7.0 (TID 83). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 84) in 577 ms on localhost (70/200)
15/08/06 17:44:51 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 102, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 86.0 in stage 7.0 (TID 102)
15/08/06 17:44:51 INFO Executor: Running task 85.0 in stage 7.0 (TID 101)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 83) in 605 ms on localhost (71/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 72.0 in stage 7.0 (TID 88). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 103, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 87.0 in stage 7.0 (TID 103)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 88) in 421 ms on localhost (72/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 74.0 in stage 7.0 (TID 90). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 104, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 88.0 in stage 7.0 (TID 104)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 90) in 471 ms on localhost (73/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 75.0 in stage 7.0 (TID 91). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 105, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 89.0 in stage 7.0 (TID 105)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 91) in 483 ms on localhost (74/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 71.0 in stage 7.0 (TID 87). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 106, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 90.0 in stage 7.0 (TID 106)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 87) in 544 ms on localhost (75/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO Executor: Finished task 76.0 in stage 7.0 (TID 92). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 107, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 91.0 in stage 7.0 (TID 107)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 92) in 523 ms on localhost (76/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO Executor: Finished task 78.0 in stage 7.0 (TID 94). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 108, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 92.0 in stage 7.0 (TID 108)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 94) in 511 ms on localhost (77/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 73.0 in stage 7.0 (TID 89). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 109, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 93.0 in stage 7.0 (TID 109)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 89) in 614 ms on localhost (78/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 79.0 in stage 7.0 (TID 95). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 110, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 94.0 in stage 7.0 (TID 110)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 95) in 516 ms on localhost (79/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO Executor: Finished task 80.0 in stage 7.0 (TID 96). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO Executor: Finished task 77.0 in stage 7.0 (TID 93). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 111, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 95.0 in stage 7.0 (TID 111)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 96) in 537 ms on localhost (80/200)
15/08/06 17:44:51 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 112, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 96.0 in stage 7.0 (TID 112)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 93) in 616 ms on localhost (81/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO Executor: Finished task 81.0 in stage 7.0 (TID 97). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 113, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 97.0 in stage 7.0 (TID 113)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 97) in 518 ms on localhost (82/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 94.0 in stage 7.0 (TID 110). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 114, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 98.0 in stage 7.0 (TID 114)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 110) in 54 ms on localhost (83/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 83.0 in stage 7.0 (TID 99). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 115, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 99.0 in stage 7.0 (TID 115)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 99) in 510 ms on localhost (84/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO Executor: Finished task 84.0 in stage 7.0 (TID 100). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 116, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 100.0 in stage 7.0 (TID 116)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 100) in 522 ms on localhost (85/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 97.0 in stage 7.0 (TID 113). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 117, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 101.0 in stage 7.0 (TID 117)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 113) in 57 ms on localhost (86/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 85.0 in stage 7.0 (TID 101). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 118, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 102.0 in stage 7.0 (TID 118)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 101) in 517 ms on localhost (87/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO Executor: Finished task 86.0 in stage 7.0 (TID 102). 1124 bytes result sent to driver
15/08/06 17:44:51 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 119, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:51 INFO Executor: Running task 103.0 in stage 7.0 (TID 119)
15/08/06 17:44:51 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 102) in 521 ms on localhost (88/200)
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO Executor: Finished task 82.0 in stage 7.0 (TID 98). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 120, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 104.0 in stage 7.0 (TID 120)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 98) in 676 ms on localhost (89/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 87.0 in stage 7.0 (TID 103). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 121, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 103) in 512 ms on localhost (90/200)
15/08/06 17:44:52 INFO Executor: Running task 105.0 in stage 7.0 (TID 121)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO Executor: Finished task 88.0 in stage 7.0 (TID 104). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 122, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 106.0 in stage 7.0 (TID 122)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 104) in 480 ms on localhost (91/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 89.0 in stage 7.0 (TID 105). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 123, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 107.0 in stage 7.0 (TID 123)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 105) in 506 ms on localhost (92/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 91.0 in stage 7.0 (TID 107). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 124, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 108.0 in stage 7.0 (TID 124)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 107) in 500 ms on localhost (93/200)
15/08/06 17:44:52 INFO Executor: Finished task 90.0 in stage 7.0 (TID 106). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 125, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 109.0 in stage 7.0 (TID 125)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 106) in 519 ms on localhost (94/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO Executor: Finished task 92.0 in stage 7.0 (TID 108). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO Executor: Finished task 93.0 in stage 7.0 (TID 109). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 126, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 110.0 in stage 7.0 (TID 126)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 108) in 543 ms on localhost (95/200)
15/08/06 17:44:52 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 127, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 111.0 in stage 7.0 (TID 127)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 109) in 513 ms on localhost (96/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 96.0 in stage 7.0 (TID 112). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 128, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 112.0 in stage 7.0 (TID 128)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 112) in 547 ms on localhost (97/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 95.0 in stage 7.0 (TID 111). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 129, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 113.0 in stage 7.0 (TID 129)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 111) in 565 ms on localhost (98/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 98.0 in stage 7.0 (TID 114). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 130, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 114.0 in stage 7.0 (TID 130)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 114) in 585 ms on localhost (99/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 99.0 in stage 7.0 (TID 115). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 131, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 115.0 in stage 7.0 (TID 131)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 115) in 611 ms on localhost (100/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 101.0 in stage 7.0 (TID 117). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 132, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 116.0 in stage 7.0 (TID 132)
15/08/06 17:44:52 INFO Executor: Finished task 100.0 in stage 7.0 (TID 116). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 117) in 597 ms on localhost (101/200)
15/08/06 17:44:52 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 133, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 117.0 in stage 7.0 (TID 133)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 116) in 619 ms on localhost (102/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 102.0 in stage 7.0 (TID 118). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 134, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 118.0 in stage 7.0 (TID 134)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 118) in 621 ms on localhost (103/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 103.0 in stage 7.0 (TID 119). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 135, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 119.0 in stage 7.0 (TID 135)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 119) in 626 ms on localhost (104/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO Executor: Finished task 104.0 in stage 7.0 (TID 120). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 136, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 120.0 in stage 7.0 (TID 136)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 120) in 745 ms on localhost (105/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 105.0 in stage 7.0 (TID 121). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 137, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 121.0 in stage 7.0 (TID 137)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 121) in 750 ms on localhost (106/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 106.0 in stage 7.0 (TID 122). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 138, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 122.0 in stage 7.0 (TID 138)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 122) in 767 ms on localhost (107/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO Executor: Finished task 107.0 in stage 7.0 (TID 123). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 139, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 123.0 in stage 7.0 (TID 139)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 123) in 757 ms on localhost (108/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Finished task 109.0 in stage 7.0 (TID 125). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 140, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 124.0 in stage 7.0 (TID 140)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 125) in 729 ms on localhost (109/200)
15/08/06 17:44:52 INFO Executor: Finished task 108.0 in stage 7.0 (TID 124). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 141, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:52 INFO Executor: Running task 125.0 in stage 7.0 (TID 141)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 124) in 744 ms on localhost (110/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:52 INFO Executor: Finished task 110.0 in stage 7.0 (TID 126). 1124 bytes result sent to driver
15/08/06 17:44:52 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 142, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:52 INFO Executor: Running task 126.0 in stage 7.0 (TID 142)
15/08/06 17:44:52 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 126) in 706 ms on localhost (111/200)
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 111.0 in stage 7.0 (TID 127). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 143, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 127.0 in stage 7.0 (TID 143)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 127) in 726 ms on localhost (112/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 113.0 in stage 7.0 (TID 129). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 144, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 128.0 in stage 7.0 (TID 144)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 129) in 677 ms on localhost (113/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 112.0 in stage 7.0 (TID 128). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 145, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 129.0 in stage 7.0 (TID 145)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 128) in 699 ms on localhost (114/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 114.0 in stage 7.0 (TID 130). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 146, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 130.0 in stage 7.0 (TID 146)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 130) in 737 ms on localhost (115/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 115.0 in stage 7.0 (TID 131). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 147, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 131.0 in stage 7.0 (TID 147)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 131) in 706 ms on localhost (116/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 116.0 in stage 7.0 (TID 132). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO Executor: Finished task 117.0 in stage 7.0 (TID 133). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 148, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 132.0 in stage 7.0 (TID 148)
15/08/06 17:44:53 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 149, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 132) in 721 ms on localhost (117/200)
15/08/06 17:44:53 INFO Executor: Running task 133.0 in stage 7.0 (TID 149)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 133) in 711 ms on localhost (118/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 133.0 in stage 7.0 (TID 149). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 150, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 134.0 in stage 7.0 (TID 150)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 149) in 49 ms on localhost (119/200)
15/08/06 17:44:53 INFO Executor: Finished task 119.0 in stage 7.0 (TID 135). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 151, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 135.0 in stage 7.0 (TID 151)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 135) in 673 ms on localhost (120/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 118.0 in stage 7.0 (TID 134). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 152, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 136.0 in stage 7.0 (TID 152)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 134) in 696 ms on localhost (121/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO Executor: Finished task 120.0 in stage 7.0 (TID 136). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 153, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 137.0 in stage 7.0 (TID 153)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 136) in 581 ms on localhost (122/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 121.0 in stage 7.0 (TID 137). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 154, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 138.0 in stage 7.0 (TID 154)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 137) in 546 ms on localhost (123/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 122.0 in stage 7.0 (TID 138). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 155, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 139.0 in stage 7.0 (TID 155)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 138) in 537 ms on localhost (124/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 123.0 in stage 7.0 (TID 139). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 156, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 140.0 in stage 7.0 (TID 156)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 139) in 538 ms on localhost (125/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 124.0 in stage 7.0 (TID 140). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 157, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 141.0 in stage 7.0 (TID 157)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 140) in 546 ms on localhost (126/200)
15/08/06 17:44:53 INFO Executor: Finished task 125.0 in stage 7.0 (TID 141). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 158, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 142.0 in stage 7.0 (TID 158)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 141) in 543 ms on localhost (127/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 126.0 in stage 7.0 (TID 142). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 159, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 143.0 in stage 7.0 (TID 159)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 142) in 520 ms on localhost (128/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 127.0 in stage 7.0 (TID 143). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 160, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 144.0 in stage 7.0 (TID 160)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 143) in 518 ms on localhost (129/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO Executor: Finished task 144.0 in stage 7.0 (TID 160). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 161, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 145.0 in stage 7.0 (TID 161)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 160) in 56 ms on localhost (130/200)
15/08/06 17:44:53 INFO Executor: Finished task 128.0 in stage 7.0 (TID 144). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 162, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 146.0 in stage 7.0 (TID 162)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 144) in 510 ms on localhost (131/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 129.0 in stage 7.0 (TID 145). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 163, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 147.0 in stage 7.0 (TID 163)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 145) in 507 ms on localhost (132/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 130.0 in stage 7.0 (TID 146). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 164, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 148.0 in stage 7.0 (TID 164)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 146) in 506 ms on localhost (133/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO Executor: Finished task 131.0 in stage 7.0 (TID 147). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 165, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 149.0 in stage 7.0 (TID 165)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 147) in 526 ms on localhost (134/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 132.0 in stage 7.0 (TID 148). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 166, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 150.0 in stage 7.0 (TID 166)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 148) in 545 ms on localhost (135/200)
15/08/06 17:44:53 INFO Executor: Finished task 135.0 in stage 7.0 (TID 151). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 167, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 151.0 in stage 7.0 (TID 167)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 151) in 498 ms on localhost (136/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 136.0 in stage 7.0 (TID 152). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 168, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 152) in 500 ms on localhost (137/200)
15/08/06 17:44:53 INFO Executor: Running task 152.0 in stage 7.0 (TID 168)
15/08/06 17:44:53 INFO Executor: Finished task 134.0 in stage 7.0 (TID 150). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 169, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Running task 153.0 in stage 7.0 (TID 169)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 150) in 518 ms on localhost (138/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 150.0 in stage 7.0 (TID 166). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 170, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 154.0 in stage 7.0 (TID 170)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 166) in 77 ms on localhost (139/200)
15/08/06 17:44:53 INFO Executor: Finished task 153.0 in stage 7.0 (TID 169). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 171, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 155.0 in stage 7.0 (TID 171)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 169) in 58 ms on localhost (140/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 137.0 in stage 7.0 (TID 153). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 172, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 156.0 in stage 7.0 (TID 172)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 153) in 539 ms on localhost (141/200)
15/08/06 17:44:53 INFO Executor: Finished task 138.0 in stage 7.0 (TID 154). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 173, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 157.0 in stage 7.0 (TID 173)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 154) in 540 ms on localhost (142/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO Executor: Finished task 139.0 in stage 7.0 (TID 155). 1124 bytes result sent to driver
15/08/06 17:44:53 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 174, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:53 INFO Executor: Running task 158.0 in stage 7.0 (TID 174)
15/08/06 17:44:53 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 155) in 572 ms on localhost (143/200)
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO Executor: Finished task 140.0 in stage 7.0 (TID 156). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 175, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 159.0 in stage 7.0 (TID 175)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 156) in 587 ms on localhost (144/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 142.0 in stage 7.0 (TID 158). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 176, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 158) in 585 ms on localhost (145/200)
15/08/06 17:44:54 INFO Executor: Running task 160.0 in stage 7.0 (TID 176)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO Executor: Finished task 141.0 in stage 7.0 (TID 157). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 177, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 161.0 in stage 7.0 (TID 177)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 157) in 606 ms on localhost (146/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 143.0 in stage 7.0 (TID 159). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 178, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 162.0 in stage 7.0 (TID 178)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 159) in 582 ms on localhost (147/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 162.0 in stage 7.0 (TID 178). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 179, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 163.0 in stage 7.0 (TID 179)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 178) in 89 ms on localhost (148/200)
15/08/06 17:44:54 INFO Executor: Finished task 145.0 in stage 7.0 (TID 161). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 180, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 164.0 in stage 7.0 (TID 180)
15/08/06 17:44:54 INFO Executor: Finished task 147.0 in stage 7.0 (TID 163). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 161) in 599 ms on localhost (149/200)
15/08/06 17:44:54 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 181, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 165.0 in stage 7.0 (TID 181)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 163) in 592 ms on localhost (150/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO Executor: Finished task 146.0 in stage 7.0 (TID 162). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 182, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 166.0 in stage 7.0 (TID 182)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 162) in 614 ms on localhost (151/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 164.0 in stage 7.0 (TID 180). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO Executor: Finished task 148.0 in stage 7.0 (TID 164). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 183, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 167.0 in stage 7.0 (TID 183)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 180) in 74 ms on localhost (152/200)
15/08/06 17:44:54 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 184, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 168.0 in stage 7.0 (TID 184)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 164) in 578 ms on localhost (153/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 149.0 in stage 7.0 (TID 165). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 185, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 169.0 in stage 7.0 (TID 185)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 165) in 567 ms on localhost (154/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO Executor: Finished task 167.0 in stage 7.0 (TID 183). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 170.0 in stage 7.0 (TID 186, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 170.0 in stage 7.0 (TID 186)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 183) in 74 ms on localhost (155/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 169.0 in stage 7.0 (TID 185). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 171.0 in stage 7.0 (TID 187, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 171.0 in stage 7.0 (TID 187)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 185) in 55 ms on localhost (156/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 151.0 in stage 7.0 (TID 167). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 172.0 in stage 7.0 (TID 188, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 172.0 in stage 7.0 (TID 188)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 167) in 582 ms on localhost (157/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 152.0 in stage 7.0 (TID 168). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 173.0 in stage 7.0 (TID 189, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 173.0 in stage 7.0 (TID 189)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 168) in 587 ms on localhost (158/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 155.0 in stage 7.0 (TID 171). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 174.0 in stage 7.0 (TID 190, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 174.0 in stage 7.0 (TID 190)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 171) in 722 ms on localhost (159/200)
15/08/06 17:44:54 INFO Executor: Finished task 154.0 in stage 7.0 (TID 170). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 175.0 in stage 7.0 (TID 191, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 175.0 in stage 7.0 (TID 191)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 170) in 748 ms on localhost (160/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 173.0 in stage 7.0 (TID 189). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO TaskSetManager: Starting task 176.0 in stage 7.0 (TID 192, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 176.0 in stage 7.0 (TID 192)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 173.0 in stage 7.0 (TID 189) in 226 ms on localhost (161/200)
15/08/06 17:44:54 INFO Executor: Finished task 157.0 in stage 7.0 (TID 173). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 177.0 in stage 7.0 (TID 193, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 177.0 in stage 7.0 (TID 193)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 173) in 695 ms on localhost (162/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO Executor: Finished task 156.0 in stage 7.0 (TID 172). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 178.0 in stage 7.0 (TID 194, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 178.0 in stage 7.0 (TID 194)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 172) in 740 ms on localhost (163/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO Executor: Finished task 158.0 in stage 7.0 (TID 174). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 179.0 in stage 7.0 (TID 195, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 179.0 in stage 7.0 (TID 195)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 174) in 690 ms on localhost (164/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 159.0 in stage 7.0 (TID 175). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 180.0 in stage 7.0 (TID 196, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 180.0 in stage 7.0 (TID 196)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 175) in 684 ms on localhost (165/200)
15/08/06 17:44:54 INFO Executor: Finished task 160.0 in stage 7.0 (TID 176). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 181.0 in stage 7.0 (TID 197, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 181.0 in stage 7.0 (TID 197)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 176) in 669 ms on localhost (166/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 161.0 in stage 7.0 (TID 177). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 182.0 in stage 7.0 (TID 198, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 182.0 in stage 7.0 (TID 198)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 177) in 675 ms on localhost (167/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 180.0 in stage 7.0 (TID 196). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 183.0 in stage 7.0 (TID 199, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 183.0 in stage 7.0 (TID 199)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 180.0 in stage 7.0 (TID 196) in 67 ms on localhost (168/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 163.0 in stage 7.0 (TID 179). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 184.0 in stage 7.0 (TID 200, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 184.0 in stage 7.0 (TID 200)
15/08/06 17:44:54 INFO Executor: Finished task 165.0 in stage 7.0 (TID 181). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 179) in 686 ms on localhost (169/200)
15/08/06 17:44:54 INFO TaskSetManager: Starting task 185.0 in stage 7.0 (TID 201, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 185.0 in stage 7.0 (TID 201)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 181) in 682 ms on localhost (170/200)
15/08/06 17:44:54 INFO Executor: Finished task 166.0 in stage 7.0 (TID 182). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 186.0 in stage 7.0 (TID 202, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 182) in 669 ms on localhost (171/200)
15/08/06 17:44:54 INFO Executor: Running task 186.0 in stage 7.0 (TID 202)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO Executor: Finished task 168.0 in stage 7.0 (TID 184). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 187.0 in stage 7.0 (TID 203, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 187.0 in stage 7.0 (TID 203)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 184) in 640 ms on localhost (172/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:54 INFO Executor: Finished task 184.0 in stage 7.0 (TID 200). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 188.0 in stage 7.0 (TID 204, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 188.0 in stage 7.0 (TID 204)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 184.0 in stage 7.0 (TID 200) in 73 ms on localhost (173/200)
15/08/06 17:44:54 INFO Executor: Finished task 186.0 in stage 7.0 (TID 202). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 189.0 in stage 7.0 (TID 205, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 189.0 in stage 7.0 (TID 205)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 186.0 in stage 7.0 (TID 202) in 66 ms on localhost (174/200)
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:54 INFO Executor: Finished task 171.0 in stage 7.0 (TID 187). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 190.0 in stage 7.0 (TID 206, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 190.0 in stage 7.0 (TID 206)
15/08/06 17:44:54 INFO TaskSetManager: Finished task 171.0 in stage 7.0 (TID 187) in 645 ms on localhost (175/200)
15/08/06 17:44:54 INFO Executor: Finished task 170.0 in stage 7.0 (TID 186). 1124 bytes result sent to driver
15/08/06 17:44:54 INFO TaskSetManager: Starting task 191.0 in stage 7.0 (TID 207, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:54 INFO Executor: Running task 191.0 in stage 7.0 (TID 207)
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO TaskSetManager: Finished task 170.0 in stage 7.0 (TID 186) in 671 ms on localhost (176/200)
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO Executor: Finished task 172.0 in stage 7.0 (TID 188). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Starting task 192.0 in stage 7.0 (TID 208, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:55 INFO Executor: Running task 192.0 in stage 7.0 (TID 208)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 172.0 in stage 7.0 (TID 188) in 666 ms on localhost (177/200)
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO Executor: Finished task 190.0 in stage 7.0 (TID 206). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Starting task 193.0 in stage 7.0 (TID 209, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:55 INFO Executor: Running task 193.0 in stage 7.0 (TID 209)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 190.0 in stage 7.0 (TID 206) in 63 ms on localhost (178/200)
15/08/06 17:44:55 INFO Executor: Finished task 174.0 in stage 7.0 (TID 190). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Starting task 194.0 in stage 7.0 (TID 210, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:55 INFO Executor: Running task 194.0 in stage 7.0 (TID 210)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 174.0 in stage 7.0 (TID 190) in 492 ms on localhost (179/200)
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO Executor: Finished task 176.0 in stage 7.0 (TID 192). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO TaskSetManager: Starting task 195.0 in stage 7.0 (TID 211, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:55 INFO Executor: Running task 195.0 in stage 7.0 (TID 211)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 176.0 in stage 7.0 (TID 192) in 483 ms on localhost (180/200)
15/08/06 17:44:55 INFO Executor: Finished task 175.0 in stage 7.0 (TID 191). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Starting task 196.0 in stage 7.0 (TID 212, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:55 INFO Executor: Running task 196.0 in stage 7.0 (TID 212)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 175.0 in stage 7.0 (TID 191) in 499 ms on localhost (181/200)
15/08/06 17:44:55 INFO Executor: Finished task 177.0 in stage 7.0 (TID 193). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Starting task 197.0 in stage 7.0 (TID 213, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:55 INFO Executor: Running task 197.0 in stage 7.0 (TID 213)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 177.0 in stage 7.0 (TID 193) in 493 ms on localhost (182/200)
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO Executor: Finished task 178.0 in stage 7.0 (TID 194). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Starting task 198.0 in stage 7.0 (TID 214, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:55 INFO Executor: Running task 198.0 in stage 7.0 (TID 214)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 178.0 in stage 7.0 (TID 194) in 509 ms on localhost (183/200)
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO Executor: Finished task 197.0 in stage 7.0 (TID 213). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO Executor: Finished task 195.0 in stage 7.0 (TID 211). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Starting task 199.0 in stage 7.0 (TID 215, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/06 17:44:55 INFO Executor: Running task 199.0 in stage 7.0 (TID 215)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 197.0 in stage 7.0 (TID 213) in 66 ms on localhost (184/200)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 195.0 in stage 7.0 (TID 211) in 88 ms on localhost (185/200)
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/06 17:44:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:55 INFO Executor: Finished task 179.0 in stage 7.0 (TID 195). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 179.0 in stage 7.0 (TID 195) in 540 ms on localhost (186/200)
15/08/06 17:44:55 INFO Executor: Finished task 181.0 in stage 7.0 (TID 197). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 181.0 in stage 7.0 (TID 197) in 556 ms on localhost (187/200)
15/08/06 17:44:55 INFO Executor: Finished task 182.0 in stage 7.0 (TID 198). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 182.0 in stage 7.0 (TID 198) in 538 ms on localhost (188/200)
15/08/06 17:44:55 INFO Executor: Finished task 183.0 in stage 7.0 (TID 199). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 183.0 in stage 7.0 (TID 199) in 527 ms on localhost (189/200)
15/08/06 17:44:55 INFO Executor: Finished task 185.0 in stage 7.0 (TID 201). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 185.0 in stage 7.0 (TID 201) in 494 ms on localhost (190/200)
15/08/06 17:44:55 INFO Executor: Finished task 187.0 in stage 7.0 (TID 203). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 187.0 in stage 7.0 (TID 203) in 512 ms on localhost (191/200)
15/08/06 17:44:55 INFO Executor: Finished task 189.0 in stage 7.0 (TID 205). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 189.0 in stage 7.0 (TID 205) in 486 ms on localhost (192/200)
15/08/06 17:44:55 INFO Executor: Finished task 188.0 in stage 7.0 (TID 204). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 188.0 in stage 7.0 (TID 204) in 507 ms on localhost (193/200)
15/08/06 17:44:55 INFO Executor: Finished task 191.0 in stage 7.0 (TID 207). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 191.0 in stage 7.0 (TID 207) in 454 ms on localhost (194/200)
15/08/06 17:44:55 INFO Executor: Finished task 193.0 in stage 7.0 (TID 209). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 193.0 in stage 7.0 (TID 209) in 433 ms on localhost (195/200)
15/08/06 17:44:55 INFO Executor: Finished task 192.0 in stage 7.0 (TID 208). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 192.0 in stage 7.0 (TID 208) in 474 ms on localhost (196/200)
15/08/06 17:44:55 INFO Executor: Finished task 194.0 in stage 7.0 (TID 210). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 194.0 in stage 7.0 (TID 210) in 435 ms on localhost (197/200)
15/08/06 17:44:55 INFO Executor: Finished task 196.0 in stage 7.0 (TID 212). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO Executor: Finished task 198.0 in stage 7.0 (TID 214). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO Executor: Finished task 199.0 in stage 7.0 (TID 215). 1124 bytes result sent to driver
15/08/06 17:44:55 INFO TaskSetManager: Finished task 196.0 in stage 7.0 (TID 212) in 426 ms on localhost (198/200)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 198.0 in stage 7.0 (TID 214) in 379 ms on localhost (199/200)
15/08/06 17:44:55 INFO TaskSetManager: Finished task 199.0 in stage 7.0 (TID 215) in 357 ms on localhost (200/200)
15/08/06 17:44:55 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/06 17:44:55 INFO DAGScheduler: Stage 7 (mapPartitions at Exchange.scala:64) finished in 7.809 s
15/08/06 17:44:55 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:44:55 INFO DAGScheduler: running: Set()
15/08/06 17:44:55 INFO DAGScheduler: waiting: Set(Stage 8)
15/08/06 17:44:55 INFO DAGScheduler: failed: Set()
15/08/06 17:44:55 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@13ffccbe
15/08/06 17:44:55 INFO StatsReportListener: task runtime:(count: 200, mean: 616.100000, stdev: 407.603705, max: 2031.000000, min: 45.000000)
15/08/06 17:44:55 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:55 INFO StatsReportListener: 	45.0 ms	57.0 ms	74.0 ms	494.0 ms	577.0 ms	696.0 ms	770.0 ms	1.9 s	2.0 s
15/08/06 17:44:55 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 7580.820000, stdev: 3876.470167, max: 17153.000000, min: 0.000000)
15/08/06 17:44:55 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:55 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	6.3 KB	8.1 KB	9.9 KB	11.7 KB	13.4 KB	16.8 KB
15/08/06 17:44:55 INFO DAGScheduler: Missing parents for Stage 8: List()
15/08/06 17:44:55 INFO DAGScheduler: Submitting Stage 8 (MapPartitionsRDD[48] at mapPartitions at Aggregate.scala:151), which is now runnable
15/08/06 17:44:55 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.110000, stdev: 0.328481, max: 2.000000, min: 0.000000)
15/08/06 17:44:55 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:55 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/06 17:44:55 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:44:55 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:55 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:44:55 INFO StatsReportListener: task result size:(count: 200, mean: 1124.000000, stdev: 0.000000, max: 1124.000000, min: 1124.000000)
15/08/06 17:44:55 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:55 INFO StatsReportListener: 	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B
15/08/06 17:44:55 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.283529, stdev: 3.900138, max: 99.577465, min: 74.137931)
15/08/06 17:44:55 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:55 INFO StatsReportListener: 	74 %	89 %	92 %	98 %	99 %	99 %	99 %	99 %	100 %
15/08/06 17:44:55 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.054610, stdev: 0.298287, max: 2.739726, min: 0.000000)
15/08/06 17:44:55 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:55 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 3 %
15/08/06 17:44:55 INFO StatsReportListener: other time pct: (count: 200, mean: 2.661861, stdev: 3.769159, max: 24.137931, min: 0.422535)
15/08/06 17:44:55 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:44:55 INFO StatsReportListener: 	 0 %	 1 %	 1 %	 1 %	 1 %	 2 %	 8 %	11 %	24 %
15/08/06 17:44:55 INFO MemoryStore: ensureFreeSpace(149960) called with curMem=768433, maxMem=3333968363
15/08/06 17:44:55 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 146.4 KB, free 3.1 GB)
15/08/06 17:44:55 INFO MemoryStore: ensureFreeSpace(65189) called with curMem=918393, maxMem=3333968363
15/08/06 17:44:55 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 63.7 KB, free 3.1 GB)
15/08/06 17:44:55 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:42907 (size: 63.7 KB, free: 3.1 GB)
15/08/06 17:44:55 INFO BlockManagerMaster: Updated info of block broadcast_12_piece0
15/08/06 17:44:55 INFO DefaultExecutionContext: Created broadcast 12 from broadcast at DAGScheduler.scala:838
15/08/06 17:44:55 INFO DAGScheduler: Submitting 200 missing tasks from Stage 8 (MapPartitionsRDD[48] at mapPartitions at Aggregate.scala:151)
15/08/06 17:44:55 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/06 17:44:55 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 216, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 217, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 218, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 219, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 220, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 221, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 222, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 223, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 224, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 225, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 226, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 227, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 228, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 229, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 230, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 231, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:55 INFO Executor: Running task 1.0 in stage 8.0 (TID 217)
15/08/06 17:44:55 INFO Executor: Running task 0.0 in stage 8.0 (TID 216)
15/08/06 17:44:55 INFO Executor: Running task 5.0 in stage 8.0 (TID 221)
15/08/06 17:44:55 INFO Executor: Running task 9.0 in stage 8.0 (TID 225)
15/08/06 17:44:55 INFO Executor: Running task 7.0 in stage 8.0 (TID 223)
15/08/06 17:44:55 INFO Executor: Running task 8.0 in stage 8.0 (TID 224)
15/08/06 17:44:55 INFO Executor: Running task 2.0 in stage 8.0 (TID 218)
15/08/06 17:44:55 INFO Executor: Running task 12.0 in stage 8.0 (TID 228)
15/08/06 17:44:55 INFO Executor: Running task 13.0 in stage 8.0 (TID 229)
15/08/06 17:44:55 INFO Executor: Running task 10.0 in stage 8.0 (TID 226)
15/08/06 17:44:55 INFO Executor: Running task 6.0 in stage 8.0 (TID 222)
15/08/06 17:44:55 INFO Executor: Running task 4.0 in stage 8.0 (TID 220)
15/08/06 17:44:55 INFO Executor: Running task 3.0 in stage 8.0 (TID 219)
15/08/06 17:44:55 INFO Executor: Running task 15.0 in stage 8.0 (TID 231)
15/08/06 17:44:55 INFO Executor: Running task 11.0 in stage 8.0 (TID 227)
15/08/06 17:44:55 INFO Executor: Running task 14.0 in stage 8.0 (TID 230)
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4e258d31
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000005_221/part-00005
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@384c5396
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@11967031
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7f50f5c3
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000015_231/part-00015
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000012_228/part-00012
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000006_222/part-00006
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c371682
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000011_227/part-00011
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4fc69005
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000009_225/part-00009
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ff7726a
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000007_223/part-00007
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22510071
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000010_226/part-00010
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b39d11e
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@49f9a4b9
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000000_216/part-00000
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24ad93a0
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000004_220/part-00004
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000014_230/part-00014
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@619f4d36
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000013_229/part-00013
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6f14dc32
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000001_217/part-00001
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ef8d281
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000003_219/part-00003
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@68d8d2c5
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000002_218/part-00002
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4ff65421
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000008_224/part-00008
15/08/06 17:44:56 INFO CodecConfig: Compression set to false
15/08/06 17:44:56 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:56 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:56 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e9cac48
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4900acf3
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d8e2320
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ea4213d
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ccffd32
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b5aaaf6
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32108fda
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@113ecd2e
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d544ae3
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ff2c2e0
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6ad818b7
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3af81da2
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@392e0709
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7dae3935
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d7c1a7c
15/08/06 17:44:56 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79825420
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,836
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,616
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,396
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,076
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,156
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,816
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,136
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 555B for [ps_partkey] INT32: 128 values, 519B raw, 519B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,075B for [part_value] DOUBLE: 128 values, 1,031B raw, 1,031B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,136
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 619B for [ps_partkey] INT32: 144 values, 583B raw, 583B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,203B for [part_value] DOUBLE: 144 values, 1,159B raw, 1,159B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,356
15/08/06 17:44:56 INFO BlockManager: Removing broadcast 11
15/08/06 17:44:56 INFO BlockManager: Removing block broadcast_11
15/08/06 17:44:56 INFO MemoryStore: Block broadcast_11 of size 13544 dropped from memory (free 3332998325)
15/08/06 17:44:56 INFO BlockManager: Removing block broadcast_11_piece0
15/08/06 17:44:56 INFO MemoryStore: Block broadcast_11_piece0 of size 7369 dropped from memory (free 3333005694)
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:42907 in memory (size: 7.2 KB, free: 3.1 GB)
15/08/06 17:44:56 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ContextCleaner: Cleaned broadcast 11
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 603B for [ps_partkey] INT32: 140 values, 567B raw, 567B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 659B for [ps_partkey] INT32: 154 values, 623B raw, 623B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,171B for [part_value] DOUBLE: 140 values, 1,127B raw, 1,127B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 711B for [ps_partkey] INT32: 167 values, 675B raw, 675B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,283B for [part_value] DOUBLE: 154 values, 1,239B raw, 1,239B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 615B for [ps_partkey] INT32: 143 values, 579B raw, 579B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 667B for [ps_partkey] INT32: 156 values, 631B raw, 631B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,387B for [part_value] DOUBLE: 167 values, 1,343B raw, 1,343B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,195B for [part_value] DOUBLE: 143 values, 1,151B raw, 1,151B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,299B for [part_value] DOUBLE: 156 values, 1,255B raw, 1,255B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 751B for [ps_partkey] INT32: 177 values, 715B raw, 715B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 815B for [ps_partkey] INT32: 193 values, 779B raw, 779B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,467B for [part_value] DOUBLE: 177 values, 1,423B raw, 1,423B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:56 INFO ColumnChunkPageWriteStore: written 1,595B for [part_value] DOUBLE: 193 values, 1,551B raw, 1,551B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000011_227' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000011
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000015_231' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000015
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000005_221' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000005
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000011_227: Committed
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000005_221: Committed
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000015_231: Committed
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000000_216' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000000
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000000_216: Committed
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000009_225' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000009
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000013_229' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000013
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000009_225: Committed
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000013_229: Committed
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000014_230' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000014
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000006_222' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000006
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000004_220' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000004
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000006_222: Committed
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000004_220: Committed
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000002_218' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000002
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000014_230: Committed
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000002_218: Committed
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000001_217' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000001
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000012_228' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000012
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000001_217: Committed
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000012_228: Committed
15/08/06 17:44:57 INFO Executor: Finished task 0.0 in stage 8.0 (TID 216). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 5.0 in stage 8.0 (TID 221). 781 bytes result sent to driver
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000008_224' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000008
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000008_224: Committed
15/08/06 17:44:57 INFO Executor: Finished task 12.0 in stage 8.0 (TID 228). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 2.0 in stage 8.0 (TID 218). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 1.0 in stage 8.0 (TID 217). 781 bytes result sent to driver
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000003_219' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000003
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000003_219: Committed
15/08/06 17:44:57 INFO Executor: Finished task 11.0 in stage 8.0 (TID 227). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 6.0 in stage 8.0 (TID 222). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 9.0 in stage 8.0 (TID 225). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 15.0 in stage 8.0 (TID 231). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 13.0 in stage 8.0 (TID 229). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 8.0 in stage 8.0 (TID 224). 781 bytes result sent to driver
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000010_226' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000010
15/08/06 17:44:57 INFO Executor: Finished task 14.0 in stage 8.0 (TID 230). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 232, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000010_226: Committed
15/08/06 17:44:57 INFO Executor: Finished task 4.0 in stage 8.0 (TID 220). 781 bytes result sent to driver
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000007_223' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000007
15/08/06 17:44:57 INFO Executor: Finished task 3.0 in stage 8.0 (TID 219). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Running task 16.0 in stage 8.0 (TID 232)
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000007_223: Committed
15/08/06 17:44:57 INFO Executor: Finished task 10.0 in stage 8.0 (TID 226). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 216) in 1316 ms on localhost (1/200)
15/08/06 17:44:57 INFO Executor: Finished task 7.0 in stage 8.0 (TID 223). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 233, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 17.0 in stage 8.0 (TID 233)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 221) in 1314 ms on localhost (2/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 234, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 18.0 in stage 8.0 (TID 234)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 228) in 1308 ms on localhost (3/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 235, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 19.0 in stage 8.0 (TID 235)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 218) in 1319 ms on localhost (4/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 236, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 20.0 in stage 8.0 (TID 236)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 217) in 1322 ms on localhost (5/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 237, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 21.0 in stage 8.0 (TID 237)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 227) in 1321 ms on localhost (6/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 238, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 22.0 in stage 8.0 (TID 238)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 222) in 1328 ms on localhost (7/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 239, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 23.0 in stage 8.0 (TID 239)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 225) in 1327 ms on localhost (8/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 240, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 24.0 in stage 8.0 (TID 240)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 231) in 1323 ms on localhost (9/200)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 229) in 1327 ms on localhost (10/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 241, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 25.0 in stage 8.0 (TID 241)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 242, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 26.0 in stage 8.0 (TID 242)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 224) in 1335 ms on localhost (11/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 243, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 27.0 in stage 8.0 (TID 243)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 230) in 1331 ms on localhost (12/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 244, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 28.0 in stage 8.0 (TID 244)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 220) in 1345 ms on localhost (13/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 245, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 29.0 in stage 8.0 (TID 245)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 246, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 30.0 in stage 8.0 (TID 246)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 219) in 1352 ms on localhost (14/200)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 226) in 1348 ms on localhost (15/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 247, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 31.0 in stage 8.0 (TID 247)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 223) in 1355 ms on localhost (16/200)
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6acf1cb5
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000018_234/part-00018
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@163e7a94
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@533bf759
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000017_233/part-00017
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@124fc546
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000023_239/part-00023
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ab7315b
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000018_234' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000018
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000018_234: Committed
15/08/06 17:44:57 INFO Executor: Finished task 18.0 in stage 8.0 (TID 234). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 248, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 32.0 in stage 8.0 (TID 248)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 234) in 524 ms on localhost (17/200)
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c42dd57
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c4cd67f
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000019_235/part-00019
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16223e95
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000026_242/part-00026
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7af5d737
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000016_232/part-00016
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6796351b
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ea241e2
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000017_233' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000017
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000017_233: Committed
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@13c359ab
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000029_245/part-00029
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO Executor: Finished task 17.0 in stage 8.0 (TID 233). 781 bytes result sent to driver
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 249, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 33.0 in stage 8.0 (TID 249)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 233) in 593 ms on localhost (18/200)
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@46d75cc0
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000023_239' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000023
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000023_239: Committed
15/08/06 17:44:57 INFO Executor: Finished task 23.0 in stage 8.0 (TID 239). 781 bytes result sent to driver
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@429c04f9
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000030_246/part-00030
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 250, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO Executor: Running task 34.0 in stage 8.0 (TID 250)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 239) in 584 ms on localhost (19/200)
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4556ed0a
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6f5ef320
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3f49c0e
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000027_243/part-00027
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000031_247/part-00031
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000026_242' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000026
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000026_242: Committed
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO Executor: Finished task 26.0 in stage 8.0 (TID 242). 781 bytes result sent to driver
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000019_235' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000019
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000019_235: Committed
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7cadb683
15/08/06 17:44:57 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 251, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 35.0 in stage 8.0 (TID 251)
15/08/06 17:44:57 INFO Executor: Finished task 19.0 in stage 8.0 (TID 235). 781 bytes result sent to driver
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d94a824
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000028_244/part-00028
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 242) in 617 ms on localhost (20/200)
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@59068910
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@694a791d
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000025_241/part-00025
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 252, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000022_238/part-00022
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO Executor: Running task 36.0 in stage 8.0 (TID 252)
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 235) in 643 ms on localhost (21/200)
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@65e182a7
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000020_236/part-00020
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14dfd754
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000021_237/part-00021
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2cc7d6e6
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@73edad01
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000024_240/part-00024
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,036
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@477ed07f
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ad07463
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 395B for [ps_partkey] INT32: 88 values, 359B raw, 359B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 755B for [part_value] DOUBLE: 88 values, 711B raw, 711B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@315e8e4
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@136cd251
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000016_232' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000016
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000016_232: Committed
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42f95421
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@26442197
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000029_245' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000029
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000029_245: Committed
15/08/06 17:44:57 INFO Executor: Finished task 16.0 in stage 8.0 (TID 232). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 29.0 in stage 8.0 (TID 245). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 253, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 232) in 702 ms on localhost (22/200)
15/08/06 17:44:57 INFO Executor: Running task 37.0 in stage 8.0 (TID 253)
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@49950c0d
15/08/06 17:44:57 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 254, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 38.0 in stage 8.0 (TID 254)
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,296
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:44:57 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 245) in 678 ms on localhost (23/200)
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000030_246' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000030
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000030_246: Committed
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO Executor: Finished task 30.0 in stage 8.0 (TID 246). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 255, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 39.0 in stage 8.0 (TID 255)
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,856
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 647B for [ps_partkey] INT32: 151 values, 611B raw, 611B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 246) in 683 ms on localhost (24/200)
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,259B for [part_value] DOUBLE: 151 values, 1,215B raw, 1,215B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 759B for [ps_partkey] INT32: 179 values, 723B raw, 723B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,483B for [part_value] DOUBLE: 179 values, 1,439B raw, 1,439B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,356
15/08/06 17:44:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,276
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 659B for [ps_partkey] INT32: 154 values, 623B raw, 623B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,283B for [part_value] DOUBLE: 154 values, 1,239B raw, 1,239B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 643B for [ps_partkey] INT32: 150 values, 607B raw, 607B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO ColumnChunkPageWriteStore: written 1,251B for [part_value] DOUBLE: 150 values, 1,207B raw, 1,207B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000027_243' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000027
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000027_243: Committed
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000031_247' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000031
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000031_247: Committed
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000025_241' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000025
15/08/06 17:44:57 INFO Executor: Finished task 27.0 in stage 8.0 (TID 243). 781 bytes result sent to driver
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000025_241: Committed
15/08/06 17:44:57 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 256, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 40.0 in stage 8.0 (TID 256)
15/08/06 17:44:57 INFO Executor: Finished task 31.0 in stage 8.0 (TID 247). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Finished task 25.0 in stage 8.0 (TID 241). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 243) in 707 ms on localhost (25/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 257, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 41.0 in stage 8.0 (TID 257)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 247) in 693 ms on localhost (26/200)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 258, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 241) in 718 ms on localhost (27/200)
15/08/06 17:44:57 INFO Executor: Running task 42.0 in stage 8.0 (TID 258)
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000028_244' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000028
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000028_244: Committed
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000022_238' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000022
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000022_238: Committed
15/08/06 17:44:57 INFO Executor: Finished task 28.0 in stage 8.0 (TID 244). 781 bytes result sent to driver
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000021_237' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000021
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000021_237: Committed
15/08/06 17:44:57 INFO Executor: Finished task 22.0 in stage 8.0 (TID 238). 781 bytes result sent to driver
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000020_236' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000020
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000020_236: Committed
15/08/06 17:44:57 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 259, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Finished task 21.0 in stage 8.0 (TID 237). 781 bytes result sent to driver
15/08/06 17:44:57 INFO Executor: Running task 43.0 in stage 8.0 (TID 259)
15/08/06 17:44:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000024_240' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000024
15/08/06 17:44:57 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000024_240: Committed
15/08/06 17:44:57 INFO Executor: Finished task 20.0 in stage 8.0 (TID 236). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 244) in 736 ms on localhost (28/200)
15/08/06 17:44:57 INFO Executor: Finished task 24.0 in stage 8.0 (TID 240). 781 bytes result sent to driver
15/08/06 17:44:57 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 260, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 261, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 262, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 46.0 in stage 8.0 (TID 262)
15/08/06 17:44:57 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 263, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:57 INFO Executor: Running task 45.0 in stage 8.0 (TID 261)
15/08/06 17:44:57 INFO Executor: Running task 47.0 in stage 8.0 (TID 263)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 238) in 864 ms on localhost (29/200)
15/08/06 17:44:57 INFO Executor: Running task 44.0 in stage 8.0 (TID 260)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 236) in 881 ms on localhost (30/200)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 240) in 868 ms on localhost (31/200)
15/08/06 17:44:57 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 237) in 882 ms on localhost (32/200)
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:57 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34560063
15/08/06 17:44:57 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000032_248/part-00032
15/08/06 17:44:57 INFO CodecConfig: Compression set to false
15/08/06 17:44:57 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:57 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:57 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a20b300
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000034_250/part-00034
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51ef4cff
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000035_251/part-00035
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76ddc042
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@b1f9ba4
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7500c8f4
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000033_249/part-00033
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,576
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 703B for [ps_partkey] INT32: 165 values, 667B raw, 667B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3382c2aa
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,371B for [part_value] DOUBLE: 165 values, 1,327B raw, 1,327B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,116
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 811B for [ps_partkey] INT32: 192 values, 775B raw, 775B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,587B for [part_value] DOUBLE: 192 values, 1,543B raw, 1,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2789d9a6
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@60208583
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000036_252/part-00036
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000032_248' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000032
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000032_248: Committed
15/08/06 17:44:58 INFO Executor: Finished task 32.0 in stage 8.0 (TID 248). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 264, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 48.0 in stage 8.0 (TID 264)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 248) in 530 ms on localhost (33/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000034_250' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000034
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000034_250: Committed
15/08/06 17:44:58 INFO Executor: Finished task 34.0 in stage 8.0 (TID 250). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 265, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@36c4120
15/08/06 17:44:58 INFO Executor: Running task 49.0 in stage 8.0 (TID 265)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 250) in 464 ms on localhost (34/200)
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@58fd27e8
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000040_256/part-00040
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d7416d1
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000037_253/part-00037
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000033_249' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000033
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000033_249: Committed
15/08/06 17:44:58 INFO Executor: Finished task 33.0 in stage 8.0 (TID 249). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 266, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 50.0 in stage 8.0 (TID 266)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 249) in 496 ms on localhost (35/200)
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c02874e
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3fa7532e
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@455d9b43
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000039_255/part-00039
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10ac90e0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000042_258/part-00042
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5bb6a9b9
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000041_257/part-00041
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2fca0245
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@21f3bc8b
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000044_260/part-00044
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4a8c1587
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000037_253' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000037
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000037_253: Committed
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@b09fc86
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000036_252' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000036
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000036_252: Committed
15/08/06 17:44:58 INFO Executor: Finished task 37.0 in stage 8.0 (TID 253). 781 bytes result sent to driver
15/08/06 17:44:58 INFO Executor: Finished task 36.0 in stage 8.0 (TID 252). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 267, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 51.0 in stage 8.0 (TID 267)
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:44:58 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 253) in 458 ms on localhost (36/200)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 268, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 52.0 in stage 8.0 (TID 268)
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@599d451b
15/08/06 17:44:58 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 252) in 514 ms on localhost (37/200)
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000047_263/part-00047
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e5bd704
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b8df63d
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,676
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d723ba2
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000038_254/part-00038
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000046_262/part-00046
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000043_259/part-00043
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,636
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 523B for [ps_partkey] INT32: 120 values, 487B raw, 487B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,011B for [part_value] DOUBLE: 120 values, 967B raw, 967B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 515B for [ps_partkey] INT32: 118 values, 479B raw, 479B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 995B for [part_value] DOUBLE: 118 values, 951B raw, 951B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1e90c401
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d589ff5
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7861c343
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a5aa173
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000045_261/part-00045
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000040_256' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000040
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000040_256: Committed
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,816
15/08/06 17:44:58 INFO Executor: Finished task 40.0 in stage 8.0 (TID 256). 781 bytes result sent to driver
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 269, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 256) in 464 ms on localhost (38/200)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 551B for [ps_partkey] INT32: 127 values, 515B raw, 515B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO Executor: Running task 53.0 in stage 8.0 (TID 269)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,067B for [part_value] DOUBLE: 127 values, 1,023B raw, 1,023B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ff81a9b
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@501c1ce
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@365abdf3
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000039_255' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000039
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000039_255: Committed
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000042_258' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000042
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000042_258: Committed
15/08/06 17:44:58 INFO Executor: Finished task 39.0 in stage 8.0 (TID 255). 781 bytes result sent to driver
15/08/06 17:44:58 INFO Executor: Finished task 42.0 in stage 8.0 (TID 258). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 270, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 54.0 in stage 8.0 (TID 270)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 271, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 55.0 in stage 8.0 (TID 271)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 258) in 477 ms on localhost (39/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000041_257' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000041
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000041_257: Committed
15/08/06 17:44:58 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 255) in 499 ms on localhost (40/200)
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:44:58 INFO Executor: Finished task 41.0 in stage 8.0 (TID 257). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 272, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 56.0 in stage 8.0 (TID 272)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 257) in 485 ms on localhost (41/200)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000043_259' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000043
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000043_259: Committed
15/08/06 17:44:58 INFO Executor: Finished task 43.0 in stage 8.0 (TID 259). 781 bytes result sent to driver
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000047_263' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000047
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000047_263: Committed
15/08/06 17:44:58 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 273, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 57.0 in stage 8.0 (TID 273)
15/08/06 17:44:58 INFO Executor: Finished task 47.0 in stage 8.0 (TID 263). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 259) in 470 ms on localhost (42/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000044_260' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000044
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000044_260: Committed
15/08/06 17:44:58 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 274, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 58.0 in stage 8.0 (TID 274)
15/08/06 17:44:58 INFO Executor: Finished task 44.0 in stage 8.0 (TID 260). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 263) in 358 ms on localhost (43/200)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 275, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 59.0 in stage 8.0 (TID 275)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 260) in 362 ms on localhost (44/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000046_262' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000046
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000046_262: Committed
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO Executor: Finished task 46.0 in stage 8.0 (TID 262). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 276, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 60.0 in stage 8.0 (TID 276)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 262) in 369 ms on localhost (45/200)
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@9c2a521
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000049_265/part-00049
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000038_254' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000038
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000038_254: Committed
15/08/06 17:44:58 INFO Executor: Finished task 38.0 in stage 8.0 (TID 254). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 277, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 61.0 in stage 8.0 (TID 277)
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 254) in 564 ms on localhost (46/200)
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79219faf
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,176
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 423B for [ps_partkey] INT32: 95 values, 387B raw, 387B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 811B for [part_value] DOUBLE: 95 values, 767B raw, 767B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@149c9739
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000050_266/part-00050
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17889743
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000048_264/part-00048
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c6358c4
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d52377a
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000049_265' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000049
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000049_265: Committed
15/08/06 17:44:58 INFO Executor: Finished task 49.0 in stage 8.0 (TID 265). 781 bytes result sent to driver
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 278, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 62.0 in stage 8.0 (TID 278)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 265) in 345 ms on localhost (47/200)
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000050_266' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000050
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000050_266: Committed
15/08/06 17:44:58 INFO Executor: Finished task 50.0 in stage 8.0 (TID 266). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 279, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 63.0 in stage 8.0 (TID 279)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 266) in 349 ms on localhost (48/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000048_264' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000048
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000048_264: Committed
15/08/06 17:44:58 INFO Executor: Finished task 48.0 in stage 8.0 (TID 264). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 280, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 64.0 in stage 8.0 (TID 280)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 264) in 388 ms on localhost (49/200)
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@489d28a0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000051_267/part-00051
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000035_251' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000035
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000035_251: Committed
15/08/06 17:44:58 INFO Executor: Finished task 35.0 in stage 8.0 (TID 251). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 281, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 65.0 in stage 8.0 (TID 281)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 251) in 820 ms on localhost (50/200)
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2a651afd
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@59653639
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000052_268/part-00052
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1af03e98
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000057_273/part-00057
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000051_267' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000051
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000051_267: Committed
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@20f47afb
15/08/06 17:44:58 INFO Executor: Finished task 51.0 in stage 8.0 (TID 267). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 282, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 66.0 in stage 8.0 (TID 282)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 267) in 348 ms on localhost (51/200)
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34f2e1df
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000056_272/part-00056
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35cb12a1
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000055_271/part-00055
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@500e6e2d
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000060_276/part-00060
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16b1b520
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6cea5383
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000054_270/part-00054
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000053_269/part-00053
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@18b7d725
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6363756a
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000059_275/part-00059
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17fe4577
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000058_274/part-00058
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5eee79fb
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7229f6a3
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000061_277/part-00061
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@36a36689
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@75fd89c1
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@585f1a6b
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@9067068
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5b24efda
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@55fefff5
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,336
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d2a4591
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 455B for [ps_partkey] INT32: 103 values, 419B raw, 419B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 875B for [part_value] DOUBLE: 103 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,636
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 515B for [ps_partkey] INT32: 118 values, 479B raw, 479B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 995B for [part_value] DOUBLE: 118 values, 951B raw, 951B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,516
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000052_268' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000052
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000052_268: Committed
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 491B for [ps_partkey] INT32: 112 values, 455B raw, 455B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:44:58 INFO Executor: Finished task 52.0 in stage 8.0 (TID 268). 781 bytes result sent to driver
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 947B for [part_value] DOUBLE: 112 values, 903B raw, 903B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 283, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,336
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000057_273' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000057
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000057_273: Committed
15/08/06 17:44:58 INFO Executor: Running task 67.0 in stage 8.0 (TID 283)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO Executor: Finished task 57.0 in stage 8.0 (TID 273). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 268) in 410 ms on localhost (52/200)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 284, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 855B for [ps_partkey] INT32: 203 values, 819B raw, 819B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO Executor: Running task 68.0 in stage 8.0 (TID 284)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,675B for [part_value] DOUBLE: 203 values, 1,631B raw, 1,631B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3077df72
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000062_278/part-00062
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 273) in 336 ms on localhost (53/200)
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000055_271' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000055
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000055_271: Committed
15/08/06 17:44:58 INFO Executor: Finished task 55.0 in stage 8.0 (TID 271). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 285, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 69.0 in stage 8.0 (TID 285)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 271) in 364 ms on localhost (54/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000054_270' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000054
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000054_270: Committed
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@574da02f
15/08/06 17:44:58 INFO Executor: Finished task 54.0 in stage 8.0 (TID 270). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 286, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 70.0 in stage 8.0 (TID 286)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000056_272' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000056
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000056_272: Committed
15/08/06 17:44:58 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 270) in 375 ms on localhost (55/200)
15/08/06 17:44:58 INFO Executor: Finished task 56.0 in stage 8.0 (TID 272). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 287, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 71.0 in stage 8.0 (TID 287)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 272) in 373 ms on localhost (56/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000053_269' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000053
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000053_269: Committed
15/08/06 17:44:58 INFO Executor: Finished task 53.0 in stage 8.0 (TID 269). 781 bytes result sent to driver
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000058_274' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000058
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000058_274: Committed
15/08/06 17:44:58 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 288, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 72.0 in stage 8.0 (TID 288)
15/08/06 17:44:58 INFO Executor: Finished task 58.0 in stage 8.0 (TID 274). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 269) in 401 ms on localhost (57/200)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 289, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 73.0 in stage 8.0 (TID 289)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 274) in 362 ms on localhost (58/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000060_276' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000060
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000060_276: Committed
15/08/06 17:44:58 INFO Executor: Finished task 60.0 in stage 8.0 (TID 276). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 290, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 74.0 in stage 8.0 (TID 290)
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:44:58 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 276) in 359 ms on localhost (59/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000059_275' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000059
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000059_275: Committed
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO Executor: Finished task 59.0 in stage 8.0 (TID 275). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 291, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 75.0 in stage 8.0 (TID 291)
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e7b8f29
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000063_279/part-00063
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3974a393
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000064_280/part-00064
15/08/06 17:44:58 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 275) in 374 ms on localhost (60/200)
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000061_277' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000061
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000061_277: Committed
15/08/06 17:44:58 INFO Executor: Finished task 61.0 in stage 8.0 (TID 277). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 292, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 76.0 in stage 8.0 (TID 292)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 277) in 349 ms on localhost (61/200)
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6291c832
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a51987e
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b7cebc3
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000065_281/part-00065
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000062_278' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000062
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000062_278: Committed
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO Executor: Finished task 62.0 in stage 8.0 (TID 278). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 293, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 77.0 in stage 8.0 (TID 293)
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 278) in 247 ms on localhost (62/200)
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,056
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000045_261' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000045
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000045_261: Committed
15/08/06 17:44:58 INFO Executor: Finished task 45.0 in stage 8.0 (TID 261). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 294, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 78.0 in stage 8.0 (TID 294)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 261) in 786 ms on localhost (63/200)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 599B for [ps_partkey] INT32: 139 values, 563B raw, 563B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6a98ab4d
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,163B for [part_value] DOUBLE: 139 values, 1,119B raw, 1,119B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@783fffdd
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000066_282/part-00066
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@75757ced
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000063_279' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000063
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000063_279: Committed
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000065_281' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000065
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000065_281: Committed
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000064_280' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000064
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000064_280: Committed
15/08/06 17:44:58 INFO Executor: Finished task 63.0 in stage 8.0 (TID 279). 781 bytes result sent to driver
15/08/06 17:44:58 INFO Executor: Finished task 65.0 in stage 8.0 (TID 281). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 295, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Finished task 64.0 in stage 8.0 (TID 280). 781 bytes result sent to driver
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,816
15/08/06 17:44:58 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 279) in 289 ms on localhost (64/200)
15/08/06 17:44:58 INFO Executor: Running task 79.0 in stage 8.0 (TID 295)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 296, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 80.0 in stage 8.0 (TID 296)
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fa57753
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000068_284/part-00068
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 281) in 267 ms on localhost (65/200)
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 280) in 287 ms on localhost (66/200)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 751B for [ps_partkey] INT32: 177 values, 715B raw, 715B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 297, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 81.0 in stage 8.0 (TID 297)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,467B for [part_value] DOUBLE: 177 values, 1,423B raw, 1,423B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@76e47eb7
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000067_283/part-00067
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ecb0150
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@eeed792
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,716
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 531B for [ps_partkey] INT32: 122 values, 495B raw, 495B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,027B for [part_value] DOUBLE: 122 values, 983B raw, 983B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4638ce0f
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000074_290/part-00074
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25d2e4e6
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000071_287/part-00071
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@432e3d3d
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000069_285/part-00069
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4228abeb
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7c2ef33b
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000073_289/part-00073
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000070_286/part-00070
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@190f3ef6
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@122fa822
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000075_291/part-00075
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000066_282' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000066
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000066_282: Committed
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO Executor: Finished task 66.0 in stage 8.0 (TID 282). 781 bytes result sent to driver
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1405786c
15/08/06 17:44:58 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 298, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000072_288/part-00072
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO Executor: Running task 82.0 in stage 8.0 (TID 298)
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 282) in 337 ms on localhost (67/200)
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1cdcc76a
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3352a79e
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@777a9064
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@23372bac
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000067_283' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000067
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000067_283: Committed
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3dab7dfb
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000068_284' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000068
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000068_284: Committed
15/08/06 17:44:58 INFO Executor: Finished task 67.0 in stage 8.0 (TID 283). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 299, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Finished task 68.0 in stage 8.0 (TID 284). 781 bytes result sent to driver
15/08/06 17:44:58 INFO Executor: Running task 83.0 in stage 8.0 (TID 299)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 283) in 287 ms on localhost (68/200)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 300, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b0f29f3
15/08/06 17:44:58 INFO Executor: Running task 84.0 in stage 8.0 (TID 300)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 284) in 287 ms on localhost (69/200)
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000076_292/part-00076
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1cd49d15
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,236
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 635B for [ps_partkey] INT32: 148 values, 599B raw, 599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,235B for [part_value] DOUBLE: 148 values, 1,191B raw, 1,191B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c83ed00
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000078_294/part-00078
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e296472
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000077_293/part-00077
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4e877288
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@16e92c34
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f136e5
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000071_287' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000071
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000071_287: Committed
15/08/06 17:44:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,096
15/08/06 17:44:58 INFO Executor: Finished task 71.0 in stage 8.0 (TID 287). 781 bytes result sent to driver
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 301, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO Executor: Running task 85.0 in stage 8.0 (TID 301)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000074_290' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000074
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000074_290: Committed
15/08/06 17:44:58 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 287) in 322 ms on localhost (70/200)
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 807B for [ps_partkey] INT32: 191 values, 771B raw, 771B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO Executor: Finished task 74.0 in stage 8.0 (TID 290). 781 bytes result sent to driver
15/08/06 17:44:58 INFO ColumnChunkPageWriteStore: written 1,579B for [part_value] DOUBLE: 191 values, 1,535B raw, 1,535B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:58 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 302, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 86.0 in stage 8.0 (TID 302)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 290) in 312 ms on localhost (71/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000075_291' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000075
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000075_291: Committed
15/08/06 17:44:58 INFO Executor: Finished task 75.0 in stage 8.0 (TID 291). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 303, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 87.0 in stage 8.0 (TID 303)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 291) in 314 ms on localhost (72/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000070_286' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000070
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000070_286: Committed
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000072_288' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000072
15/08/06 17:44:58 INFO Executor: Finished task 70.0 in stage 8.0 (TID 286). 781 bytes result sent to driver
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000072_288: Committed
15/08/06 17:44:58 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 304, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 88.0 in stage 8.0 (TID 304)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000069_285' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000069
15/08/06 17:44:58 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 286) in 342 ms on localhost (73/200)
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000069_285: Committed
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000073_289' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000073
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000073_289: Committed
15/08/06 17:44:58 INFO Executor: Finished task 73.0 in stage 8.0 (TID 289). 781 bytes result sent to driver
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000076_292' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000076
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000076_292: Committed
15/08/06 17:44:58 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 305, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Finished task 76.0 in stage 8.0 (TID 292). 781 bytes result sent to driver
15/08/06 17:44:58 INFO Executor: Running task 89.0 in stage 8.0 (TID 305)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 289) in 336 ms on localhost (74/200)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 306, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 90.0 in stage 8.0 (TID 306)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 292) in 315 ms on localhost (75/200)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000078_294' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000078
15/08/06 17:44:58 INFO Executor: Finished task 69.0 in stage 8.0 (TID 285). 781 bytes result sent to driver
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000078_294: Committed
15/08/06 17:44:58 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 307, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 91.0 in stage 8.0 (TID 307)
15/08/06 17:44:58 INFO Executor: Finished task 78.0 in stage 8.0 (TID 294). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 285) in 358 ms on localhost (76/200)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 308, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 92.0 in stage 8.0 (TID 308)
15/08/06 17:44:58 INFO Executor: Finished task 72.0 in stage 8.0 (TID 288). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 294) in 277 ms on localhost (77/200)
15/08/06 17:44:58 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 309, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 288) in 346 ms on localhost (78/200)
15/08/06 17:44:58 INFO Executor: Running task 93.0 in stage 8.0 (TID 309)
15/08/06 17:44:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000077_293' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000077
15/08/06 17:44:58 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000077_293: Committed
15/08/06 17:44:58 INFO Executor: Finished task 77.0 in stage 8.0 (TID 293). 781 bytes result sent to driver
15/08/06 17:44:58 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 310, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:58 INFO Executor: Running task 94.0 in stage 8.0 (TID 310)
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:58 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 293) in 298 ms on localhost (79/200)
15/08/06 17:44:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35a637d2
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7abab245
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000079_295/part-00079
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000080_296/part-00080
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a873987
15/08/06 17:44:58 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000081_297/part-00081
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO CodecConfig: Compression set to false
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:58 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:58 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:58 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64cd05b2
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76eecb9d
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ffb8edb
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@518bdbf7
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000082_298/part-00082
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c392506
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000084_300/part-00084
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3df589ba
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1f0904da
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000083_299/part-00083
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3c420449
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000079_295' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000079
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000079_295: Committed
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4456d19e
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000080_296' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000080
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000080_296: Committed
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000081_297' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000081
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000081_297: Committed
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000082_298' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000082
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000082_298: Committed
15/08/06 17:44:59 INFO Executor: Finished task 79.0 in stage 8.0 (TID 295). 781 bytes result sent to driver
15/08/06 17:44:59 INFO Executor: Finished task 81.0 in stage 8.0 (TID 297). 781 bytes result sent to driver
15/08/06 17:44:59 INFO Executor: Finished task 80.0 in stage 8.0 (TID 296). 781 bytes result sent to driver
15/08/06 17:44:59 INFO Executor: Finished task 82.0 in stage 8.0 (TID 298). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 311, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 95.0 in stage 8.0 (TID 311)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 295) in 320 ms on localhost (80/200)
15/08/06 17:44:59 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 312, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 96.0 in stage 8.0 (TID 312)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 297) in 317 ms on localhost (81/200)
15/08/06 17:44:59 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 313, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 97.0 in stage 8.0 (TID 313)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 296) in 321 ms on localhost (82/200)
15/08/06 17:44:59 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 314, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 98.0 in stage 8.0 (TID 314)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 298) in 211 ms on localhost (83/200)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000084_300' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000084
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000084_300: Committed
15/08/06 17:44:59 INFO Executor: Finished task 84.0 in stage 8.0 (TID 300). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 315, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 99.0 in stage 8.0 (TID 315)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 300) in 206 ms on localhost (84/200)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000083_299' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000083
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000083_299: Committed
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@494c702a
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000086_302/part-00086
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO Executor: Finished task 83.0 in stage 8.0 (TID 299). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 316, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 100.0 in stage 8.0 (TID 316)
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 299) in 218 ms on localhost (85/200)
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@339d568b
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 53 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@738d6728
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000091_307/part-00091
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a50503d
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000093_309/part-00093
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10c62dfe
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42f5874c
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000092_308/part-00092
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@67bedf0f
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000094_310/part-00094
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@148dc5f3
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d9e7a75
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000085_301/part-00085
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22f2fa58
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@585b0ec
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000089_305/part-00089
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000087_303/part-00087
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000088_304/part-00088
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1fd8bac8
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000090_306/part-00090
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,656
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 519B for [ps_partkey] INT32: 119 values, 483B raw, 483B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,003B for [part_value] DOUBLE: 119 values, 959B raw, 959B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78fc3d77
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78278dae
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ee4158e
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@57e311c5
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,276
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 843B for [ps_partkey] INT32: 200 values, 807B raw, 807B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000086_302' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000086
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000086_302: Committed
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,651B for [part_value] DOUBLE: 200 values, 1,607B raw, 1,607B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ca3c996
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO Executor: Finished task 86.0 in stage 8.0 (TID 302). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 317, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 101.0 in stage 8.0 (TID 317)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 302) in 311 ms on localhost (86/200)
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32dfb243
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,316
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@aee9cd9
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 651B for [ps_partkey] INT32: 152 values, 615B raw, 615B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3b155df7
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,736
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 735B for [ps_partkey] INT32: 173 values, 699B raw, 699B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,267B for [part_value] DOUBLE: 152 values, 1,223B raw, 1,223B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,435B for [part_value] DOUBLE: 173 values, 1,391B raw, 1,391B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000093_309' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000093
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000093_309: Committed
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e09517b
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000098_314/part-00098
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO Executor: Finished task 93.0 in stage 8.0 (TID 309). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000091_307' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000091
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000091_307: Committed
15/08/06 17:44:59 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 318, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 102.0 in stage 8.0 (TID 318)
15/08/06 17:44:59 INFO Executor: Finished task 91.0 in stage 8.0 (TID 307). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 309) in 317 ms on localhost (87/200)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000088_304' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000088
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000088_304: Committed
15/08/06 17:44:59 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 319, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 103.0 in stage 8.0 (TID 319)
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2514337
15/08/06 17:44:59 INFO Executor: Finished task 88.0 in stage 8.0 (TID 304). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000096_312/part-00096
15/08/06 17:44:59 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 307) in 324 ms on localhost (88/200)
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 320, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 304) in 334 ms on localhost (89/200)
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19e4983a
15/08/06 17:44:59 INFO Executor: Running task 104.0 in stage 8.0 (TID 320)
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000099_315/part-00099
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000092_308' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000092
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000092_308: Committed
15/08/06 17:44:59 INFO Executor: Finished task 92.0 in stage 8.0 (TID 308). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 321, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 105.0 in stage 8.0 (TID 321)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000085_301' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000085
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000085_301: Committed
15/08/06 17:44:59 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 308) in 328 ms on localhost (90/200)
15/08/06 17:44:59 INFO Executor: Finished task 85.0 in stage 8.0 (TID 301). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 322, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 301) in 357 ms on localhost (91/200)
15/08/06 17:44:59 INFO Executor: Running task 106.0 in stage 8.0 (TID 322)
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000089_305' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000089
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000089_305: Committed
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@400902cd
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO Executor: Finished task 89.0 in stage 8.0 (TID 305). 781 bytes result sent to driver
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@68361f03
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000095_311/part-00095
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000100_316/part-00100
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 323, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 107.0 in stage 8.0 (TID 323)
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 305) in 347 ms on localhost (92/200)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000094_310' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000094
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000094_310: Committed
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@496d7570
15/08/06 17:44:59 INFO Executor: Finished task 94.0 in stage 8.0 (TID 310). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 324, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 108.0 in stage 8.0 (TID 324)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 310) in 333 ms on localhost (93/200)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000090_306' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000090
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000090_306: Committed
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a807ffc
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000097_313/part-00097
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO Executor: Finished task 90.0 in stage 8.0 (TID 306). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66a6c898
15/08/06 17:44:59 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 325, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 109.0 in stage 8.0 (TID 325)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 306) in 353 ms on localhost (94/200)
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,516
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 691B for [ps_partkey] INT32: 162 values, 655B raw, 655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,347B for [part_value] DOUBLE: 162 values, 1,303B raw, 1,303B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000087_303' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000087
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@454a937f
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b8837bd
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000087_303: Committed
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO Executor: Finished task 87.0 in stage 8.0 (TID 303). 781 bytes result sent to driver
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35444d63
15/08/06 17:44:59 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 326, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 110.0 in stage 8.0 (TID 326)
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,556
15/08/06 17:44:59 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 303) in 376 ms on localhost (95/200)
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 499B for [ps_partkey] INT32: 114 values, 463B raw, 463B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 963B for [part_value] DOUBLE: 114 values, 919B raw, 919B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3c935226
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,396
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 667B for [ps_partkey] INT32: 156 values, 631B raw, 631B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,299B for [part_value] DOUBLE: 156 values, 1,255B raw, 1,255B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000098_314' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000098
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000098_314: Committed
15/08/06 17:44:59 INFO Executor: Finished task 98.0 in stage 8.0 (TID 314). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 327, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 111.0 in stage 8.0 (TID 327)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 314) in 282 ms on localhost (96/200)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000100_316' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000100
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000100_316: Committed
15/08/06 17:44:59 INFO Executor: Finished task 100.0 in stage 8.0 (TID 316). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 328, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 112.0 in stage 8.0 (TID 328)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000096_312' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000096
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000096_312: Committed
15/08/06 17:44:59 INFO Executor: Finished task 96.0 in stage 8.0 (TID 312). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 316) in 275 ms on localhost (97/200)
15/08/06 17:44:59 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 329, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 113.0 in stage 8.0 (TID 329)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 312) in 303 ms on localhost (98/200)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000097_313' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000097
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000097_313: Committed
15/08/06 17:44:59 INFO Executor: Finished task 97.0 in stage 8.0 (TID 313). 781 bytes result sent to driver
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000095_311' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000095
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000095_311: Committed
15/08/06 17:44:59 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 330, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Finished task 95.0 in stage 8.0 (TID 311). 781 bytes result sent to driver
15/08/06 17:44:59 INFO Executor: Running task 114.0 in stage 8.0 (TID 330)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 313) in 314 ms on localhost (99/200)
15/08/06 17:44:59 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 331, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 311) in 319 ms on localhost (100/200)
15/08/06 17:44:59 INFO Executor: Running task 115.0 in stage 8.0 (TID 331)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@15f47c9d
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000104_320/part-00104
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@662132ad
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000101_317/part-00101
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@264db59d
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,796
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@37ce6a10
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000109_325/part-00109
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 747B for [ps_partkey] INT32: 176 values, 711B raw, 711B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,459B for [part_value] DOUBLE: 176 values, 1,415B raw, 1,415B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e10ad1d
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,756
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 739B for [ps_partkey] INT32: 174 values, 703B raw, 703B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,443B for [part_value] DOUBLE: 174 values, 1,399B raw, 1,399B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@bbe9ddf
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000106_322/part-00106
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5219045
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@59a33b77
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e86a4a4
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000107_323/part-00107
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000108_324/part-00108
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d115cf
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2440b99a
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000105_321/part-00105
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1559400e
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000102_318/part-00102
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a1cec20
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000110_326/part-00110
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@626a479f
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000111_327/part-00111
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b9695a9
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a1e9838
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000113_329/part-00113
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@356ebf8e
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,616
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 711B for [ps_partkey] INT32: 167 values, 675B raw, 675B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,387B for [part_value] DOUBLE: 167 values, 1,343B raw, 1,343B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@79e21f09
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000112_328/part-00112
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c63c662
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000103_319/part-00103
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a758386
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@11054e7f
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@fb18bbb
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000101_317' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000101
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000101_317: Committed
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@334901ad
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f78bbbf
15/08/06 17:44:59 INFO Executor: Finished task 101.0 in stage 8.0 (TID 317). 781 bytes result sent to driver
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34109064
15/08/06 17:44:59 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 332, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 116.0 in stage 8.0 (TID 332)
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:44:59 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 317) in 406 ms on localhost (101/200)
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000104_320' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000104
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000104_320: Committed
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO Executor: Finished task 104.0 in stage 8.0 (TID 320). 781 bytes result sent to driver
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000106_322' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000106
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000106_322: Committed
15/08/06 17:44:59 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 333, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 117.0 in stage 8.0 (TID 333)
15/08/06 17:44:59 INFO Executor: Finished task 106.0 in stage 8.0 (TID 322). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 320) in 374 ms on localhost (102/200)
15/08/06 17:44:59 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 334, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 118.0 in stage 8.0 (TID 334)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000109_325' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000109
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000109_325: Committed
15/08/06 17:44:59 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 322) in 370 ms on localhost (103/200)
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@37bc1452
15/08/06 17:44:59 INFO Executor: Finished task 109.0 in stage 8.0 (TID 325). 781 bytes result sent to driver
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000107_323' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000107
15/08/06 17:44:59 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 335, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000107_323: Committed
15/08/06 17:44:59 INFO Executor: Running task 119.0 in stage 8.0 (TID 335)
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@75cc020e
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000115_331/part-00115
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO Executor: Finished task 107.0 in stage 8.0 (TID 323). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 325) in 355 ms on localhost (104/200)
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:44:59 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 336, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO Executor: Running task 120.0 in stage 8.0 (TID 336)
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 323) in 364 ms on localhost (105/200)
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000108_324' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000108
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000108_324: Committed
15/08/06 17:44:59 INFO Executor: Finished task 108.0 in stage 8.0 (TID 324). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 337, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000111_327' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000111
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000111_327: Committed
15/08/06 17:44:59 INFO Executor: Running task 121.0 in stage 8.0 (TID 337)
15/08/06 17:44:59 INFO Executor: Finished task 111.0 in stage 8.0 (TID 327). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 324) in 371 ms on localhost (106/200)
15/08/06 17:44:59 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 338, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 122.0 in stage 8.0 (TID 338)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 327) in 326 ms on localhost (107/200)
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@40590bfd
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000114_330/part-00114
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000110_326' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000110
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000110_326: Committed
15/08/06 17:44:59 INFO Executor: Finished task 110.0 in stage 8.0 (TID 326). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4edf85e9
15/08/06 17:44:59 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 339, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 123.0 in stage 8.0 (TID 339)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000103_319' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000103
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000103_319: Committed
15/08/06 17:44:59 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 326) in 366 ms on localhost (108/200)
15/08/06 17:44:59 INFO Executor: Finished task 103.0 in stage 8.0 (TID 319). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 340, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000113_329' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000113
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000113_329: Committed
15/08/06 17:44:59 INFO Executor: Running task 124.0 in stage 8.0 (TID 340)
15/08/06 17:44:59 INFO Executor: Finished task 113.0 in stage 8.0 (TID 329). 781 bytes result sent to driver
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000105_321' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000105
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000105_321: Committed
15/08/06 17:44:59 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 319) in 408 ms on localhost (109/200)
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,836
15/08/06 17:44:59 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 341, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 125.0 in stage 8.0 (TID 341)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 329) in 322 ms on localhost (110/200)
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 555B for [ps_partkey] INT32: 128 values, 519B raw, 519B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO Executor: Finished task 105.0 in stage 8.0 (TID 321). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,075B for [part_value] DOUBLE: 128 values, 1,031B raw, 1,031B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 342, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 126.0 in stage 8.0 (TID 342)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 321) in 409 ms on localhost (111/200)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d0a557f
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000112_328' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000112
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000112_328: Committed
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO Executor: Finished task 112.0 in stage 8.0 (TID 328). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 343, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 127.0 in stage 8.0 (TID 343)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 328) in 341 ms on localhost (112/200)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000114_330' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000114
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000114_330: Committed
15/08/06 17:44:59 INFO Executor: Finished task 114.0 in stage 8.0 (TID 330). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 344, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 128.0 in stage 8.0 (TID 344)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 330) in 353 ms on localhost (113/200)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000099_315' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000099
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000099_315: Committed
15/08/06 17:44:59 INFO Executor: Finished task 99.0 in stage 8.0 (TID 315). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 345, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 129.0 in stage 8.0 (TID 345)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 315) in 670 ms on localhost (114/200)
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@356e0dc6
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000120_336/part-00120
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@264501b8
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000116_332/part-00116
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e32fb6d
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000117_333/part-00117
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c112cc5
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000119_335/part-00119
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@591a0429
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000121_337/part-00121
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c6d95a4
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3780d9bb
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c56a074
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000125_341/part-00125
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,056
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 799B for [ps_partkey] INT32: 189 values, 763B raw, 763B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e0460d1
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@41704087
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,563B for [part_value] DOUBLE: 189 values, 1,519B raw, 1,519B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000118_334/part-00118
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,116
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 811B for [ps_partkey] INT32: 192 values, 775B raw, 775B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,587B for [part_value] DOUBLE: 192 values, 1,543B raw, 1,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@55747dae
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@309c80a1
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000123_339/part-00123
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51d5529f
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1ed4483c
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000122_338/part-00122
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@23c71437
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,736
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7fde17df
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 535B for [ps_partkey] INT32: 123 values, 499B raw, 499B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,035B for [part_value] DOUBLE: 123 values, 991B raw, 991B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@72fc87a8
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c07c942
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000126_342/part-00126
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3da0cce7
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000124_340/part-00124
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,876
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000117_333' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000117
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000117_333: Committed
15/08/06 17:44:59 INFO Executor: Finished task 117.0 in stage 8.0 (TID 333). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 763B for [ps_partkey] INT32: 180 values, 727B raw, 727B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000120_336' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000120
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000120_336: Committed
15/08/06 17:44:59 INFO Executor: Finished task 120.0 in stage 8.0 (TID 336). 781 bytes result sent to driver
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,491B for [part_value] DOUBLE: 180 values, 1,447B raw, 1,447B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1bf48957
15/08/06 17:44:59 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 346, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7aba5bc6
15/08/06 17:44:59 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 333) in 201 ms on localhost (115/200)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000121_337' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000121
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,776
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000116_332' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000116
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000116_332: Committed
15/08/06 17:44:59 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 347, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 743B for [ps_partkey] INT32: 175 values, 707B raw, 707B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO Executor: Running task 130.0 in stage 8.0 (TID 346)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 336) in 194 ms on localhost (116/200)
15/08/06 17:44:59 INFO Executor: Running task 131.0 in stage 8.0 (TID 347)
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000127_343/part-00127
15/08/06 17:44:59 INFO Executor: Finished task 116.0 in stage 8.0 (TID 332). 781 bytes result sent to driver
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000121_337: Committed
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,451B for [part_value] DOUBLE: 175 values, 1,407B raw, 1,407B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO Executor: Finished task 121.0 in stage 8.0 (TID 337). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 348, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000119_335' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000119
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000119_335: Committed
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c187010
15/08/06 17:44:59 INFO Executor: Running task 132.0 in stage 8.0 (TID 348)
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@186530a4
15/08/06 17:44:59 INFO Executor: Finished task 119.0 in stage 8.0 (TID 335). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 332) in 214 ms on localhost (117/200)
15/08/06 17:44:59 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 349, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,236
15/08/06 17:44:59 INFO Executor: Running task 133.0 in stage 8.0 (TID 349)
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,656
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 635B for [ps_partkey] INT32: 148 values, 599B raw, 599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 337) in 195 ms on localhost (118/200)
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,235B for [part_value] DOUBLE: 148 values, 1,191B raw, 1,191B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 719B for [ps_partkey] INT32: 169 values, 683B raw, 683B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 350, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,403B for [part_value] DOUBLE: 169 values, 1,359B raw, 1,359B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO Executor: Running task 134.0 in stage 8.0 (TID 350)
15/08/06 17:44:59 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 335) in 211 ms on localhost (119/200)
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6df30379
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000128_344/part-00128
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d693553
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@162c1443
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000129_345/part-00129
15/08/06 17:44:59 INFO CodecConfig: Compression set to false
15/08/06 17:44:59 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:44:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:44:59 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:44:59 INFO ParquetOutputFormat: Validation is off
15/08/06 17:44:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000122_338' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000122
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000122_338: Committed
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000118_334' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000118
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000118_334: Committed
15/08/06 17:44:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000123_339' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000123
15/08/06 17:44:59 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000123_339: Committed
15/08/06 17:44:59 INFO Executor: Finished task 122.0 in stage 8.0 (TID 338). 781 bytes result sent to driver
15/08/06 17:44:59 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 351, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:44:59 INFO Executor: Running task 135.0 in stage 8.0 (TID 351)
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2af5b1a
15/08/06 17:44:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:44:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:44:59 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1554648
15/08/06 17:45:00 INFO Executor: Finished task 123.0 in stage 8.0 (TID 339). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 338) in 304 ms on localhost (120/200)
15/08/06 17:45:00 INFO Executor: Finished task 118.0 in stage 8.0 (TID 334). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:00 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 352, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO Executor: Running task 136.0 in stage 8.0 (TID 352)
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:45:00 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 339) in 300 ms on localhost (121/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 353, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 137.0 in stage 8.0 (TID 353)
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 334) in 331 ms on localhost (122/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000124_340' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000124
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000124_340: Committed
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO Executor: Finished task 124.0 in stage 8.0 (TID 340). 781 bytes result sent to driver
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000126_342' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000126
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000126_342: Committed
15/08/06 17:45:00 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 354, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Finished task 126.0 in stage 8.0 (TID 342). 781 bytes result sent to driver
15/08/06 17:45:00 INFO Executor: Running task 138.0 in stage 8.0 (TID 354)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 340) in 306 ms on localhost (123/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 355, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 139.0 in stage 8.0 (TID 355)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 342) in 301 ms on localhost (124/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000127_343' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000127
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000127_343: Committed
15/08/06 17:45:00 INFO Executor: Finished task 127.0 in stage 8.0 (TID 343). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 356, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 140.0 in stage 8.0 (TID 356)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 343) in 303 ms on localhost (125/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000128_344' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000128
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000128_344: Committed
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000129_345' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000129
15/08/06 17:45:00 INFO Executor: Finished task 128.0 in stage 8.0 (TID 344). 781 bytes result sent to driver
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000129_345: Committed
15/08/06 17:45:00 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 357, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 141.0 in stage 8.0 (TID 357)
15/08/06 17:45:00 INFO Executor: Finished task 129.0 in stage 8.0 (TID 345). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 344) in 282 ms on localhost (126/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 358, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 142.0 in stage 8.0 (TID 358)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 345) in 267 ms on localhost (127/200)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3e4d106f
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000130_346/part-00130
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51418614
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000133_349/part-00133
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24c9b4b9
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000134_350/part-00134
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4bc7277f
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000132_348/part-00132
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ff229b6
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1a12db8d
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2afd655a
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,056
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 599B for [ps_partkey] INT32: 139 values, 563B raw, 563B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,163B for [part_value] DOUBLE: 139 values, 1,119B raw, 1,119B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000102_318' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000102
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000102_318: Committed
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ae50168
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000136_352/part-00136
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO Executor: Finished task 102.0 in stage 8.0 (TID 318). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 359, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 143.0 in stage 8.0 (TID 359)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@280cd1c1
15/08/06 17:45:00 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 318) in 821 ms on localhost (128/200)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000137_353/part-00137
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7fbad8c3
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6466dc69
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@723398f3
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@197eb418
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000138_354/part-00138
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d61a0bb
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000131_347/part-00131
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000139_355/part-00139
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000133_349' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000133
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000133_349: Committed
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO Executor: Finished task 133.0 in stage 8.0 (TID 349). 781 bytes result sent to driver
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000134_350' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000134
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000134_350: Committed
15/08/06 17:45:00 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 360, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 144.0 in stage 8.0 (TID 360)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3f57eac5
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000135_351/part-00135
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO Executor: Finished task 134.0 in stage 8.0 (TID 350). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 349) in 246 ms on localhost (129/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 361, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO Executor: Running task 145.0 in stage 8.0 (TID 361)
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 350) in 245 ms on localhost (130/200)
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6d78b0de
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,736
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 535B for [ps_partkey] INT32: 123 values, 499B raw, 499B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,035B for [part_value] DOUBLE: 123 values, 991B raw, 991B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000115_331' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000115
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000115_331: Committed
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000130_346' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000130
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000130_346: Committed
15/08/06 17:45:00 INFO Executor: Finished task 115.0 in stage 8.0 (TID 331). 781 bytes result sent to driver
15/08/06 17:45:00 INFO Executor: Finished task 130.0 in stage 8.0 (TID 346). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 362, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 146.0 in stage 8.0 (TID 362)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6390d9f6
15/08/06 17:45:00 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 331) in 744 ms on localhost (131/200)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@607f28e8
15/08/06 17:45:00 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 363, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 147.0 in stage 8.0 (TID 363)
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:45:00 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 346) in 273 ms on localhost (132/200)
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,536
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 495B for [ps_partkey] INT32: 113 values, 459B raw, 459B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 955B for [part_value] DOUBLE: 113 values, 911B raw, 911B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@642ad3c1
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d3c55a5
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@656598d7
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3105a9a3
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000141_357/part-00141
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000140_356/part-00140
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000136_352' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000136
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000136_352: Committed
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000132_348' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000132
15/08/06 17:45:00 INFO Executor: Finished task 136.0 in stage 8.0 (TID 352). 781 bytes result sent to driver
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000132_348: Committed
15/08/06 17:45:00 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 364, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 148.0 in stage 8.0 (TID 364)
15/08/06 17:45:00 INFO Executor: Finished task 132.0 in stage 8.0 (TID 348). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 352) in 159 ms on localhost (133/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 365, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 149.0 in stage 8.0 (TID 365)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 348) in 282 ms on localhost (134/200)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3ad98205
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000137_353' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000137
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000137_353: Committed
15/08/06 17:45:00 INFO Executor: Finished task 137.0 in stage 8.0 (TID 353). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 366, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 150.0 in stage 8.0 (TID 366)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 353) in 170 ms on localhost (135/200)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@531a2fe2
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42b75224
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000142_358/part-00142
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000138_354' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000138
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000138_354: Committed
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:00 INFO Executor: Finished task 138.0 in stage 8.0 (TID 354). 781 bytes result sent to driver
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 367, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO Executor: Running task 151.0 in stage 8.0 (TID 367)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 354) in 173 ms on localhost (136/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000131_347' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000131
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000131_347: Committed
15/08/06 17:45:00 INFO Executor: Finished task 131.0 in stage 8.0 (TID 347). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 368, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 347) in 312 ms on localhost (137/200)
15/08/06 17:45:00 INFO Executor: Running task 152.0 in stage 8.0 (TID 368)
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000135_351' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000135
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000135_351: Committed
15/08/06 17:45:00 INFO Executor: Finished task 135.0 in stage 8.0 (TID 351). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 369, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 153.0 in stage 8.0 (TID 369)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000139_355' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000139
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000139_355: Committed
15/08/06 17:45:00 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 351) in 208 ms on localhost (138/200)
15/08/06 17:45:00 INFO Executor: Finished task 139.0 in stage 8.0 (TID 355). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 370, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 355) in 190 ms on localhost (139/200)
15/08/06 17:45:00 INFO Executor: Running task 154.0 in stage 8.0 (TID 370)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000141_357' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000141
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000141_357: Committed
15/08/06 17:45:00 INFO Executor: Finished task 141.0 in stage 8.0 (TID 357). 781 bytes result sent to driver
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000140_356' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000140
15/08/06 17:45:00 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 371, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000140_356: Committed
15/08/06 17:45:00 INFO Executor: Running task 155.0 in stage 8.0 (TID 371)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 357) in 178 ms on localhost (140/200)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ab73441
15/08/06 17:45:00 INFO Executor: Finished task 140.0 in stage 8.0 (TID 356). 781 bytes result sent to driver
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,656
15/08/06 17:45:00 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 372, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO Executor: Running task 156.0 in stage 8.0 (TID 372)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 356) in 190 ms on localhost (141/200)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 719B for [ps_partkey] INT32: 169 values, 683B raw, 683B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,403B for [part_value] DOUBLE: 169 values, 1,359B raw, 1,359B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d07812
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000143_359/part-00143
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34ad1845
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,336
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 455B for [ps_partkey] INT32: 103 values, 419B raw, 419B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 875B for [part_value] DOUBLE: 103 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000142_358' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000142
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000142_358: Committed
15/08/06 17:45:00 INFO Executor: Finished task 142.0 in stage 8.0 (TID 358). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 373, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 157.0 in stage 8.0 (TID 373)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 358) in 226 ms on localhost (142/200)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@934c53b
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000147_363/part-00147
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@28476589
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000144_360/part-00144
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b568fa9
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000146_362/part-00146
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000143_359' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000143
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000143_359: Committed
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@38a7896d
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@685cb6b7
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000149_365/part-00149
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000125_341' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000125
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1edfdde8
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO Executor: Finished task 143.0 in stage 8.0 (TID 359). 781 bytes result sent to driver
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000125_341: Committed
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 374, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO Executor: Running task 158.0 in stage 8.0 (TID 374)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 359) in 319 ms on localhost (143/200)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b03954b
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000145_361/part-00145
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@583e9566
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:45:00 INFO Executor: Finished task 125.0 in stage 8.0 (TID 341). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 375, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO Executor: Running task 159.0 in stage 8.0 (TID 375)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 341) in 737 ms on localhost (144/200)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7768386
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000148_364/part-00148
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42d6572d
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f56da3
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000152_368/part-00152
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000154_370/part-00154
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4360e483
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000153_369/part-00153
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e3a7704
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7621d673
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000151_367/part-00151
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@22a2690a
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@459dd379
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000150_366/part-00150
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@615c6e8d
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000144_360' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000144
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000144_360: Committed
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO Executor: Finished task 144.0 in stage 8.0 (TID 360). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 376, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 160.0 in stage 8.0 (TID 376)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4acd90d0
15/08/06 17:45:00 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 360) in 343 ms on localhost (145/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000147_363' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000147
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000147_363: Committed
15/08/06 17:45:00 INFO Executor: Finished task 147.0 in stage 8.0 (TID 363). 781 bytes result sent to driver
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,816
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c5845d0
15/08/06 17:45:00 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 377, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 161.0 in stage 8.0 (TID 377)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000146_362' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000146
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000146_362: Committed
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,076
15/08/06 17:45:00 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 363) in 335 ms on localhost (146/200)
15/08/06 17:45:00 INFO Executor: Finished task 146.0 in stage 8.0 (TID 362). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 378, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 162.0 in stage 8.0 (TID 378)
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 803B for [ps_partkey] INT32: 190 values, 767B raw, 767B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 551B for [ps_partkey] INT32: 127 values, 515B raw, 515B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,571B for [part_value] DOUBLE: 190 values, 1,527B raw, 1,527B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1278ec1d
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5b217939
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,536
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,067B for [part_value] DOUBLE: 127 values, 1,023B raw, 1,023B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 362) in 341 ms on localhost (147/200)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c53d600
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 495B for [ps_partkey] INT32: 113 values, 459B raw, 459B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 955B for [part_value] DOUBLE: 113 values, 911B raw, 911B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,056
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 799B for [ps_partkey] INT32: 189 values, 763B raw, 763B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,563B for [part_value] DOUBLE: 189 values, 1,519B raw, 1,519B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000148_364' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000148
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000148_364: Committed
15/08/06 17:45:00 INFO Executor: Finished task 148.0 in stage 8.0 (TID 364). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 379, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 163.0 in stage 8.0 (TID 379)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 364) in 339 ms on localhost (148/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000145_361' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000145
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000145_361: Committed
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@32db816e
15/08/06 17:45:00 INFO Executor: Finished task 145.0 in stage 8.0 (TID 361). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000156_372/part-00156
15/08/06 17:45:00 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 380, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 164.0 in stage 8.0 (TID 380)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 361) in 371 ms on localhost (149/200)
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000152_368' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000152
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000152_368: Committed
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000153_369' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000153
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000153_369: Committed
15/08/06 17:45:00 INFO Executor: Finished task 152.0 in stage 8.0 (TID 368). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 381, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 165.0 in stage 8.0 (TID 381)
15/08/06 17:45:00 INFO Executor: Finished task 153.0 in stage 8.0 (TID 369). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 368) in 324 ms on localhost (150/200)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 382, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:00 INFO Executor: Running task 166.0 in stage 8.0 (TID 382)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000151_367' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000151
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000151_367: Committed
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 369) in 314 ms on localhost (151/200)
15/08/06 17:45:00 INFO Executor: Finished task 151.0 in stage 8.0 (TID 367). 781 bytes result sent to driver
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@21aac2cb
15/08/06 17:45:00 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 383, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 167.0 in stage 8.0 (TID 383)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000155_371/part-00155
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c031072
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000150_366' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000150
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000150_366: Committed
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 367) in 333 ms on localhost (152/200)
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO Executor: Finished task 150.0 in stage 8.0 (TID 366). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 384, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO Executor: Running task 168.0 in stage 8.0 (TID 384)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 366) in 342 ms on localhost (153/200)
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2defac6e
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@67effa1c
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000157_373/part-00157
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000156_372' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000156
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000156_372: Committed
15/08/06 17:45:00 INFO Executor: Finished task 156.0 in stage 8.0 (TID 372). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 385, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 169.0 in stage 8.0 (TID 385)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 372) in 343 ms on localhost (154/200)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1dd44625
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000155_371' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000155
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000155_371: Committed
15/08/06 17:45:00 INFO Executor: Finished task 155.0 in stage 8.0 (TID 371). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 386, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 170.0 in stage 8.0 (TID 386)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 371) in 362 ms on localhost (155/200)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5be541ea
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000158_374/part-00158
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@469c0f20
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000159_375/part-00159
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2aeb8587
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000162_378/part-00162
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@29c21a2f
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000161_377/part-00161
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@52e1311
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@13e5c3bf
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000160_376/part-00160
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000157_373' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000157
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000157_373: Committed
15/08/06 17:45:00 INFO Executor: Finished task 157.0 in stage 8.0 (TID 373). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 387, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 171.0 in stage 8.0 (TID 387)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 373) in 336 ms on localhost (156/200)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a30f467
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@717b8264
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@63adad70
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,996
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,636
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,596
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1f55c730
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000163_379/part-00163
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 715B for [ps_partkey] INT32: 168 values, 679B raw, 679B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 587B for [ps_partkey] INT32: 136 values, 551B raw, 551B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,395B for [part_value] DOUBLE: 168 values, 1,351B raw, 1,351B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 507B for [ps_partkey] INT32: 116 values, 471B raw, 471B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 979B for [part_value] DOUBLE: 116 values, 935B raw, 935B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,139B for [part_value] DOUBLE: 136 values, 1,095B raw, 1,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@635722d9
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000159_375' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000159
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000159_375: Committed
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d7f7b1c
15/08/06 17:45:00 INFO Executor: Finished task 159.0 in stage 8.0 (TID 375). 781 bytes result sent to driver
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,476
15/08/06 17:45:00 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 388, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 172.0 in stage 8.0 (TID 388)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 375) in 177 ms on localhost (157/200)
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 483B for [ps_partkey] INT32: 110 values, 447B raw, 447B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1481cb6e
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000168_384/part-00168
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 931B for [part_value] DOUBLE: 110 values, 887B raw, 887B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@26a59da1
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000167_383/part-00167
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000158_374' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000158
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000158_374: Committed
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000162_378' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000162
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000162_378: Committed
15/08/06 17:45:00 INFO Executor: Finished task 158.0 in stage 8.0 (TID 374). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 389, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Finished task 162.0 in stage 8.0 (TID 378). 781 bytes result sent to driver
15/08/06 17:45:00 INFO Executor: Running task 173.0 in stage 8.0 (TID 389)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 374) in 341 ms on localhost (158/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 390, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 174.0 in stage 8.0 (TID 390)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@63f6210e
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@33f45167
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000166_382/part-00166
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000160_376' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000160
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000160_376: Committed
15/08/06 17:45:00 INFO Executor: Finished task 160.0 in stage 8.0 (TID 376). 781 bytes result sent to driver
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7186458b
15/08/06 17:45:00 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 378) in 152 ms on localhost (159/200)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000164_380/part-00164
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 391, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 175.0 in stage 8.0 (TID 391)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 376) in 173 ms on localhost (160/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000163_379' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000163
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000163_379: Committed
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d73bb63
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000165_381/part-00165
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO Executor: Finished task 163.0 in stage 8.0 (TID 379). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 392, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b5b086
15/08/06 17:45:00 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 379) in 155 ms on localhost (161/200)
15/08/06 17:45:00 INFO Executor: Running task 176.0 in stage 8.0 (TID 392)
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,556
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7a93b935
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,576
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 499B for [ps_partkey] INT32: 114 values, 463B raw, 463B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 963B for [part_value] DOUBLE: 114 values, 919B raw, 919B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 903B for [ps_partkey] INT32: 215 values, 867B raw, 867B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,771B for [part_value] DOUBLE: 215 values, 1,727B raw, 1,727B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bc88ac3
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000168_384' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000168
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000168_384: Committed
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:45:00 INFO Executor: Finished task 168.0 in stage 8.0 (TID 384). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 393, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 177.0 in stage 8.0 (TID 393)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 384) in 241 ms on localhost (162/200)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7163bb3e
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3272cf71
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000170_386/part-00170
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e8fba3
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000169_385/part-00169
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fda244d
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000171_387/part-00171
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000167_383' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000167
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000167_383: Committed
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO Executor: Finished task 167.0 in stage 8.0 (TID 383). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 394, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 178.0 in stage 8.0 (TID 394)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 383) in 278 ms on localhost (163/200)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34c4e8c8
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,996
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 787B for [ps_partkey] INT32: 186 values, 751B raw, 751B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,539B for [part_value] DOUBLE: 186 values, 1,495B raw, 1,495B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000166_382' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000166
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000166_382: Committed
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@597460d9
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/06 17:45:00 INFO Executor: Finished task 166.0 in stage 8.0 (TID 382). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000164_380' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000164
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000164_380: Committed
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 395, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Finished task 164.0 in stage 8.0 (TID 380). 781 bytes result sent to driver
15/08/06 17:45:00 INFO Executor: Running task 179.0 in stage 8.0 (TID 395)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 382) in 297 ms on localhost (164/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 396, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 180.0 in stage 8.0 (TID 396)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@b697a5
15/08/06 17:45:00 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 380) in 308 ms on localhost (165/200)
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000165_381' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000165
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000165_381: Committed
15/08/06 17:45:00 INFO Executor: Finished task 165.0 in stage 8.0 (TID 381). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 397, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 181.0 in stage 8.0 (TID 397)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 381) in 314 ms on localhost (166/200)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a6266b8
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000172_388/part-00172
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000170_386' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000170
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@180cef6a
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000170_386: Committed
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000173_389/part-00173
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO Executor: Finished task 170.0 in stage 8.0 (TID 386). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 398, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 182.0 in stage 8.0 (TID 398)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 386) in 283 ms on localhost (167/200)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2765ff0c
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000176_392/part-00176
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c0482d8
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000174_390/part-00174
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a228f42
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000175_391/part-00175
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@118bbd46
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7a6b43c1
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,596
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 707B for [ps_partkey] INT32: 166 values, 671B raw, 671B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,379B for [part_value] DOUBLE: 166 values, 1,335B raw, 1,335B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1062fb97
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@60bea6b8
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2fca635a
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,756
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,216
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 939B for [ps_partkey] INT32: 224 values, 903B raw, 903B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 831B for [ps_partkey] INT32: 197 values, 795B raw, 795B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,843B for [part_value] DOUBLE: 224 values, 1,799B raw, 1,799B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,627B for [part_value] DOUBLE: 197 values, 1,583B raw, 1,583B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61130377
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000177_393/part-00177
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000169_385' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000169
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000169_385: Committed
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000171_387' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000171
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000171_387: Committed
15/08/06 17:45:00 INFO Executor: Finished task 169.0 in stage 8.0 (TID 385). 781 bytes result sent to driver
15/08/06 17:45:00 INFO Executor: Finished task 171.0 in stage 8.0 (TID 387). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 399, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 183.0 in stage 8.0 (TID 399)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 385) in 329 ms on localhost (168/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 400, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 184.0 in stage 8.0 (TID 400)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 387) in 294 ms on localhost (169/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000172_388' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000172
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000172_388: Committed
15/08/06 17:45:00 INFO Executor: Finished task 172.0 in stage 8.0 (TID 388). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 401, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 185.0 in stage 8.0 (TID 401)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000173_389' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000173
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000173_389: Committed
15/08/06 17:45:00 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 388) in 274 ms on localhost (170/200)
15/08/06 17:45:00 INFO Executor: Finished task 173.0 in stage 8.0 (TID 389). 781 bytes result sent to driver
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000175_391' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000175
15/08/06 17:45:00 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 402, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000175_391: Committed
15/08/06 17:45:00 INFO Executor: Running task 186.0 in stage 8.0 (TID 402)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 389) in 264 ms on localhost (171/200)
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35fd34b5
15/08/06 17:45:00 INFO Executor: Finished task 175.0 in stage 8.0 (TID 391). 781 bytes result sent to driver
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,996
15/08/06 17:45:00 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 403, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 187.0 in stage 8.0 (TID 403)
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 787B for [ps_partkey] INT32: 186 values, 751B raw, 751B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,539B for [part_value] DOUBLE: 186 values, 1,495B raw, 1,495B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 391) in 253 ms on localhost (172/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000174_390' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000174
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000174_390: Committed
15/08/06 17:45:00 INFO Executor: Finished task 174.0 in stage 8.0 (TID 390). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 404, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 188.0 in stage 8.0 (TID 404)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 390) in 270 ms on localhost (173/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000149_365' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000149
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000149_365: Committed
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000176_392' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000176
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000176_392: Committed
15/08/06 17:45:00 INFO Executor: Finished task 149.0 in stage 8.0 (TID 365). 781 bytes result sent to driver
15/08/06 17:45:00 INFO Executor: Finished task 176.0 in stage 8.0 (TID 392). 781 bytes result sent to driver
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000154_370' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000154
15/08/06 17:45:00 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 405, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000154_370: Committed
15/08/06 17:45:00 INFO Executor: Finished task 154.0 in stage 8.0 (TID 370). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 365) in 746 ms on localhost (174/200)
15/08/06 17:45:00 INFO Executor: Running task 189.0 in stage 8.0 (TID 405)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 406, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 190.0 in stage 8.0 (TID 406)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 392) in 256 ms on localhost (175/200)
15/08/06 17:45:00 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 407, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 191.0 in stage 8.0 (TID 407)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 370) in 711 ms on localhost (176/200)
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000177_393' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000177
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000177_393: Committed
15/08/06 17:45:00 INFO Executor: Finished task 177.0 in stage 8.0 (TID 393). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 408, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO Executor: Running task 192.0 in stage 8.0 (TID 408)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 393) in 169 ms on localhost (177/200)
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@165fbf0
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000179_395/part-00179
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e736cbe
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000178_394/part-00178
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@91dfb35
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@309d4f5b
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@27c4f84c
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000181_397/part-00181
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000180_396/part-00180
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2df94433
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@47ec87d9
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,462,196
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,796
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,027B for [ps_partkey] INT32: 246 values, 991B raw, 991B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 747B for [ps_partkey] INT32: 176 values, 711B raw, 711B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,459B for [part_value] DOUBLE: 176 values, 1,415B raw, 1,415B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 2,019B for [part_value] DOUBLE: 246 values, 1,975B raw, 1,975B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000179_395' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000179
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000179_395: Committed
15/08/06 17:45:00 INFO Executor: Finished task 179.0 in stage 8.0 (TID 395). 781 bytes result sent to driver
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@19999add
15/08/06 17:45:00 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ecce801
15/08/06 17:45:00 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 409, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/06 17:45:00 INFO Executor: Running task 193.0 in stage 8.0 (TID 409)
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:00 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 395) in 173 ms on localhost (178/200)
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:00 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000182_398/part-00182
15/08/06 17:45:00 INFO CodecConfig: Compression set to false
15/08/06 17:45:00 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:00 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:00 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000181_397' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000181
15/08/06 17:45:00 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000181_397: Committed
15/08/06 17:45:00 INFO Executor: Finished task 181.0 in stage 8.0 (TID 397). 781 bytes result sent to driver
15/08/06 17:45:00 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 410, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:00 INFO Executor: Running task 194.0 in stage 8.0 (TID 410)
15/08/06 17:45:00 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 397) in 171 ms on localhost (179/200)
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@39b5a00b
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000180_396' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000180
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000180_396: Committed
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:01 INFO Executor: Finished task 180.0 in stage 8.0 (TID 396). 781 bytes result sent to driver
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:01 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 411, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:01 INFO Executor: Running task 195.0 in stage 8.0 (TID 411)
15/08/06 17:45:01 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 396) in 204 ms on localhost (180/200)
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@26186b9f
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000161_377' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000161
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000161_377: Committed
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000183_399/part-00183
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@d41a743
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000184_400/part-00184
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO Executor: Finished task 161.0 in stage 8.0 (TID 377). 781 bytes result sent to driver
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@557e5f72
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000185_401/part-00185
15/08/06 17:45:01 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 412, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO Executor: Running task 196.0 in stage 8.0 (TID 412)
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 377) in 554 ms on localhost (181/200)
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@237186d0
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000186_402/part-00186
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000182_398' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000182
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000182_398: Committed
15/08/06 17:45:01 INFO Executor: Finished task 182.0 in stage 8.0 (TID 398). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 413, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:01 INFO Executor: Running task 197.0 in stage 8.0 (TID 413)
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:01 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 398) in 193 ms on localhost (182/200)
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@641f6a1e
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@b319bc2
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42bcf59e
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@480cf8d6
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000187_403/part-00187
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@25fb7cb8
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@21b92783
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000189_405/part-00189
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5dc0477f
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000188_404/part-00188
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1a7576a0
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000184_400' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000184
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000184_400: Committed
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2a2b4b2b
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5409ae
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,156
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/06 17:45:01 INFO Executor: Finished task 184.0 in stage 8.0 (TID 400). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 414, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO Executor: Running task 198.0 in stage 8.0 (TID 414)
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 819B for [ps_partkey] INT32: 194 values, 783B raw, 783B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,603B for [part_value] DOUBLE: 194 values, 1,559B raw, 1,559B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 400) in 374 ms on localhost (183/200)
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@28b0a2ea
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000191_407/part-00191
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000186_402' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000186
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000186_402: Committed
15/08/06 17:45:01 INFO Executor: Finished task 186.0 in stage 8.0 (TID 402). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 415, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@15a2f11f
15/08/06 17:45:01 INFO Executor: Running task 199.0 in stage 8.0 (TID 415)
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000190_406/part-00190
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 402) in 380 ms on localhost (184/200)
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b9084f6
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000192_408/part-00192
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2dc37fd
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@33f1e08d
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000189_405' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000189
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000189_405: Committed
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000193_409/part-00193
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO Executor: Finished task 189.0 in stage 8.0 (TID 405). 781 bytes result sent to driver
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 405) in 381 ms on localhost (185/200)
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000187_403' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000187
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000187_403: Committed
15/08/06 17:45:01 INFO Executor: Finished task 187.0 in stage 8.0 (TID 403). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 403) in 396 ms on localhost (186/200)
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@48d520af
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,136
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2054e684
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 615B for [ps_partkey] INT32: 143 values, 579B raw, 579B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,195B for [part_value] DOUBLE: 143 values, 1,151B raw, 1,151B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000191_407' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000191
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000191_407: Committed
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5214c3de
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000195_411/part-00195
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e0ea6e
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO Executor: Finished task 191.0 in stage 8.0 (TID 407). 781 bytes result sent to driver
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 407) in 399 ms on localhost (187/200)
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8462259
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000194_410/part-00194
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000192_408' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000192
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000192_408: Committed
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@ddb8be9
15/08/06 17:45:01 INFO Executor: Finished task 192.0 in stage 8.0 (TID 408). 781 bytes result sent to driver
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,276
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000190_406' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000190
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000190_406: Committed
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/06 17:45:01 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 408) in 397 ms on localhost (188/200)
15/08/06 17:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 643B for [ps_partkey] INT32: 150 values, 607B raw, 607B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,251B for [part_value] DOUBLE: 150 values, 1,207B raw, 1,207B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO Executor: Finished task 190.0 in stage 8.0 (TID 406). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 406) in 414 ms on localhost (189/200)
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@edeff75
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,896
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 567B for [ps_partkey] INT32: 131 values, 531B raw, 531B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,099B for [part_value] DOUBLE: 131 values, 1,055B raw, 1,055B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000193_409' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000193
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000193_409: Committed
15/08/06 17:45:01 INFO Executor: Finished task 193.0 in stage 8.0 (TID 409). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 409) in 358 ms on localhost (190/200)
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000195_411' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000195
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000195_411: Committed
15/08/06 17:45:01 INFO Executor: Finished task 195.0 in stage 8.0 (TID 411). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 411) in 336 ms on localhost (191/200)
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000194_410' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000194
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000194_410: Committed
15/08/06 17:45:01 INFO Executor: Finished task 194.0 in stage 8.0 (TID 410). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 410) in 360 ms on localhost (192/200)
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@65f1d3e7
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@678f16fa
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000197_413/part-00197
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000196_412/part-00196
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@651c9e67
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@39e3b00d
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@543a16f3
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000198_414/part-00198
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000196_412' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000196
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000196_412: Committed
15/08/06 17:45:01 INFO Executor: Finished task 196.0 in stage 8.0 (TID 412). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 412) in 371 ms on localhost (193/200)
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000178_394' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000178
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000178_394: Committed
15/08/06 17:45:01 INFO Executor: Finished task 178.0 in stage 8.0 (TID 394). 781 bytes result sent to driver
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@74c6985c
15/08/06 17:45:01 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a3cfbe9
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/_temporary/attempt_201508061744_0008_m_000199_415/part-00199
15/08/06 17:45:01 INFO CodecConfig: Compression set to false
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,776
15/08/06 17:45:01 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:01 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:01 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:01 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 394) in 620 ms on localhost (194/200)
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 743B for [ps_partkey] INT32: 175 values, 707B raw, 707B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,451B for [part_value] DOUBLE: 175 values, 1,407B raw, 1,407B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@63387b2
15/08/06 17:45:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000198_414' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000198
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000198_414: Committed
15/08/06 17:45:01 INFO Executor: Finished task 198.0 in stage 8.0 (TID 414). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 414) in 177 ms on localhost (195/200)
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000199_415' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000199
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000199_415: Committed
15/08/06 17:45:01 INFO Executor: Finished task 199.0 in stage 8.0 (TID 415). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 415) in 170 ms on localhost (196/200)
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000183_399' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000183
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000183_399: Committed
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000185_401' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000185
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000185_401: Committed
15/08/06 17:45:01 INFO Executor: Finished task 183.0 in stage 8.0 (TID 399). 781 bytes result sent to driver
15/08/06 17:45:01 INFO Executor: Finished task 185.0 in stage 8.0 (TID 401). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 399) in 780 ms on localhost (197/200)
15/08/06 17:45:01 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 401) in 772 ms on localhost (198/200)
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000188_404' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000188
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000188_404: Committed
15/08/06 17:45:01 INFO Executor: Finished task 188.0 in stage 8.0 (TID 404). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 404) in 780 ms on localhost (199/200)
15/08/06 17:45:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508061744_0008_m_000197_413' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_temporary/0/task_201508061744_0008_m_000197
15/08/06 17:45:01 INFO SparkHiveWriterContainer: attempt_201508061744_0008_m_000197_413: Committed
15/08/06 17:45:01 INFO Executor: Finished task 197.0 in stage 8.0 (TID 413). 781 bytes result sent to driver
15/08/06 17:45:01 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 413) in 765 ms on localhost (200/200)
15/08/06 17:45:01 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/06 17:45:01 INFO DAGScheduler: Stage 8 (runJob at InsertIntoHiveTable.scala:93) finished in 6.088 s
15/08/06 17:45:01 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@17c96394
15/08/06 17:45:01 INFO DAGScheduler: Job 5 finished: runJob at InsertIntoHiveTable.scala:93, took 16.918414 s
15/08/06 17:45:01 INFO StatsReportListener: task runtime:(count: 200, mean: 458.950000, stdev: 305.024634, max: 1355.000000, min: 152.000000)
15/08/06 17:45:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:01 INFO StatsReportListener: 	152.0 ms	177.0 ms	206.0 ms	294.0 ms	346.0 ms	499.0 ms	864.0 ms	1.3 s	1.4 s
15/08/06 17:45:01 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.865000, stdev: 1.409530, max: 9.000000, min: 0.000000)
15/08/06 17:45:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:01 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	4.0 ms	9.0 ms
15/08/06 17:45:01 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:45:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:01 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:45:01 INFO StatsReportListener: task result size:(count: 200, mean: 781.000000, stdev: 0.000000, max: 781.000000, min: 781.000000)
15/08/06 17:45:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:01 INFO StatsReportListener: 	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B
15/08/06 17:45:01 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.041063, stdev: 5.916425, max: 99.695122, min: 54.231975)
15/08/06 17:45:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:01 INFO StatsReportListener: 	54 %	94 %	96 %	98 %	99 %	99 %	99 %	99 %	100 %
15/08/06 17:45:01 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.211608, stdev: 0.329540, max: 1.895735, min: 0.000000)
15/08/06 17:45:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:01 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 2 %
15/08/06 17:45:01 INFO StatsReportListener: other time pct: (count: 200, mean: 2.747329, stdev: 5.906184, max: 45.768025, min: 0.304878)
15/08/06 17:45:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:01 INFO StatsReportListener: 	 0 %	 1 %	 1 %	 1 %	 1 %	 2 %	 4 %	14 %	46 %
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/_SUCCESS;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/_SUCCESS;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00000;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00001;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00002;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00003;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00004;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00005;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00006;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00007;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00008;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00009;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00010;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00011;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00012;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00013;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00014;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00015;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00016;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00017;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00018;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00019;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00020;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00021;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00022;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00023;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00024;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00025;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00026;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00027;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00028;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00029;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00030;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00031;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00032;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00033;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00034;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00035;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00036;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00037;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00038;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00039;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00040;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00041;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00042;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00043;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00044;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00045;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00046;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00047;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00048;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00049;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00050;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00051;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00052;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00053;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00054;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00055;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00056;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00057;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00058;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00059;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00060;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00061;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00062;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00063;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00064;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00065;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00066;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00067;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00068;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00069;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00070;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00071;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00072;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00073;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00074;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00075;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00076;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00077;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00078;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00079;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00080;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00081;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00082;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00083;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00084;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00085;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00086;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00087;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00088;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00089;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00090;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00091;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00092;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00093;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00094;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00095;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00096;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00097;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00098;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00099;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00100;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00101;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00102;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00103;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00104;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00105;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00106;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00107;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00108;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00109;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00110;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00111;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00112;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00113;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00114;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00115;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00116;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00117;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00118;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00119;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00120;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00121;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00122;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00123;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00124;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00125;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00126;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00127;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00128;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00129;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00130;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00131;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00132;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00133;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00134;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00135;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00136;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00137;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00138;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00139;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00140;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00141;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00142;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00143;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00144;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00145;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00146;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00147;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00148;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00149;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00150;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00151;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00152;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00153;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00154;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00155;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00156;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00157;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00158;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00159;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00160;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00161;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00162;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00163;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00164;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00165;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00166;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00167;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00168;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00169;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00170;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00171;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00172;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00173;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00174;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00175;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00176;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00177;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00178;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00179;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00180;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00181;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00182;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00183;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00184;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00185;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00186;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00187;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00188;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00189;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00190;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00191;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00192;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00193;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00194;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00195;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00196;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00197;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00198;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198;Status:true
15/08/06 17:45:03 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-44-43_315_1015622453070685964-1/-ext-10000/part-00199;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199;Status:true
15/08/06 17:45:03 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:45:03 INFO DAGScheduler: Got job 6 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:45:03 INFO DAGScheduler: Final stage: Stage 9(collect at SparkPlan.scala:84)
15/08/06 17:45:03 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:45:03 INFO DAGScheduler: Missing parents: List()
15/08/06 17:45:03 INFO DAGScheduler: Submitting Stage 9 (MappedRDD[50] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:45:03 INFO MemoryStore: ensureFreeSpace(3240) called with curMem=962669, maxMem=3333968363
15/08/06 17:45:03 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 3.2 KB, free 3.1 GB)
15/08/06 17:45:03 INFO MemoryStore: ensureFreeSpace(1941) called with curMem=965909, maxMem=3333968363
15/08/06 17:45:03 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1941.0 B, free 3.1 GB)
15/08/06 17:45:03 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:42907 (size: 1941.0 B, free: 3.1 GB)
15/08/06 17:45:03 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/08/06 17:45:03 INFO DefaultExecutionContext: Created broadcast 13 from broadcast at DAGScheduler.scala:838
15/08/06 17:45:03 INFO DAGScheduler: Submitting 1 missing tasks from Stage 9 (MappedRDD[50] at map at SparkPlan.scala:84)
15/08/06 17:45:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/08/06 17:45:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 416, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:45:03 INFO Executor: Running task 0.0 in stage 9.0 (TID 416)
15/08/06 17:45:03 INFO Executor: Finished task 0.0 in stage 9.0 (TID 416). 618 bytes result sent to driver
15/08/06 17:45:03 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 416) in 6 ms on localhost (1/1)
15/08/06 17:45:03 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/06 17:45:03 INFO DAGScheduler: Stage 9 (collect at SparkPlan.scala:84) finished in 0.007 s
15/08/06 17:45:03 INFO DAGScheduler: Job 6 finished: collect at SparkPlan.scala:84, took 0.023636 s
15/08/06 17:45:03 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4e2853a
Time taken: 22.079 seconds
15/08/06 17:45:03 INFO CliDriver: Time taken: 22.079 seconds
15/08/06 17:45:03 INFO StatsReportListener: task runtime:(count: 1, mean: 6.000000, stdev: 0.000000, max: 6.000000, min: 6.000000)
15/08/06 17:45:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:03 INFO StatsReportListener: 	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms
15/08/06 17:45:03 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:45:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:03 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:45:03 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 33.333333, stdev: 0.000000, max: 33.333333, min: 33.333333)
15/08/06 17:45:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:03 INFO StatsReportListener: 	33 %	33 %	33 %	33 %	33 %	33 %	33 %	33 %	33 %
15/08/06 17:45:03 INFO StatsReportListener: other time pct: (count: 1, mean: 66.666667, stdev: 0.000000, max: 66.666667, min: 66.666667)
15/08/06 17:45:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:03 INFO StatsReportListener: 	67 %	67 %	67 %	67 %	67 %	67 %	67 %	67 %	67 %
15/08/06 17:45:03 INFO ParseDriver: Parsing command: insert into table q11_important_stock_par_spark
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par_spark) sum_tmp
        join q11_part_tmp_par_spark
where part_value > total_value * 0.0001
order by value desc
15/08/06 17:45:03 INFO ParseDriver: Parse Completed
15/08/06 17:45:03 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#114, part_value#115)
15/08/06 17:45:04 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#118, part_value#119)
15/08/06 17:45:04 INFO MemoryStore: ensureFreeSpace(280818) called with curMem=967850, maxMem=3333968363
15/08/06 17:45:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 274.2 KB, free 3.1 GB)
15/08/06 17:45:04 INFO MemoryStore: ensureFreeSpace(31760) called with curMem=1248668, maxMem=3333968363
15/08/06 17:45:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 31.0 KB, free 3.1 GB)
15/08/06 17:45:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:42907 (size: 31.0 KB, free: 3.1 GB)
15/08/06 17:45:04 INFO BlockManagerMaster: Updated info of block broadcast_14_piece0
15/08/06 17:45:04 INFO DefaultExecutionContext: Created broadcast 14 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:45:04 INFO MemoryStore: ensureFreeSpace(280962) called with curMem=1280428, maxMem=3333968363
15/08/06 17:45:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 274.4 KB, free 3.1 GB)
15/08/06 17:45:04 INFO MemoryStore: ensureFreeSpace(31842) called with curMem=1561390, maxMem=3333968363
15/08/06 17:45:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/06 17:45:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:42907 (size: 31.1 KB, free: 3.1 GB)
15/08/06 17:45:04 INFO BlockManagerMaster: Updated info of block broadcast_15_piece0
15/08/06 17:45:04 INFO DefaultExecutionContext: Created broadcast 15 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/06 17:45:04 INFO FileInputFormat: Total input paths to process : 200
15/08/06 17:45:04 INFO ParquetInputFormat: Total input paths to process : 200
15/08/06 17:45:04 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:45:04 INFO ParquetFileReader: reading another 200 footers
15/08/06 17:45:04 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/06 17:45:04 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000; isDirectory=false; length=2638; replication=1; blocksize=134217728; modification_time=1438883097005; access_time=1438883096284; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001; isDirectory=false; length=2326; replication=1; blocksize=134217728; modification_time=1438883097003; access_time=1438883096290; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438883097002; access_time=1438883096307; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883097003; access_time=1438883096292; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004; isDirectory=false; length=2194; replication=1; blocksize=134217728; modification_time=1438883097008; access_time=1438883096286; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005; isDirectory=false; length=2446; replication=1; blocksize=134217728; modification_time=1438883097006; access_time=1438883096276; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006; isDirectory=false; length=1858; replication=1; blocksize=134217728; modification_time=1438883097002; access_time=1438883096272; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883097003; access_time=1438883096276; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883097001; access_time=1438883096316; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009; isDirectory=false; length=2050; replication=1; blocksize=134217728; modification_time=1438883097009; access_time=1438883096272; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010; isDirectory=false; length=2002; replication=1; blocksize=134217728; modification_time=1438883097008; access_time=1438883096280; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883097006; access_time=1438883096276; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012; isDirectory=false; length=2038; replication=1; blocksize=134217728; modification_time=1438883097008; access_time=1438883096270; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013; isDirectory=false; length=2170; replication=1; blocksize=134217728; modification_time=1438883097008; access_time=1438883096289; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438883097002; access_time=1438883096289; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883097008; access_time=1438883096273; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438883097693; access_time=1438883097622; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438883097590; access_time=1438883097523; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438883097544; access_time=1438883097474; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883097664; access_time=1438883097590; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438883097778; access_time=1438883097695; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021; isDirectory=false; length=2122; replication=1; blocksize=134217728; modification_time=1438883097786; access_time=1438883097713; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022; isDirectory=false; length=2170; replication=1; blocksize=134217728; modification_time=1438883097785; access_time=1438883097696; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438883097623; access_time=1438883097554; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024; isDirectory=false; length=2470; replication=1; blocksize=134217728; modification_time=1438883097791; access_time=1438883097708; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438883097756; access_time=1438883097693; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438883097663; access_time=1438883097587; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438883097753; access_time=1438883097674; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028; isDirectory=false; length=2134; replication=1; blocksize=134217728; modification_time=1438883097779; access_time=1438883097694; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438883097706; access_time=1438883097632; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438883097728; access_time=1438883097664; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031; isDirectory=false; length=1378; replication=1; blocksize=134217728; modification_time=1438883097749; access_time=1438883097676; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032; isDirectory=false; length=2302; replication=1; blocksize=134217728; modification_time=1438883098080; access_time=1438883097998; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438883098110; access_time=1438883098037; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034; isDirectory=false; length=2626; replication=1; blocksize=134217728; modification_time=1438883098084; access_time=1438883098005; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883098485; access_time=1438883098023; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438883098158; access_time=1438883098084; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883098170; access_time=1438883098114; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883098295; access_time=1438883098213; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438883098213; access_time=1438883098166; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438883098203; access_time=1438883098110; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041; isDirectory=false; length=1738; replication=1; blocksize=134217728; modification_time=1438883098243; access_time=1438883098181; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042; isDirectory=false; length=1762; replication=1; blocksize=134217728; modification_time=1438883098228; access_time=1438883098170; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438883098256; access_time=1438883098211; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044; isDirectory=false; length=1846; replication=1; blocksize=134217728; modification_time=1438883098260; access_time=1438883098190; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438883098689; access_time=1438883098236; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438883098272; access_time=1438883098212; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438883098259; access_time=1438883098209; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438883098465; access_time=1438883098339; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049; isDirectory=false; length=1462; replication=1; blocksize=134217728; modification_time=1438883098424; access_time=1438883098298; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883098462; access_time=1438883098336; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1438883098526; access_time=1438883098492; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438883098587; access_time=1438883098521; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053; isDirectory=false; length=1738; replication=1; blocksize=134217728; modification_time=1438883098614; access_time=1438883098557; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438883098608; access_time=1438883098557; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055; isDirectory=false; length=1558; replication=1; blocksize=134217728; modification_time=1438883098597; access_time=1438883098560; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883098614; access_time=1438883098549; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883098593; access_time=1438883098540; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058; isDirectory=false; length=1666; replication=1; blocksize=134217728; modification_time=1438883098618; access_time=1438883098563; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438883098617; access_time=1438883098562; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060; isDirectory=false; length=2758; replication=1; blocksize=134217728; modification_time=1438883098623; access_time=1438883098559; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438883098633; access_time=1438883098573; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883098676; access_time=1438883098614; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063; isDirectory=false; length=1990; replication=1; blocksize=134217728; modification_time=1438883098736; access_time=1438883098654; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438883098748; access_time=1438883098656; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438883098749; access_time=1438883098682; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066; isDirectory=false; length=2446; replication=1; blocksize=134217728; modification_time=1438883098862; access_time=1438883098724; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883098877; access_time=1438883098779; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068; isDirectory=false; length=1786; replication=1; blocksize=134217728; modification_time=1438883098879; access_time=1438883098769; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438883098944; access_time=1438883098861; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438883098947; access_time=1438883098868; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438883098922; access_time=1438883098861; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883098948; access_time=1438883098892; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1438883098947; access_time=1438883098868; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074; isDirectory=false; length=2098; replication=1; blocksize=134217728; modification_time=1438883098925; access_time=1438883098858; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438883098943; access_time=1438883098877; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438883098950; access_time=1438883098909; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438883098965; access_time=1438883098923; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078; isDirectory=false; length=2614; replication=1; blocksize=134217728; modification_time=1438883098966; access_time=1438883098917; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883099062; access_time=1438883098997; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438883099062; access_time=1438883098994; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883099068; access_time=1438883099000; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438883099069; access_time=1438883099040; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438883099096; access_time=1438883099061; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438883099088; access_time=1438883099047; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883099285; access_time=1438883099234; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438883099235; access_time=1438883099111; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087; isDirectory=false; length=2146; replication=1; blocksize=134217728; modification_time=1438883099318; access_time=1438883099242; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088; isDirectory=false; length=2722; replication=1; blocksize=134217728; modification_time=1438883099281; access_time=1438883099233; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438883099286; access_time=1438883099241; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090; isDirectory=false; length=2398; replication=1; blocksize=134217728; modification_time=1438883099310; access_time=1438883099247; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091; isDirectory=false; length=1750; replication=1; blocksize=134217728; modification_time=1438883099271; access_time=1438883099205; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883099285; access_time=1438883099233; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438883099275; access_time=1438883099221; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438883099309; access_time=1438883099245; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883099380; access_time=1438883099321; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096; isDirectory=false; length=1690; replication=1; blocksize=134217728; modification_time=1438883099371; access_time=1438883099318; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097; isDirectory=false; length=2194; replication=1; blocksize=134217728; modification_time=1438883099379; access_time=1438883099332; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438883099354; access_time=1438883099295; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099; isDirectory=false; length=2266; replication=1; blocksize=134217728; modification_time=1438883099757; access_time=1438883099314; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883099370; access_time=1438883099323; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101; isDirectory=false; length=2410; replication=1; blocksize=134217728; modification_time=1438883099536; access_time=1438883099455; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883100092; access_time=1438883099537; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883099691; access_time=1438883099653; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104; isDirectory=false; length=2434; replication=1; blocksize=134217728; modification_time=1438883099539; access_time=1438883099443; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1438883099683; access_time=1438883099539; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438883099650; access_time=1438883099495; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107; isDirectory=false; length=2326; replication=1; blocksize=134217728; modification_time=1438883099664; access_time=1438883099514; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438883099673; access_time=1438883099525; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883099652; access_time=1438883099480; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883099690; access_time=1438883099540; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883099675; access_time=1438883099537; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438883099708; access_time=1438883099652; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1438883099689; access_time=1438883099649; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1438883099735; access_time=1438883099701; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115; isDirectory=false; length=1858; replication=1; blocksize=134217728; modification_time=1438883100127; access_time=1438883099690; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438883099856; access_time=1438883099817; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117; isDirectory=false; length=2590; replication=1; blocksize=134217728; modification_time=1438883099856; access_time=1438883099823; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883099885; access_time=1438883099845; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1438883099866; access_time=1438883099829; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120; isDirectory=false; length=2626; replication=1; blocksize=134217728; modification_time=1438883099857; access_time=1438883099816; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1438883099862; access_time=1438883099833; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122; isDirectory=false; length=2422; replication=1; blocksize=134217728; modification_time=1438883099895; access_time=1438883099861; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123; isDirectory=false; length=2482; replication=1; blocksize=134217728; modification_time=1438883099893; access_time=1438883099847; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124; isDirectory=false; length=2098; replication=1; blocksize=134217728; modification_time=1438883099900; access_time=1438883099870; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125; isDirectory=false; length=1798; replication=1; blocksize=134217728; modification_time=1438883100279; access_time=1438883099838; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126; isDirectory=false; length=2350; replication=1; blocksize=134217728; modification_time=1438883099903; access_time=1438883099868; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883100007; access_time=1438883099884; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438883100021; access_time=1438883099905; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438883100021; access_time=1438883099904; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883100130; access_time=1438883100076; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883100173; access_time=1438883100131; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438883100146; access_time=1438883100092; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883100117; access_time=1438883100078; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134; isDirectory=false; length=1990; replication=1; blocksize=134217728; modification_time=1438883100120; access_time=1438883100084; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883100178; access_time=1438883100137; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438883100146; access_time=1438883100114; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137; isDirectory=false; length=1798; replication=1; blocksize=134217728; modification_time=1438883100158; access_time=1438883100119; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138; isDirectory=false; length=1678; replication=1; blocksize=134217728; modification_time=1438883100163; access_time=1438883100128; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1438883100179; access_time=1438883100134; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1438883100191; access_time=1438883100160; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1438883100192; access_time=1438883100159; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142; isDirectory=false; length=2350; replication=1; blocksize=134217728; modification_time=1438883100240; access_time=1438883100201; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143; isDirectory=false; length=1558; replication=1; blocksize=134217728; modification_time=1438883100277; access_time=1438883100242; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883100458; access_time=1438883100267; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438883100494; access_time=1438883100444; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438883100465; access_time=1438883100289; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883100465; access_time=1438883100265; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883100490; access_time=1438883100459; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1438883100891; access_time=1438883100443; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1438883100504; access_time=1438883100469; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151; isDirectory=false; length=2602; replication=1; blocksize=134217728; modification_time=1438883100501; access_time=1438883100468; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152; isDirectory=false; length=1678; replication=1; blocksize=134217728; modification_time=1438883100498; access_time=1438883100466; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153; isDirectory=false; length=1846; replication=1; blocksize=134217728; modification_time=1438883100501; access_time=1438883100466; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154; isDirectory=false; length=2590; replication=1; blocksize=134217728; modification_time=1438883100900; access_time=1438883100468; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1438883100553; access_time=1438883100522; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883100545; access_time=1438883100506; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1438883100579; access_time=1438883100545; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158; isDirectory=false; length=2338; replication=1; blocksize=134217728; modification_time=1438883100621; access_time=1438883100576; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1438883100605; access_time=1438883100577; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438883100627; access_time=1438883100594; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161; isDirectory=false; length=1714; replication=1; blocksize=134217728; modification_time=1438883101022; access_time=1438883100589; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162; isDirectory=false; length=1954; replication=1; blocksize=134217728; modification_time=1438883100622; access_time=1438883100588; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163; isDirectory=false; length=1642; replication=1; blocksize=134217728; modification_time=1438883100645; access_time=1438883100606; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883100800; access_time=1438883100646; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1438883100800; access_time=1438883100656; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166; isDirectory=false; length=1690; replication=1; blocksize=134217728; modification_time=1438883100798; access_time=1438883100644; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167; isDirectory=false; length=2902; replication=1; blocksize=134217728; modification_time=1438883100778; access_time=1438883100645; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1438883100654; access_time=1438883100625; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1438883100873; access_time=1438883100789; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170; isDirectory=false; length=2554; replication=1; blocksize=134217728; modification_time=1438883100837; access_time=1438883100779; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1438883100874; access_time=1438883100792; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1438883100882; access_time=1438883100838; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173; isDirectory=false; length=2314; replication=1; blocksize=134217728; modification_time=1438883100885; access_time=1438883100853; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174; isDirectory=false; length=3010; replication=1; blocksize=134217728; modification_time=1438883100888; access_time=1438883100858; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1438883100887; access_time=1438883100862; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176; isDirectory=false; length=2686; replication=1; blocksize=134217728; modification_time=1438883100897; access_time=1438883100860; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177; isDirectory=false; length=2554; replication=1; blocksize=134217728; modification_time=1438883100917; access_time=1438883100883; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178; isDirectory=false; length=3274; replication=1; blocksize=134217728; modification_time=1438883101397; access_time=1438883100942; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883100965; access_time=1438883100931; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1438883100999; access_time=1438883100960; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181; isDirectory=false; length=2434; replication=1; blocksize=134217728; modification_time=1438883100984; access_time=1438883100959; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1438883101030; access_time=1438883100988; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1438883101647; access_time=1438883101035; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438883101245; access_time=1438883101033; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1438883101647; access_time=1438883101036; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1438883101259; access_time=1438883101040; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187; isDirectory=false; length=2650; replication=1; blocksize=134217728; modification_time=1438883101275; access_time=1438883101242; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1438883101674; access_time=1438883101246; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1438883101272; access_time=1438883101244; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1438883101315; access_time=1438883101279; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883101299; access_time=1438883101268; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192; isDirectory=false; length=2038; replication=1; blocksize=134217728; modification_time=1438883101313; access_time=1438883101285; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1438883101329; access_time=1438883101289; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194; isDirectory=false; length=1894; replication=1; blocksize=134217728; modification_time=1438883101344; access_time=1438883101318; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195; isDirectory=false; length=2122; replication=1; blocksize=134217728; modification_time=1438883101337; access_time=1438883101311; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1438883101389; access_time=1438883101362; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1438883101791; access_time=1438883101362; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198; isDirectory=false; length=2422; replication=1; blocksize=134217728; modification_time=1438883101426; access_time=1438883101390; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1438883101436; access_time=1438883101415; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 232 ms
15/08/06 17:45:04 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:45:04 INFO DefaultExecutionContext: Starting job: RangePartitioner at Exchange.scala:88
15/08/06 17:45:04 INFO FileInputFormat: Total input paths to process : 200
15/08/06 17:45:04 INFO ParquetInputFormat: Total input paths to process : 200
15/08/06 17:45:04 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/06 17:45:04 INFO DAGScheduler: Registering RDD 63 (mapPartitions at Exchange.scala:100)
15/08/06 17:45:04 INFO DAGScheduler: Got job 7 (RangePartitioner at Exchange.scala:88) with 200 output partitions (allowLocal=false)
15/08/06 17:45:04 INFO DAGScheduler: Final stage: Stage 11(RangePartitioner at Exchange.scala:88)
15/08/06 17:45:04 INFO DAGScheduler: Parents of final stage: List(Stage 10)
15/08/06 17:45:05 INFO DAGScheduler: Missing parents: List(Stage 10)
15/08/06 17:45:05 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[63] at mapPartitions at Exchange.scala:100), which has no missing parents
15/08/06 17:45:05 INFO MemoryStore: ensureFreeSpace(8200) called with curMem=1593232, maxMem=3333968363
15/08/06 17:45:05 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 8.0 KB, free 3.1 GB)
15/08/06 17:45:05 INFO MemoryStore: ensureFreeSpace(4450) called with curMem=1601432, maxMem=3333968363
15/08/06 17:45:05 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.1 GB)
15/08/06 17:45:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:42907 (size: 4.3 KB, free: 3.1 GB)
15/08/06 17:45:05 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/08/06 17:45:05 INFO DefaultExecutionContext: Created broadcast 16 from broadcast at DAGScheduler.scala:838
15/08/06 17:45:05 INFO DAGScheduler: Submitting 200 missing tasks from Stage 10 (MapPartitionsRDD[63] at mapPartitions at Exchange.scala:100)
15/08/06 17:45:05 INFO TaskSchedulerImpl: Adding task set 10.0 with 200 tasks
15/08/06 17:45:05 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 417, localhost, ANY, 1525 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 418, localhost, ANY, 1526 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 419, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 420, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 421, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 422, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 423, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 424, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 425, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 426, localhost, ANY, 1524 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 427, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 428, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 429, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 430, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 431, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 15.0 in stage 10.0 (TID 432, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 0.0 in stage 10.0 (TID 417)
15/08/06 17:45:05 INFO Executor: Running task 1.0 in stage 10.0 (TID 418)
15/08/06 17:45:05 INFO Executor: Running task 2.0 in stage 10.0 (TID 419)
15/08/06 17:45:05 INFO Executor: Running task 4.0 in stage 10.0 (TID 421)
15/08/06 17:45:05 INFO Executor: Running task 7.0 in stage 10.0 (TID 424)
15/08/06 17:45:05 INFO Executor: Running task 3.0 in stage 10.0 (TID 420)
15/08/06 17:45:05 INFO Executor: Running task 9.0 in stage 10.0 (TID 426)
15/08/06 17:45:05 INFO Executor: Running task 8.0 in stage 10.0 (TID 425)
15/08/06 17:45:05 INFO Executor: Running task 6.0 in stage 10.0 (TID 423)
15/08/06 17:45:05 INFO Executor: Running task 5.0 in stage 10.0 (TID 422)
15/08/06 17:45:05 INFO Executor: Running task 10.0 in stage 10.0 (TID 427)
15/08/06 17:45:05 INFO Executor: Running task 11.0 in stage 10.0 (TID 428)
15/08/06 17:45:05 INFO Executor: Running task 12.0 in stage 10.0 (TID 429)
15/08/06 17:45:05 INFO Executor: Running task 14.0 in stage 10.0 (TID 431)
15/08/06 17:45:05 INFO Executor: Running task 15.0 in stage 10.0 (TID 432)
15/08/06 17:45:05 INFO Executor: Running task 13.0 in stage 10.0 (TID 430)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 154
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 142
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 140
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 129
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 126
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 144
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 193
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 182
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 125
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 177
15/08/06 17:45:05 INFO Executor: Finished task 10.0 in stage 10.0 (TID 427). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 12.0 in stage 10.0 (TID 429). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 8.0 in stage 10.0 (TID 425). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 167
15/08/06 17:45:05 INFO Executor: Finished task 3.0 in stage 10.0 (TID 420). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 13.0 in stage 10.0 (TID 430). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 156
15/08/06 17:45:05 INFO Executor: Finished task 14.0 in stage 10.0 (TID 431). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 6.0 in stage 10.0 (TID 423). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 0.0 in stage 10.0 (TID 417). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 7.0 in stage 10.0 (TID 424). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 9.0 in stage 10.0 (TID 426). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 11.0 in stage 10.0 (TID 428). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 2.0 in stage 10.0 (TID 419). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 15.0 in stage 10.0 (TID 432). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 16.0 in stage 10.0 (TID 433, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 5.0 in stage 10.0 (TID 422). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 16.0 in stage 10.0 (TID 433)
15/08/06 17:45:05 INFO Executor: Finished task 1.0 in stage 10.0 (TID 418). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 4.0 in stage 10.0 (TID 421). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 429) in 61 ms on localhost (1/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 17.0 in stage 10.0 (TID 434, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO Executor: Running task 17.0 in stage 10.0 (TID 434)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 427) in 64 ms on localhost (2/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 18.0 in stage 10.0 (TID 435, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 18.0 in stage 10.0 (TID 435)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 425) in 68 ms on localhost (3/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 19.0 in stage 10.0 (TID 436, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 19.0 in stage 10.0 (TID 436)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 20.0 in stage 10.0 (TID 437, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 20.0 in stage 10.0 (TID 437)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 420) in 74 ms on localhost (4/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 21.0 in stage 10.0 (TID 438, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 21.0 in stage 10.0 (TID 438)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 430) in 71 ms on localhost (5/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 22.0 in stage 10.0 (TID 439, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 22.0 in stage 10.0 (TID 439)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 431) in 74 ms on localhost (6/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 423) in 80 ms on localhost (7/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:45:05 INFO TaskSetManager: Starting task 23.0 in stage 10.0 (TID 440, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 23.0 in stage 10.0 (TID 440)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 16.0 in stage 10.0 (TID 433). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 24.0 in stage 10.0 (TID 441, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 24.0 in stage 10.0 (TID 441)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 17.0 in stage 10.0 (TID 434). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:45:05 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 417) in 88 ms on localhost (8/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 25.0 in stage 10.0 (TID 442, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 171
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 25.0 in stage 10.0 (TID 442)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 424) in 91 ms on localhost (9/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:45:05 INFO Executor: Finished task 20.0 in stage 10.0 (TID 437). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 159
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 19.0 in stage 10.0 (TID 436). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 426) in 91 ms on localhost (10/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 154
15/08/06 17:45:05 INFO TaskSetManager: Starting task 26.0 in stage 10.0 (TID 443, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 26.0 in stage 10.0 (TID 443)
15/08/06 17:45:05 INFO Executor: Finished task 21.0 in stage 10.0 (TID 438). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 428) in 96 ms on localhost (11/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 179
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:45:05 INFO Executor: Finished task 22.0 in stage 10.0 (TID 439). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 419) in 103 ms on localhost (12/200)
15/08/06 17:45:05 INFO Executor: Finished task 18.0 in stage 10.0 (TID 435). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 27.0 in stage 10.0 (TID 444, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 27.0 in stage 10.0 (TID 444)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 28.0 in stage 10.0 (TID 445, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 28.0 in stage 10.0 (TID 445)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 121
15/08/06 17:45:05 INFO TaskSetManager: Finished task 15.0 in stage 10.0 (TID 432) in 98 ms on localhost (13/200)
15/08/06 17:45:05 INFO Executor: Finished task 23.0 in stage 10.0 (TID 440). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 24.0 in stage 10.0 (TID 441). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 29.0 in stage 10.0 (TID 446, localhost, ANY, 1526 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 422) in 108 ms on localhost (14/200)
15/08/06 17:45:05 INFO Executor: Running task 29.0 in stage 10.0 (TID 446)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 25.0 in stage 10.0 (TID 442). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 30.0 in stage 10.0 (TID 447, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 30.0 in stage 10.0 (TID 447)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 418) in 119 ms on localhost (15/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 31.0 in stage 10.0 (TID 448, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 31.0 in stage 10.0 (TID 448)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 421) in 120 ms on localhost (16/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 26.0 in stage 10.0 (TID 443). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 32.0 in stage 10.0 (TID 449, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 32.0 in stage 10.0 (TID 449)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 16.0 in stage 10.0 (TID 433) in 67 ms on localhost (17/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 151
15/08/06 17:45:05 INFO TaskSetManager: Starting task 33.0 in stage 10.0 (TID 450, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 33.0 in stage 10.0 (TID 450)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 17.0 in stage 10.0 (TID 434) in 59 ms on localhost (18/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 34.0 in stage 10.0 (TID 451, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 34.0 in stage 10.0 (TID 451)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 20.0 in stage 10.0 (TID 437) in 55 ms on localhost (19/200)
15/08/06 17:45:05 INFO Executor: Finished task 28.0 in stage 10.0 (TID 445). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 27.0 in stage 10.0 (TID 444). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 19.0 in stage 10.0 (TID 436) in 58 ms on localhost (20/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:45:05 INFO TaskSetManager: Starting task 35.0 in stage 10.0 (TID 452, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:45:05 INFO Executor: Running task 35.0 in stage 10.0 (TID 452)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 36.0 in stage 10.0 (TID 453, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 165
15/08/06 17:45:05 INFO TaskSetManager: Finished task 21.0 in stage 10.0 (TID 438) in 58 ms on localhost (21/200)
15/08/06 17:45:05 INFO Executor: Finished task 30.0 in stage 10.0 (TID 447). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 29.0 in stage 10.0 (TID 446). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 36.0 in stage 10.0 (TID 453)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 37.0 in stage 10.0 (TID 454, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 37.0 in stage 10.0 (TID 454)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 32.0 in stage 10.0 (TID 449). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 88
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:45:05 INFO TaskSetManager: Starting task 38.0 in stage 10.0 (TID 455, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 38.0 in stage 10.0 (TID 455)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 39.0 in stage 10.0 (TID 456, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 34.0 in stage 10.0 (TID 451). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 33.0 in stage 10.0 (TID 450). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 22.0 in stage 10.0 (TID 439) in 64 ms on localhost (22/200)
15/08/06 17:45:05 INFO Executor: Finished task 31.0 in stage 10.0 (TID 448). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 39.0 in stage 10.0 (TID 456)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 23.0 in stage 10.0 (TID 440) in 60 ms on localhost (23/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 18.0 in stage 10.0 (TID 435) in 75 ms on localhost (24/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 40.0 in stage 10.0 (TID 457, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 41.0 in stage 10.0 (TID 458, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 40.0 in stage 10.0 (TID 457)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 41.0 in stage 10.0 (TID 458)
15/08/06 17:45:05 INFO Executor: Finished task 35.0 in stage 10.0 (TID 452). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:05 INFO TaskSetManager: Finished task 24.0 in stage 10.0 (TID 441) in 63 ms on localhost (25/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 25.0 in stage 10.0 (TID 442) in 62 ms on localhost (26/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:45:05 INFO TaskSetManager: Starting task 42.0 in stage 10.0 (TID 459, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 42.0 in stage 10.0 (TID 459)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 26.0 in stage 10.0 (TID 443) in 54 ms on localhost (27/200)
15/08/06 17:45:05 INFO Executor: Finished task 37.0 in stage 10.0 (TID 454). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 43.0 in stage 10.0 (TID 460, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 43.0 in stage 10.0 (TID 460)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 28.0 in stage 10.0 (TID 445) in 51 ms on localhost (28/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 178
15/08/06 17:45:05 INFO TaskSetManager: Starting task 44.0 in stage 10.0 (TID 461, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 44.0 in stage 10.0 (TID 461)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 27.0 in stage 10.0 (TID 444) in 55 ms on localhost (29/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 30.0 in stage 10.0 (TID 447) in 42 ms on localhost (30/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 36.0 in stage 10.0 (TID 453). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 45.0 in stage 10.0 (TID 462, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 45.0 in stage 10.0 (TID 462)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 46.0 in stage 10.0 (TID 463, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 46.0 in stage 10.0 (TID 463)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 118
15/08/06 17:45:05 INFO TaskSetManager: Finished task 29.0 in stage 10.0 (TID 446) in 56 ms on localhost (31/200)
15/08/06 17:45:05 INFO Executor: Finished task 40.0 in stage 10.0 (TID 457). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 38.0 in stage 10.0 (TID 455). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 39.0 in stage 10.0 (TID 456). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 32.0 in stage 10.0 (TID 449) in 43 ms on localhost (32/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 47.0 in stage 10.0 (TID 464, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 47.0 in stage 10.0 (TID 464)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 48.0 in stage 10.0 (TID 465, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 48.0 in stage 10.0 (TID 465)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 49.0 in stage 10.0 (TID 466, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 41.0 in stage 10.0 (TID 458). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:45:05 INFO Executor: Running task 49.0 in stage 10.0 (TID 466)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:45:05 INFO TaskSetManager: Finished task 34.0 in stage 10.0 (TID 451) in 42 ms on localhost (33/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 33.0 in stage 10.0 (TID 450) in 46 ms on localhost (34/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 50.0 in stage 10.0 (TID 467, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 50.0 in stage 10.0 (TID 467)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:45:05 INFO Executor: Finished task 44.0 in stage 10.0 (TID 461). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 31.0 in stage 10.0 (TID 448) in 54 ms on localhost (35/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 35.0 in stage 10.0 (TID 452) in 44 ms on localhost (36/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 120
15/08/06 17:45:05 INFO TaskSetManager: Starting task 51.0 in stage 10.0 (TID 468, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 43.0 in stage 10.0 (TID 460). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 46.0 in stage 10.0 (TID 463). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 51.0 in stage 10.0 (TID 468)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 37.0 in stage 10.0 (TID 454) in 42 ms on localhost (37/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 52.0 in stage 10.0 (TID 469, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 42.0 in stage 10.0 (TID 459). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 52.0 in stage 10.0 (TID 469)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 36.0 in stage 10.0 (TID 453) in 47 ms on localhost (38/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 53.0 in stage 10.0 (TID 470, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 53.0 in stage 10.0 (TID 470)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 54.0 in stage 10.0 (TID 471, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 54.0 in stage 10.0 (TID 471)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 40.0 in stage 10.0 (TID 457) in 37 ms on localhost (39/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 38.0 in stage 10.0 (TID 455) in 48 ms on localhost (40/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 95
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 121
15/08/06 17:45:05 INFO TaskSetManager: Starting task 55.0 in stage 10.0 (TID 472, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:05 INFO Executor: Running task 55.0 in stage 10.0 (TID 472)
15/08/06 17:45:05 INFO Executor: Finished task 45.0 in stage 10.0 (TID 462). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 48.0 in stage 10.0 (TID 465). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 50.0 in stage 10.0 (TID 467). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 39.0 in stage 10.0 (TID 456) in 47 ms on localhost (41/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 56.0 in stage 10.0 (TID 473, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 121
15/08/06 17:45:05 INFO Executor: Running task 56.0 in stage 10.0 (TID 473)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 41.0 in stage 10.0 (TID 458) in 44 ms on localhost (42/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 57.0 in stage 10.0 (TID 474, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 57.0 in stage 10.0 (TID 474)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 124
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 58.0 in stage 10.0 (TID 475, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 49.0 in stage 10.0 (TID 466). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 51.0 in stage 10.0 (TID 468). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 47.0 in stage 10.0 (TID 464). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:45:05 INFO Executor: Running task 58.0 in stage 10.0 (TID 475)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 44.0 in stage 10.0 (TID 461) in 36 ms on localhost (43/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 52.0 in stage 10.0 (TID 469). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 59.0 in stage 10.0 (TID 476, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 118
15/08/06 17:45:05 INFO Executor: Running task 59.0 in stage 10.0 (TID 476)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 103
15/08/06 17:45:05 INFO TaskSetManager: Finished task 43.0 in stage 10.0 (TID 460) in 41 ms on localhost (44/200)
15/08/06 17:45:05 INFO Executor: Finished task 54.0 in stage 10.0 (TID 471). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 60.0 in stage 10.0 (TID 477, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 60.0 in stage 10.0 (TID 477)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 46.0 in stage 10.0 (TID 463) in 36 ms on localhost (45/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 61.0 in stage 10.0 (TID 478, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 61.0 in stage 10.0 (TID 478)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 42.0 in stage 10.0 (TID 459) in 48 ms on localhost (46/200)
15/08/06 17:45:05 INFO Executor: Finished task 55.0 in stage 10.0 (TID 472). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 62.0 in stage 10.0 (TID 479, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 62.0 in stage 10.0 (TID 479)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 53.0 in stage 10.0 (TID 470). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 45.0 in stage 10.0 (TID 462) in 41 ms on localhost (47/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 63.0 in stage 10.0 (TID 480, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 63.0 in stage 10.0 (TID 480)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 48.0 in stage 10.0 (TID 465) in 37 ms on localhost (48/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 64.0 in stage 10.0 (TID 481, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 64.0 in stage 10.0 (TID 481)
15/08/06 17:45:05 INFO Executor: Finished task 56.0 in stage 10.0 (TID 473). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 50.0 in stage 10.0 (TID 467) in 34 ms on localhost (49/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:05 INFO TaskSetManager: Finished task 49.0 in stage 10.0 (TID 466) in 39 ms on localhost (50/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 112
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 65.0 in stage 10.0 (TID 482, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 65.0 in stage 10.0 (TID 482)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 203
15/08/06 17:45:05 INFO TaskSetManager: Starting task 66.0 in stage 10.0 (TID 483, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 66.0 in stage 10.0 (TID 483)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 145
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 51.0 in stage 10.0 (TID 468) in 42 ms on localhost (51/200)
15/08/06 17:45:05 INFO Executor: Finished task 58.0 in stage 10.0 (TID 475). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 47.0 in stage 10.0 (TID 464) in 55 ms on localhost (52/200)
15/08/06 17:45:05 INFO Executor: Finished task 60.0 in stage 10.0 (TID 477). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 57.0 in stage 10.0 (TID 474). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 67.0 in stage 10.0 (TID 484, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:05 INFO Executor: Finished task 61.0 in stage 10.0 (TID 478). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 68.0 in stage 10.0 (TID 485, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 67.0 in stage 10.0 (TID 484)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 52.0 in stage 10.0 (TID 469) in 45 ms on localhost (53/200)
15/08/06 17:45:05 INFO Executor: Running task 68.0 in stage 10.0 (TID 485)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:05 INFO Executor: Finished task 59.0 in stage 10.0 (TID 476). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 69.0 in stage 10.0 (TID 486, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:45:05 INFO Executor: Running task 69.0 in stage 10.0 (TID 486)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:45:05 INFO Executor: Finished task 64.0 in stage 10.0 (TID 481). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 54.0 in stage 10.0 (TID 471) in 48 ms on localhost (54/200)
15/08/06 17:45:05 INFO Executor: Finished task 62.0 in stage 10.0 (TID 479). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 177
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 55.0 in stage 10.0 (TID 472) in 45 ms on localhost (55/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 70.0 in stage 10.0 (TID 487, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 65.0 in stage 10.0 (TID 482). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 70.0 in stage 10.0 (TID 487)
15/08/06 17:45:05 INFO Executor: Finished task 63.0 in stage 10.0 (TID 480). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 71.0 in stage 10.0 (TID 488, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 71.0 in stage 10.0 (TID 488)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 53.0 in stage 10.0 (TID 470) in 54 ms on localhost (56/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 72.0 in stage 10.0 (TID 489, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 122
15/08/06 17:45:05 INFO Executor: Finished task 66.0 in stage 10.0 (TID 483). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 72.0 in stage 10.0 (TID 489)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:05 INFO TaskSetManager: Finished task 56.0 in stage 10.0 (TID 473) in 48 ms on localhost (57/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 73.0 in stage 10.0 (TID 490, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:05 INFO Executor: Running task 73.0 in stage 10.0 (TID 490)
15/08/06 17:45:05 INFO Executor: Finished task 68.0 in stage 10.0 (TID 485). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 58.0 in stage 10.0 (TID 475) in 48 ms on localhost (58/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 67.0 in stage 10.0 (TID 484). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 69.0 in stage 10.0 (TID 486). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 74.0 in stage 10.0 (TID 491, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 74.0 in stage 10.0 (TID 491)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 60.0 in stage 10.0 (TID 477) in 49 ms on localhost (59/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 160
15/08/06 17:45:05 INFO TaskSetManager: Starting task 75.0 in stage 10.0 (TID 492, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:45:05 INFO Executor: Running task 75.0 in stage 10.0 (TID 492)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 57.0 in stage 10.0 (TID 474) in 58 ms on localhost (60/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 76.0 in stage 10.0 (TID 493, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 76.0 in stage 10.0 (TID 493)
15/08/06 17:45:05 INFO Executor: Finished task 70.0 in stage 10.0 (TID 487). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 71.0 in stage 10.0 (TID 488). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 61.0 in stage 10.0 (TID 478) in 53 ms on localhost (61/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:45:05 INFO Executor: Finished task 72.0 in stage 10.0 (TID 489). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 59.0 in stage 10.0 (TID 476) in 59 ms on localhost (62/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 77.0 in stage 10.0 (TID 494, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 77.0 in stage 10.0 (TID 494)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 78.0 in stage 10.0 (TID 495, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 78.0 in stage 10.0 (TID 495)
15/08/06 17:45:05 INFO Executor: Finished task 73.0 in stage 10.0 (TID 490). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:45:05 INFO TaskSetManager: Finished task 64.0 in stage 10.0 (TID 481) in 52 ms on localhost (63/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:45:05 INFO TaskSetManager: Finished task 62.0 in stage 10.0 (TID 479) in 58 ms on localhost (64/200)
15/08/06 17:45:05 INFO Executor: Finished task 74.0 in stage 10.0 (TID 491). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 79.0 in stage 10.0 (TID 496, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 79.0 in stage 10.0 (TID 496)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 134
15/08/06 17:45:05 INFO TaskSetManager: Starting task 80.0 in stage 10.0 (TID 497, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 80.0 in stage 10.0 (TID 497)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 65.0 in stage 10.0 (TID 482) in 55 ms on localhost (65/200)
15/08/06 17:45:05 INFO Executor: Finished task 75.0 in stage 10.0 (TID 492). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 63.0 in stage 10.0 (TID 480) in 63 ms on localhost (66/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 81.0 in stage 10.0 (TID 498, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 76.0 in stage 10.0 (TID 493). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 81.0 in stage 10.0 (TID 498)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 66.0 in stage 10.0 (TID 483) in 51 ms on localhost (67/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 82.0 in stage 10.0 (TID 499, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 191
15/08/06 17:45:05 INFO TaskSetManager: Starting task 83.0 in stage 10.0 (TID 500, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 83.0 in stage 10.0 (TID 500)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 124
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 68.0 in stage 10.0 (TID 485) in 49 ms on localhost (68/200)
15/08/06 17:45:05 INFO Executor: Running task 82.0 in stage 10.0 (TID 499)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 84.0 in stage 10.0 (TID 501, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 84.0 in stage 10.0 (TID 501)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:05 INFO Executor: Finished task 78.0 in stage 10.0 (TID 495). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 67.0 in stage 10.0 (TID 484) in 54 ms on localhost (69/200)
15/08/06 17:45:05 INFO Executor: Finished task 77.0 in stage 10.0 (TID 494). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 80.0 in stage 10.0 (TID 497). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 69.0 in stage 10.0 (TID 486) in 50 ms on localhost (70/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 85.0 in stage 10.0 (TID 502, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 85.0 in stage 10.0 (TID 502)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 70.0 in stage 10.0 (TID 487) in 50 ms on localhost (71/200)
15/08/06 17:45:05 INFO Executor: Finished task 79.0 in stage 10.0 (TID 496). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 86.0 in stage 10.0 (TID 503, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 81.0 in stage 10.0 (TID 498). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 86.0 in stage 10.0 (TID 503)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 71.0 in stage 10.0 (TID 488) in 51 ms on localhost (72/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 87.0 in stage 10.0 (TID 504, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 87.0 in stage 10.0 (TID 504)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:05 INFO TaskSetManager: Starting task 88.0 in stage 10.0 (TID 505, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 88.0 in stage 10.0 (TID 505)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 89.0 in stage 10.0 (TID 506, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 89.0 in stage 10.0 (TID 506)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 83.0 in stage 10.0 (TID 500). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 82.0 in stage 10.0 (TID 499). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 84.0 in stage 10.0 (TID 501). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 90.0 in stage 10.0 (TID 507, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO Executor: Running task 90.0 in stage 10.0 (TID 507)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 72.0 in stage 10.0 (TID 489) in 74 ms on localhost (73/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 158
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 74.0 in stage 10.0 (TID 491) in 73 ms on localhost (74/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 73.0 in stage 10.0 (TID 490) in 77 ms on localhost (75/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 91.0 in stage 10.0 (TID 508, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 91.0 in stage 10.0 (TID 508)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 75.0 in stage 10.0 (TID 492) in 72 ms on localhost (76/200)
15/08/06 17:45:05 INFO Executor: Finished task 85.0 in stage 10.0 (TID 502). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/06 17:45:05 INFO TaskSetManager: Starting task 92.0 in stage 10.0 (TID 509, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 76.0 in stage 10.0 (TID 493) in 70 ms on localhost (77/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 92.0 in stage 10.0 (TID 509)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 93.0 in stage 10.0 (TID 510, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 93.0 in stage 10.0 (TID 510)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 78.0 in stage 10.0 (TID 495) in 66 ms on localhost (78/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 89.0 in stage 10.0 (TID 506). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 86.0 in stage 10.0 (TID 503). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 94.0 in stage 10.0 (TID 511, localhost, ANY, 1526 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 173
15/08/06 17:45:05 INFO Executor: Running task 94.0 in stage 10.0 (TID 511)
15/08/06 17:45:05 INFO Executor: Finished task 87.0 in stage 10.0 (TID 504). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 77.0 in stage 10.0 (TID 494) in 73 ms on localhost (79/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 95.0 in stage 10.0 (TID 512, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 95.0 in stage 10.0 (TID 512)
15/08/06 17:45:05 INFO Executor: Finished task 90.0 in stage 10.0 (TID 507). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 80.0 in stage 10.0 (TID 497) in 66 ms on localhost (80/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 200
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 79.0 in stage 10.0 (TID 496) in 71 ms on localhost (81/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 119
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 96.0 in stage 10.0 (TID 513, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 96.0 in stage 10.0 (TID 513)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 97.0 in stage 10.0 (TID 514, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 97.0 in stage 10.0 (TID 514)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 81.0 in stage 10.0 (TID 498) in 70 ms on localhost (82/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:45:05 INFO Executor: Finished task 91.0 in stage 10.0 (TID 508). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:05 INFO Executor: Finished task 93.0 in stage 10.0 (TID 510). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 98.0 in stage 10.0 (TID 515, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 98.0 in stage 10.0 (TID 515)
15/08/06 17:45:05 INFO Executor: Finished task 92.0 in stage 10.0 (TID 509). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 88.0 in stage 10.0 (TID 505). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 83.0 in stage 10.0 (TID 500) in 70 ms on localhost (83/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 99.0 in stage 10.0 (TID 516, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 99.0 in stage 10.0 (TID 516)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 82.0 in stage 10.0 (TID 499) in 75 ms on localhost (84/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/06 17:45:05 INFO TaskSetManager: Finished task 84.0 in stage 10.0 (TID 501) in 73 ms on localhost (85/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:45:05 INFO TaskSetManager: Starting task 100.0 in stage 10.0 (TID 517, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 94.0 in stage 10.0 (TID 511). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 100.0 in stage 10.0 (TID 517)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 101.0 in stage 10.0 (TID 518, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 101.0 in stage 10.0 (TID 518)
15/08/06 17:45:05 INFO Executor: Finished task 95.0 in stage 10.0 (TID 512). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 96.0 in stage 10.0 (TID 513). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 85.0 in stage 10.0 (TID 502) in 72 ms on localhost (86/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 97.0 in stage 10.0 (TID 514). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 102.0 in stage 10.0 (TID 519, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 102.0 in stage 10.0 (TID 519)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 89.0 in stage 10.0 (TID 506) in 49 ms on localhost (87/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 98.0 in stage 10.0 (TID 515). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 103.0 in stage 10.0 (TID 520, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 103.0 in stage 10.0 (TID 520)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 162
15/08/06 17:45:05 INFO TaskSetManager: Finished task 86.0 in stage 10.0 (TID 503) in 73 ms on localhost (88/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 87.0 in stage 10.0 (TID 504) in 73 ms on localhost (89/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 104.0 in stage 10.0 (TID 521, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 104.0 in stage 10.0 (TID 521)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 105.0 in stage 10.0 (TID 522, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 105.0 in stage 10.0 (TID 522)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:05 INFO TaskSetManager: Finished task 90.0 in stage 10.0 (TID 507) in 53 ms on localhost (90/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 91.0 in stage 10.0 (TID 508) in 45 ms on localhost (91/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 106.0 in stage 10.0 (TID 523, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 106.0 in stage 10.0 (TID 523)
15/08/06 17:45:05 INFO Executor: Finished task 99.0 in stage 10.0 (TID 516). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 174
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:05 INFO Executor: Finished task 100.0 in stage 10.0 (TID 517). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 93.0 in stage 10.0 (TID 510) in 44 ms on localhost (92/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 107.0 in stage 10.0 (TID 524, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 107.0 in stage 10.0 (TID 524)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 108.0 in stage 10.0 (TID 525, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 101.0 in stage 10.0 (TID 518). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:05 INFO Executor: Running task 108.0 in stage 10.0 (TID 525)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 92.0 in stage 10.0 (TID 509) in 48 ms on localhost (93/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 88.0 in stage 10.0 (TID 505) in 82 ms on localhost (94/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 109.0 in stage 10.0 (TID 526, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 109.0 in stage 10.0 (TID 526)
15/08/06 17:45:05 INFO Executor: Finished task 103.0 in stage 10.0 (TID 520). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 110.0 in stage 10.0 (TID 527, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 110.0 in stage 10.0 (TID 527)
15/08/06 17:45:05 INFO Executor: Finished task 102.0 in stage 10.0 (TID 519). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 94.0 in stage 10.0 (TID 511) in 49 ms on localhost (95/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 111.0 in stage 10.0 (TID 528, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:05 INFO Executor: Running task 111.0 in stage 10.0 (TID 528)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 95.0 in stage 10.0 (TID 512) in 45 ms on localhost (96/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 112.0 in stage 10.0 (TID 529, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 176
15/08/06 17:45:05 INFO Executor: Running task 112.0 in stage 10.0 (TID 529)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 96.0 in stage 10.0 (TID 513) in 41 ms on localhost (97/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 113.0 in stage 10.0 (TID 530, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 167
15/08/06 17:45:05 INFO Executor: Running task 113.0 in stage 10.0 (TID 530)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 97.0 in stage 10.0 (TID 514) in 43 ms on localhost (98/200)
15/08/06 17:45:05 INFO Executor: Finished task 104.0 in stage 10.0 (TID 521). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 106.0 in stage 10.0 (TID 523). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 107.0 in stage 10.0 (TID 524). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 155
15/08/06 17:45:05 INFO TaskSetManager: Starting task 114.0 in stage 10.0 (TID 531, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 108.0 in stage 10.0 (TID 525). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 114.0 in stage 10.0 (TID 531)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 98.0 in stage 10.0 (TID 515) in 43 ms on localhost (99/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 105.0 in stage 10.0 (TID 522). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 109.0 in stage 10.0 (TID 526). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 115.0 in stage 10.0 (TID 532, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 115.0 in stage 10.0 (TID 532)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 178
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 99.0 in stage 10.0 (TID 516) in 43 ms on localhost (100/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:45:05 INFO TaskSetManager: Starting task 116.0 in stage 10.0 (TID 533, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 116.0 in stage 10.0 (TID 533)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 149
15/08/06 17:45:05 INFO TaskSetManager: Finished task 100.0 in stage 10.0 (TID 517) in 41 ms on localhost (101/200)
15/08/06 17:45:05 INFO Executor: Finished task 110.0 in stage 10.0 (TID 527). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 111.0 in stage 10.0 (TID 528). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 117.0 in stage 10.0 (TID 534, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 117.0 in stage 10.0 (TID 534)
15/08/06 17:45:05 INFO Executor: Finished task 113.0 in stage 10.0 (TID 530). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 101.0 in stage 10.0 (TID 518) in 43 ms on localhost (102/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 118.0 in stage 10.0 (TID 535, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 103.0 in stage 10.0 (TID 520) in 41 ms on localhost (103/200)
15/08/06 17:45:05 INFO Executor: Finished task 112.0 in stage 10.0 (TID 529). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 118.0 in stage 10.0 (TID 535)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 119.0 in stage 10.0 (TID 536, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 128
15/08/06 17:45:05 INFO Executor: Running task 119.0 in stage 10.0 (TID 536)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 102.0 in stage 10.0 (TID 519) in 46 ms on localhost (104/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 120.0 in stage 10.0 (TID 537, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 120.0 in stage 10.0 (TID 537)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 104.0 in stage 10.0 (TID 521) in 43 ms on localhost (105/200)
15/08/06 17:45:05 INFO Executor: Finished task 115.0 in stage 10.0 (TID 532). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 114.0 in stage 10.0 (TID 531). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 121.0 in stage 10.0 (TID 538, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO Executor: Running task 121.0 in stage 10.0 (TID 538)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/06 17:45:05 INFO TaskSetManager: Finished task 106.0 in stage 10.0 (TID 523) in 44 ms on localhost (106/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 122.0 in stage 10.0 (TID 539, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 116.0 in stage 10.0 (TID 533). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 122.0 in stage 10.0 (TID 539)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 107.0 in stage 10.0 (TID 524) in 48 ms on localhost (107/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 123.0 in stage 10.0 (TID 540, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/06 17:45:05 INFO TaskSetManager: Finished task 108.0 in stage 10.0 (TID 525) in 49 ms on localhost (108/200)
15/08/06 17:45:05 INFO Executor: Finished task 117.0 in stage 10.0 (TID 534). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 123.0 in stage 10.0 (TID 540)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 124.0 in stage 10.0 (TID 541, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 124.0 in stage 10.0 (TID 541)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 105.0 in stage 10.0 (TID 522) in 58 ms on localhost (109/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 118.0 in stage 10.0 (TID 535). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:05 INFO TaskSetManager: Starting task 125.0 in stage 10.0 (TID 542, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 125.0 in stage 10.0 (TID 542)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 109.0 in stage 10.0 (TID 526) in 51 ms on localhost (110/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 126.0 in stage 10.0 (TID 543, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 126.0 in stage 10.0 (TID 543)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 120.0 in stage 10.0 (TID 537). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 110.0 in stage 10.0 (TID 527) in 52 ms on localhost (111/200)
15/08/06 17:45:05 INFO Executor: Finished task 119.0 in stage 10.0 (TID 536). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 127.0 in stage 10.0 (TID 544, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 127.0 in stage 10.0 (TID 544)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 128.0 in stage 10.0 (TID 545, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 128.0 in stage 10.0 (TID 545)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 111.0 in stage 10.0 (TID 528) in 56 ms on localhost (112/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 129.0 in stage 10.0 (TID 546, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 129.0 in stage 10.0 (TID 546)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 121.0 in stage 10.0 (TID 538). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 148
15/08/06 17:45:05 INFO TaskSetManager: Finished task 113.0 in stage 10.0 (TID 530) in 54 ms on localhost (113/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 112.0 in stage 10.0 (TID 529) in 59 ms on localhost (114/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 175
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 180
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 130.0 in stage 10.0 (TID 547, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 130.0 in stage 10.0 (TID 547)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:45:05 INFO Executor: Finished task 124.0 in stage 10.0 (TID 541). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 115.0 in stage 10.0 (TID 532) in 51 ms on localhost (115/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 122.0 in stage 10.0 (TID 539). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 169
15/08/06 17:45:05 INFO Executor: Finished task 123.0 in stage 10.0 (TID 540). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 131.0 in stage 10.0 (TID 548, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 131.0 in stage 10.0 (TID 548)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 114.0 in stage 10.0 (TID 531) in 60 ms on localhost (116/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 132.0 in stage 10.0 (TID 549, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 125.0 in stage 10.0 (TID 542). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 132.0 in stage 10.0 (TID 549)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:45:05 INFO Executor: Finished task 126.0 in stage 10.0 (TID 543). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 116.0 in stage 10.0 (TID 533) in 53 ms on localhost (117/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 127.0 in stage 10.0 (TID 544). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 117.0 in stage 10.0 (TID 534) in 51 ms on localhost (118/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 133.0 in stage 10.0 (TID 550, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 129.0 in stage 10.0 (TID 546). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 133.0 in stage 10.0 (TID 550)
15/08/06 17:45:05 INFO Executor: Finished task 128.0 in stage 10.0 (TID 545). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 134.0 in stage 10.0 (TID 551, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 134.0 in stage 10.0 (TID 551)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 118.0 in stage 10.0 (TID 535) in 51 ms on localhost (119/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 135.0 in stage 10.0 (TID 552, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 135.0 in stage 10.0 (TID 552)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 136.0 in stage 10.0 (TID 553, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 136.0 in stage 10.0 (TID 553)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 120.0 in stage 10.0 (TID 537) in 48 ms on localhost (120/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:45:05 INFO TaskSetManager: Finished task 119.0 in stage 10.0 (TID 536) in 51 ms on localhost (121/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 121.0 in stage 10.0 (TID 538) in 48 ms on localhost (122/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 137.0 in stage 10.0 (TID 554, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 131.0 in stage 10.0 (TID 548). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 138.0 in stage 10.0 (TID 555, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 138.0 in stage 10.0 (TID 555)
15/08/06 17:45:05 INFO Executor: Running task 137.0 in stage 10.0 (TID 554)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 124.0 in stage 10.0 (TID 541) in 37 ms on localhost (123/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 139.0 in stage 10.0 (TID 556, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:05 INFO Executor: Running task 139.0 in stage 10.0 (TID 556)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 130.0 in stage 10.0 (TID 547). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 122.0 in stage 10.0 (TID 539) in 43 ms on localhost (124/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 140.0 in stage 10.0 (TID 557, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 140.0 in stage 10.0 (TID 557)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 123.0 in stage 10.0 (TID 540) in 44 ms on localhost (125/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 132.0 in stage 10.0 (TID 549). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:45:05 INFO Executor: Finished task 133.0 in stage 10.0 (TID 550). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 134.0 in stage 10.0 (TID 551). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 141.0 in stage 10.0 (TID 558, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:45:05 INFO Executor: Running task 141.0 in stage 10.0 (TID 558)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 125.0 in stage 10.0 (TID 542) in 43 ms on localhost (126/200)
15/08/06 17:45:05 INFO Executor: Finished task 136.0 in stage 10.0 (TID 553). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:45:05 INFO Executor: Finished task 135.0 in stage 10.0 (TID 552). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 142.0 in stage 10.0 (TID 559, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 142.0 in stage 10.0 (TID 559)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:45:05 INFO TaskSetManager: Finished task 126.0 in stage 10.0 (TID 543) in 42 ms on localhost (127/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 143.0 in stage 10.0 (TID 560, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 143.0 in stage 10.0 (TID 560)
15/08/06 17:45:05 INFO Executor: Finished task 138.0 in stage 10.0 (TID 555). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 127.0 in stage 10.0 (TID 544) in 41 ms on localhost (128/200)
15/08/06 17:45:05 INFO Executor: Finished task 137.0 in stage 10.0 (TID 554). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 139.0 in stage 10.0 (TID 556). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 144.0 in stage 10.0 (TID 561, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 144.0 in stage 10.0 (TID 561)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 129.0 in stage 10.0 (TID 546) in 40 ms on localhost (129/200)
15/08/06 17:45:05 INFO Executor: Finished task 140.0 in stage 10.0 (TID 557). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 145.0 in stage 10.0 (TID 562, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 145.0 in stage 10.0 (TID 562)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 128.0 in stage 10.0 (TID 545) in 46 ms on localhost (130/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:45:05 INFO TaskSetManager: Finished task 131.0 in stage 10.0 (TID 548) in 37 ms on localhost (131/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 146.0 in stage 10.0 (TID 563, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 146.0 in stage 10.0 (TID 563)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 147.0 in stage 10.0 (TID 564, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 147.0 in stage 10.0 (TID 564)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:45:05 INFO TaskSetManager: Finished task 130.0 in stage 10.0 (TID 547) in 42 ms on localhost (132/200)
15/08/06 17:45:05 INFO Executor: Finished task 141.0 in stage 10.0 (TID 558). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 148.0 in stage 10.0 (TID 565, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:05 INFO Executor: Running task 148.0 in stage 10.0 (TID 565)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 103
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 132.0 in stage 10.0 (TID 549) in 39 ms on localhost (133/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 149.0 in stage 10.0 (TID 566, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 149.0 in stage 10.0 (TID 566)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 142.0 in stage 10.0 (TID 559). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 150.0 in stage 10.0 (TID 567, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 150.0 in stage 10.0 (TID 567)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 133.0 in stage 10.0 (TID 550) in 38 ms on localhost (134/200)
15/08/06 17:45:05 INFO Executor: Finished task 143.0 in stage 10.0 (TID 560). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 134.0 in stage 10.0 (TID 551) in 39 ms on localhost (135/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 151.0 in stage 10.0 (TID 568, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO Executor: Running task 151.0 in stage 10.0 (TID 568)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 136.0 in stage 10.0 (TID 553) in 38 ms on localhost (136/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 152.0 in stage 10.0 (TID 569, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 145.0 in stage 10.0 (TID 562). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 152.0 in stage 10.0 (TID 569)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 135.0 in stage 10.0 (TID 552) in 40 ms on localhost (137/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 153.0 in stage 10.0 (TID 570, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 153.0 in stage 10.0 (TID 570)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 178
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 154.0 in stage 10.0 (TID 571, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 154.0 in stage 10.0 (TID 571)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 155.0 in stage 10.0 (TID 572, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 155.0 in stage 10.0 (TID 572)
15/08/06 17:45:05 INFO Executor: Finished task 144.0 in stage 10.0 (TID 561). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 156.0 in stage 10.0 (TID 573, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 156.0 in stage 10.0 (TID 573)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 157.0 in stage 10.0 (TID 574, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 157.0 in stage 10.0 (TID 574)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 138.0 in stage 10.0 (TID 555) in 38 ms on localhost (138/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 140.0 in stage 10.0 (TID 557) in 35 ms on localhost (139/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 148.0 in stage 10.0 (TID 565). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 126
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 139.0 in stage 10.0 (TID 556) in 41 ms on localhost (140/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 163
15/08/06 17:45:05 INFO TaskSetManager: Finished task 137.0 in stage 10.0 (TID 554) in 45 ms on localhost (141/200)
15/08/06 17:45:05 INFO Executor: Finished task 147.0 in stage 10.0 (TID 564). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 146.0 in stage 10.0 (TID 563). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 141.0 in stage 10.0 (TID 558) in 40 ms on localhost (142/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 108
15/08/06 17:45:05 INFO Executor: Finished task 150.0 in stage 10.0 (TID 567). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 158.0 in stage 10.0 (TID 575, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 158.0 in stage 10.0 (TID 575)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 190
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Starting task 159.0 in stage 10.0 (TID 576, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 149.0 in stage 10.0 (TID 566). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 113
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:05 INFO TaskSetManager: Starting task 160.0 in stage 10.0 (TID 577, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 159.0 in stage 10.0 (TID 576)
15/08/06 17:45:05 INFO Executor: Running task 160.0 in stage 10.0 (TID 577)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 142.0 in stage 10.0 (TID 559) in 41 ms on localhost (143/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:05 INFO Executor: Finished task 151.0 in stage 10.0 (TID 568). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:05 INFO Executor: Finished task 154.0 in stage 10.0 (TID 571). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 145.0 in stage 10.0 (TID 562) in 35 ms on localhost (144/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 155.0 in stage 10.0 (TID 572). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 143.0 in stage 10.0 (TID 560) in 47 ms on localhost (145/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:45:05 INFO Executor: Finished task 152.0 in stage 10.0 (TID 569). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 159
15/08/06 17:45:05 INFO Executor: Finished task 156.0 in stage 10.0 (TID 573). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 161.0 in stage 10.0 (TID 578, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 161.0 in stage 10.0 (TID 578)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 144.0 in stage 10.0 (TID 561) in 47 ms on localhost (146/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 162.0 in stage 10.0 (TID 579, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 162.0 in stage 10.0 (TID 579)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 163.0 in stage 10.0 (TID 580, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 163.0 in stage 10.0 (TID 580)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 148.0 in stage 10.0 (TID 565) in 38 ms on localhost (147/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 157.0 in stage 10.0 (TID 574). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 164.0 in stage 10.0 (TID 581, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 153.0 in stage 10.0 (TID 570). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 164.0 in stage 10.0 (TID 581)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:45:05 INFO TaskSetManager: Finished task 146.0 in stage 10.0 (TID 563) in 45 ms on localhost (148/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 147.0 in stage 10.0 (TID 564) in 45 ms on localhost (149/200)
15/08/06 17:45:05 INFO Executor: Finished task 159.0 in stage 10.0 (TID 576). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 160.0 in stage 10.0 (TID 577). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 116
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/06 17:45:05 INFO TaskSetManager: Finished task 150.0 in stage 10.0 (TID 567) in 42 ms on localhost (150/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 165.0 in stage 10.0 (TID 582, localhost, ANY, 1531 bytes)
15/08/06 17:45:05 INFO Executor: Running task 165.0 in stage 10.0 (TID 582)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 166.0 in stage 10.0 (TID 583, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 166.0 in stage 10.0 (TID 583)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 167.0 in stage 10.0 (TID 584, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:45:05 INFO Executor: Running task 167.0 in stage 10.0 (TID 584)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 151.0 in stage 10.0 (TID 568) in 44 ms on localhost (151/200)
15/08/06 17:45:05 INFO Executor: Finished task 158.0 in stage 10.0 (TID 575). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 110
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 136
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 161.0 in stage 10.0 (TID 578). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 149.0 in stage 10.0 (TID 566) in 50 ms on localhost (152/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 154.0 in stage 10.0 (TID 571) in 44 ms on localhost (153/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 162.0 in stage 10.0 (TID 579). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 168.0 in stage 10.0 (TID 585, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 168.0 in stage 10.0 (TID 585)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 169.0 in stage 10.0 (TID 586, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 163.0 in stage 10.0 (TID 580). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 169.0 in stage 10.0 (TID 586)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Finished task 155.0 in stage 10.0 (TID 572) in 45 ms on localhost (154/200)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 170.0 in stage 10.0 (TID 587, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 170.0 in stage 10.0 (TID 587)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 152.0 in stage 10.0 (TID 569) in 51 ms on localhost (155/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 164.0 in stage 10.0 (TID 581). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 171.0 in stage 10.0 (TID 588, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 171.0 in stage 10.0 (TID 588)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 114
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 156.0 in stage 10.0 (TID 573) in 50 ms on localhost (156/200)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 172.0 in stage 10.0 (TID 589, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 172.0 in stage 10.0 (TID 589)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 173.0 in stage 10.0 (TID 590, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 215
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 173.0 in stage 10.0 (TID 590)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 157.0 in stage 10.0 (TID 574) in 51 ms on localhost (157/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 153.0 in stage 10.0 (TID 570) in 55 ms on localhost (158/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 166.0 in stage 10.0 (TID 583). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:45:05 INFO TaskSetManager: Starting task 174.0 in stage 10.0 (TID 591, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 174.0 in stage 10.0 (TID 591)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:05 INFO TaskSetManager: Starting task 175.0 in stage 10.0 (TID 592, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO Executor: Finished task 167.0 in stage 10.0 (TID 584). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 175.0 in stage 10.0 (TID 592)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 176.0 in stage 10.0 (TID 593, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:45:05 INFO Executor: Finished task 165.0 in stage 10.0 (TID 582). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 176.0 in stage 10.0 (TID 593)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 177.0 in stage 10.0 (TID 594, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 177.0 in stage 10.0 (TID 594)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 178.0 in stage 10.0 (TID 595, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 179.0 in stage 10.0 (TID 596, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 178.0 in stage 10.0 (TID 595)
15/08/06 17:45:05 INFO Executor: Finished task 169.0 in stage 10.0 (TID 586). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 168.0 in stage 10.0 (TID 585). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 179.0 in stage 10.0 (TID 596)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 180.0 in stage 10.0 (TID 597, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 180.0 in stage 10.0 (TID 597)
15/08/06 17:45:05 INFO Executor: Finished task 170.0 in stage 10.0 (TID 587). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 181.0 in stage 10.0 (TID 598, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 181.0 in stage 10.0 (TID 598)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 182.0 in stage 10.0 (TID 599, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 182.0 in stage 10.0 (TID 599)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 183.0 in stage 10.0 (TID 600, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 183.0 in stage 10.0 (TID 600)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:45:05 INFO TaskSetManager: Starting task 184.0 in stage 10.0 (TID 601, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 184.0 in stage 10.0 (TID 601)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 185.0 in stage 10.0 (TID 602, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 185.0 in stage 10.0 (TID 602)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 186.0 in stage 10.0 (TID 603, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:45:05 INFO TaskSetManager: Finished task 159.0 in stage 10.0 (TID 576) in 52 ms on localhost (159/200)
15/08/06 17:45:05 INFO Executor: Finished task 171.0 in stage 10.0 (TID 588). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Running task 186.0 in stage 10.0 (TID 603)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 161.0 in stage 10.0 (TID 578) in 42 ms on localhost (160/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 158.0 in stage 10.0 (TID 575) in 55 ms on localhost (161/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 164.0 in stage 10.0 (TID 581) in 37 ms on localhost (162/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 166.0 in stage 10.0 (TID 583) in 31 ms on localhost (163/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 160.0 in stage 10.0 (TID 577) in 55 ms on localhost (164/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 129
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:45:05 INFO TaskSetManager: Finished task 163.0 in stage 10.0 (TID 580) in 44 ms on localhost (165/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 173.0 in stage 10.0 (TID 590). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 162.0 in stage 10.0 (TID 579) in 46 ms on localhost (166/200)
15/08/06 17:45:05 INFO Executor: Finished task 172.0 in stage 10.0 (TID 589). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 246
15/08/06 17:45:05 INFO TaskSetManager: Finished task 169.0 in stage 10.0 (TID 586) in 29 ms on localhost (167/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 165.0 in stage 10.0 (TID 582) in 38 ms on localhost (168/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 197
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 186
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 196
15/08/06 17:45:05 INFO TaskSetManager: Finished task 167.0 in stage 10.0 (TID 584) in 39 ms on localhost (169/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 178.0 in stage 10.0 (TID 595). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/06 17:45:05 INFO TaskSetManager: Finished task 170.0 in stage 10.0 (TID 587) in 32 ms on localhost (170/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 177.0 in stage 10.0 (TID 594). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 168.0 in stage 10.0 (TID 585) in 36 ms on localhost (171/200)
15/08/06 17:45:05 INFO Executor: Finished task 175.0 in stage 10.0 (TID 592). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 185.0 in stage 10.0 (TID 602). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 176.0 in stage 10.0 (TID 593). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 179.0 in stage 10.0 (TID 596). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 187.0 in stage 10.0 (TID 604, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO Executor: Running task 187.0 in stage 10.0 (TID 604)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:45:05 INFO Executor: Finished task 174.0 in stage 10.0 (TID 591). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 181.0 in stage 10.0 (TID 598). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 184
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 171.0 in stage 10.0 (TID 588) in 32 ms on localhost (172/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 182.0 in stage 10.0 (TID 599). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 188.0 in stage 10.0 (TID 605, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 188.0 in stage 10.0 (TID 605)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 178
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:05 INFO TaskSetManager: Finished task 173.0 in stage 10.0 (TID 590) in 31 ms on localhost (173/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 183.0 in stage 10.0 (TID 600). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 184.0 in stage 10.0 (TID 601). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 172.0 in stage 10.0 (TID 589) in 34 ms on localhost (174/200)
15/08/06 17:45:05 INFO Executor: Finished task 186.0 in stage 10.0 (TID 603). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 189.0 in stage 10.0 (TID 606, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 189.0 in stage 10.0 (TID 606)
15/08/06 17:45:05 INFO Executor: Finished task 180.0 in stage 10.0 (TID 597). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Starting task 190.0 in stage 10.0 (TID 607, localhost, ANY, 1528 bytes)
15/08/06 17:45:05 INFO Executor: Running task 190.0 in stage 10.0 (TID 607)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 178.0 in stage 10.0 (TID 595) in 30 ms on localhost (175/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 191.0 in stage 10.0 (TID 608, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Running task 191.0 in stage 10.0 (TID 608)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 177.0 in stage 10.0 (TID 594) in 33 ms on localhost (176/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 192.0 in stage 10.0 (TID 609, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO Executor: Running task 192.0 in stage 10.0 (TID 609)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:45:05 INFO TaskSetManager: Starting task 193.0 in stage 10.0 (TID 610, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 194
15/08/06 17:45:05 INFO Executor: Running task 193.0 in stage 10.0 (TID 610)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 175.0 in stage 10.0 (TID 592) in 37 ms on localhost (177/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO TaskSetManager: Starting task 194.0 in stage 10.0 (TID 611, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 194.0 in stage 10.0 (TID 611)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 185.0 in stage 10.0 (TID 602) in 33 ms on localhost (178/200)
15/08/06 17:45:05 INFO Executor: Finished task 187.0 in stage 10.0 (TID 604). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO Executor: Finished task 188.0 in stage 10.0 (TID 605). 1819 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 195.0 in stage 10.0 (TID 612, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 195.0 in stage 10.0 (TID 612)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 179.0 in stage 10.0 (TID 596) in 40 ms on localhost (179/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Starting task 196.0 in stage 10.0 (TID 613, localhost, ANY, 1530 bytes)
15/08/06 17:45:05 INFO Executor: Running task 196.0 in stage 10.0 (TID 613)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/06 17:45:05 INFO Executor: Finished task 189.0 in stage 10.0 (TID 606). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 197.0 in stage 10.0 (TID 614, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Running task 197.0 in stage 10.0 (TID 614)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 198.0 in stage 10.0 (TID 615, localhost, ANY, 1527 bytes)
15/08/06 17:45:05 INFO Executor: Running task 198.0 in stage 10.0 (TID 615)
15/08/06 17:45:05 INFO Executor: Finished task 190.0 in stage 10.0 (TID 607). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:05 INFO TaskSetManager: Starting task 199.0 in stage 10.0 (TID 616, localhost, ANY, 1529 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Running task 199.0 in stage 10.0 (TID 616)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:45:05 INFO Executor: Finished task 191.0 in stage 10.0 (TID 608). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/06 17:45:05 INFO TaskSetManager: Finished task 176.0 in stage 10.0 (TID 593) in 47 ms on localhost (180/200)
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO Executor: Finished task 192.0 in stage 10.0 (TID 609). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 193.0 in stage 10.0 (TID 610). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO TaskSetManager: Finished task 182.0 in stage 10.0 (TID 599) in 46 ms on localhost (181/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 181.0 in stage 10.0 (TID 598) in 48 ms on localhost (182/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 174.0 in stage 10.0 (TID 591) in 54 ms on localhost (183/200)
15/08/06 17:45:05 INFO Executor: Finished task 194.0 in stage 10.0 (TID 611). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 184.0 in stage 10.0 (TID 601) in 48 ms on localhost (184/200)
15/08/06 17:45:05 INFO Executor: Finished task 195.0 in stage 10.0 (TID 612). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 183.0 in stage 10.0 (TID 600) in 51 ms on localhost (185/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO TaskSetManager: Finished task 186.0 in stage 10.0 (TID 603) in 51 ms on localhost (186/200)
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 158
15/08/06 17:45:05 INFO TaskSetManager: Finished task 187.0 in stage 10.0 (TID 604) in 35 ms on localhost (187/200)
15/08/06 17:45:05 INFO Executor: Finished task 196.0 in stage 10.0 (TID 613). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 198.0 in stage 10.0 (TID 615). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO Executor: Finished task 199.0 in stage 10.0 (TID 616). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 180.0 in stage 10.0 (TID 597) in 60 ms on localhost (188/200)
15/08/06 17:45:05 INFO Executor: Finished task 197.0 in stage 10.0 (TID 614). 1819 bytes result sent to driver
15/08/06 17:45:05 INFO TaskSetManager: Finished task 188.0 in stage 10.0 (TID 605) in 37 ms on localhost (189/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 189.0 in stage 10.0 (TID 606) in 41 ms on localhost (190/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 192.0 in stage 10.0 (TID 609) in 39 ms on localhost (191/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 191.0 in stage 10.0 (TID 608) in 42 ms on localhost (192/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 190.0 in stage 10.0 (TID 607) in 45 ms on localhost (193/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 193.0 in stage 10.0 (TID 610) in 41 ms on localhost (194/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 195.0 in stage 10.0 (TID 612) in 39 ms on localhost (195/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 194.0 in stage 10.0 (TID 611) in 41 ms on localhost (196/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 199.0 in stage 10.0 (TID 616) in 36 ms on localhost (197/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 198.0 in stage 10.0 (TID 615) in 39 ms on localhost (198/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 196.0 in stage 10.0 (TID 613) in 42 ms on localhost (199/200)
15/08/06 17:45:05 INFO TaskSetManager: Finished task 197.0 in stage 10.0 (TID 614) in 41 ms on localhost (200/200)
15/08/06 17:45:05 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/06 17:45:05 INFO DAGScheduler: Stage 10 (mapPartitions at Exchange.scala:100) finished in 0.638 s
15/08/06 17:45:05 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:45:05 INFO DAGScheduler: running: Set()
15/08/06 17:45:05 INFO DAGScheduler: waiting: Set(Stage 11)
15/08/06 17:45:05 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7c9f38b4
15/08/06 17:45:05 INFO DAGScheduler: failed: Set()
15/08/06 17:45:05 INFO StatsReportListener: task runtime:(count: 200, mean: 51.630000, stdev: 15.784901, max: 120.000000, min: 29.000000)
15/08/06 17:45:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:05 INFO StatsReportListener: 	29.0 ms	35.0 ms	37.0 ms	41.0 ms	48.0 ms	58.0 ms	73.0 ms	82.0 ms	120.0 ms
15/08/06 17:45:05 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 56.000000, stdev: 0.000000, max: 56.000000, min: 56.000000)
15/08/06 17:45:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:05 INFO StatsReportListener: 	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B
15/08/06 17:45:05 INFO DAGScheduler: Missing parents for Stage 11: List()
15/08/06 17:45:05 INFO DAGScheduler: Submitting Stage 11 (MapPartitionsRDD[77] at RangePartitioner at Exchange.scala:88), which is now runnable
15/08/06 17:45:05 INFO StatsReportListener: task result size:(count: 200, mean: 1819.000000, stdev: 0.000000, max: 1819.000000, min: 1819.000000)
15/08/06 17:45:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:05 INFO StatsReportListener: 	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B
15/08/06 17:45:05 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 38.817716, stdev: 9.421346, max: 63.934426, min: 18.055556)
15/08/06 17:45:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:05 INFO StatsReportListener: 	18 %	25 %	28 %	31 %	39 %	45 %	51 %	57 %	64 %
15/08/06 17:45:05 INFO MemoryStore: ensureFreeSpace(12256) called with curMem=1605882, maxMem=3333968363
15/08/06 17:45:05 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 12.0 KB, free 3.1 GB)
15/08/06 17:45:05 INFO StatsReportListener: other time pct: (count: 200, mean: 61.182284, stdev: 9.421346, max: 81.944444, min: 36.065574)
15/08/06 17:45:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:05 INFO StatsReportListener: 	36 %	44 %	49 %	55 %	62 %	69 %	73 %	75 %	82 %
15/08/06 17:45:05 INFO MemoryStore: ensureFreeSpace(6465) called with curMem=1618138, maxMem=3333968363
15/08/06 17:45:05 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.3 KB, free 3.1 GB)
15/08/06 17:45:05 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:42907 (size: 6.3 KB, free: 3.1 GB)
15/08/06 17:45:05 INFO BlockManagerMaster: Updated info of block broadcast_17_piece0
15/08/06 17:45:05 INFO DefaultExecutionContext: Created broadcast 17 from broadcast at DAGScheduler.scala:838
15/08/06 17:45:05 INFO DAGScheduler: Submitting 200 missing tasks from Stage 11 (MapPartitionsRDD[77] at RangePartitioner at Exchange.scala:88)
15/08/06 17:45:05 INFO TaskSchedulerImpl: Adding task set 11.0 with 200 tasks
15/08/06 17:45:05 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 617, localhost, ANY, 1821 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 618, localhost, ANY, 1823 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 619, localhost, ANY, 1824 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 620, localhost, ANY, 1823 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 621, localhost, ANY, 1825 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 622, localhost, ANY, 1824 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 623, localhost, ANY, 1824 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 624, localhost, ANY, 1825 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 625, localhost, ANY, 1824 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 626, localhost, ANY, 1821 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 627, localhost, ANY, 1826 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 11.0 in stage 11.0 (TID 628, localhost, ANY, 1825 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 12.0 in stage 11.0 (TID 629, localhost, ANY, 1825 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 13.0 in stage 11.0 (TID 630, localhost, ANY, 1824 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 14.0 in stage 11.0 (TID 631, localhost, ANY, 1825 bytes)
15/08/06 17:45:05 INFO TaskSetManager: Starting task 15.0 in stage 11.0 (TID 632, localhost, ANY, 1825 bytes)
15/08/06 17:45:05 INFO Executor: Running task 0.0 in stage 11.0 (TID 617)
15/08/06 17:45:05 INFO Executor: Running task 2.0 in stage 11.0 (TID 619)
15/08/06 17:45:05 INFO Executor: Running task 9.0 in stage 11.0 (TID 626)
15/08/06 17:45:05 INFO Executor: Running task 15.0 in stage 11.0 (TID 632)
15/08/06 17:45:05 INFO Executor: Running task 10.0 in stage 11.0 (TID 627)
15/08/06 17:45:05 INFO Executor: Running task 8.0 in stage 11.0 (TID 625)
15/08/06 17:45:05 INFO Executor: Running task 5.0 in stage 11.0 (TID 622)
15/08/06 17:45:05 INFO Executor: Running task 6.0 in stage 11.0 (TID 623)
15/08/06 17:45:05 INFO Executor: Running task 7.0 in stage 11.0 (TID 624)
15/08/06 17:45:05 INFO Executor: Running task 4.0 in stage 11.0 (TID 621)
15/08/06 17:45:05 INFO Executor: Running task 3.0 in stage 11.0 (TID 620)
15/08/06 17:45:05 INFO Executor: Running task 1.0 in stage 11.0 (TID 618)
15/08/06 17:45:05 INFO Executor: Running task 14.0 in stage 11.0 (TID 631)
15/08/06 17:45:05 INFO Executor: Running task 13.0 in stage 11.0 (TID 630)
15/08/06 17:45:05 INFO Executor: Running task 12.0 in stage 11.0 (TID 629)
15/08/06 17:45:05 INFO Executor: Running task 11.0 in stage 11.0 (TID 628)
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:05 INFO BlockManager: Removing broadcast 16
15/08/06 17:45:05 INFO BlockManager: Removing block broadcast_16_piece0
15/08/06 17:45:05 INFO MemoryStore: Block broadcast_16_piece0 of size 4450 dropped from memory (free 3332348210)
15/08/06 17:45:05 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:42907 in memory (size: 4.3 KB, free: 3.1 GB)
15/08/06 17:45:05 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/08/06 17:45:05 INFO BlockManager: Removing block broadcast_16
15/08/06 17:45:05 INFO MemoryStore: Block broadcast_16 of size 8200 dropped from memory (free 3332356410)
15/08/06 17:45:05 INFO ContextCleaner: Cleaned broadcast 16
15/08/06 17:45:05 INFO BlockManager: Removing broadcast 13
15/08/06 17:45:05 INFO BlockManager: Removing block broadcast_13_piece0
15/08/06 17:45:05 INFO MemoryStore: Block broadcast_13_piece0 of size 1941 dropped from memory (free 3332358351)
15/08/06 17:45:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:42907 in memory (size: 1941.0 B, free: 3.1 GB)
15/08/06 17:45:05 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/08/06 17:45:05 INFO BlockManager: Removing block broadcast_13
15/08/06 17:45:05 INFO MemoryStore: Block broadcast_13 of size 3240 dropped from memory (free 3332361591)
15/08/06 17:45:05 INFO ContextCleaner: Cleaned broadcast 13
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 140
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 125
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 142
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 144
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 167
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:45:05 INFO Executor: Finished task 8.0 in stage 11.0 (TID 625). 2055 bytes result sent to driver
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:05 INFO Executor: Finished task 9.0 in stage 11.0 (TID 626). 2055 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:45:05 INFO Executor: Finished task 10.0 in stage 11.0 (TID 627). 2054 bytes result sent to driver
15/08/06 17:45:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Starting task 16.0 in stage 11.0 (TID 633, localhost, ANY, 1826 bytes)
15/08/06 17:45:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 143
15/08/06 17:45:05 INFO Executor: Finished task 1.0 in stage 11.0 (TID 618). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Running task 16.0 in stage 11.0 (TID 633)
15/08/06 17:45:06 INFO Executor: Finished task 11.0 in stage 11.0 (TID 628). 2055 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 177
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:45:06 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 625) in 238 ms on localhost (1/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:45:06 INFO Executor: Finished task 3.0 in stage 11.0 (TID 620). 2111 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 17.0 in stage 11.0 (TID 634, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 193
15/08/06 17:45:06 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 626) in 241 ms on localhost (2/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO Executor: Finished task 4.0 in stage 11.0 (TID 621). 2094 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Running task 17.0 in stage 11.0 (TID 634)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 18.0 in stage 11.0 (TID 635, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Finished task 12.0 in stage 11.0 (TID 629). 2072 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 627) in 242 ms on localhost (3/200)
15/08/06 17:45:06 INFO Executor: Running task 18.0 in stage 11.0 (TID 635)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO Executor: Finished task 6.0 in stage 11.0 (TID 623). 2019 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:06 INFO Executor: Finished task 0.0 in stage 11.0 (TID 617). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 5.0 in stage 11.0 (TID 622). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 19.0 in stage 11.0 (TID 636, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Running task 19.0 in stage 11.0 (TID 636)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 618) in 250 ms on localhost (4/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 154
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO TaskSetManager: Starting task 20.0 in stage 11.0 (TID 637, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/06 17:45:06 INFO TaskSetManager: Finished task 11.0 in stage 11.0 (TID 628) in 249 ms on localhost (5/200)
15/08/06 17:45:06 INFO Executor: Running task 20.0 in stage 11.0 (TID 637)
15/08/06 17:45:06 INFO Executor: Finished task 13.0 in stage 11.0 (TID 630). 2237 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 7.0 in stage 11.0 (TID 624). 2018 bytes result sent to driver
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 620) in 254 ms on localhost (6/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO Executor: Finished task 15.0 in stage 11.0 (TID 632). 2112 bytes result sent to driver
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 21.0 in stage 11.0 (TID 638, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 21.0 in stage 11.0 (TID 638)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:06 INFO TaskSetManager: Starting task 22.0 in stage 11.0 (TID 639, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Running task 22.0 in stage 11.0 (TID 639)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 621) in 257 ms on localhost (7/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 23.0 in stage 11.0 (TID 640, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 23.0 in stage 11.0 (TID 640)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 126
15/08/06 17:45:06 INFO TaskSetManager: Finished task 12.0 in stage 11.0 (TID 629) in 256 ms on localhost (8/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 24.0 in stage 11.0 (TID 641, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 623) in 261 ms on localhost (9/200)
15/08/06 17:45:06 INFO Executor: Finished task 2.0 in stage 11.0 (TID 619). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 14.0 in stage 11.0 (TID 631). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Running task 24.0 in stage 11.0 (TID 641)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 617) in 268 ms on localhost (10/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 25.0 in stage 11.0 (TID 642, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 26.0 in stage 11.0 (TID 643, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 25.0 in stage 11.0 (TID 642)
15/08/06 17:45:06 INFO Executor: Running task 26.0 in stage 11.0 (TID 643)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 622) in 269 ms on localhost (11/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 27.0 in stage 11.0 (TID 644, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 27.0 in stage 11.0 (TID 644)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 13.0 in stage 11.0 (TID 630) in 268 ms on localhost (12/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 624) in 273 ms on localhost (13/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 28.0 in stage 11.0 (TID 645, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 28.0 in stage 11.0 (TID 645)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 29.0 in stage 11.0 (TID 646, localhost, ANY, 1822 bytes)
15/08/06 17:45:06 INFO Executor: Running task 29.0 in stage 11.0 (TID 646)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 15.0 in stage 11.0 (TID 632) in 277 ms on localhost (14/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 30.0 in stage 11.0 (TID 647, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 30.0 in stage 11.0 (TID 647)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 619) in 286 ms on localhost (15/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 31.0 in stage 11.0 (TID 648, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 31.0 in stage 11.0 (TID 648)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 14.0 in stage 11.0 (TID 631) in 287 ms on localhost (16/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 179
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:45:06 INFO Executor: Finished task 19.0 in stage 11.0 (TID 636). 2146 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Finished task 24.0 in stage 11.0 (TID 641). 2001 bytes result sent to driver
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Starting task 32.0 in stage 11.0 (TID 649, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 32.0 in stage 11.0 (TID 649)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 33.0 in stage 11.0 (TID 650, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 33.0 in stage 11.0 (TID 650)
15/08/06 17:45:06 INFO Executor: Finished task 18.0 in stage 11.0 (TID 635). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 19.0 in stage 11.0 (TID 636) in 153 ms on localhost (17/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 153
15/08/06 17:45:06 INFO TaskSetManager: Starting task 34.0 in stage 11.0 (TID 651, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 34.0 in stage 11.0 (TID 651)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 24.0 in stage 11.0 (TID 641) in 142 ms on localhost (18/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:45:06 INFO Executor: Finished task 17.0 in stage 11.0 (TID 634). 2093 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO TaskSetManager: Finished task 18.0 in stage 11.0 (TID 635) in 160 ms on localhost (19/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 151
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Starting task 35.0 in stage 11.0 (TID 652, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Finished task 17.0 in stage 11.0 (TID 634) in 166 ms on localhost (20/200)
15/08/06 17:45:06 INFO Executor: Running task 35.0 in stage 11.0 (TID 652)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:45:06 INFO Executor: Finished task 28.0 in stage 11.0 (TID 645). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 137
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 121
15/08/06 17:45:06 INFO TaskSetManager: Starting task 36.0 in stage 11.0 (TID 653, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO Executor: Running task 36.0 in stage 11.0 (TID 653)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 28.0 in stage 11.0 (TID 645) in 130 ms on localhost (21/200)
15/08/06 17:45:06 INFO Executor: Finished task 21.0 in stage 11.0 (TID 638). 2019 bytes result sent to driver
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO Executor: Finished task 20.0 in stage 11.0 (TID 637). 2055 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 37.0 in stage 11.0 (TID 654, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 37.0 in stage 11.0 (TID 654)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 21.0 in stage 11.0 (TID 638) in 158 ms on localhost (22/200)
15/08/06 17:45:06 INFO Executor: Finished task 16.0 in stage 11.0 (TID 633). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 38.0 in stage 11.0 (TID 655, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:06 INFO Executor: Running task 38.0 in stage 11.0 (TID 655)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 20.0 in stage 11.0 (TID 637) in 164 ms on localhost (23/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 39.0 in stage 11.0 (TID 656, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 16.0 in stage 11.0 (TID 633) in 179 ms on localhost (24/200)
15/08/06 17:45:06 INFO Executor: Running task 39.0 in stage 11.0 (TID 656)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 121
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:45:06 INFO Executor: Finished task 23.0 in stage 11.0 (TID 640). 2130 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 25.0 in stage 11.0 (TID 642). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 40.0 in stage 11.0 (TID 657, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 40.0 in stage 11.0 (TID 657)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 154
15/08/06 17:45:06 INFO TaskSetManager: Finished task 23.0 in stage 11.0 (TID 640) in 182 ms on localhost (25/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO TaskSetManager: Starting task 41.0 in stage 11.0 (TID 658, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO Executor: Running task 41.0 in stage 11.0 (TID 658)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 25.0 in stage 11.0 (TID 642) in 176 ms on localhost (26/200)
15/08/06 17:45:06 INFO Executor: Finished task 22.0 in stage 11.0 (TID 639). 2055 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 42.0 in stage 11.0 (TID 659, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO Executor: Running task 42.0 in stage 11.0 (TID 659)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 22.0 in stage 11.0 (TID 639) in 189 ms on localhost (27/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 196
15/08/06 17:45:06 INFO Executor: Finished task 29.0 in stage 11.0 (TID 646). 2166 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 43.0 in stage 11.0 (TID 660, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 43.0 in stage 11.0 (TID 660)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Finished task 29.0 in stage 11.0 (TID 646) in 177 ms on localhost (28/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:45:06 INFO Executor: Finished task 27.0 in stage 11.0 (TID 644). 2019 bytes result sent to driver
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 44.0 in stage 11.0 (TID 661, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 44.0 in stage 11.0 (TID 661)
15/08/06 17:45:06 INFO Executor: Finished task 30.0 in stage 11.0 (TID 647). 2055 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Finished task 27.0 in stage 11.0 (TID 644) in 196 ms on localhost (29/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO TaskSetManager: Starting task 45.0 in stage 11.0 (TID 662, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO Executor: Running task 45.0 in stage 11.0 (TID 662)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO TaskSetManager: Finished task 30.0 in stage 11.0 (TID 647) in 185 ms on localhost (30/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 88
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:45:06 INFO Executor: Finished task 31.0 in stage 11.0 (TID 648). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 26.0 in stage 11.0 (TID 643). 2036 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 46.0 in stage 11.0 (TID 663, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO Executor: Running task 46.0 in stage 11.0 (TID 663)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 31.0 in stage 11.0 (TID 648) in 192 ms on localhost (31/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 47.0 in stage 11.0 (TID 664, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO Executor: Running task 47.0 in stage 11.0 (TID 664)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 26.0 in stage 11.0 (TID 643) in 216 ms on localhost (32/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 165
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO Executor: Finished task 32.0 in stage 11.0 (TID 649). 2094 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Starting task 48.0 in stage 11.0 (TID 665, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 48.0 in stage 11.0 (TID 665)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:06 INFO TaskSetManager: Finished task 32.0 in stage 11.0 (TID 649) in 178 ms on localhost (33/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 192
15/08/06 17:45:06 INFO Executor: Finished task 38.0 in stage 11.0 (TID 655). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Starting task 49.0 in stage 11.0 (TID 666, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO Executor: Finished task 34.0 in stage 11.0 (TID 651). 2093 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Running task 49.0 in stage 11.0 (TID 666)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 50.0 in stage 11.0 (TID 667, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 50.0 in stage 11.0 (TID 667)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 34.0 in stage 11.0 (TID 651) in 179 ms on localhost (34/200)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 38.0 in stage 11.0 (TID 655) in 168 ms on localhost (35/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO Executor: Finished task 39.0 in stage 11.0 (TID 656). 2019 bytes result sent to driver
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO TaskSetManager: Starting task 51.0 in stage 11.0 (TID 668, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 39.0 in stage 11.0 (TID 656) in 172 ms on localhost (36/200)
15/08/06 17:45:06 INFO Executor: Running task 51.0 in stage 11.0 (TID 668)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO Executor: Finished task 35.0 in stage 11.0 (TID 652). 2001 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:06 INFO TaskSetManager: Starting task 52.0 in stage 11.0 (TID 669, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 120
15/08/06 17:45:06 INFO Executor: Running task 52.0 in stage 11.0 (TID 669)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:06 INFO TaskSetManager: Finished task 35.0 in stage 11.0 (TID 652) in 191 ms on localhost (37/200)
15/08/06 17:45:06 INFO Executor: Finished task 42.0 in stage 11.0 (TID 659). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 53.0 in stage 11.0 (TID 670, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO Executor: Running task 53.0 in stage 11.0 (TID 670)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 129
15/08/06 17:45:06 INFO TaskSetManager: Finished task 42.0 in stage 11.0 (TID 659) in 156 ms on localhost (38/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO Executor: Finished task 33.0 in stage 11.0 (TID 650). 2073 bytes result sent to driver
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Starting task 54.0 in stage 11.0 (TID 671, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 54.0 in stage 11.0 (TID 671)
15/08/06 17:45:06 INFO Executor: Finished task 37.0 in stage 11.0 (TID 654). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Finished task 33.0 in stage 11.0 (TID 650) in 206 ms on localhost (39/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 55.0 in stage 11.0 (TID 672, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 55.0 in stage 11.0 (TID 672)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 37.0 in stage 11.0 (TID 654) in 195 ms on localhost (40/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO Executor: Finished task 41.0 in stage 11.0 (TID 658). 2094 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 56.0 in stage 11.0 (TID 673, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 56.0 in stage 11.0 (TID 673)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 41.0 in stage 11.0 (TID 658) in 172 ms on localhost (41/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 135
15/08/06 17:45:06 INFO Executor: Finished task 43.0 in stage 11.0 (TID 660). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Starting task 57.0 in stage 11.0 (TID 674, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 57.0 in stage 11.0 (TID 674)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 43.0 in stage 11.0 (TID 660) in 172 ms on localhost (42/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Finished task 40.0 in stage 11.0 (TID 657). 2093 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:45:06 INFO TaskSetManager: Starting task 58.0 in stage 11.0 (TID 675, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 58.0 in stage 11.0 (TID 675)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 40.0 in stage 11.0 (TID 657) in 198 ms on localhost (43/200)
15/08/06 17:45:06 INFO Executor: Finished task 36.0 in stage 11.0 (TID 653). 2112 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 59.0 in stage 11.0 (TID 676, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Finished task 36.0 in stage 11.0 (TID 653) in 228 ms on localhost (44/200)
15/08/06 17:45:06 INFO Executor: Finished task 44.0 in stage 11.0 (TID 661). 2148 bytes result sent to driver
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 60.0 in stage 11.0 (TID 677, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Finished task 47.0 in stage 11.0 (TID 664). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Running task 60.0 in stage 11.0 (TID 677)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 44.0 in stage 11.0 (TID 661) in 175 ms on localhost (45/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 61.0 in stage 11.0 (TID 678, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 141
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 47.0 in stage 11.0 (TID 664) in 159 ms on localhost (46/200)
15/08/06 17:45:06 INFO Executor: Running task 61.0 in stage 11.0 (TID 678)
15/08/06 17:45:06 INFO Executor: Finished task 46.0 in stage 11.0 (TID 663). 2055 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Running task 59.0 in stage 11.0 (TID 676)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 62.0 in stage 11.0 (TID 679, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO Executor: Running task 62.0 in stage 11.0 (TID 679)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 46.0 in stage 11.0 (TID 663) in 165 ms on localhost (47/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:45:06 INFO Executor: Finished task 45.0 in stage 11.0 (TID 662). 2001 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 63.0 in stage 11.0 (TID 680, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 63.0 in stage 11.0 (TID 680)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 45.0 in stage 11.0 (TID 662) in 228 ms on localhost (48/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:06 INFO Executor: Finished task 50.0 in stage 11.0 (TID 667). 2055 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 64.0 in stage 11.0 (TID 681, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO Executor: Running task 64.0 in stage 11.0 (TID 681)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 50.0 in stage 11.0 (TID 667) in 156 ms on localhost (49/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Finished task 52.0 in stage 11.0 (TID 669). 2019 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 65.0 in stage 11.0 (TID 682, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 65.0 in stage 11.0 (TID 682)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:45:06 INFO TaskSetManager: Finished task 52.0 in stage 11.0 (TID 669) in 155 ms on localhost (50/200)
15/08/06 17:45:06 INFO Executor: Finished task 48.0 in stage 11.0 (TID 665). 2147 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 66.0 in stage 11.0 (TID 683, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 66.0 in stage 11.0 (TID 683)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 48.0 in stage 11.0 (TID 665) in 184 ms on localhost (51/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 137
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 121
15/08/06 17:45:06 INFO Executor: Finished task 51.0 in stage 11.0 (TID 668). 2072 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Finished task 54.0 in stage 11.0 (TID 671). 2093 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 67.0 in stage 11.0 (TID 684, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO Executor: Running task 67.0 in stage 11.0 (TID 684)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 68.0 in stage 11.0 (TID 685, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 68.0 in stage 11.0 (TID 685)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 103
15/08/06 17:45:06 INFO TaskSetManager: Finished task 51.0 in stage 11.0 (TID 668) in 193 ms on localhost (52/200)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 54.0 in stage 11.0 (TID 671) in 179 ms on localhost (53/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 95
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO Executor: Finished task 49.0 in stage 11.0 (TID 666). 2019 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 55.0 in stage 11.0 (TID 672). 2001 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 69.0 in stage 11.0 (TID 686, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO Executor: Running task 69.0 in stage 11.0 (TID 686)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 49.0 in stage 11.0 (TID 666) in 208 ms on localhost (54/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Starting task 70.0 in stage 11.0 (TID 687, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 70.0 in stage 11.0 (TID 687)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 55.0 in stage 11.0 (TID 672) in 184 ms on localhost (55/200)
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 118
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO Executor: Finished task 53.0 in stage 11.0 (TID 670). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 71.0 in stage 11.0 (TID 688, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 71.0 in stage 11.0 (TID 688)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 53.0 in stage 11.0 (TID 670) in 195 ms on localhost (56/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 133
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 203
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 145
15/08/06 17:45:06 INFO Executor: Finished task 56.0 in stage 11.0 (TID 673). 2019 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 60.0 in stage 11.0 (TID 677). 2111 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 61.0 in stage 11.0 (TID 678). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO TaskSetManager: Starting task 72.0 in stage 11.0 (TID 689, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 72.0 in stage 11.0 (TID 689)
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 56.0 in stage 11.0 (TID 673) in 270 ms on localhost (57/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Starting task 73.0 in stage 11.0 (TID 690, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 73.0 in stage 11.0 (TID 690)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 60.0 in stage 11.0 (TID 677) in 248 ms on localhost (58/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 74.0 in stage 11.0 (TID 691, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO Executor: Running task 74.0 in stage 11.0 (TID 691)
15/08/06 17:45:06 INFO Executor: Finished task 57.0 in stage 11.0 (TID 674). 2112 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Finished task 61.0 in stage 11.0 (TID 678) in 248 ms on localhost (59/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:06 INFO Executor: Finished task 59.0 in stage 11.0 (TID 676). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 75.0 in stage 11.0 (TID 692, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Finished task 57.0 in stage 11.0 (TID 674) in 264 ms on localhost (60/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 76.0 in stage 11.0 (TID 693, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO Executor: Running task 75.0 in stage 11.0 (TID 692)
15/08/06 17:45:06 INFO Executor: Running task 76.0 in stage 11.0 (TID 693)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 59.0 in stage 11.0 (TID 676) in 259 ms on localhost (61/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 112
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Finished task 58.0 in stage 11.0 (TID 675). 2037 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:06 INFO TaskSetManager: Starting task 77.0 in stage 11.0 (TID 694, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 77.0 in stage 11.0 (TID 694)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:45:06 INFO TaskSetManager: Finished task 58.0 in stage 11.0 (TID 675) in 272 ms on localhost (62/200)
15/08/06 17:45:06 INFO Executor: Finished task 62.0 in stage 11.0 (TID 679). 2111 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 78.0 in stage 11.0 (TID 695, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 78.0 in stage 11.0 (TID 695)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 62.0 in stage 11.0 (TID 679) in 263 ms on localhost (63/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO Executor: Finished task 63.0 in stage 11.0 (TID 680). 2019 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 79.0 in stage 11.0 (TID 696, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 63.0 in stage 11.0 (TID 680) in 218 ms on localhost (64/200)
15/08/06 17:45:06 INFO Executor: Running task 79.0 in stage 11.0 (TID 696)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 159
15/08/06 17:45:06 INFO Executor: Finished task 65.0 in stage 11.0 (TID 682). 2074 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Starting task 80.0 in stage 11.0 (TID 697, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:45:06 INFO Executor: Running task 80.0 in stage 11.0 (TID 697)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Finished task 65.0 in stage 11.0 (TID 682) in 238 ms on localhost (65/200)
15/08/06 17:45:06 INFO Executor: Finished task 66.0 in stage 11.0 (TID 683). 2220 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:45:06 INFO TaskSetManager: Starting task 81.0 in stage 11.0 (TID 698, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 81.0 in stage 11.0 (TID 698)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 66.0 in stage 11.0 (TID 683) in 234 ms on localhost (66/200)
15/08/06 17:45:06 INFO Executor: Finished task 64.0 in stage 11.0 (TID 681). 2056 bytes result sent to driver
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 82.0 in stage 11.0 (TID 699, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 82.0 in stage 11.0 (TID 699)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 64.0 in stage 11.0 (TID 681) in 261 ms on localhost (67/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 122
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:45:06 INFO Executor: Finished task 67.0 in stage 11.0 (TID 684). 2038 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:45:06 INFO Executor: Finished task 68.0 in stage 11.0 (TID 685). 2038 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 83.0 in stage 11.0 (TID 700, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 83.0 in stage 11.0 (TID 700)
15/08/06 17:45:06 INFO Executor: Finished task 71.0 in stage 11.0 (TID 688). 2074 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 72.0 in stage 11.0 (TID 689). 2038 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Finished task 67.0 in stage 11.0 (TID 684) in 253 ms on localhost (68/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Starting task 84.0 in stage 11.0 (TID 701, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 84.0 in stage 11.0 (TID 701)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 85.0 in stage 11.0 (TID 702, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 85.0 in stage 11.0 (TID 702)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 86.0 in stage 11.0 (TID 703, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 86.0 in stage 11.0 (TID 703)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO TaskSetManager: Finished task 68.0 in stage 11.0 (TID 685) in 258 ms on localhost (69/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 71.0 in stage 11.0 (TID 688) in 245 ms on localhost (70/200)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 72.0 in stage 11.0 (TID 689) in 157 ms on localhost (71/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:45:06 INFO Executor: Finished task 70.0 in stage 11.0 (TID 687). 2020 bytes result sent to driver
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Starting task 87.0 in stage 11.0 (TID 704, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 87.0 in stage 11.0 (TID 704)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 70.0 in stage 11.0 (TID 687) in 270 ms on localhost (72/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:45:06 INFO Executor: Finished task 73.0 in stage 11.0 (TID 690). 2055 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO TaskSetManager: Starting task 88.0 in stage 11.0 (TID 705, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 88.0 in stage 11.0 (TID 705)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:06 INFO TaskSetManager: Finished task 73.0 in stage 11.0 (TID 690) in 185 ms on localhost (73/200)
15/08/06 17:45:06 INFO Executor: Finished task 69.0 in stage 11.0 (TID 686). 2056 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 89.0 in stage 11.0 (TID 706, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 69.0 in stage 11.0 (TID 686) in 288 ms on localhost (74/200)
15/08/06 17:45:06 INFO Executor: Running task 89.0 in stage 11.0 (TID 706)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 191
15/08/06 17:45:06 INFO Executor: Finished task 75.0 in stage 11.0 (TID 692). 2131 bytes result sent to driver
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO TaskSetManager: Starting task 90.0 in stage 11.0 (TID 707, localhost, ANY, 1823 bytes)
15/08/06 17:45:06 INFO Executor: Finished task 78.0 in stage 11.0 (TID 695). 2056 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Running task 90.0 in stage 11.0 (TID 707)
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 75.0 in stage 11.0 (TID 692) in 207 ms on localhost (75/200)
15/08/06 17:45:06 INFO Executor: Finished task 76.0 in stage 11.0 (TID 693). 2038 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 74.0 in stage 11.0 (TID 691). 2074 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 91.0 in stage 11.0 (TID 708, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 91.0 in stage 11.0 (TID 708)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 78.0 in stage 11.0 (TID 695) in 193 ms on localhost (76/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 92.0 in stage 11.0 (TID 709, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 92.0 in stage 11.0 (TID 709)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 76.0 in stage 11.0 (TID 693) in 206 ms on localhost (77/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 93.0 in stage 11.0 (TID 710, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 93.0 in stage 11.0 (TID 710)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO TaskSetManager: Finished task 74.0 in stage 11.0 (TID 691) in 214 ms on localhost (78/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:06 INFO Executor: Finished task 79.0 in stage 11.0 (TID 696). 2038 bytes result sent to driver
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:45:06 INFO TaskSetManager: Starting task 94.0 in stage 11.0 (TID 711, localhost, ANY, 1823 bytes)
15/08/06 17:45:06 INFO Executor: Running task 94.0 in stage 11.0 (TID 711)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 79.0 in stage 11.0 (TID 696) in 198 ms on localhost (79/200)
15/08/06 17:45:06 INFO Executor: Finished task 77.0 in stage 11.0 (TID 694). 2020 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 95.0 in stage 11.0 (TID 712, localhost, ANY, 1827 bytes)
15/08/06 17:45:06 INFO Executor: Running task 95.0 in stage 11.0 (TID 712)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO TaskSetManager: Finished task 77.0 in stage 11.0 (TID 694) in 209 ms on localhost (80/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:45:06 INFO Executor: Finished task 81.0 in stage 11.0 (TID 698). 2055 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 80.0 in stage 11.0 (TID 697). 2073 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 96.0 in stage 11.0 (TID 713, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 81.0 in stage 11.0 (TID 698) in 162 ms on localhost (81/200)
15/08/06 17:45:06 INFO Executor: Running task 96.0 in stage 11.0 (TID 713)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 97.0 in stage 11.0 (TID 714, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO Executor: Running task 97.0 in stage 11.0 (TID 714)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 80.0 in stage 11.0 (TID 697) in 168 ms on localhost (82/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:06 INFO Executor: Finished task 82.0 in stage 11.0 (TID 699). 2130 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 98.0 in stage 11.0 (TID 715, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 98.0 in stage 11.0 (TID 715)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 82.0 in stage 11.0 (TID 699) in 170 ms on localhost (83/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Finished task 85.0 in stage 11.0 (TID 702). 2113 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Finished task 83.0 in stage 11.0 (TID 700). 2056 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 99.0 in stage 11.0 (TID 716, localhost, ANY, 1826 bytes)
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 141
15/08/06 17:45:06 INFO Executor: Running task 99.0 in stage 11.0 (TID 716)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 85.0 in stage 11.0 (TID 702) in 153 ms on localhost (84/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 100.0 in stage 11.0 (TID 717, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Finished task 86.0 in stage 11.0 (TID 703). 2074 bytes result sent to driver
15/08/06 17:45:06 INFO Executor: Running task 100.0 in stage 11.0 (TID 717)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 83.0 in stage 11.0 (TID 700) in 161 ms on localhost (85/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 101.0 in stage 11.0 (TID 718, localhost, ANY, 1823 bytes)
15/08/06 17:45:06 INFO Executor: Running task 101.0 in stage 11.0 (TID 718)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 86.0 in stage 11.0 (TID 703) in 160 ms on localhost (86/200)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:45:06 INFO Executor: Finished task 84.0 in stage 11.0 (TID 701). 2002 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Starting task 102.0 in stage 11.0 (TID 719, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 102.0 in stage 11.0 (TID 719)
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Finished task 84.0 in stage 11.0 (TID 701) in 175 ms on localhost (87/200)
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 200
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO Executor: Finished task 88.0 in stage 11.0 (TID 705). 2095 bytes result sent to driver
15/08/06 17:45:06 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/06 17:45:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:06 INFO TaskSetManager: Starting task 103.0 in stage 11.0 (TID 720, localhost, ANY, 1824 bytes)
15/08/06 17:45:06 INFO Executor: Running task 103.0 in stage 11.0 (TID 720)
15/08/06 17:45:06 INFO Executor: Finished task 87.0 in stage 11.0 (TID 704). 2113 bytes result sent to driver
15/08/06 17:45:06 INFO TaskSetManager: Finished task 88.0 in stage 11.0 (TID 705) in 159 ms on localhost (88/200)
15/08/06 17:45:06 INFO TaskSetManager: Starting task 104.0 in stage 11.0 (TID 721, localhost, ANY, 1825 bytes)
15/08/06 17:45:06 INFO Executor: Running task 104.0 in stage 11.0 (TID 721)
15/08/06 17:45:06 INFO TaskSetManager: Finished task 87.0 in stage 11.0 (TID 704) in 175 ms on localhost (89/200)
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:06 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 173
15/08/06 17:45:07 INFO Executor: Finished task 89.0 in stage 11.0 (TID 706). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 90.0 in stage 11.0 (TID 707). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 105.0 in stage 11.0 (TID 722, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 105.0 in stage 11.0 (TID 722)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 89.0 in stage 11.0 (TID 706) in 169 ms on localhost (90/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 106.0 in stage 11.0 (TID 723, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 106.0 in stage 11.0 (TID 723)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Finished task 90.0 in stage 11.0 (TID 707) in 147 ms on localhost (91/200)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 119
15/08/06 17:45:07 INFO Executor: Finished task 91.0 in stage 11.0 (TID 708). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 107.0 in stage 11.0 (TID 724, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 107.0 in stage 11.0 (TID 724)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Finished task 91.0 in stage 11.0 (TID 708) in 160 ms on localhost (92/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 196
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO Executor: Finished task 94.0 in stage 11.0 (TID 711). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:07 INFO TaskSetManager: Starting task 108.0 in stage 11.0 (TID 725, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 108.0 in stage 11.0 (TID 725)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 94.0 in stage 11.0 (TID 711) in 155 ms on localhost (93/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 95.0 in stage 11.0 (TID 712). 2131 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 109.0 in stage 11.0 (TID 726, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 109.0 in stage 11.0 (TID 726)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO TaskSetManager: Finished task 95.0 in stage 11.0 (TID 712) in 158 ms on localhost (94/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO Executor: Finished task 93.0 in stage 11.0 (TID 710). 2055 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/06 17:45:07 INFO TaskSetManager: Starting task 110.0 in stage 11.0 (TID 727, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 INFO Executor: Running task 110.0 in stage 11.0 (TID 727)
15/08/06 17:45:07 INFO Executor: Finished task 92.0 in stage 11.0 (TID 709). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Finished task 93.0 in stage 11.0 (TID 710) in 188 ms on localhost (95/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 111.0 in stage 11.0 (TID 728, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 111.0 in stage 11.0 (TID 728)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 92.0 in stage 11.0 (TID 709) in 196 ms on localhost (96/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/06 17:45:07 INFO Executor: Finished task 96.0 in stage 11.0 (TID 713). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 112.0 in stage 11.0 (TID 729, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 112.0 in stage 11.0 (TID 729)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Finished task 96.0 in stage 11.0 (TID 713) in 159 ms on localhost (97/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO Executor: Finished task 97.0 in stage 11.0 (TID 714). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 113.0 in stage 11.0 (TID 730, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 113.0 in stage 11.0 (TID 730)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 97.0 in stage 11.0 (TID 714) in 163 ms on localhost (98/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 162
15/08/06 17:45:07 INFO Executor: Finished task 99.0 in stage 11.0 (TID 716). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Starting task 114.0 in stage 11.0 (TID 731, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:07 INFO Executor: Running task 114.0 in stage 11.0 (TID 731)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 99.0 in stage 11.0 (TID 716) in 152 ms on localhost (99/200)
15/08/06 17:45:07 INFO Executor: Finished task 100.0 in stage 11.0 (TID 717). 2147 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 115.0 in stage 11.0 (TID 732, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 115.0 in stage 11.0 (TID 732)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 100.0 in stage 11.0 (TID 717) in 153 ms on localhost (100/200)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:07 INFO Executor: Finished task 98.0 in stage 11.0 (TID 715). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 116.0 in stage 11.0 (TID 733, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 116.0 in stage 11.0 (TID 733)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 98.0 in stage 11.0 (TID 715) in 194 ms on localhost (101/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 174
15/08/06 17:45:07 INFO Executor: Finished task 101.0 in stage 11.0 (TID 718). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO TaskSetManager: Starting task 117.0 in stage 11.0 (TID 734, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Running task 117.0 in stage 11.0 (TID 734)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 101.0 in stage 11.0 (TID 718) in 176 ms on localhost (102/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 103.0 in stage 11.0 (TID 720). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 118.0 in stage 11.0 (TID 735, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 118.0 in stage 11.0 (TID 735)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 103.0 in stage 11.0 (TID 720) in 165 ms on localhost (103/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:07 INFO Executor: Finished task 102.0 in stage 11.0 (TID 719). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 119.0 in stage 11.0 (TID 736, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 INFO Executor: Running task 119.0 in stage 11.0 (TID 736)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 102.0 in stage 11.0 (TID 719) in 190 ms on localhost (104/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 176
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 104.0 in stage 11.0 (TID 721). 2073 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 120.0 in stage 11.0 (TID 737, localhost, ANY, 1823 bytes)
15/08/06 17:45:07 INFO Executor: Running task 120.0 in stage 11.0 (TID 737)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 104.0 in stage 11.0 (TID 721) in 180 ms on localhost (105/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO Executor: Finished task 105.0 in stage 11.0 (TID 722). 2094 bytes result sent to driver
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 106.0 in stage 11.0 (TID 723). 2167 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 121.0 in stage 11.0 (TID 738, localhost, ANY, 1823 bytes)
15/08/06 17:45:07 INFO Executor: Running task 121.0 in stage 11.0 (TID 738)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 167
15/08/06 17:45:07 INFO TaskSetManager: Finished task 105.0 in stage 11.0 (TID 722) in 176 ms on localhost (106/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 122.0 in stage 11.0 (TID 739, localhost, ANY, 1823 bytes)
15/08/06 17:45:07 INFO Executor: Finished task 107.0 in stage 11.0 (TID 724). 2113 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Running task 122.0 in stage 11.0 (TID 739)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 106.0 in stage 11.0 (TID 723) in 175 ms on localhost (107/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Starting task 123.0 in stage 11.0 (TID 740, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 123.0 in stage 11.0 (TID 740)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 107.0 in stage 11.0 (TID 724) in 162 ms on localhost (108/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 147
15/08/06 17:45:07 INFO Executor: Finished task 108.0 in stage 11.0 (TID 725). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 124.0 in stage 11.0 (TID 741, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 INFO Executor: Running task 124.0 in stage 11.0 (TID 741)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 108.0 in stage 11.0 (TID 725) in 170 ms on localhost (109/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:07 INFO Executor: Finished task 109.0 in stage 11.0 (TID 726). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 125.0 in stage 11.0 (TID 742, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 125.0 in stage 11.0 (TID 742)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 109.0 in stage 11.0 (TID 726) in 171 ms on localhost (110/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:07 INFO Executor: Finished task 110.0 in stage 11.0 (TID 727). 2149 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 111.0 in stage 11.0 (TID 728). 2002 bytes result sent to driver
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Starting task 126.0 in stage 11.0 (TID 743, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 INFO Executor: Running task 126.0 in stage 11.0 (TID 743)
15/08/06 17:45:07 INFO Executor: Finished task 112.0 in stage 11.0 (TID 729). 2055 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Finished task 110.0 in stage 11.0 (TID 727) in 180 ms on localhost (111/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:07 INFO TaskSetManager: Starting task 127.0 in stage 11.0 (TID 744, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 127.0 in stage 11.0 (TID 744)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 111.0 in stage 11.0 (TID 728) in 184 ms on localhost (112/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 128.0 in stage 11.0 (TID 745, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 128.0 in stage 11.0 (TID 745)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 112.0 in stage 11.0 (TID 729) in 174 ms on localhost (113/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 128
15/08/06 17:45:07 INFO Executor: Finished task 113.0 in stage 11.0 (TID 730). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 115.0 in stage 11.0 (TID 732). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 129.0 in stage 11.0 (TID 746, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 129.0 in stage 11.0 (TID 746)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 113.0 in stage 11.0 (TID 730) in 177 ms on localhost (114/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 130.0 in stage 11.0 (TID 747, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 130.0 in stage 11.0 (TID 747)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 115.0 in stage 11.0 (TID 732) in 160 ms on localhost (115/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 116.0 in stage 11.0 (TID 733). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 114.0 in stage 11.0 (TID 731). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 131.0 in stage 11.0 (TID 748, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 131.0 in stage 11.0 (TID 748)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 116.0 in stage 11.0 (TID 733) in 175 ms on localhost (116/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 132.0 in stage 11.0 (TID 749, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Running task 132.0 in stage 11.0 (TID 749)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 114.0 in stage 11.0 (TID 731) in 195 ms on localhost (117/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:45:07 INFO Executor: Finished task 117.0 in stage 11.0 (TID 734). 2113 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 133.0 in stage 11.0 (TID 750, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 133.0 in stage 11.0 (TID 750)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 117.0 in stage 11.0 (TID 734) in 172 ms on localhost (118/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/06 17:45:07 INFO Executor: Finished task 118.0 in stage 11.0 (TID 735). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 134.0 in stage 11.0 (TID 751, localhost, ANY, 1827 bytes)
15/08/06 17:45:07 INFO Executor: Running task 134.0 in stage 11.0 (TID 751)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 118.0 in stage 11.0 (TID 735) in 218 ms on localhost (119/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 120.0 in stage 11.0 (TID 737). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Starting task 135.0 in stage 11.0 (TID 752, localhost, ANY, 1827 bytes)
15/08/06 17:45:07 INFO Executor: Finished task 119.0 in stage 11.0 (TID 736). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Running task 135.0 in stage 11.0 (TID 752)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 120.0 in stage 11.0 (TID 737) in 225 ms on localhost (120/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 136.0 in stage 11.0 (TID 753, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 136.0 in stage 11.0 (TID 753)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 119.0 in stage 11.0 (TID 736) in 236 ms on localhost (121/200)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 180
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO Executor: Finished task 123.0 in stage 11.0 (TID 740). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/06 17:45:07 INFO TaskSetManager: Starting task 137.0 in stage 11.0 (TID 754, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 137.0 in stage 11.0 (TID 754)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 123.0 in stage 11.0 (TID 740) in 227 ms on localhost (122/200)
15/08/06 17:45:07 INFO Executor: Finished task 121.0 in stage 11.0 (TID 738). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Starting task 138.0 in stage 11.0 (TID 755, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 138.0 in stage 11.0 (TID 755)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 121.0 in stage 11.0 (TID 738) in 233 ms on localhost (123/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 123
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO Executor: Finished task 125.0 in stage 11.0 (TID 742). 2002 bytes result sent to driver
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Starting task 139.0 in stage 11.0 (TID 756, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 139.0 in stage 11.0 (TID 756)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 125.0 in stage 11.0 (TID 742) in 214 ms on localhost (124/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:45:07 INFO Executor: Finished task 124.0 in stage 11.0 (TID 741). 2130 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 140.0 in stage 11.0 (TID 757, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 140.0 in stage 11.0 (TID 757)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:45:07 INFO TaskSetManager: Finished task 124.0 in stage 11.0 (TID 741) in 237 ms on localhost (125/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO Executor: Finished task 122.0 in stage 11.0 (TID 739). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 141.0 in stage 11.0 (TID 758, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 INFO Executor: Running task 141.0 in stage 11.0 (TID 758)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO TaskSetManager: Finished task 122.0 in stage 11.0 (TID 739) in 256 ms on localhost (126/200)
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO Executor: Finished task 128.0 in stage 11.0 (TID 745). 2149 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:45:07 INFO TaskSetManager: Starting task 142.0 in stage 11.0 (TID 759, localhost, ANY, 1823 bytes)
15/08/06 17:45:07 INFO Executor: Running task 142.0 in stage 11.0 (TID 759)
15/08/06 17:45:07 INFO Executor: Finished task 127.0 in stage 11.0 (TID 744). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Finished task 128.0 in stage 11.0 (TID 745) in 211 ms on localhost (127/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 143.0 in stage 11.0 (TID 760, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 143.0 in stage 11.0 (TID 760)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 127.0 in stage 11.0 (TID 744) in 227 ms on localhost (128/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 163
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:45:07 INFO Executor: Finished task 126.0 in stage 11.0 (TID 743). 2094 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 144.0 in stage 11.0 (TID 761, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 144.0 in stage 11.0 (TID 761)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:07 INFO Executor: Finished task 129.0 in stage 11.0 (TID 746). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Finished task 126.0 in stage 11.0 (TID 743) in 247 ms on localhost (129/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:45:07 INFO TaskSetManager: Starting task 145.0 in stage 11.0 (TID 762, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 145.0 in stage 11.0 (TID 762)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 129.0 in stage 11.0 (TID 746) in 226 ms on localhost (130/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO Executor: Finished task 130.0 in stage 11.0 (TID 747). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 146.0 in stage 11.0 (TID 763, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 146.0 in stage 11.0 (TID 763)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO TaskSetManager: Finished task 130.0 in stage 11.0 (TID 747) in 219 ms on localhost (131/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 133.0 in stage 11.0 (TID 750). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:45:07 INFO TaskSetManager: Starting task 147.0 in stage 11.0 (TID 764, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 147.0 in stage 11.0 (TID 764)
15/08/06 17:45:07 INFO Executor: Finished task 131.0 in stage 11.0 (TID 748). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Finished task 133.0 in stage 11.0 (TID 750) in 206 ms on localhost (132/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 148.0 in stage 11.0 (TID 765, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 148.0 in stage 11.0 (TID 765)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 131.0 in stage 11.0 (TID 748) in 219 ms on localhost (133/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 108
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 132.0 in stage 11.0 (TID 749). 2002 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 149.0 in stage 11.0 (TID 766, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 149.0 in stage 11.0 (TID 766)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 132.0 in stage 11.0 (TID 749) in 227 ms on localhost (134/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 139
15/08/06 17:45:07 INFO Executor: Finished task 134.0 in stage 11.0 (TID 751). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 150.0 in stage 11.0 (TID 767, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 150.0 in stage 11.0 (TID 767)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 134.0 in stage 11.0 (TID 751) in 164 ms on localhost (135/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 130
15/08/06 17:45:07 INFO Executor: Finished task 139.0 in stage 11.0 (TID 756). 2002 bytes result sent to driver
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO Executor: Finished task 136.0 in stage 11.0 (TID 753). 2038 bytes result sent to driver
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 135.0 in stage 11.0 (TID 752). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 151.0 in stage 11.0 (TID 768, localhost, ANY, 1823 bytes)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 139.0 in stage 11.0 (TID 756) in 162 ms on localhost (136/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 152.0 in stage 11.0 (TID 769, localhost, ANY, 1827 bytes)
15/08/06 17:45:07 INFO Executor: Running task 152.0 in stage 11.0 (TID 769)
15/08/06 17:45:07 INFO Executor: Running task 151.0 in stage 11.0 (TID 768)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Finished task 136.0 in stage 11.0 (TID 753) in 182 ms on localhost (137/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 153.0 in stage 11.0 (TID 770, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 153.0 in stage 11.0 (TID 770)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Finished task 135.0 in stage 11.0 (TID 752) in 187 ms on localhost (138/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO Executor: Finished task 137.0 in stage 11.0 (TID 754). 2113 bytes result sent to driver
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO TaskSetManager: Starting task 154.0 in stage 11.0 (TID 771, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 INFO Executor: Running task 154.0 in stage 11.0 (TID 771)
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Finished task 137.0 in stage 11.0 (TID 754) in 180 ms on localhost (139/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:45:07 INFO Executor: Finished task 141.0 in stage 11.0 (TID 758). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Starting task 155.0 in stage 11.0 (TID 772, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 155.0 in stage 11.0 (TID 772)
15/08/06 17:45:07 INFO Executor: Finished task 138.0 in stage 11.0 (TID 755). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:45:07 INFO TaskSetManager: Finished task 141.0 in stage 11.0 (TID 758) in 163 ms on localhost (140/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 156.0 in stage 11.0 (TID 773, localhost, ANY, 1827 bytes)
15/08/06 17:45:07 INFO Executor: Running task 156.0 in stage 11.0 (TID 773)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 138.0 in stage 11.0 (TID 755) in 190 ms on localhost (141/200)
15/08/06 17:45:07 INFO Executor: Finished task 140.0 in stage 11.0 (TID 757). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Starting task 157.0 in stage 11.0 (TID 774, localhost, ANY, 1827 bytes)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO TaskSetManager: Finished task 140.0 in stage 11.0 (TID 757) in 173 ms on localhost (142/200)
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Running task 157.0 in stage 11.0 (TID 774)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO Executor: Finished task 143.0 in stage 11.0 (TID 760). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Starting task 158.0 in stage 11.0 (TID 775, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:45:07 INFO Executor: Running task 158.0 in stage 11.0 (TID 775)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:07 INFO TaskSetManager: Finished task 143.0 in stage 11.0 (TID 760) in 155 ms on localhost (143/200)
15/08/06 17:45:07 INFO Executor: Finished task 144.0 in stage 11.0 (TID 761). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 126
15/08/06 17:45:07 INFO Executor: Finished task 142.0 in stage 11.0 (TID 759). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 159.0 in stage 11.0 (TID 776, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO Executor: Finished task 146.0 in stage 11.0 (TID 763). 2113 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Running task 159.0 in stage 11.0 (TID 776)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO TaskSetManager: Finished task 144.0 in stage 11.0 (TID 761) in 144 ms on localhost (144/200)
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Starting task 160.0 in stage 11.0 (TID 777, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 160.0 in stage 11.0 (TID 777)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 142.0 in stage 11.0 (TID 759) in 167 ms on localhost (145/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO TaskSetManager: Starting task 161.0 in stage 11.0 (TID 778, localhost, ANY, 1827 bytes)
15/08/06 17:45:07 INFO Executor: Running task 161.0 in stage 11.0 (TID 778)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 146.0 in stage 11.0 (TID 763) in 142 ms on localhost (146/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:45:07 INFO Executor: Finished task 145.0 in stage 11.0 (TID 762). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 162.0 in stage 11.0 (TID 779, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 162.0 in stage 11.0 (TID 779)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 145.0 in stage 11.0 (TID 762) in 156 ms on localhost (147/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 148.0 in stage 11.0 (TID 765). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 163.0 in stage 11.0 (TID 780, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 163.0 in stage 11.0 (TID 780)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 148.0 in stage 11.0 (TID 765) in 133 ms on localhost (148/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 178
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO Executor: Finished task 147.0 in stage 11.0 (TID 764). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 164.0 in stage 11.0 (TID 781, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 164.0 in stage 11.0 (TID 781)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 108
15/08/06 17:45:07 INFO TaskSetManager: Finished task 147.0 in stage 11.0 (TID 764) in 146 ms on localhost (149/200)
15/08/06 17:45:07 INFO Executor: Finished task 149.0 in stage 11.0 (TID 766). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 165.0 in stage 11.0 (TID 782, localhost, ANY, 1827 bytes)
15/08/06 17:45:07 INFO Executor: Running task 165.0 in stage 11.0 (TID 782)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 149.0 in stage 11.0 (TID 766) in 137 ms on localhost (150/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/06 17:45:07 INFO Executor: Finished task 150.0 in stage 11.0 (TID 767). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 166.0 in stage 11.0 (TID 783, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 166.0 in stage 11.0 (TID 783)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 150.0 in stage 11.0 (TID 767) in 163 ms on localhost (151/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:45:07 INFO Executor: Finished task 154.0 in stage 11.0 (TID 771). 2130 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 167.0 in stage 11.0 (TID 784, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 167.0 in stage 11.0 (TID 784)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 154.0 in stage 11.0 (TID 771) in 130 ms on localhost (152/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:45:07 INFO Executor: Finished task 152.0 in stage 11.0 (TID 769). 1913 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 168.0 in stage 11.0 (TID 785, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 168.0 in stage 11.0 (TID 785)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Finished task 152.0 in stage 11.0 (TID 769) in 159 ms on localhost (153/200)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 190
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO Executor: Finished task 157.0 in stage 11.0 (TID 774). 2113 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:45:07 INFO Executor: Finished task 151.0 in stage 11.0 (TID 768). 2148 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Starting task 169.0 in stage 11.0 (TID 786, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 169.0 in stage 11.0 (TID 786)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO TaskSetManager: Finished task 157.0 in stage 11.0 (TID 774) in 141 ms on localhost (154/200)
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Starting task 170.0 in stage 11.0 (TID 787, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 170.0 in stage 11.0 (TID 787)
15/08/06 17:45:07 INFO Executor: Finished task 153.0 in stage 11.0 (TID 770). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Finished task 151.0 in stage 11.0 (TID 768) in 171 ms on localhost (155/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Starting task 171.0 in stage 11.0 (TID 788, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 171.0 in stage 11.0 (TID 788)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:07 INFO TaskSetManager: Finished task 153.0 in stage 11.0 (TID 770) in 169 ms on localhost (156/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO Executor: Finished task 155.0 in stage 11.0 (TID 772). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Starting task 172.0 in stage 11.0 (TID 789, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 172.0 in stage 11.0 (TID 789)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 155.0 in stage 11.0 (TID 772) in 157 ms on localhost (157/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 138
15/08/06 17:45:07 INFO Executor: Finished task 156.0 in stage 11.0 (TID 773). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Starting task 173.0 in stage 11.0 (TID 790, localhost, ANY, 1823 bytes)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:45:07 INFO Executor: Running task 173.0 in stage 11.0 (TID 790)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 156.0 in stage 11.0 (TID 773) in 172 ms on localhost (158/200)
15/08/06 17:45:07 INFO Executor: Finished task 159.0 in stage 11.0 (TID 776). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 174.0 in stage 11.0 (TID 791, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 174.0 in stage 11.0 (TID 791)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Finished task 159.0 in stage 11.0 (TID 776) in 154 ms on localhost (159/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO Executor: Finished task 158.0 in stage 11.0 (TID 775). 2074 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 175.0 in stage 11.0 (TID 792, localhost, ANY, 1823 bytes)
15/08/06 17:45:07 INFO Executor: Running task 175.0 in stage 11.0 (TID 792)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 158.0 in stage 11.0 (TID 775) in 178 ms on localhost (160/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO Executor: Finished task 160.0 in stage 11.0 (TID 777). 2020 bytes result sent to driver
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Starting task 176.0 in stage 11.0 (TID 793, localhost, ANY, 1824 bytes)
15/08/06 17:45:07 INFO Executor: Running task 176.0 in stage 11.0 (TID 793)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO TaskSetManager: Finished task 160.0 in stage 11.0 (TID 777) in 185 ms on localhost (161/200)
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 136
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 116
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 155
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 110
15/08/06 17:45:07 INFO Executor: Finished task 162.0 in stage 11.0 (TID 779). 2094 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 161.0 in stage 11.0 (TID 778). 2038 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 164.0 in stage 11.0 (TID 781). 2111 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 163.0 in stage 11.0 (TID 780). 1913 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 177.0 in stage 11.0 (TID 794, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 177.0 in stage 11.0 (TID 794)
15/08/06 17:45:07 INFO Executor: Finished task 165.0 in stage 11.0 (TID 782). 2094 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Finished task 162.0 in stage 11.0 (TID 779) in 189 ms on localhost (162/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 178.0 in stage 11.0 (TID 795, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 178.0 in stage 11.0 (TID 795)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 161.0 in stage 11.0 (TID 778) in 202 ms on localhost (163/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Starting task 179.0 in stage 11.0 (TID 796, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO Executor: Running task 179.0 in stage 11.0 (TID 796)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 164.0 in stage 11.0 (TID 781) in 173 ms on localhost (164/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 180.0 in stage 11.0 (TID 797, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 180.0 in stage 11.0 (TID 797)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 163.0 in stage 11.0 (TID 780) in 187 ms on localhost (165/200)
15/08/06 17:45:07 INFO TaskSetManager: Starting task 181.0 in stage 11.0 (TID 798, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO TaskSetManager: Finished task 165.0 in stage 11.0 (TID 782) in 175 ms on localhost (166/200)
15/08/06 17:45:07 INFO Executor: Running task 181.0 in stage 11.0 (TID 798)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 114
15/08/06 17:45:07 INFO Executor: Finished task 166.0 in stage 11.0 (TID 783). 2073 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Starting task 182.0 in stage 11.0 (TID 799, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/06 17:45:07 INFO Executor: Running task 182.0 in stage 11.0 (TID 799)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 166.0 in stage 11.0 (TID 783) in 186 ms on localhost (167/200)
15/08/06 17:45:07 INFO Executor: Finished task 167.0 in stage 11.0 (TID 784). 2094 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 183.0 in stage 11.0 (TID 800, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 183.0 in stage 11.0 (TID 800)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 167.0 in stage 11.0 (TID 784) in 169 ms on localhost (168/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:07 INFO Executor: Finished task 171.0 in stage 11.0 (TID 788). 2131 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 172.0 in stage 11.0 (TID 789). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 184.0 in stage 11.0 (TID 801, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 184.0 in stage 11.0 (TID 801)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 171.0 in stage 11.0 (TID 788) in 180 ms on localhost (169/200)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Starting task 185.0 in stage 11.0 (TID 802, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 185.0 in stage 11.0 (TID 802)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 186
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO TaskSetManager: Finished task 172.0 in stage 11.0 (TID 789) in 179 ms on localhost (170/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO Executor: Finished task 170.0 in stage 11.0 (TID 787). 2131 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO TaskSetManager: Starting task 186.0 in stage 11.0 (TID 803, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO Executor: Running task 186.0 in stage 11.0 (TID 803)
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:45:07 INFO TaskSetManager: Finished task 170.0 in stage 11.0 (TID 787) in 188 ms on localhost (171/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO Executor: Finished task 168.0 in stage 11.0 (TID 785). 2002 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 187.0 in stage 11.0 (TID 804, localhost, ANY, 1823 bytes)
15/08/06 17:45:07 INFO Executor: Running task 187.0 in stage 11.0 (TID 804)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 168.0 in stage 11.0 (TID 785) in 201 ms on localhost (172/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 155
15/08/06 17:45:07 INFO Executor: Finished task 169.0 in stage 11.0 (TID 786). 2020 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 188.0 in stage 11.0 (TID 805, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO Executor: Running task 188.0 in stage 11.0 (TID 805)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Finished task 169.0 in stage 11.0 (TID 786) in 204 ms on localhost (173/200)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/06 17:45:07 INFO Executor: Finished task 174.0 in stage 11.0 (TID 791). 2130 bytes result sent to driver
15/08/06 17:45:07 INFO Executor: Finished task 173.0 in stage 11.0 (TID 790). 1913 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 189.0 in stage 11.0 (TID 806, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 189.0 in stage 11.0 (TID 806)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 174.0 in stage 11.0 (TID 791) in 196 ms on localhost (174/200)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO TaskSetManager: Starting task 190.0 in stage 11.0 (TID 807, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO Executor: Running task 190.0 in stage 11.0 (TID 807)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 173.0 in stage 11.0 (TID 790) in 205 ms on localhost (175/200)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 196
15/08/06 17:45:07 INFO Executor: Finished task 175.0 in stage 11.0 (TID 792). 2113 bytes result sent to driver
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO TaskSetManager: Starting task 191.0 in stage 11.0 (TID 808, localhost, ANY, 1825 bytes)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/06 17:45:07 INFO Executor: Running task 191.0 in stage 11.0 (TID 808)
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO TaskSetManager: Finished task 175.0 in stage 11.0 (TID 792) in 191 ms on localhost (176/200)
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 197
15/08/06 17:45:07 INFO Executor: Finished task 180.0 in stage 11.0 (TID 797). 2056 bytes result sent to driver
15/08/06 17:45:07 INFO TaskSetManager: Starting task 192.0 in stage 11.0 (TID 809, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 192.0 in stage 11.0 (TID 809)
15/08/06 17:45:07 INFO TaskSetManager: Finished task 180.0 in stage 11.0 (TID 797) in 165 ms on localhost (177/200)
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:07 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:45:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:07 INFO Executor: Finished task 176.0 in stage 11.0 (TID 793). 2095 bytes result sent to driver
15/08/06 17:45:07 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 186
15/08/06 17:45:07 INFO TaskSetManager: Starting task 193.0 in stage 11.0 (TID 810, localhost, ANY, 1826 bytes)
15/08/06 17:45:07 INFO Executor: Running task 193.0 in stage 11.0 (TID 810)
15/08/06 17:45:08 INFO TaskSetManager: Finished task 176.0 in stage 11.0 (TID 793) in 196 ms on localhost (178/200)
15/08/06 17:45:08 INFO Executor: Finished task 177.0 in stage 11.0 (TID 794). 2038 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Starting task 194.0 in stage 11.0 (TID 811, localhost, ANY, 1825 bytes)
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO TaskSetManager: Finished task 177.0 in stage 11.0 (TID 794) in 181 ms on localhost (179/200)
15/08/06 17:45:08 INFO Executor: Running task 194.0 in stage 11.0 (TID 811)
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO Executor: Finished task 179.0 in stage 11.0 (TID 796). 2020 bytes result sent to driver
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 246
15/08/06 17:45:08 INFO Executor: Finished task 178.0 in stage 11.0 (TID 795). 2185 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Starting task 195.0 in stage 11.0 (TID 812, localhost, ANY, 1826 bytes)
15/08/06 17:45:08 INFO Executor: Running task 195.0 in stage 11.0 (TID 812)
15/08/06 17:45:08 INFO TaskSetManager: Finished task 179.0 in stage 11.0 (TID 796) in 188 ms on localhost (180/200)
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:08 INFO TaskSetManager: Starting task 196.0 in stage 11.0 (TID 813, localhost, ANY, 1826 bytes)
15/08/06 17:45:08 INFO Executor: Running task 196.0 in stage 11.0 (TID 813)
15/08/06 17:45:08 INFO TaskSetManager: Finished task 178.0 in stage 11.0 (TID 795) in 194 ms on localhost (181/200)
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO Executor: Finished task 183.0 in stage 11.0 (TID 800). 2113 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Starting task 197.0 in stage 11.0 (TID 814, localhost, ANY, 1826 bytes)
15/08/06 17:45:08 INFO Executor: Running task 197.0 in stage 11.0 (TID 814)
15/08/06 17:45:08 INFO TaskSetManager: Finished task 183.0 in stage 11.0 (TID 800) in 215 ms on localhost (182/200)
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/06 17:45:08 INFO Executor: Finished task 181.0 in stage 11.0 (TID 798). 2038 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Starting task 198.0 in stage 11.0 (TID 815, localhost, ANY, 1824 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Finished task 181.0 in stage 11.0 (TID 798) in 276 ms on localhost (183/200)
15/08/06 17:45:08 INFO Executor: Running task 198.0 in stage 11.0 (TID 815)
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:45:08 INFO Executor: Finished task 186.0 in stage 11.0 (TID 803). 2074 bytes result sent to driver
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO TaskSetManager: Starting task 199.0 in stage 11.0 (TID 816, localhost, ANY, 1826 bytes)
15/08/06 17:45:08 INFO Executor: Running task 199.0 in stage 11.0 (TID 816)
15/08/06 17:45:08 INFO Executor: Finished task 182.0 in stage 11.0 (TID 799). 2056 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Finished task 186.0 in stage 11.0 (TID 803) in 224 ms on localhost (184/200)
15/08/06 17:45:08 INFO TaskSetManager: Finished task 182.0 in stage 11.0 (TID 799) in 279 ms on localhost (185/200)
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:45:08 INFO Executor: Finished task 185.0 in stage 11.0 (TID 802). 2038 bytes result sent to driver
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 194
15/08/06 17:45:08 INFO TaskSetManager: Finished task 185.0 in stage 11.0 (TID 802) in 239 ms on localhost (186/200)
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 145
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO Executor: Finished task 188.0 in stage 11.0 (TID 805). 2002 bytes result sent to driver
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO Executor: Finished task 187.0 in stage 11.0 (TID 804). 2074 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Finished task 188.0 in stage 11.0 (TID 805) in 245 ms on localhost (187/200)
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 184
15/08/06 17:45:08 INFO TaskSetManager: Finished task 187.0 in stage 11.0 (TID 804) in 260 ms on localhost (188/200)
15/08/06 17:45:08 INFO Executor: Finished task 184.0 in stage 11.0 (TID 801). 2020 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Finished task 184.0 in stage 11.0 (TID 801) in 271 ms on localhost (189/200)
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:08 INFO Executor: Finished task 190.0 in stage 11.0 (TID 807). 2038 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Finished task 190.0 in stage 11.0 (TID 807) in 240 ms on localhost (190/200)
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO Executor: Finished task 196.0 in stage 11.0 (TID 813). 2113 bytes result sent to driver
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 150
15/08/06 17:45:08 INFO TaskSetManager: Finished task 196.0 in stage 11.0 (TID 813) in 206 ms on localhost (191/200)
15/08/06 17:45:08 INFO Executor: Finished task 195.0 in stage 11.0 (TID 812). 2074 bytes result sent to driver
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO TaskSetManager: Finished task 195.0 in stage 11.0 (TID 812) in 216 ms on localhost (192/200)
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:08 INFO Executor: Finished task 193.0 in stage 11.0 (TID 810). 2037 bytes result sent to driver
15/08/06 17:45:08 INFO Executor: Finished task 189.0 in stage 11.0 (TID 806). 2055 bytes result sent to driver
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO Executor: Finished task 192.0 in stage 11.0 (TID 809). 2020 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Finished task 193.0 in stage 11.0 (TID 810) in 238 ms on localhost (193/200)
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/06 17:45:08 INFO TaskSetManager: Finished task 189.0 in stage 11.0 (TID 806) in 272 ms on localhost (194/200)
15/08/06 17:45:08 INFO Executor: Finished task 194.0 in stage 11.0 (TID 811). 2054 bytes result sent to driver
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO TaskSetManager: Finished task 192.0 in stage 11.0 (TID 809) in 248 ms on localhost (195/200)
15/08/06 17:45:08 INFO TaskSetManager: Finished task 194.0 in stage 11.0 (TID 811) in 240 ms on localhost (196/200)
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO Executor: Finished task 191.0 in stage 11.0 (TID 808). 2112 bytes result sent to driver
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO TaskSetManager: Finished task 191.0 in stage 11.0 (TID 808) in 268 ms on localhost (197/200)
15/08/06 17:45:08 INFO Executor: Finished task 197.0 in stage 11.0 (TID 814). 2056 bytes result sent to driver
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:45:08 INFO TaskSetManager: Finished task 197.0 in stage 11.0 (TID 814) in 156 ms on localhost (198/200)
15/08/06 17:45:08 INFO Executor: Finished task 198.0 in stage 11.0 (TID 815). 2095 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Finished task 198.0 in stage 11.0 (TID 815) in 150 ms on localhost (199/200)
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:45:08 INFO Executor: Finished task 199.0 in stage 11.0 (TID 816). 2131 bytes result sent to driver
15/08/06 17:45:08 INFO TaskSetManager: Finished task 199.0 in stage 11.0 (TID 816) in 128 ms on localhost (200/200)
15/08/06 17:45:08 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/08/06 17:45:08 INFO DAGScheduler: Stage 11 (RangePartitioner at Exchange.scala:88) finished in 2.527 s
15/08/06 17:45:08 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@99f07b
15/08/06 17:45:08 INFO DAGScheduler: Job 7 finished: RangePartitioner at Exchange.scala:88, took 3.555411 s
15/08/06 17:45:08 INFO StatsReportListener: task runtime:(count: 200, mean: 197.810000, stdev: 39.535982, max: 288.000000, min: 128.000000)
15/08/06 17:45:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:08 INFO StatsReportListener: 	128.0 ms	147.0 ms	156.0 ms	168.0 ms	188.0 ms	228.0 ms	261.0 ms	271.0 ms	288.0 ms
15/08/06 17:45:08 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.240000, stdev: 0.482079, max: 3.000000, min: 0.000000)
15/08/06 17:45:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:08 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	3.0 ms
15/08/06 17:45:08 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:45:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:08 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:45:08 INFO StatsReportListener: task result size:(count: 200, mean: 2066.055000, stdev: 46.469151, max: 2237.000000, min: 1913.000000)
15/08/06 17:45:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:08 INFO StatsReportListener: 	1913.0 B	2002.0 B	2019.0 B	2037.0 B	2.0 KB	2.0 KB	2.1 KB	2.1 KB	2.2 KB
15/08/06 17:45:08 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 95.497978, stdev: 3.637108, max: 98.750000, min: 66.511628)
15/08/06 17:45:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:08 INFO StatsReportListener: 	67 %	89 %	92 %	95 %	96 %	97 %	98 %	98 %	99 %
15/08/06 17:45:08 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.132110, stdev: 0.283727, max: 2.112676, min: 0.000000)
15/08/06 17:45:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:08 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 2 %
15/08/06 17:45:08 INFO StatsReportListener: other time pct: (count: 200, mean: 4.369911, stdev: 3.609280, max: 33.488372, min: 1.250000)
15/08/06 17:45:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:08 INFO StatsReportListener: 	 1 %	 2 %	 2 %	 2 %	 3 %	 5 %	 8 %	11 %	33 %
15/08/06 17:45:08 INFO DefaultExecutionContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/08/06 17:45:08 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 205 bytes
15/08/06 17:45:08 INFO DAGScheduler: Registering RDD 75 (mapPartitions at Exchange.scala:77)
15/08/06 17:45:08 INFO DAGScheduler: Got job 8 (runJob at InsertIntoHiveTable.scala:93) with 200 output partitions (allowLocal=false)
15/08/06 17:45:08 INFO DAGScheduler: Final stage: Stage 14(runJob at InsertIntoHiveTable.scala:93)
15/08/06 17:45:08 INFO DAGScheduler: Parents of final stage: List(Stage 13)
15/08/06 17:45:08 INFO DAGScheduler: Missing parents: List(Stage 13)
15/08/06 17:45:08 INFO DAGScheduler: Submitting Stage 13 (MapPartitionsRDD[75] at mapPartitions at Exchange.scala:77), which has no missing parents
15/08/06 17:45:08 INFO MemoryStore: ensureFreeSpace(16352) called with curMem=1606772, maxMem=3333968363
15/08/06 17:45:08 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 16.0 KB, free 3.1 GB)
15/08/06 17:45:08 INFO MemoryStore: ensureFreeSpace(9186) called with curMem=1623124, maxMem=3333968363
15/08/06 17:45:08 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 9.0 KB, free 3.1 GB)
15/08/06 17:45:08 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:42907 (size: 9.0 KB, free: 3.1 GB)
15/08/06 17:45:08 INFO BlockManagerMaster: Updated info of block broadcast_18_piece0
15/08/06 17:45:08 INFO DefaultExecutionContext: Created broadcast 18 from broadcast at DAGScheduler.scala:838
15/08/06 17:45:08 INFO DAGScheduler: Submitting 200 missing tasks from Stage 13 (MapPartitionsRDD[75] at mapPartitions at Exchange.scala:77)
15/08/06 17:45:08 INFO TaskSchedulerImpl: Adding task set 13.0 with 200 tasks
15/08/06 17:45:08 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 817, localhost, ANY, 1810 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 818, localhost, ANY, 1812 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 819, localhost, ANY, 1813 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 820, localhost, ANY, 1812 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 821, localhost, ANY, 1814 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 822, localhost, ANY, 1813 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 823, localhost, ANY, 1813 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 824, localhost, ANY, 1814 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 825, localhost, ANY, 1813 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 826, localhost, ANY, 1810 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 827, localhost, ANY, 1815 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 828, localhost, ANY, 1814 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 829, localhost, ANY, 1814 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 830, localhost, ANY, 1813 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 14.0 in stage 13.0 (TID 831, localhost, ANY, 1814 bytes)
15/08/06 17:45:08 INFO TaskSetManager: Starting task 15.0 in stage 13.0 (TID 832, localhost, ANY, 1814 bytes)
15/08/06 17:45:08 INFO Executor: Running task 0.0 in stage 13.0 (TID 817)
15/08/06 17:45:08 INFO Executor: Running task 2.0 in stage 13.0 (TID 819)
15/08/06 17:45:08 INFO Executor: Running task 8.0 in stage 13.0 (TID 825)
15/08/06 17:45:08 INFO Executor: Running task 6.0 in stage 13.0 (TID 823)
15/08/06 17:45:08 INFO Executor: Running task 3.0 in stage 13.0 (TID 820)
15/08/06 17:45:08 INFO Executor: Running task 4.0 in stage 13.0 (TID 821)
15/08/06 17:45:08 INFO Executor: Running task 1.0 in stage 13.0 (TID 818)
15/08/06 17:45:08 INFO Executor: Running task 7.0 in stage 13.0 (TID 824)
15/08/06 17:45:08 INFO Executor: Running task 5.0 in stage 13.0 (TID 822)
15/08/06 17:45:08 INFO Executor: Running task 9.0 in stage 13.0 (TID 826)
15/08/06 17:45:08 INFO Executor: Running task 10.0 in stage 13.0 (TID 827)
15/08/06 17:45:08 INFO Executor: Running task 11.0 in stage 13.0 (TID 828)
15/08/06 17:45:08 INFO Executor: Running task 12.0 in stage 13.0 (TID 829)
15/08/06 17:45:08 INFO Executor: Running task 13.0 in stage 13.0 (TID 830)
15/08/06 17:45:08 INFO Executor: Running task 14.0 in stage 13.0 (TID 831)
15/08/06 17:45:08 INFO Executor: Running task 15.0 in stage 13.0 (TID 832)
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 193
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 144
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 142
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 125
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 167
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 140
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 156
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 154
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 126
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 128
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:45:08 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/06 17:45:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:08 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:45:09 INFO Executor: Finished task 7.0 in stage 13.0 (TID 824). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 16.0 in stage 13.0 (TID 833, localhost, ANY, 1815 bytes)
15/08/06 17:45:09 INFO Executor: Running task 16.0 in stage 13.0 (TID 833)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 824) in 719 ms on localhost (1/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:09 INFO Executor: Finished task 0.0 in stage 13.0 (TID 817). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 4.0 in stage 13.0 (TID 821). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 17.0 in stage 13.0 (TID 834, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 17.0 in stage 13.0 (TID 834)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 817) in 748 ms on localhost (2/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 18.0 in stage 13.0 (TID 835, localhost, ANY, 1815 bytes)
15/08/06 17:45:09 INFO Executor: Running task 18.0 in stage 13.0 (TID 835)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 821) in 748 ms on localhost (3/200)
15/08/06 17:45:09 INFO Executor: Finished task 12.0 in stage 13.0 (TID 829). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO Executor: Finished task 11.0 in stage 13.0 (TID 828). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 19.0 in stage 13.0 (TID 836, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 19.0 in stage 13.0 (TID 836)
15/08/06 17:45:09 INFO Executor: Finished task 8.0 in stage 13.0 (TID 825). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 829) in 763 ms on localhost (4/200)
15/08/06 17:45:09 INFO Executor: Finished task 1.0 in stage 13.0 (TID 818). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 20.0 in stage 13.0 (TID 837, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO Executor: Running task 20.0 in stage 13.0 (TID 837)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 828) in 766 ms on localhost (5/200)
15/08/06 17:45:09 INFO Executor: Finished task 5.0 in stage 13.0 (TID 822). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 21.0 in stage 13.0 (TID 838, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 21.0 in stage 13.0 (TID 838)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 825) in 770 ms on localhost (6/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Starting task 22.0 in stage 13.0 (TID 839, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 22.0 in stage 13.0 (TID 839)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 818) in 774 ms on localhost (7/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Starting task 23.0 in stage 13.0 (TID 840, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO Executor: Running task 23.0 in stage 13.0 (TID 840)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 822) in 776 ms on localhost (8/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO Executor: Finished task 13.0 in stage 13.0 (TID 830). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 6.0 in stage 13.0 (TID 823). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 15.0 in stage 13.0 (TID 832). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 24.0 in stage 13.0 (TID 841, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 24.0 in stage 13.0 (TID 841)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 830) in 784 ms on localhost (9/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 25.0 in stage 13.0 (TID 842, localhost, ANY, 1816 bytes)
15/08/06 17:45:09 INFO Executor: Running task 25.0 in stage 13.0 (TID 842)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 823) in 790 ms on localhost (10/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 26.0 in stage 13.0 (TID 843, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 26.0 in stage 13.0 (TID 843)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 15.0 in stage 13.0 (TID 832) in 788 ms on localhost (11/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:09 INFO Executor: Finished task 10.0 in stage 13.0 (TID 827). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 27.0 in stage 13.0 (TID 844, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 27.0 in stage 13.0 (TID 844)
15/08/06 17:45:09 INFO Executor: Finished task 3.0 in stage 13.0 (TID 820). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 14.0 in stage 13.0 (TID 831). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 827) in 805 ms on localhost (12/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 28.0 in stage 13.0 (TID 845, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 29.0 in stage 13.0 (TID 846, localhost, ANY, 1811 bytes)
15/08/06 17:45:09 INFO Executor: Running task 29.0 in stage 13.0 (TID 846)
15/08/06 17:45:09 INFO Executor: Running task 28.0 in stage 13.0 (TID 845)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 820) in 813 ms on localhost (13/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO Executor: Finished task 9.0 in stage 13.0 (TID 826). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Finished task 14.0 in stage 13.0 (TID 831) in 809 ms on localhost (14/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 30.0 in stage 13.0 (TID 847, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 826) in 812 ms on localhost (15/200)
15/08/06 17:45:09 INFO Executor: Running task 30.0 in stage 13.0 (TID 847)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO Executor: Finished task 2.0 in stage 13.0 (TID 819). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 31.0 in stage 13.0 (TID 848, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO Executor: Running task 31.0 in stage 13.0 (TID 848)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 819) in 821 ms on localhost (16/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 159
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 121
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 151
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 179
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 88
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:45:09 INFO Executor: Finished task 17.0 in stage 13.0 (TID 834). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 32.0 in stage 13.0 (TID 849, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 32.0 in stage 13.0 (TID 849)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 17.0 in stage 13.0 (TID 834) in 427 ms on localhost (17/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO Executor: Finished task 25.0 in stage 13.0 (TID 842). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 33.0 in stage 13.0 (TID 850, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO Executor: Running task 33.0 in stage 13.0 (TID 850)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 25.0 in stage 13.0 (TID 842) in 480 ms on localhost (18/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 165
15/08/06 17:45:09 INFO Executor: Finished task 16.0 in stage 13.0 (TID 833). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 34.0 in stage 13.0 (TID 851, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 34.0 in stage 13.0 (TID 851)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 16.0 in stage 13.0 (TID 833) in 634 ms on localhost (19/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 149
15/08/06 17:45:09 INFO Executor: Finished task 18.0 in stage 13.0 (TID 835). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 35.0 in stage 13.0 (TID 852, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO Executor: Running task 35.0 in stage 13.0 (TID 852)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 18.0 in stage 13.0 (TID 835) in 706 ms on localhost (20/200)
15/08/06 17:45:09 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:09 INFO Executor: Finished task 20.0 in stage 13.0 (TID 837). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:45:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:09 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 192
15/08/06 17:45:09 INFO TaskSetManager: Finished task 20.0 in stage 13.0 (TID 837) in 702 ms on localhost (21/200)
15/08/06 17:45:09 INFO Executor: Finished task 24.0 in stage 13.0 (TID 841). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 36.0 in stage 13.0 (TID 853, localhost, ANY, 1815 bytes)
15/08/06 17:45:09 INFO Executor: Running task 36.0 in stage 13.0 (TID 853)
15/08/06 17:45:09 INFO Executor: Finished task 19.0 in stage 13.0 (TID 836). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 37.0 in stage 13.0 (TID 854, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO Executor: Running task 37.0 in stage 13.0 (TID 854)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 24.0 in stage 13.0 (TID 841) in 705 ms on localhost (22/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO Executor: Finished task 21.0 in stage 13.0 (TID 838). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO Executor: Finished task 30.0 in stage 13.0 (TID 847). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 23.0 in stage 13.0 (TID 840). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 28.0 in stage 13.0 (TID 845). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 22.0 in stage 13.0 (TID 839). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 29.0 in stage 13.0 (TID 846). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 38.0 in stage 13.0 (TID 855, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO Executor: Running task 38.0 in stage 13.0 (TID 855)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 19.0 in stage 13.0 (TID 836) in 761 ms on localhost (23/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 39.0 in stage 13.0 (TID 856, localhost, ANY, 1815 bytes)
15/08/06 17:45:09 INFO Executor: Running task 39.0 in stage 13.0 (TID 856)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 21.0 in stage 13.0 (TID 838) in 746 ms on localhost (24/200)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 30.0 in stage 13.0 (TID 847) in 704 ms on localhost (25/200)
15/08/06 17:45:09 INFO Executor: Finished task 27.0 in stage 13.0 (TID 844). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO Executor: Finished task 26.0 in stage 13.0 (TID 843). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO Executor: Finished task 31.0 in stage 13.0 (TID 848). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO TaskSetManager: Starting task 40.0 in stage 13.0 (TID 857, localhost, ANY, 1815 bytes)
15/08/06 17:45:09 INFO Executor: Running task 40.0 in stage 13.0 (TID 857)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 41.0 in stage 13.0 (TID 858, localhost, ANY, 1815 bytes)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 23.0 in stage 13.0 (TID 840) in 753 ms on localhost (26/200)
15/08/06 17:45:09 INFO Executor: Running task 41.0 in stage 13.0 (TID 858)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 42.0 in stage 13.0 (TID 859, localhost, ANY, 1816 bytes)
15/08/06 17:45:09 INFO Executor: Running task 42.0 in stage 13.0 (TID 859)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 28.0 in stage 13.0 (TID 845) in 721 ms on localhost (27/200)
15/08/06 17:45:09 INFO Executor: Finished task 32.0 in stage 13.0 (TID 849). 2182 bytes result sent to driver
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Finished task 22.0 in stage 13.0 (TID 839) in 760 ms on localhost (28/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 43.0 in stage 13.0 (TID 860, localhost, ANY, 1813 bytes)
15/08/06 17:45:09 INFO Executor: Running task 43.0 in stage 13.0 (TID 860)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 44.0 in stage 13.0 (TID 861, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO Executor: Running task 44.0 in stage 13.0 (TID 861)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Finished task 29.0 in stage 13.0 (TID 846) in 725 ms on localhost (29/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 45.0 in stage 13.0 (TID 862, localhost, ANY, 1814 bytes)
15/08/06 17:45:09 INFO Executor: Running task 45.0 in stage 13.0 (TID 862)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 27.0 in stage 13.0 (TID 844) in 734 ms on localhost (30/200)
15/08/06 17:45:09 INFO TaskSetManager: Starting task 46.0 in stage 13.0 (TID 863, localhost, ANY, 1816 bytes)
15/08/06 17:45:09 INFO Executor: Running task 46.0 in stage 13.0 (TID 863)
15/08/06 17:45:09 INFO TaskSetManager: Finished task 26.0 in stage 13.0 (TID 843) in 751 ms on localhost (31/200)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO TaskSetManager: Starting task 47.0 in stage 13.0 (TID 864, localhost, ANY, 1816 bytes)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:09 INFO Executor: Running task 47.0 in stage 13.0 (TID 864)
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:09 INFO TaskSetManager: Finished task 31.0 in stage 13.0 (TID 848) in 726 ms on localhost (32/200)
15/08/06 17:45:10 INFO TaskSetManager: Starting task 48.0 in stage 13.0 (TID 865, localhost, ANY, 1814 bytes)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:10 INFO Executor: Running task 48.0 in stage 13.0 (TID 865)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 32.0 in stage 13.0 (TID 849) in 378 ms on localhost (33/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO Executor: Finished task 33.0 in stage 13.0 (TID 850). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 49.0 in stage 13.0 (TID 866, localhost, ANY, 1816 bytes)
15/08/06 17:45:10 INFO Executor: Running task 49.0 in stage 13.0 (TID 866)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 33.0 in stage 13.0 (TID 850) in 297 ms on localhost (34/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/06 17:45:10 INFO Executor: Finished task 34.0 in stage 13.0 (TID 851). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 50.0 in stage 13.0 (TID 867, localhost, ANY, 1814 bytes)
15/08/06 17:45:10 INFO Executor: Running task 50.0 in stage 13.0 (TID 867)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 34.0 in stage 13.0 (TID 851) in 275 ms on localhost (35/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 120
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 95
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 141
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 118
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 135
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:10 INFO Executor: Finished task 35.0 in stage 13.0 (TID 852). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 51.0 in stage 13.0 (TID 868, localhost, ANY, 1816 bytes)
15/08/06 17:45:10 INFO Executor: Running task 51.0 in stage 13.0 (TID 868)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 35.0 in stage 13.0 (TID 852) in 359 ms on localhost (36/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO Executor: Finished task 37.0 in stage 13.0 (TID 854). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 52.0 in stage 13.0 (TID 869, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 52.0 in stage 13.0 (TID 869)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 37.0 in stage 13.0 (TID 854) in 331 ms on localhost (37/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:45:10 INFO Executor: Finished task 36.0 in stage 13.0 (TID 853). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO Executor: Finished task 45.0 in stage 13.0 (TID 862). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 53.0 in stage 13.0 (TID 870, localhost, ANY, 1816 bytes)
15/08/06 17:45:10 INFO Executor: Running task 53.0 in stage 13.0 (TID 870)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 36.0 in stage 13.0 (TID 853) in 665 ms on localhost (38/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO TaskSetManager: Starting task 54.0 in stage 13.0 (TID 871, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 54.0 in stage 13.0 (TID 871)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 45.0 in stage 13.0 (TID 862) in 612 ms on localhost (39/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:10 INFO Executor: Finished task 43.0 in stage 13.0 (TID 860). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 55.0 in stage 13.0 (TID 872, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 55.0 in stage 13.0 (TID 872)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 43.0 in stage 13.0 (TID 860) in 647 ms on localhost (40/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:10 INFO Executor: Finished task 38.0 in stage 13.0 (TID 855). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO Executor: Finished task 39.0 in stage 13.0 (TID 856). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 56.0 in stage 13.0 (TID 873, localhost, ANY, 1814 bytes)
15/08/06 17:45:10 INFO Executor: Running task 56.0 in stage 13.0 (TID 873)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 38.0 in stage 13.0 (TID 855) in 719 ms on localhost (41/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO TaskSetManager: Starting task 57.0 in stage 13.0 (TID 874, localhost, ANY, 1814 bytes)
15/08/06 17:45:10 INFO Executor: Running task 57.0 in stage 13.0 (TID 874)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 39.0 in stage 13.0 (TID 856) in 705 ms on localhost (42/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 INFO Executor: Finished task 41.0 in stage 13.0 (TID 858). 2182 bytes result sent to driver
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO TaskSetManager: Starting task 58.0 in stage 13.0 (TID 875, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 58.0 in stage 13.0 (TID 875)
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO TaskSetManager: Finished task 41.0 in stage 13.0 (TID 858) in 737 ms on localhost (43/200)
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 118
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 137
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:10 INFO Executor: Finished task 48.0 in stage 13.0 (TID 865). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 59.0 in stage 13.0 (TID 876, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 59.0 in stage 13.0 (TID 876)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 48.0 in stage 13.0 (TID 865) in 733 ms on localhost (44/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 INFO Executor: Finished task 47.0 in stage 13.0 (TID 864). 2182 bytes result sent to driver
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO Executor: Finished task 46.0 in stage 13.0 (TID 863). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO Executor: Finished task 44.0 in stage 13.0 (TID 861). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:45:10 INFO TaskSetManager: Starting task 60.0 in stage 13.0 (TID 877, localhost, ANY, 1814 bytes)
15/08/06 17:45:10 INFO Executor: Running task 60.0 in stage 13.0 (TID 877)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 47.0 in stage 13.0 (TID 864) in 751 ms on localhost (45/200)
15/08/06 17:45:10 INFO TaskSetManager: Starting task 61.0 in stage 13.0 (TID 878, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Finished task 49.0 in stage 13.0 (TID 866). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO Executor: Running task 61.0 in stage 13.0 (TID 878)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO Executor: Finished task 42.0 in stage 13.0 (TID 859). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Finished task 46.0 in stage 13.0 (TID 863) in 759 ms on localhost (46/200)
15/08/06 17:45:10 INFO Executor: Finished task 40.0 in stage 13.0 (TID 857). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO TaskSetManager: Starting task 62.0 in stage 13.0 (TID 879, localhost, ANY, 1814 bytes)
15/08/06 17:45:10 INFO Executor: Running task 62.0 in stage 13.0 (TID 879)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 44.0 in stage 13.0 (TID 861) in 771 ms on localhost (47/200)
15/08/06 17:45:10 INFO Executor: Finished task 50.0 in stage 13.0 (TID 867). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 63.0 in stage 13.0 (TID 880, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 63.0 in stage 13.0 (TID 880)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 49.0 in stage 13.0 (TID 866) in 755 ms on localhost (48/200)
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:10 INFO TaskSetManager: Starting task 64.0 in stage 13.0 (TID 881, localhost, ANY, 1816 bytes)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:10 INFO Executor: Running task 64.0 in stage 13.0 (TID 881)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 42.0 in stage 13.0 (TID 859) in 787 ms on localhost (49/200)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 40.0 in stage 13.0 (TID 857) in 798 ms on localhost (50/200)
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO TaskSetManager: Starting task 65.0 in stage 13.0 (TID 882, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 65.0 in stage 13.0 (TID 882)
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:10 INFO TaskSetManager: Starting task 66.0 in stage 13.0 (TID 883, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 66.0 in stage 13.0 (TID 883)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:10 INFO TaskSetManager: Finished task 50.0 in stage 13.0 (TID 867) in 718 ms on localhost (51/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO Executor: Finished task 52.0 in stage 13.0 (TID 869). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 67.0 in stage 13.0 (TID 884, localhost, ANY, 1816 bytes)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 52.0 in stage 13.0 (TID 869) in 541 ms on localhost (52/200)
15/08/06 17:45:10 INFO Executor: Running task 67.0 in stage 13.0 (TID 884)
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:10 INFO Executor: Finished task 51.0 in stage 13.0 (TID 868). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO TaskSetManager: Finished task 51.0 in stage 13.0 (TID 868) in 570 ms on localhost (53/200)
15/08/06 17:45:10 INFO TaskSetManager: Starting task 68.0 in stage 13.0 (TID 885, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 68.0 in stage 13.0 (TID 885)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 112
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 203
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:10 INFO Executor: Finished task 54.0 in stage 13.0 (TID 871). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO Executor: Finished task 53.0 in stage 13.0 (TID 870). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 159
15/08/06 17:45:10 INFO TaskSetManager: Starting task 69.0 in stage 13.0 (TID 886, localhost, ANY, 1816 bytes)
15/08/06 17:45:10 INFO TaskSetManager: Starting task 70.0 in stage 13.0 (TID 887, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 69.0 in stage 13.0 (TID 886)
15/08/06 17:45:10 INFO Executor: Running task 70.0 in stage 13.0 (TID 887)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 54.0 in stage 13.0 (TID 871) in 330 ms on localhost (54/200)
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO TaskSetManager: Finished task 53.0 in stage 13.0 (TID 870) in 347 ms on localhost (55/200)
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO Executor: Finished task 55.0 in stage 13.0 (TID 872). 2182 bytes result sent to driver
15/08/06 17:45:10 INFO TaskSetManager: Starting task 71.0 in stage 13.0 (TID 888, localhost, ANY, 1815 bytes)
15/08/06 17:45:10 INFO Executor: Running task 71.0 in stage 13.0 (TID 888)
15/08/06 17:45:10 INFO TaskSetManager: Finished task 55.0 in stage 13.0 (TID 872) in 313 ms on localhost (56/200)
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 178
15/08/06 17:45:10 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/06 17:45:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 122
15/08/06 17:45:11 INFO Executor: Finished task 56.0 in stage 13.0 (TID 873). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 72.0 in stage 13.0 (TID 889, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 72.0 in stage 13.0 (TID 889)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 56.0 in stage 13.0 (TID 873) in 376 ms on localhost (57/200)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 160
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/06 17:45:11 INFO Executor: Finished task 57.0 in stage 13.0 (TID 874). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 73.0 in stage 13.0 (TID 890, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 73.0 in stage 13.0 (TID 890)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 57.0 in stage 13.0 (TID 874) in 423 ms on localhost (58/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:45:11 INFO Executor: Finished task 60.0 in stage 13.0 (TID 877). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 74.0 in stage 13.0 (TID 891, localhost, ANY, 1813 bytes)
15/08/06 17:45:11 INFO Executor: Running task 74.0 in stage 13.0 (TID 891)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 60.0 in stage 13.0 (TID 877) in 447 ms on localhost (59/200)
15/08/06 17:45:11 INFO Executor: Finished task 58.0 in stage 13.0 (TID 875). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO TaskSetManager: Starting task 75.0 in stage 13.0 (TID 892, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 75.0 in stage 13.0 (TID 892)
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO TaskSetManager: Finished task 58.0 in stage 13.0 (TID 875) in 510 ms on localhost (60/200)
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:11 INFO Executor: Finished task 61.0 in stage 13.0 (TID 878). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 76.0 in stage 13.0 (TID 893, localhost, ANY, 1816 bytes)
15/08/06 17:45:11 INFO Executor: Running task 76.0 in stage 13.0 (TID 893)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 61.0 in stage 13.0 (TID 878) in 488 ms on localhost (61/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:11 INFO Executor: Finished task 63.0 in stage 13.0 (TID 880). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 77.0 in stage 13.0 (TID 894, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 77.0 in stage 13.0 (TID 894)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 63.0 in stage 13.0 (TID 880) in 507 ms on localhost (62/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:11 INFO Executor: Finished task 59.0 in stage 13.0 (TID 876). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO Executor: Finished task 65.0 in stage 13.0 (TID 882). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 78.0 in stage 13.0 (TID 895, localhost, ANY, 1813 bytes)
15/08/06 17:45:11 INFO Executor: Running task 78.0 in stage 13.0 (TID 895)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO Executor: Finished task 66.0 in stage 13.0 (TID 883). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 79.0 in stage 13.0 (TID 896, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 79.0 in stage 13.0 (TID 896)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 59.0 in stage 13.0 (TID 876) in 581 ms on localhost (63/200)
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO TaskSetManager: Finished task 65.0 in stage 13.0 (TID 882) in 540 ms on localhost (64/200)
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO TaskSetManager: Starting task 80.0 in stage 13.0 (TID 897, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 80.0 in stage 13.0 (TID 897)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 66.0 in stage 13.0 (TID 883) in 547 ms on localhost (65/200)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO Executor: Finished task 67.0 in stage 13.0 (TID 884). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO Executor: Finished task 64.0 in stage 13.0 (TID 881). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 184
15/08/06 17:45:11 INFO TaskSetManager: Starting task 81.0 in stage 13.0 (TID 898, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 81.0 in stage 13.0 (TID 898)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO TaskSetManager: Starting task 82.0 in stage 13.0 (TID 899, localhost, ANY, 1813 bytes)
15/08/06 17:45:11 INFO Executor: Running task 82.0 in stage 13.0 (TID 899)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 67.0 in stage 13.0 (TID 884) in 539 ms on localhost (66/200)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 64.0 in stage 13.0 (TID 881) in 574 ms on localhost (67/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO Executor: Finished task 62.0 in stage 13.0 (TID 879). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 83.0 in stage 13.0 (TID 900, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 83.0 in stage 13.0 (TID 900)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 62.0 in stage 13.0 (TID 879) in 662 ms on localhost (68/200)
15/08/06 17:45:11 INFO Executor: Finished task 68.0 in stage 13.0 (TID 885). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO TaskSetManager: Starting task 84.0 in stage 13.0 (TID 901, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 84.0 in stage 13.0 (TID 901)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 68.0 in stage 13.0 (TID 885) in 609 ms on localhost (69/200)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:11 INFO Executor: Finished task 70.0 in stage 13.0 (TID 887). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO Executor: Finished task 71.0 in stage 13.0 (TID 888). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 INFO Executor: Finished task 69.0 in stage 13.0 (TID 886). 2182 bytes result sent to driver
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO TaskSetManager: Starting task 85.0 in stage 13.0 (TID 902, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 85.0 in stage 13.0 (TID 902)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 70.0 in stage 13.0 (TID 887) in 546 ms on localhost (70/200)
15/08/06 17:45:11 INFO TaskSetManager: Starting task 86.0 in stage 13.0 (TID 903, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 86.0 in stage 13.0 (TID 903)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 71.0 in stage 13.0 (TID 888) in 537 ms on localhost (71/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO TaskSetManager: Starting task 87.0 in stage 13.0 (TID 904, localhost, ANY, 1813 bytes)
15/08/06 17:45:11 INFO Executor: Running task 87.0 in stage 13.0 (TID 904)
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO TaskSetManager: Finished task 69.0 in stage 13.0 (TID 886) in 554 ms on localhost (72/200)
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO Executor: Finished task 72.0 in stage 13.0 (TID 889). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO Executor: Finished task 73.0 in stage 13.0 (TID 890). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 88.0 in stage 13.0 (TID 905, localhost, ANY, 1813 bytes)
15/08/06 17:45:11 INFO Executor: Running task 88.0 in stage 13.0 (TID 905)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 72.0 in stage 13.0 (TID 889) in 482 ms on localhost (73/200)
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 191
15/08/06 17:45:11 INFO TaskSetManager: Starting task 89.0 in stage 13.0 (TID 906, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 89.0 in stage 13.0 (TID 906)
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO TaskSetManager: Finished task 73.0 in stage 13.0 (TID 890) in 424 ms on localhost (74/200)
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/06 17:45:11 INFO Executor: Finished task 74.0 in stage 13.0 (TID 891). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 90.0 in stage 13.0 (TID 907, localhost, ANY, 1812 bytes)
15/08/06 17:45:11 INFO Executor: Running task 90.0 in stage 13.0 (TID 907)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO TaskSetManager: Finished task 74.0 in stage 13.0 (TID 891) in 372 ms on localhost (75/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:11 INFO Executor: Finished task 75.0 in stage 13.0 (TID 892). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 163
15/08/06 17:45:11 INFO TaskSetManager: Starting task 91.0 in stage 13.0 (TID 908, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 91.0 in stage 13.0 (TID 908)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 75.0 in stage 13.0 (TID 892) in 377 ms on localhost (76/200)
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 161
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:11 INFO Executor: Finished task 76.0 in stage 13.0 (TID 893). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 92.0 in stage 13.0 (TID 909, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 92.0 in stage 13.0 (TID 909)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 76.0 in stage 13.0 (TID 893) in 361 ms on localhost (77/200)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO Executor: Finished task 77.0 in stage 13.0 (TID 894). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO TaskSetManager: Starting task 93.0 in stage 13.0 (TID 910, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 93.0 in stage 13.0 (TID 910)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 77.0 in stage 13.0 (TID 894) in 380 ms on localhost (78/200)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 182
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 200
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 173
15/08/06 17:45:11 INFO Executor: Finished task 78.0 in stage 13.0 (TID 895). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 94.0 in stage 13.0 (TID 911, localhost, ANY, 1812 bytes)
15/08/06 17:45:11 INFO Executor: Running task 94.0 in stage 13.0 (TID 911)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 78.0 in stage 13.0 (TID 895) in 419 ms on localhost (79/200)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 119
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:11 INFO Executor: Finished task 80.0 in stage 13.0 (TID 897). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 95.0 in stage 13.0 (TID 912, localhost, ANY, 1816 bytes)
15/08/06 17:45:11 INFO Executor: Running task 95.0 in stage 13.0 (TID 912)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 80.0 in stage 13.0 (TID 897) in 444 ms on localhost (80/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:45:11 INFO Executor: Finished task 79.0 in stage 13.0 (TID 896). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 196
15/08/06 17:45:11 INFO TaskSetManager: Starting task 96.0 in stage 13.0 (TID 913, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 96.0 in stage 13.0 (TID 913)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 79.0 in stage 13.0 (TID 896) in 538 ms on localhost (81/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:45:11 INFO Executor: Finished task 82.0 in stage 13.0 (TID 899). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 97.0 in stage 13.0 (TID 914, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO Executor: Running task 97.0 in stage 13.0 (TID 914)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 82.0 in stage 13.0 (TID 899) in 548 ms on localhost (82/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO Executor: Finished task 81.0 in stage 13.0 (TID 898). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 98.0 in stage 13.0 (TID 915, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 98.0 in stage 13.0 (TID 915)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 81.0 in stage 13.0 (TID 898) in 568 ms on localhost (83/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO Executor: Finished task 84.0 in stage 13.0 (TID 901). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 99.0 in stage 13.0 (TID 916, localhost, ANY, 1815 bytes)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 84.0 in stage 13.0 (TID 901) in 519 ms on localhost (84/200)
15/08/06 17:45:11 INFO Executor: Running task 99.0 in stage 13.0 (TID 916)
15/08/06 17:45:11 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:45:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 114
15/08/06 17:45:11 INFO Executor: Finished task 83.0 in stage 13.0 (TID 900). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 100.0 in stage 13.0 (TID 917, localhost, ANY, 1814 bytes)
15/08/06 17:45:11 INFO Executor: Running task 100.0 in stage 13.0 (TID 917)
15/08/06 17:45:11 INFO TaskSetManager: Finished task 83.0 in stage 13.0 (TID 900) in 559 ms on localhost (85/200)
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:11 INFO Executor: Finished task 85.0 in stage 13.0 (TID 902). 2182 bytes result sent to driver
15/08/06 17:45:11 INFO TaskSetManager: Starting task 101.0 in stage 13.0 (TID 918, localhost, ANY, 1812 bytes)
15/08/06 17:45:11 INFO Executor: Running task 101.0 in stage 13.0 (TID 918)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 85.0 in stage 13.0 (TID 902) in 540 ms on localhost (86/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:12 INFO Executor: Finished task 89.0 in stage 13.0 (TID 906). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO Executor: Finished task 87.0 in stage 13.0 (TID 904). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 102.0 in stage 13.0 (TID 919, localhost, ANY, 1813 bytes)
15/08/06 17:45:12 INFO Executor: Running task 102.0 in stage 13.0 (TID 919)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 89.0 in stage 13.0 (TID 906) in 559 ms on localhost (87/200)
15/08/06 17:45:12 INFO Executor: Finished task 86.0 in stage 13.0 (TID 903). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 103.0 in stage 13.0 (TID 920, localhost, ANY, 1813 bytes)
15/08/06 17:45:12 INFO Executor: Running task 103.0 in stage 13.0 (TID 920)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 87.0 in stage 13.0 (TID 904) in 593 ms on localhost (88/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO TaskSetManager: Starting task 104.0 in stage 13.0 (TID 921, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 104.0 in stage 13.0 (TID 921)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 86.0 in stage 13.0 (TID 903) in 603 ms on localhost (89/200)
15/08/06 17:45:12 INFO Executor: Finished task 88.0 in stage 13.0 (TID 905). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO TaskSetManager: Starting task 105.0 in stage 13.0 (TID 922, localhost, ANY, 1815 bytes)
15/08/06 17:45:12 INFO Executor: Running task 105.0 in stage 13.0 (TID 922)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 88.0 in stage 13.0 (TID 905) in 579 ms on localhost (90/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO Executor: Finished task 90.0 in stage 13.0 (TID 907). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO Executor: Finished task 91.0 in stage 13.0 (TID 908). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 162
15/08/06 17:45:12 INFO TaskSetManager: Starting task 106.0 in stage 13.0 (TID 923, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 106.0 in stage 13.0 (TID 923)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 90.0 in stage 13.0 (TID 907) in 547 ms on localhost (91/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO TaskSetManager: Starting task 107.0 in stage 13.0 (TID 924, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 107.0 in stage 13.0 (TID 924)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 91.0 in stage 13.0 (TID 908) in 535 ms on localhost (92/200)
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 92.0 in stage 13.0 (TID 909). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 108.0 in stage 13.0 (TID 925, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 108.0 in stage 13.0 (TID 925)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 92.0 in stage 13.0 (TID 909) in 523 ms on localhost (93/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 93.0 in stage 13.0 (TID 910). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 109.0 in stage 13.0 (TID 926, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 109.0 in stage 13.0 (TID 926)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 93.0 in stage 13.0 (TID 910) in 490 ms on localhost (94/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO Executor: Finished task 94.0 in stage 13.0 (TID 911). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO TaskSetManager: Starting task 110.0 in stage 13.0 (TID 927, localhost, ANY, 1813 bytes)
15/08/06 17:45:12 INFO Executor: Running task 110.0 in stage 13.0 (TID 927)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 94.0 in stage 13.0 (TID 911) in 432 ms on localhost (95/200)
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 174
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 95.0 in stage 13.0 (TID 912). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 111.0 in stage 13.0 (TID 928, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 111.0 in stage 13.0 (TID 928)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 95.0 in stage 13.0 (TID 912) in 410 ms on localhost (96/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO Executor: Finished task 96.0 in stage 13.0 (TID 913). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 112.0 in stage 13.0 (TID 929, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO Executor: Running task 112.0 in stage 13.0 (TID 929)
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO TaskSetManager: Finished task 96.0 in stage 13.0 (TID 913) in 387 ms on localhost (97/200)
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 176
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 INFO Executor: Finished task 98.0 in stage 13.0 (TID 915). 2182 bytes result sent to driver
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO Executor: Finished task 97.0 in stage 13.0 (TID 914). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 113.0 in stage 13.0 (TID 930, localhost, ANY, 1815 bytes)
15/08/06 17:45:12 INFO Executor: Running task 113.0 in stage 13.0 (TID 930)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 98.0 in stage 13.0 (TID 915) in 425 ms on localhost (98/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO TaskSetManager: Starting task 114.0 in stage 13.0 (TID 931, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 114.0 in stage 13.0 (TID 931)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:12 INFO TaskSetManager: Finished task 97.0 in stage 13.0 (TID 914) in 443 ms on localhost (99/200)
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:45:12 INFO Executor: Finished task 100.0 in stage 13.0 (TID 917). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 115.0 in stage 13.0 (TID 932, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 115.0 in stage 13.0 (TID 932)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 100.0 in stage 13.0 (TID 917) in 462 ms on localhost (100/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO Executor: Finished task 99.0 in stage 13.0 (TID 916). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/06 17:45:12 INFO TaskSetManager: Starting task 116.0 in stage 13.0 (TID 933, localhost, ANY, 1815 bytes)
15/08/06 17:45:12 INFO Executor: Running task 116.0 in stage 13.0 (TID 933)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 99.0 in stage 13.0 (TID 916) in 518 ms on localhost (101/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 101.0 in stage 13.0 (TID 918). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO TaskSetManager: Starting task 117.0 in stage 13.0 (TID 934, localhost, ANY, 1813 bytes)
15/08/06 17:45:12 INFO Executor: Running task 117.0 in stage 13.0 (TID 934)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 101.0 in stage 13.0 (TID 918) in 467 ms on localhost (102/200)
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 160
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 103.0 in stage 13.0 (TID 920). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 118.0 in stage 13.0 (TID 935, localhost, ANY, 1815 bytes)
15/08/06 17:45:12 INFO Executor: Running task 118.0 in stage 13.0 (TID 935)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 103.0 in stage 13.0 (TID 920) in 481 ms on localhost (103/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/06 17:45:12 INFO Executor: Finished task 104.0 in stage 13.0 (TID 921). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO Executor: Finished task 102.0 in stage 13.0 (TID 919). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 119.0 in stage 13.0 (TID 936, localhost, ANY, 1813 bytes)
15/08/06 17:45:12 INFO Executor: Running task 119.0 in stage 13.0 (TID 936)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 104.0 in stage 13.0 (TID 921) in 559 ms on localhost (104/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO TaskSetManager: Starting task 120.0 in stage 13.0 (TID 937, localhost, ANY, 1812 bytes)
15/08/06 17:45:12 INFO Executor: Running task 120.0 in stage 13.0 (TID 937)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 102.0 in stage 13.0 (TID 919) in 586 ms on localhost (105/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 105.0 in stage 13.0 (TID 922). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 138
15/08/06 17:45:12 INFO TaskSetManager: Starting task 121.0 in stage 13.0 (TID 938, localhost, ANY, 1812 bytes)
15/08/06 17:45:12 INFO Executor: Running task 121.0 in stage 13.0 (TID 938)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 105.0 in stage 13.0 (TID 922) in 584 ms on localhost (106/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO Executor: Finished task 106.0 in stage 13.0 (TID 923). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 122.0 in stage 13.0 (TID 939, localhost, ANY, 1812 bytes)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 106.0 in stage 13.0 (TID 923) in 595 ms on localhost (107/200)
15/08/06 17:45:12 INFO Executor: Running task 122.0 in stage 13.0 (TID 939)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 108.0 in stage 13.0 (TID 925). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 123.0 in stage 13.0 (TID 940, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 123.0 in stage 13.0 (TID 940)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 108.0 in stage 13.0 (TID 925) in 604 ms on localhost (108/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO Executor: Finished task 110.0 in stage 13.0 (TID 927). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO Executor: Finished task 109.0 in stage 13.0 (TID 926). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/06 17:45:12 INFO TaskSetManager: Starting task 124.0 in stage 13.0 (TID 941, localhost, ANY, 1813 bytes)
15/08/06 17:45:12 INFO Executor: Running task 124.0 in stage 13.0 (TID 941)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 110.0 in stage 13.0 (TID 927) in 627 ms on localhost (109/200)
15/08/06 17:45:12 INFO Executor: Finished task 107.0 in stage 13.0 (TID 924). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 111.0 in stage 13.0 (TID 928). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 125.0 in stage 13.0 (TID 942, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 125.0 in stage 13.0 (TID 942)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 INFO TaskSetManager: Finished task 109.0 in stage 13.0 (TID 926) in 648 ms on localhost (110/200)
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO TaskSetManager: Starting task 126.0 in stage 13.0 (TID 943, localhost, ANY, 1813 bytes)
15/08/06 17:45:12 INFO Executor: Running task 126.0 in stage 13.0 (TID 943)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 107.0 in stage 13.0 (TID 924) in 682 ms on localhost (111/200)
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO TaskSetManager: Starting task 127.0 in stage 13.0 (TID 944, localhost, ANY, 1815 bytes)
15/08/06 17:45:12 INFO Executor: Running task 127.0 in stage 13.0 (TID 944)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 111.0 in stage 13.0 (TID 928) in 629 ms on localhost (112/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO Executor: Finished task 112.0 in stage 13.0 (TID 929). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 175
15/08/06 17:45:12 INFO TaskSetManager: Starting task 128.0 in stage 13.0 (TID 945, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 112.0 in stage 13.0 (TID 929) in 612 ms on localhost (113/200)
15/08/06 17:45:12 INFO Executor: Running task 128.0 in stage 13.0 (TID 945)
15/08/06 17:45:12 INFO Executor: Finished task 113.0 in stage 13.0 (TID 930). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 129.0 in stage 13.0 (TID 946, localhost, ANY, 1815 bytes)
15/08/06 17:45:12 INFO Executor: Running task 129.0 in stage 13.0 (TID 946)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 113.0 in stage 13.0 (TID 930) in 511 ms on localhost (114/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 114.0 in stage 13.0 (TID 931). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 130.0 in stage 13.0 (TID 947, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 130.0 in stage 13.0 (TID 947)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 114.0 in stage 13.0 (TID 931) in 516 ms on localhost (115/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 115.0 in stage 13.0 (TID 932). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 131.0 in stage 13.0 (TID 948, localhost, ANY, 1815 bytes)
15/08/06 17:45:12 INFO Executor: Running task 131.0 in stage 13.0 (TID 948)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 115.0 in stage 13.0 (TID 932) in 440 ms on localhost (116/200)
15/08/06 17:45:12 INFO Executor: Finished task 116.0 in stage 13.0 (TID 933). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 132.0 in stage 13.0 (TID 949, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 132.0 in stage 13.0 (TID 949)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 116.0 in stage 13.0 (TID 933) in 429 ms on localhost (117/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO Executor: Finished task 117.0 in stage 13.0 (TID 934). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 133.0 in stage 13.0 (TID 950, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 133.0 in stage 13.0 (TID 950)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 117.0 in stage 13.0 (TID 934) in 426 ms on localhost (118/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO Executor: Finished task 118.0 in stage 13.0 (TID 935). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 134.0 in stage 13.0 (TID 951, localhost, ANY, 1816 bytes)
15/08/06 17:45:12 INFO Executor: Running task 134.0 in stage 13.0 (TID 951)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 118.0 in stage 13.0 (TID 935) in 356 ms on localhost (119/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 180
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 148
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 123
15/08/06 17:45:12 INFO Executor: Finished task 120.0 in stage 13.0 (TID 937). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO Executor: Finished task 119.0 in stage 13.0 (TID 936). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 135.0 in stage 13.0 (TID 952, localhost, ANY, 1816 bytes)
15/08/06 17:45:12 INFO Executor: Running task 135.0 in stage 13.0 (TID 952)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 120.0 in stage 13.0 (TID 937) in 292 ms on localhost (120/200)
15/08/06 17:45:12 INFO TaskSetManager: Starting task 136.0 in stage 13.0 (TID 953, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 136.0 in stage 13.0 (TID 953)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 119.0 in stage 13.0 (TID 936) in 303 ms on localhost (121/200)
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 169
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/06 17:45:12 INFO Executor: Finished task 121.0 in stage 13.0 (TID 938). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 137.0 in stage 13.0 (TID 954, localhost, ANY, 1814 bytes)
15/08/06 17:45:12 INFO Executor: Running task 137.0 in stage 13.0 (TID 954)
15/08/06 17:45:12 INFO TaskSetManager: Finished task 121.0 in stage 13.0 (TID 938) in 304 ms on localhost (122/200)
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:45:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:12 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 163
15/08/06 17:45:12 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:12 INFO Executor: Finished task 122.0 in stage 13.0 (TID 939). 2182 bytes result sent to driver
15/08/06 17:45:12 INFO TaskSetManager: Starting task 138.0 in stage 13.0 (TID 955, localhost, ANY, 1815 bytes)
15/08/06 17:45:12 INFO Executor: Running task 138.0 in stage 13.0 (TID 955)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 122.0 in stage 13.0 (TID 939) in 320 ms on localhost (123/200)
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO Executor: Finished task 123.0 in stage 13.0 (TID 940). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:45:13 INFO TaskSetManager: Starting task 139.0 in stage 13.0 (TID 956, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 139.0 in stage 13.0 (TID 956)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 123.0 in stage 13.0 (TID 940) in 353 ms on localhost (124/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:13 INFO Executor: Finished task 125.0 in stage 13.0 (TID 942). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 140.0 in stage 13.0 (TID 957, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 140.0 in stage 13.0 (TID 957)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 125.0 in stage 13.0 (TID 942) in 429 ms on localhost (125/200)
15/08/06 17:45:13 INFO Executor: Finished task 124.0 in stage 13.0 (TID 941). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO TaskSetManager: Starting task 141.0 in stage 13.0 (TID 958, localhost, ANY, 1813 bytes)
15/08/06 17:45:13 INFO Executor: Running task 141.0 in stage 13.0 (TID 958)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 124.0 in stage 13.0 (TID 941) in 454 ms on localhost (126/200)
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 123
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/06 17:45:13 INFO Executor: Finished task 126.0 in stage 13.0 (TID 943). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 142.0 in stage 13.0 (TID 959, localhost, ANY, 1812 bytes)
15/08/06 17:45:13 INFO Executor: Running task 142.0 in stage 13.0 (TID 959)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 126.0 in stage 13.0 (TID 943) in 571 ms on localhost (127/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO Executor: Finished task 127.0 in stage 13.0 (TID 944). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 143.0 in stage 13.0 (TID 960, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 143.0 in stage 13.0 (TID 960)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 127.0 in stage 13.0 (TID 944) in 598 ms on localhost (128/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO Executor: Finished task 129.0 in stage 13.0 (TID 946). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 144.0 in stage 13.0 (TID 961, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 144.0 in stage 13.0 (TID 961)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 129.0 in stage 13.0 (TID 946) in 612 ms on localhost (129/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO Executor: Finished task 128.0 in stage 13.0 (TID 945). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/06 17:45:13 INFO TaskSetManager: Starting task 145.0 in stage 13.0 (TID 962, localhost, ANY, 1814 bytes)
15/08/06 17:45:13 INFO Executor: Running task 145.0 in stage 13.0 (TID 962)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 128.0 in stage 13.0 (TID 945) in 652 ms on localhost (130/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO Executor: Finished task 131.0 in stage 13.0 (TID 948). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO Executor: Finished task 130.0 in stage 13.0 (TID 947). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 146.0 in stage 13.0 (TID 963, localhost, ANY, 1814 bytes)
15/08/06 17:45:13 INFO Executor: Running task 146.0 in stage 13.0 (TID 963)
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 INFO TaskSetManager: Finished task 131.0 in stage 13.0 (TID 948) in 645 ms on localhost (131/200)
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/06 17:45:13 INFO Executor: Finished task 132.0 in stage 13.0 (TID 949). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 147.0 in stage 13.0 (TID 964, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 147.0 in stage 13.0 (TID 964)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 130.0 in stage 13.0 (TID 947) in 695 ms on localhost (132/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO TaskSetManager: Starting task 148.0 in stage 13.0 (TID 965, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 148.0 in stage 13.0 (TID 965)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 132.0 in stage 13.0 (TID 949) in 667 ms on localhost (133/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO Executor: Finished task 133.0 in stage 13.0 (TID 950). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO Executor: Finished task 134.0 in stage 13.0 (TID 951). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO Executor: Finished task 136.0 in stage 13.0 (TID 953). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 149.0 in stage 13.0 (TID 966, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 133.0 in stage 13.0 (TID 950) in 674 ms on localhost (134/200)
15/08/06 17:45:13 INFO Executor: Running task 149.0 in stage 13.0 (TID 966)
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:13 INFO TaskSetManager: Starting task 150.0 in stage 13.0 (TID 967, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 150.0 in stage 13.0 (TID 967)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 134.0 in stage 13.0 (TID 951) in 672 ms on localhost (135/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO TaskSetManager: Starting task 151.0 in stage 13.0 (TID 968, localhost, ANY, 1812 bytes)
15/08/06 17:45:13 INFO Executor: Running task 151.0 in stage 13.0 (TID 968)
15/08/06 17:45:13 INFO Executor: Finished task 135.0 in stage 13.0 (TID 952). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 INFO TaskSetManager: Finished task 136.0 in stage 13.0 (TID 953) in 647 ms on localhost (136/200)
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:45:13 INFO TaskSetManager: Starting task 152.0 in stage 13.0 (TID 969, localhost, ANY, 1816 bytes)
15/08/06 17:45:13 INFO Executor: Running task 152.0 in stage 13.0 (TID 969)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 135.0 in stage 13.0 (TID 952) in 663 ms on localhost (137/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO Executor: Finished task 137.0 in stage 13.0 (TID 954). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO Executor: Finished task 138.0 in stage 13.0 (TID 955). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 153.0 in stage 13.0 (TID 970, localhost, ANY, 1814 bytes)
15/08/06 17:45:13 INFO Executor: Running task 153.0 in stage 13.0 (TID 970)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 137.0 in stage 13.0 (TID 954) in 655 ms on localhost (138/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO TaskSetManager: Starting task 154.0 in stage 13.0 (TID 971, localhost, ANY, 1813 bytes)
15/08/06 17:45:13 INFO Executor: Running task 154.0 in stage 13.0 (TID 971)
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 INFO TaskSetManager: Finished task 138.0 in stage 13.0 (TID 955) in 616 ms on localhost (139/200)
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 126
15/08/06 17:45:13 INFO Executor: Finished task 139.0 in stage 13.0 (TID 956). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 155.0 in stage 13.0 (TID 972, localhost, ANY, 1814 bytes)
15/08/06 17:45:13 INFO Executor: Running task 155.0 in stage 13.0 (TID 972)
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO TaskSetManager: Finished task 139.0 in stage 13.0 (TID 956) in 588 ms on localhost (140/200)
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO Executor: Finished task 141.0 in stage 13.0 (TID 958). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:13 INFO TaskSetManager: Starting task 156.0 in stage 13.0 (TID 973, localhost, ANY, 1816 bytes)
15/08/06 17:45:13 INFO Executor: Running task 156.0 in stage 13.0 (TID 973)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 141.0 in stage 13.0 (TID 958) in 460 ms on localhost (141/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO Executor: Finished task 140.0 in stage 13.0 (TID 957). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO TaskSetManager: Starting task 157.0 in stage 13.0 (TID 974, localhost, ANY, 1816 bytes)
15/08/06 17:45:13 INFO Executor: Running task 157.0 in stage 13.0 (TID 974)
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 163
15/08/06 17:45:13 INFO TaskSetManager: Finished task 140.0 in stage 13.0 (TID 957) in 583 ms on localhost (142/200)
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 108
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO Executor: Finished task 142.0 in stage 13.0 (TID 959). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 190
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/06 17:45:13 INFO Executor: Finished task 152.0 in stage 13.0 (TID 969). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 158.0 in stage 13.0 (TID 975, localhost, ANY, 1813 bytes)
15/08/06 17:45:13 INFO Executor: Running task 158.0 in stage 13.0 (TID 975)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 142.0 in stage 13.0 (TID 959) in 380 ms on localhost (143/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO TaskSetManager: Starting task 159.0 in stage 13.0 (TID 976, localhost, ANY, 1814 bytes)
15/08/06 17:45:13 INFO Executor: Running task 159.0 in stage 13.0 (TID 976)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 152.0 in stage 13.0 (TID 969) in 155 ms on localhost (144/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO Executor: Finished task 143.0 in stage 13.0 (TID 960). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/06 17:45:13 INFO TaskSetManager: Starting task 160.0 in stage 13.0 (TID 977, localhost, ANY, 1814 bytes)
15/08/06 17:45:13 INFO Executor: Running task 160.0 in stage 13.0 (TID 977)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 143.0 in stage 13.0 (TID 960) in 375 ms on localhost (145/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/06 17:45:13 INFO Executor: Finished task 144.0 in stage 13.0 (TID 961). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 161.0 in stage 13.0 (TID 978, localhost, ANY, 1816 bytes)
15/08/06 17:45:13 INFO Executor: Running task 161.0 in stage 13.0 (TID 978)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 144.0 in stage 13.0 (TID 961) in 351 ms on localhost (146/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 149
15/08/06 17:45:13 INFO Executor: Finished task 145.0 in stage 13.0 (TID 962). 2182 bytes result sent to driver
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/06 17:45:13 INFO TaskSetManager: Starting task 162.0 in stage 13.0 (TID 979, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 162.0 in stage 13.0 (TID 979)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 145.0 in stage 13.0 (TID 962) in 387 ms on localhost (147/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO Executor: Finished task 146.0 in stage 13.0 (TID 963). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 147
15/08/06 17:45:13 INFO TaskSetManager: Starting task 163.0 in stage 13.0 (TID 980, localhost, ANY, 1814 bytes)
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 INFO Executor: Running task 163.0 in stage 13.0 (TID 980)
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO TaskSetManager: Finished task 146.0 in stage 13.0 (TID 963) in 386 ms on localhost (148/200)
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 116
15/08/06 17:45:13 INFO Executor: Finished task 147.0 in stage 13.0 (TID 964). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO Executor: Finished task 148.0 in stage 13.0 (TID 965). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 164.0 in stage 13.0 (TID 981, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 164.0 in stage 13.0 (TID 981)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 147.0 in stage 13.0 (TID 964) in 406 ms on localhost (149/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:13 INFO TaskSetManager: Starting task 165.0 in stage 13.0 (TID 982, localhost, ANY, 1816 bytes)
15/08/06 17:45:13 INFO Executor: Running task 165.0 in stage 13.0 (TID 982)
15/08/06 17:45:13 INFO Executor: Finished task 150.0 in stage 13.0 (TID 967). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Finished task 148.0 in stage 13.0 (TID 965) in 410 ms on localhost (150/200)
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO Executor: Finished task 149.0 in stage 13.0 (TID 966). 2182 bytes result sent to driver
15/08/06 17:45:13 INFO TaskSetManager: Starting task 166.0 in stage 13.0 (TID 983, localhost, ANY, 1815 bytes)
15/08/06 17:45:13 INFO Executor: Running task 166.0 in stage 13.0 (TID 983)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 150.0 in stage 13.0 (TID 967) in 400 ms on localhost (151/200)
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO TaskSetManager: Starting task 167.0 in stage 13.0 (TID 984, localhost, ANY, 1814 bytes)
15/08/06 17:45:13 INFO Executor: Running task 167.0 in stage 13.0 (TID 984)
15/08/06 17:45:13 INFO TaskSetManager: Finished task 149.0 in stage 13.0 (TID 966) in 418 ms on localhost (152/200)
15/08/06 17:45:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/06 17:45:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:13 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 136
15/08/06 17:45:13 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 110
15/08/06 17:45:14 INFO Executor: Finished task 163.0 in stage 13.0 (TID 980). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 168.0 in stage 13.0 (TID 985, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 168.0 in stage 13.0 (TID 985)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 163.0 in stage 13.0 (TID 980) in 163 ms on localhost (153/200)
15/08/06 17:45:14 INFO Executor: Finished task 151.0 in stage 13.0 (TID 968). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO TaskSetManager: Starting task 169.0 in stage 13.0 (TID 986, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:14 INFO Executor: Running task 169.0 in stage 13.0 (TID 986)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 151.0 in stage 13.0 (TID 968) in 476 ms on localhost (154/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/06 17:45:14 INFO Executor: Finished task 153.0 in stage 13.0 (TID 970). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO Executor: Finished task 154.0 in stage 13.0 (TID 971). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 170.0 in stage 13.0 (TID 987, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 170.0 in stage 13.0 (TID 987)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 153.0 in stage 13.0 (TID 970) in 492 ms on localhost (155/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO TaskSetManager: Starting task 171.0 in stage 13.0 (TID 988, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 154.0 in stage 13.0 (TID 971) in 505 ms on localhost (156/200)
15/08/06 17:45:14 INFO Executor: Running task 171.0 in stage 13.0 (TID 988)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO Executor: Finished task 156.0 in stage 13.0 (TID 973). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 172.0 in stage 13.0 (TID 989, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 172.0 in stage 13.0 (TID 989)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 156.0 in stage 13.0 (TID 973) in 479 ms on localhost (157/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/06 17:45:14 INFO Executor: Finished task 159.0 in stage 13.0 (TID 976). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO Executor: Finished task 158.0 in stage 13.0 (TID 975). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/06 17:45:14 INFO TaskSetManager: Starting task 173.0 in stage 13.0 (TID 990, localhost, ANY, 1812 bytes)
15/08/06 17:45:14 INFO Executor: Running task 173.0 in stage 13.0 (TID 990)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 159.0 in stage 13.0 (TID 976) in 563 ms on localhost (158/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO TaskSetManager: Starting task 174.0 in stage 13.0 (TID 991, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 174.0 in stage 13.0 (TID 991)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 158.0 in stage 13.0 (TID 975) in 583 ms on localhost (159/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO Executor: Finished task 161.0 in stage 13.0 (TID 978). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO Executor: Finished task 160.0 in stage 13.0 (TID 977). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 175.0 in stage 13.0 (TID 992, localhost, ANY, 1812 bytes)
15/08/06 17:45:14 INFO Executor: Running task 175.0 in stage 13.0 (TID 992)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 161.0 in stage 13.0 (TID 978) in 548 ms on localhost (160/200)
15/08/06 17:45:14 INFO TaskSetManager: Starting task 176.0 in stage 13.0 (TID 993, localhost, ANY, 1813 bytes)
15/08/06 17:45:14 INFO Executor: Running task 176.0 in stage 13.0 (TID 993)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO Executor: Finished task 155.0 in stage 13.0 (TID 972). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Finished task 160.0 in stage 13.0 (TID 977) in 578 ms on localhost (161/200)
15/08/06 17:45:14 INFO TaskSetManager: Starting task 177.0 in stage 13.0 (TID 994, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 177.0 in stage 13.0 (TID 994)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 155.0 in stage 13.0 (TID 972) in 695 ms on localhost (162/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO Executor: Finished task 157.0 in stage 13.0 (TID 974). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:45:14 INFO TaskSetManager: Starting task 178.0 in stage 13.0 (TID 995, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 178.0 in stage 13.0 (TID 995)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 157.0 in stage 13.0 (TID 974) in 686 ms on localhost (163/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO Executor: Finished task 162.0 in stage 13.0 (TID 979). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO TaskSetManager: Starting task 179.0 in stage 13.0 (TID 996, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/06 17:45:14 INFO Executor: Running task 179.0 in stage 13.0 (TID 996)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 162.0 in stage 13.0 (TID 979) in 553 ms on localhost (164/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/06 17:45:14 INFO Executor: Finished task 173.0 in stage 13.0 (TID 990). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 180.0 in stage 13.0 (TID 997, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 180.0 in stage 13.0 (TID 997)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 173.0 in stage 13.0 (TID 990) in 146 ms on localhost (165/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 197
15/08/06 17:45:14 INFO Executor: Finished task 165.0 in stage 13.0 (TID 982). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/06 17:45:14 INFO TaskSetManager: Starting task 181.0 in stage 13.0 (TID 998, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 181.0 in stage 13.0 (TID 998)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 165.0 in stage 13.0 (TID 982) in 523 ms on localhost (166/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO Executor: Finished task 166.0 in stage 13.0 (TID 983). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 182.0 in stage 13.0 (TID 999, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 182.0 in stage 13.0 (TID 999)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 166.0 in stage 13.0 (TID 983) in 517 ms on localhost (167/200)
15/08/06 17:45:14 INFO Executor: Finished task 164.0 in stage 13.0 (TID 981). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO Executor: Finished task 167.0 in stage 13.0 (TID 984). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 183.0 in stage 13.0 (TID 1000, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 183.0 in stage 13.0 (TID 1000)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 164.0 in stage 13.0 (TID 981) in 550 ms on localhost (168/200)
15/08/06 17:45:14 INFO TaskSetManager: Starting task 184.0 in stage 13.0 (TID 1001, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 167.0 in stage 13.0 (TID 984) in 507 ms on localhost (169/200)
15/08/06 17:45:14 INFO Executor: Running task 184.0 in stage 13.0 (TID 1001)
15/08/06 17:45:14 INFO Executor: Finished task 169.0 in stage 13.0 (TID 986). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 185.0 in stage 13.0 (TID 1002, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 185.0 in stage 13.0 (TID 1002)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO TaskSetManager: Finished task 169.0 in stage 13.0 (TID 986) in 442 ms on localhost (170/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO Executor: Finished task 168.0 in stage 13.0 (TID 985). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 186.0 in stage 13.0 (TID 1003, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 186.0 in stage 13.0 (TID 1003)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 168.0 in stage 13.0 (TID 985) in 465 ms on localhost (171/200)
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 246
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO Executor: Finished task 171.0 in stage 13.0 (TID 988). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 187.0 in stage 13.0 (TID 1004, localhost, ANY, 1812 bytes)
15/08/06 17:45:14 INFO Executor: Running task 187.0 in stage 13.0 (TID 1004)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO TaskSetManager: Finished task 171.0 in stage 13.0 (TID 988) in 399 ms on localhost (172/200)
15/08/06 17:45:14 INFO Executor: Finished task 170.0 in stage 13.0 (TID 987). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 188.0 in stage 13.0 (TID 1005, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 188.0 in stage 13.0 (TID 1005)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 170.0 in stage 13.0 (TID 987) in 434 ms on localhost (173/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO Executor: Finished task 172.0 in stage 13.0 (TID 989). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 189.0 in stage 13.0 (TID 1006, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 INFO Executor: Running task 189.0 in stage 13.0 (TID 1006)
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO Executor: Finished task 174.0 in stage 13.0 (TID 991). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO TaskSetManager: Finished task 172.0 in stage 13.0 (TID 989) in 411 ms on localhost (174/200)
15/08/06 17:45:14 INFO TaskSetManager: Starting task 190.0 in stage 13.0 (TID 1007, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:14 INFO Executor: Running task 190.0 in stage 13.0 (TID 1007)
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO TaskSetManager: Finished task 174.0 in stage 13.0 (TID 991) in 261 ms on localhost (175/200)
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 176
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 147
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 194
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 145
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/06 17:45:14 INFO Executor: Finished task 177.0 in stage 13.0 (TID 994). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO Executor: Finished task 176.0 in stage 13.0 (TID 993). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 191.0 in stage 13.0 (TID 1008, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 191.0 in stage 13.0 (TID 1008)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 177.0 in stage 13.0 (TID 994) in 389 ms on localhost (176/200)
15/08/06 17:45:14 INFO TaskSetManager: Starting task 192.0 in stage 13.0 (TID 1009, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 192.0 in stage 13.0 (TID 1009)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 176.0 in stage 13.0 (TID 993) in 404 ms on localhost (177/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO Executor: Finished task 175.0 in stage 13.0 (TID 992). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 193.0 in stage 13.0 (TID 1010, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 193.0 in stage 13.0 (TID 1010)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 175.0 in stage 13.0 (TID 992) in 433 ms on localhost (178/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO Executor: Finished task 178.0 in stage 13.0 (TID 995). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 194.0 in stage 13.0 (TID 1011, localhost, ANY, 1814 bytes)
15/08/06 17:45:14 INFO Executor: Running task 194.0 in stage 13.0 (TID 1011)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 178.0 in stage 13.0 (TID 995) in 437 ms on localhost (179/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 158
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/06 17:45:14 INFO Executor: Finished task 179.0 in stage 13.0 (TID 996). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 195.0 in stage 13.0 (TID 1012, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 179.0 in stage 13.0 (TID 996) in 497 ms on localhost (180/200)
15/08/06 17:45:14 INFO Executor: Running task 195.0 in stage 13.0 (TID 1012)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO Executor: Finished task 180.0 in stage 13.0 (TID 997). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 196.0 in stage 13.0 (TID 1013, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 196.0 in stage 13.0 (TID 1013)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 180.0 in stage 13.0 (TID 997) in 483 ms on localhost (181/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 131
15/08/06 17:45:14 INFO Executor: Finished task 185.0 in stage 13.0 (TID 1002). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO Executor: Finished task 181.0 in stage 13.0 (TID 998). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO Executor: Finished task 184.0 in stage 13.0 (TID 1001). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Starting task 197.0 in stage 13.0 (TID 1014, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 197.0 in stage 13.0 (TID 1014)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 185.0 in stage 13.0 (TID 1002) in 483 ms on localhost (182/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO TaskSetManager: Starting task 198.0 in stage 13.0 (TID 1015, localhost, ANY, 1813 bytes)
15/08/06 17:45:14 INFO Executor: Running task 198.0 in stage 13.0 (TID 1015)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 181.0 in stage 13.0 (TID 998) in 522 ms on localhost (183/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:14 INFO TaskSetManager: Starting task 199.0 in stage 13.0 (TID 1016, localhost, ANY, 1815 bytes)
15/08/06 17:45:14 INFO Executor: Running task 199.0 in stage 13.0 (TID 1016)
15/08/06 17:45:14 INFO TaskSetManager: Finished task 184.0 in stage 13.0 (TID 1001) in 507 ms on localhost (184/200)
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/06 17:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:14 INFO Executor: Finished task 186.0 in stage 13.0 (TID 1003). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Finished task 186.0 in stage 13.0 (TID 1003) in 494 ms on localhost (185/200)
15/08/06 17:45:14 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:14 INFO Executor: Finished task 187.0 in stage 13.0 (TID 1004). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Finished task 187.0 in stage 13.0 (TID 1004) in 497 ms on localhost (186/200)
15/08/06 17:45:14 INFO Executor: Finished task 183.0 in stage 13.0 (TID 1000). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/06 17:45:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:14 INFO TaskSetManager: Finished task 183.0 in stage 13.0 (TID 1000) in 540 ms on localhost (187/200)
15/08/06 17:45:14 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/06 17:45:14 INFO Executor: Finished task 182.0 in stage 13.0 (TID 999). 2182 bytes result sent to driver
15/08/06 17:45:14 INFO TaskSetManager: Finished task 182.0 in stage 13.0 (TID 999) in 553 ms on localhost (188/200)
15/08/06 17:45:15 INFO Executor: Finished task 190.0 in stage 13.0 (TID 1007). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 190.0 in stage 13.0 (TID 1007) in 468 ms on localhost (189/200)
15/08/06 17:45:15 INFO Executor: Finished task 188.0 in stage 13.0 (TID 1005). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 188.0 in stage 13.0 (TID 1005) in 491 ms on localhost (190/200)
15/08/06 17:45:15 INFO Executor: Finished task 189.0 in stage 13.0 (TID 1006). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 189.0 in stage 13.0 (TID 1006) in 486 ms on localhost (191/200)
15/08/06 17:45:15 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/06 17:45:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:15 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/06 17:45:15 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:15 INFO Executor: Finished task 192.0 in stage 13.0 (TID 1009). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:15 INFO TaskSetManager: Finished task 192.0 in stage 13.0 (TID 1009) in 351 ms on localhost (192/200)
15/08/06 17:45:15 INFO Executor: Finished task 191.0 in stage 13.0 (TID 1008). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 191.0 in stage 13.0 (TID 1008) in 368 ms on localhost (193/200)
15/08/06 17:45:15 INFO Executor: Finished task 193.0 in stage 13.0 (TID 1010). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/06 17:45:15 INFO TaskSetManager: Finished task 193.0 in stage 13.0 (TID 1010) in 339 ms on localhost (194/200)
15/08/06 17:45:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/06 17:45:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:15 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/06 17:45:15 INFO Executor: Finished task 194.0 in stage 13.0 (TID 1011). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/06 17:45:15 INFO TaskSetManager: Finished task 194.0 in stage 13.0 (TID 1011) in 297 ms on localhost (195/200)
15/08/06 17:45:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/06 17:45:15 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 158
15/08/06 17:45:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/06 17:45:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/06 17:45:15 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 161
15/08/06 17:45:15 INFO Executor: Finished task 195.0 in stage 13.0 (TID 1012). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 195.0 in stage 13.0 (TID 1012) in 262 ms on localhost (196/200)
15/08/06 17:45:15 INFO Executor: Finished task 196.0 in stage 13.0 (TID 1013). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 196.0 in stage 13.0 (TID 1013) in 278 ms on localhost (197/200)
15/08/06 17:45:15 INFO Executor: Finished task 198.0 in stage 13.0 (TID 1015). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 198.0 in stage 13.0 (TID 1015) in 344 ms on localhost (198/200)
15/08/06 17:45:15 INFO Executor: Finished task 197.0 in stage 13.0 (TID 1014). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 197.0 in stage 13.0 (TID 1014) in 366 ms on localhost (199/200)
15/08/06 17:45:15 INFO Executor: Finished task 199.0 in stage 13.0 (TID 1016). 2182 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 199.0 in stage 13.0 (TID 1016) in 352 ms on localhost (200/200)
15/08/06 17:45:15 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/08/06 17:45:15 INFO DAGScheduler: Stage 13 (mapPartitions at Exchange.scala:77) finished in 6.860 s
15/08/06 17:45:15 INFO DAGScheduler: looking for newly runnable stages
15/08/06 17:45:15 INFO DAGScheduler: running: Set()
15/08/06 17:45:15 INFO DAGScheduler: waiting: Set(Stage 14)
15/08/06 17:45:15 INFO DAGScheduler: failed: Set()
15/08/06 17:45:15 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@495c5433
15/08/06 17:45:15 INFO StatsReportListener: task runtime:(count: 200, mean: 536.930000, stdev: 151.433864, max: 821.000000, min: 146.000000)
15/08/06 17:45:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:15 INFO StatsReportListener: 	146.0 ms	303.0 ms	351.0 ms	425.0 ms	538.0 ms	662.0 ms	755.0 ms	784.0 ms	821.0 ms
15/08/06 17:45:15 INFO DAGScheduler: Missing parents for Stage 14: List()
15/08/06 17:45:15 INFO DAGScheduler: Submitting Stage 14 (MapPartitionsRDD[80] at mapPartitions at basicOperators.scala:207), which is now runnable
15/08/06 17:45:15 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 3352.795000, stdev: 425.781685, max: 3815.000000, min: 0.000000)
15/08/06 17:45:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:15 INFO StatsReportListener: 	0.0 B	3.2 KB	3.2 KB	3.3 KB	3.3 KB	3.4 KB	3.5 KB	3.5 KB	3.7 KB
15/08/06 17:45:15 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.240000, stdev: 0.618385, max: 6.000000, min: 0.000000)
15/08/06 17:45:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:15 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	6.0 ms
15/08/06 17:45:15 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:45:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:15 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:45:15 INFO StatsReportListener: task result size:(count: 200, mean: 2182.000000, stdev: 0.000000, max: 2182.000000, min: 2182.000000)
15/08/06 17:45:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:15 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/06 17:45:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 96.247963, stdev: 2.933582, max: 99.430199, min: 74.825175)
15/08/06 17:45:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:15 INFO StatsReportListener: 	75 %	93 %	94 %	96 %	97 %	98 %	98 %	99 %	99 %
15/08/06 17:45:15 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.059937, stdev: 0.301204, max: 4.109589, min: 0.000000)
15/08/06 17:45:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:15 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 4 %
15/08/06 17:45:15 INFO StatsReportListener: other time pct: (count: 200, mean: 3.692100, stdev: 2.858200, max: 25.174825, min: 0.569801)
15/08/06 17:45:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:15 INFO StatsReportListener: 	 1 %	 1 %	 2 %	 2 %	 3 %	 4 %	 6 %	 7 %	25 %
15/08/06 17:45:15 INFO MemoryStore: ensureFreeSpace(151624) called with curMem=1632310, maxMem=3333968363
15/08/06 17:45:15 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 148.1 KB, free 3.1 GB)
15/08/06 17:45:15 INFO MemoryStore: ensureFreeSpace(66865) called with curMem=1783934, maxMem=3333968363
15/08/06 17:45:15 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 65.3 KB, free 3.1 GB)
15/08/06 17:45:15 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:42907 (size: 65.3 KB, free: 3.1 GB)
15/08/06 17:45:15 INFO BlockManagerMaster: Updated info of block broadcast_19_piece0
15/08/06 17:45:15 INFO DefaultExecutionContext: Created broadcast 19 from broadcast at DAGScheduler.scala:838
15/08/06 17:45:15 INFO DAGScheduler: Submitting 200 missing tasks from Stage 14 (MapPartitionsRDD[80] at mapPartitions at basicOperators.scala:207)
15/08/06 17:45:15 INFO TaskSchedulerImpl: Adding task set 14.0 with 200 tasks
15/08/06 17:45:15 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 1017, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 1018, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 1019, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 1020, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 1021, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 1022, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 1023, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 7.0 in stage 14.0 (TID 1024, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 8.0 in stage 14.0 (TID 1025, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 9.0 in stage 14.0 (TID 1026, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 10.0 in stage 14.0 (TID 1027, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 11.0 in stage 14.0 (TID 1028, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 12.0 in stage 14.0 (TID 1029, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 13.0 in stage 14.0 (TID 1030, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 14.0 in stage 14.0 (TID 1031, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 15.0 in stage 14.0 (TID 1032, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO Executor: Running task 3.0 in stage 14.0 (TID 1020)
15/08/06 17:45:15 INFO Executor: Running task 0.0 in stage 14.0 (TID 1017)
15/08/06 17:45:15 INFO Executor: Running task 11.0 in stage 14.0 (TID 1028)
15/08/06 17:45:15 INFO Executor: Running task 6.0 in stage 14.0 (TID 1023)
15/08/06 17:45:15 INFO Executor: Running task 8.0 in stage 14.0 (TID 1025)
15/08/06 17:45:15 INFO Executor: Running task 10.0 in stage 14.0 (TID 1027)
15/08/06 17:45:15 INFO Executor: Running task 15.0 in stage 14.0 (TID 1032)
15/08/06 17:45:15 INFO Executor: Running task 9.0 in stage 14.0 (TID 1026)
15/08/06 17:45:15 INFO Executor: Running task 2.0 in stage 14.0 (TID 1019)
15/08/06 17:45:15 INFO Executor: Running task 7.0 in stage 14.0 (TID 1024)
15/08/06 17:45:15 INFO Executor: Running task 5.0 in stage 14.0 (TID 1022)
15/08/06 17:45:15 INFO Executor: Running task 4.0 in stage 14.0 (TID 1021)
15/08/06 17:45:15 INFO Executor: Running task 1.0 in stage 14.0 (TID 1018)
15/08/06 17:45:15 INFO Executor: Running task 14.0 in stage 14.0 (TID 1031)
15/08/06 17:45:15 INFO Executor: Running task 13.0 in stage 14.0 (TID 1030)
15/08/06 17:45:15 INFO Executor: Running task 12.0 in stage 14.0 (TID 1029)
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2efba895
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000010_1027/part-00010
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d78db8
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000011_1028/part-00011
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6e992bf9
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a162129
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000006_1023/part-00006
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000003_1020/part-00003
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@32db0feb
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000004_1021/part-00004
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4bf22762
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000014_1031/part-00014
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6f7b9e2
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000013_1030/part-00013
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@37de1cda
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000009_1026/part-00009
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3037d1e7
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000001_1018/part-00001
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@30e82f27
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000007_1024/part-00007
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b6fc286
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000005_1022/part-00005
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64c1a50
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b4ef2bc
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@661f6cc3
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000000_1017/part-00000
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a4551b2
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1e286a90
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6aaa6abb
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000002_1019/part-00002
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@266ffc3e
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6f6c2367
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000008_1025/part-00008
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24887857
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000015_1032/part-00015
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@44f7ccbb
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000012_1029/part-00012
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@98a1406
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@48289ace
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72fe4064
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ea98437
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@80bfe55
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79ad86e9
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@43b4225b
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@372fc50
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ad62060
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f0b588a
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@41e20d2f
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000011_1028' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000011
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000011_1028: Committed
15/08/06 17:45:15 INFO Executor: Finished task 11.0 in stage 14.0 (TID 1028). 781 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Starting task 16.0 in stage 14.0 (TID 1033, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000010_1027' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000010
15/08/06 17:45:15 INFO Executor: Running task 16.0 in stage 14.0 (TID 1033)
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000010_1027: Committed
15/08/06 17:45:15 INFO TaskSetManager: Finished task 11.0 in stage 14.0 (TID 1028) in 258 ms on localhost (1/200)
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000006_1023' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000006
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000006_1023: Committed
15/08/06 17:45:15 INFO Executor: Finished task 10.0 in stage 14.0 (TID 1027). 781 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Starting task 17.0 in stage 14.0 (TID 1034, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO Executor: Running task 17.0 in stage 14.0 (TID 1034)
15/08/06 17:45:15 INFO Executor: Finished task 6.0 in stage 14.0 (TID 1023). 781 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Starting task 18.0 in stage 14.0 (TID 1035, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO Executor: Running task 18.0 in stage 14.0 (TID 1035)
15/08/06 17:45:15 INFO TaskSetManager: Finished task 10.0 in stage 14.0 (TID 1027) in 264 ms on localhost (2/200)
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000003_1020' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000003
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000003_1020: Committed
15/08/06 17:45:15 INFO TaskSetManager: Finished task 6.0 in stage 14.0 (TID 1023) in 266 ms on localhost (3/200)
15/08/06 17:45:15 INFO Executor: Finished task 3.0 in stage 14.0 (TID 1020). 781 bytes result sent to driver
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000002_1019' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000002
15/08/06 17:45:15 INFO TaskSetManager: Starting task 19.0 in stage 14.0 (TID 1036, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000002_1019: Committed
15/08/06 17:45:15 INFO Executor: Running task 19.0 in stage 14.0 (TID 1036)
15/08/06 17:45:15 INFO Executor: Finished task 2.0 in stage 14.0 (TID 1019). 781 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 1020) in 270 ms on localhost (4/200)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 20.0 in stage 14.0 (TID 1037, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO Executor: Running task 20.0 in stage 14.0 (TID 1037)
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000005_1022' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000005
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000005_1022: Committed
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000014_1031' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000014
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000001_1018' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000001
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000014_1031: Committed
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000001_1018: Committed
15/08/06 17:45:15 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 1019) in 273 ms on localhost (5/200)
15/08/06 17:45:15 INFO Executor: Finished task 5.0 in stage 14.0 (TID 1022). 781 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Starting task 21.0 in stage 14.0 (TID 1038, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO Executor: Running task 21.0 in stage 14.0 (TID 1038)
15/08/06 17:45:15 INFO Executor: Finished task 1.0 in stage 14.0 (TID 1018). 781 bytes result sent to driver
15/08/06 17:45:15 INFO Executor: Finished task 14.0 in stage 14.0 (TID 1031). 781 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Finished task 5.0 in stage 14.0 (TID 1022) in 275 ms on localhost (6/200)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 22.0 in stage 14.0 (TID 1039, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 1018) in 276 ms on localhost (7/200)
15/08/06 17:45:15 INFO Executor: Running task 22.0 in stage 14.0 (TID 1039)
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000008_1025' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000008
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000008_1025: Committed
15/08/06 17:45:15 INFO TaskSetManager: Finished task 14.0 in stage 14.0 (TID 1031) in 274 ms on localhost (8/200)
15/08/06 17:45:15 INFO TaskSetManager: Starting task 23.0 in stage 14.0 (TID 1040, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO Executor: Running task 23.0 in stage 14.0 (TID 1040)
15/08/06 17:45:15 INFO Executor: Finished task 8.0 in stage 14.0 (TID 1025). 781 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Starting task 24.0 in stage 14.0 (TID 1041, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO Executor: Running task 24.0 in stage 14.0 (TID 1041)
15/08/06 17:45:15 INFO TaskSetManager: Finished task 8.0 in stage 14.0 (TID 1025) in 279 ms on localhost (9/200)
15/08/06 17:45:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000012_1029' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000012
15/08/06 17:45:15 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000012_1029: Committed
15/08/06 17:45:15 INFO Executor: Finished task 12.0 in stage 14.0 (TID 1029). 781 bytes result sent to driver
15/08/06 17:45:15 INFO TaskSetManager: Starting task 25.0 in stage 14.0 (TID 1042, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:15 INFO Executor: Running task 25.0 in stage 14.0 (TID 1042)
15/08/06 17:45:15 INFO TaskSetManager: Finished task 12.0 in stage 14.0 (TID 1029) in 282 ms on localhost (10/200)
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@55365d85
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000018_1035/part-00018
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@11c19607
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000017_1034/part-00017
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c60aeac
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000022_1039/part-00022
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6f6bbd2b
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a508b33
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000016_1033/part-00016
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000020_1037/part-00020
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@50c77b6
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000021_1038/part-00021
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@470d8196
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000019_1036/part-00019
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2fcf95b5
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@656db698
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@80ece0d
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000024_1041/part-00024
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f9bb9d
15/08/06 17:45:15 INFO CodecConfig: Compression set to false
15/08/06 17:45:15 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:15 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@8c4275
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:15 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@785f0f9a
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:15 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2028dd1b
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000025_1042/part-00025
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a130e9c
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@bf07ee0
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bfa8bdd
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3aa856dc
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000020_1037' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000020
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000020_1037: Committed
15/08/06 17:45:16 INFO Executor: Finished task 20.0 in stage 14.0 (TID 1037). 781 bytes result sent to driver
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000016_1033' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000016
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000016_1033: Committed
15/08/06 17:45:16 INFO TaskSetManager: Starting task 26.0 in stage 14.0 (TID 1043, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 26.0 in stage 14.0 (TID 1043)
15/08/06 17:45:16 INFO Executor: Finished task 16.0 in stage 14.0 (TID 1033). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Finished task 20.0 in stage 14.0 (TID 1037) in 333 ms on localhost (11/200)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 27.0 in stage 14.0 (TID 1044, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19a26102
15/08/06 17:45:16 INFO Executor: Running task 27.0 in stage 14.0 (TID 1044)
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000023_1040/part-00023
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO TaskSetManager: Finished task 16.0 in stage 14.0 (TID 1033) in 347 ms on localhost (12/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000021_1038' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000021
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000021_1038: Committed
15/08/06 17:45:16 INFO Executor: Finished task 21.0 in stage 14.0 (TID 1038). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 28.0 in stage 14.0 (TID 1045, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 28.0 in stage 14.0 (TID 1045)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000019_1036' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000019
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000019_1036: Committed
15/08/06 17:45:16 INFO TaskSetManager: Finished task 21.0 in stage 14.0 (TID 1038) in 336 ms on localhost (13/200)
15/08/06 17:45:16 INFO Executor: Finished task 19.0 in stage 14.0 (TID 1036). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 29.0 in stage 14.0 (TID 1046, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 19.0 in stage 14.0 (TID 1036) in 343 ms on localhost (14/200)
15/08/06 17:45:16 INFO Executor: Running task 29.0 in stage 14.0 (TID 1046)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000025_1042' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000025
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000025_1042: Committed
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000024_1041' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000024
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000024_1041: Committed
15/08/06 17:45:16 INFO Executor: Finished task 25.0 in stage 14.0 (TID 1042). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Finished task 24.0 in stage 14.0 (TID 1041). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 30.0 in stage 14.0 (TID 1047, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 30.0 in stage 14.0 (TID 1047)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 25.0 in stage 14.0 (TID 1042) in 337 ms on localhost (15/200)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 31.0 in stage 14.0 (TID 1048, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 31.0 in stage 14.0 (TID 1048)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 24.0 in stage 14.0 (TID 1041) in 342 ms on localhost (16/200)
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6fb76b74
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000023_1040' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000023
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000023_1040: Committed
15/08/06 17:45:16 INFO Executor: Finished task 23.0 in stage 14.0 (TID 1040). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 32.0 in stage 14.0 (TID 1049, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO Executor: Running task 32.0 in stage 14.0 (TID 1049)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO TaskSetManager: Finished task 23.0 in stage 14.0 (TID 1040) in 366 ms on localhost (17/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000004_1021' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000004
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000004_1021: Committed
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO Executor: Finished task 4.0 in stage 14.0 (TID 1021). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 33.0 in stage 14.0 (TID 1050, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 33.0 in stage 14.0 (TID 1050)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 1021) in 655 ms on localhost (18/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000009_1026' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000009
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000009_1026: Committed
15/08/06 17:45:16 INFO Executor: Finished task 9.0 in stage 14.0 (TID 1026). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 34.0 in stage 14.0 (TID 1051, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000007_1024' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000007
15/08/06 17:45:16 INFO Executor: Running task 34.0 in stage 14.0 (TID 1051)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000013_1030' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000013
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000007_1024: Committed
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000013_1030: Committed
15/08/06 17:45:16 INFO Executor: Finished task 13.0 in stage 14.0 (TID 1030). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Finished task 9.0 in stage 14.0 (TID 1026) in 670 ms on localhost (19/200)
15/08/06 17:45:16 INFO Executor: Finished task 7.0 in stage 14.0 (TID 1024). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 35.0 in stage 14.0 (TID 1052, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000000_1017' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000000
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000000_1017: Committed
15/08/06 17:45:16 INFO TaskSetManager: Starting task 36.0 in stage 14.0 (TID 1053, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 36.0 in stage 14.0 (TID 1053)
15/08/06 17:45:16 INFO Executor: Finished task 0.0 in stage 14.0 (TID 1017). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Running task 35.0 in stage 14.0 (TID 1052)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 13.0 in stage 14.0 (TID 1030) in 670 ms on localhost (20/200)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 7.0 in stage 14.0 (TID 1024) in 673 ms on localhost (21/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO TaskSetManager: Starting task 37.0 in stage 14.0 (TID 1054, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 37.0 in stage 14.0 (TID 1054)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000015_1032' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000015
15/08/06 17:45:16 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 1017) in 677 ms on localhost (22/200)
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000015_1032: Committed
15/08/06 17:45:16 INFO Executor: Finished task 15.0 in stage 14.0 (TID 1032). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 38.0 in stage 14.0 (TID 1055, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 38.0 in stage 14.0 (TID 1055)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 15.0 in stage 14.0 (TID 1032) in 676 ms on localhost (23/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ee8d33b
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000027_1044/part-00027
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3bb7b46f
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000030_1047/part-00030
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@19a7020a
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c1003ee
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@f7235af
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c1003ee
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000031_1048/part-00031
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000026_1043/part-00026
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000028_1045/part-00028
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4c5a4264
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1da63a80
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000029_1046/part-00029
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@121cdb62
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@24870838
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@62e2d353
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000027_1044' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000027
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000027_1044: Committed
15/08/06 17:45:16 INFO Executor: Finished task 27.0 in stage 14.0 (TID 1044). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 39.0 in stage 14.0 (TID 1056, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 39.0 in stage 14.0 (TID 1056)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 27.0 in stage 14.0 (TID 1044) in 161 ms on localhost (24/200)
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@38047f04
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17bac498
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000032_1049/part-00032
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6f05894a
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000030_1047' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000030
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000030_1047: Committed
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000038_1055/part-00038
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000028_1045' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000028
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000028_1045: Committed
15/08/06 17:45:16 INFO Executor: Finished task 28.0 in stage 14.0 (TID 1045). 781 bytes result sent to driver
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000031_1048' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000031
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000031_1048: Committed
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3bfcdbb8
15/08/06 17:45:16 INFO Executor: Finished task 30.0 in stage 14.0 (TID 1047). 781 bytes result sent to driver
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO TaskSetManager: Starting task 40.0 in stage 14.0 (TID 1057, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 40.0 in stage 14.0 (TID 1057)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000026_1043' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000026
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000026_1043: Committed
15/08/06 17:45:16 INFO Executor: Finished task 31.0 in stage 14.0 (TID 1048). 781 bytes result sent to driver
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO TaskSetManager: Finished task 28.0 in stage 14.0 (TID 1045) in 184 ms on localhost (25/200)
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO TaskSetManager: Starting task 41.0 in stage 14.0 (TID 1058, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Finished task 26.0 in stage 14.0 (TID 1043). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Running task 41.0 in stage 14.0 (TID 1058)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 42.0 in stage 14.0 (TID 1059, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 42.0 in stage 14.0 (TID 1059)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 43.0 in stage 14.0 (TID 1060, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 43.0 in stage 14.0 (TID 1060)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6cfc07d3
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000033_1050/part-00033
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO TaskSetManager: Finished task 31.0 in stage 14.0 (TID 1048) in 174 ms on localhost (26/200)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 30.0 in stage 14.0 (TID 1047) in 179 ms on localhost (27/200)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 26.0 in stage 14.0 (TID 1043) in 194 ms on localhost (28/200)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a7efb8c
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000036_1053/part-00036
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f0ea88f
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a2beec
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5569a330
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000035_1052/part-00035
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d09cdcd
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3ea6753e
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000034_1051/part-00034
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@27c33c23
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000037_1054/part-00037
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f282409
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000038_1055' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000038
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000038_1055: Committed
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@900481
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6f17b170
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO Executor: Finished task 38.0 in stage 14.0 (TID 1055). 781 bytes result sent to driver
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000033_1050' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000033
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000033_1050: Committed
15/08/06 17:45:16 INFO TaskSetManager: Starting task 44.0 in stage 14.0 (TID 1061, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO Executor: Finished task 33.0 in stage 14.0 (TID 1050). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Running task 44.0 in stage 14.0 (TID 1061)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 38.0 in stage 14.0 (TID 1055) in 175 ms on localhost (29/200)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 45.0 in stage 14.0 (TID 1062, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 45.0 in stage 14.0 (TID 1062)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 33.0 in stage 14.0 (TID 1050) in 201 ms on localhost (30/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000036_1053' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000036
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000036_1053: Committed
15/08/06 17:45:16 INFO Executor: Finished task 36.0 in stage 14.0 (TID 1053). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 46.0 in stage 14.0 (TID 1063, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 46.0 in stage 14.0 (TID 1063)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 36.0 in stage 14.0 (TID 1053) in 189 ms on localhost (31/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000035_1052' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000035
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000035_1052: Committed
15/08/06 17:45:16 INFO Executor: Finished task 35.0 in stage 14.0 (TID 1052). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 47.0 in stage 14.0 (TID 1064, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 47.0 in stage 14.0 (TID 1064)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 35.0 in stage 14.0 (TID 1052) in 195 ms on localhost (32/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000034_1051' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000034
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000034_1051: Committed
15/08/06 17:45:16 INFO Executor: Finished task 34.0 in stage 14.0 (TID 1051). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 48.0 in stage 14.0 (TID 1065, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 48.0 in stage 14.0 (TID 1065)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 34.0 in stage 14.0 (TID 1051) in 203 ms on localhost (33/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000037_1054' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000037
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000037_1054: Committed
15/08/06 17:45:16 INFO Executor: Finished task 37.0 in stage 14.0 (TID 1054). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 49.0 in stage 14.0 (TID 1066, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 49.0 in stage 14.0 (TID 1066)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO TaskSetManager: Finished task 37.0 in stage 14.0 (TID 1054) in 206 ms on localhost (34/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7866543c
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000039_1056/part-00039
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19cecb01
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000042_1059/part-00042
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@61e5e099
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1b11af84
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@261a10fe
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000043_1060/part-00043
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000040_1057/part-00040
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@484cd89d
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c019b38
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000041_1058/part-00041
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@fe28458
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ed5fd26
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000039_1056' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000039
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000039_1056: Committed
15/08/06 17:45:16 INFO Executor: Finished task 39.0 in stage 14.0 (TID 1056). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 50.0 in stage 14.0 (TID 1067, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7cee57b3
15/08/06 17:45:16 INFO Executor: Running task 50.0 in stage 14.0 (TID 1067)
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO TaskSetManager: Finished task 39.0 in stage 14.0 (TID 1056) in 198 ms on localhost (35/200)
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000042_1059' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000042
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000042_1059: Committed
15/08/06 17:45:16 INFO Executor: Finished task 42.0 in stage 14.0 (TID 1059). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 51.0 in stage 14.0 (TID 1068, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 51.0 in stage 14.0 (TID 1068)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 42.0 in stage 14.0 (TID 1059) in 176 ms on localhost (36/200)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c1da657
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000044_1061/part-00044
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4470fcd2
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000046_1063/part-00046
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000043_1060' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000043
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000043_1060: Committed
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000040_1057' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000040
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000040_1057: Committed
15/08/06 17:45:16 INFO Executor: Finished task 43.0 in stage 14.0 (TID 1060). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 52.0 in stage 14.0 (TID 1069, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Finished task 40.0 in stage 14.0 (TID 1057). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Running task 52.0 in stage 14.0 (TID 1069)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 53.0 in stage 14.0 (TID 1070, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 53.0 in stage 14.0 (TID 1070)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 43.0 in stage 14.0 (TID 1060) in 184 ms on localhost (37/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000041_1058' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000041
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000041_1058: Committed
15/08/06 17:45:16 INFO TaskSetManager: Finished task 40.0 in stage 14.0 (TID 1057) in 189 ms on localhost (38/200)
15/08/06 17:45:16 INFO Executor: Finished task 41.0 in stage 14.0 (TID 1058). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 54.0 in stage 14.0 (TID 1071, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 54.0 in stage 14.0 (TID 1071)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 41.0 in stage 14.0 (TID 1058) in 191 ms on localhost (39/200)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@509d6d0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000045_1062/part-00045
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6f9ad429
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000022_1039' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000022
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000022_1039: Committed
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ddf08fd
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO Executor: Finished task 22.0 in stage 14.0 (TID 1039). 781 bytes result sent to driver
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO TaskSetManager: Starting task 55.0 in stage 14.0 (TID 1072, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 55.0 in stage 14.0 (TID 1072)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 22.0 in stage 14.0 (TID 1039) in 902 ms on localhost (40/200)
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@25e262c
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000018_1035' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000018
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000017_1034' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000017
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000018_1035: Committed
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000017_1034: Committed
15/08/06 17:45:16 INFO Executor: Finished task 17.0 in stage 14.0 (TID 1034). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Finished task 18.0 in stage 14.0 (TID 1035). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 56.0 in stage 14.0 (TID 1073, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 56.0 in stage 14.0 (TID 1073)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 17.0 in stage 14.0 (TID 1034) in 924 ms on localhost (41/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000029_1046' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000029
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000029_1046: Committed
15/08/06 17:45:16 INFO Executor: Finished task 29.0 in stage 14.0 (TID 1046). 781 bytes result sent to driver
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4823ea4c
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000048_1065/part-00048
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO TaskSetManager: Finished task 18.0 in stage 14.0 (TID 1035) in 925 ms on localhost (42/200)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 57.0 in stage 14.0 (TID 1074, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 58.0 in stage 14.0 (TID 1075, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 57.0 in stage 14.0 (TID 1074)
15/08/06 17:45:16 INFO Executor: Running task 58.0 in stage 14.0 (TID 1075)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 29.0 in stage 14.0 (TID 1046) in 585 ms on localhost (43/200)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4492fcc0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000047_1064/part-00047
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35463c8f
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67fb4be9
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d249320
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000049_1066/part-00049
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000032_1049' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000032
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000032_1049: Committed
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO Executor: Finished task 32.0 in stage 14.0 (TID 1049). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 59.0 in stage 14.0 (TID 1076, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 59.0 in stage 14.0 (TID 1076)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 32.0 in stage 14.0 (TID 1049) in 587 ms on localhost (44/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000045_1062' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000045
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000046_1063' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000046
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000045_1062: Committed
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000046_1063: Committed
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000044_1061' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000044
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000044_1061: Committed
15/08/06 17:45:16 INFO Executor: Finished task 45.0 in stage 14.0 (TID 1062). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Finished task 46.0 in stage 14.0 (TID 1063). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 60.0 in stage 14.0 (TID 1077, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 60.0 in stage 14.0 (TID 1077)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO TaskSetManager: Finished task 45.0 in stage 14.0 (TID 1062) in 381 ms on localhost (45/200)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 61.0 in stage 14.0 (TID 1078, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 61.0 in stage 14.0 (TID 1078)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 46.0 in stage 14.0 (TID 1063) in 376 ms on localhost (46/200)
15/08/06 17:45:16 INFO Executor: Finished task 44.0 in stage 14.0 (TID 1061). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 62.0 in stage 14.0 (TID 1079, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 44.0 in stage 14.0 (TID 1061) in 387 ms on localhost (47/200)
15/08/06 17:45:16 INFO Executor: Running task 62.0 in stage 14.0 (TID 1079)
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ade786c
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000048_1065' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000048
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000048_1065: Committed
15/08/06 17:45:16 INFO Executor: Finished task 48.0 in stage 14.0 (TID 1065). 781 bytes result sent to driver
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO TaskSetManager: Starting task 63.0 in stage 14.0 (TID 1080, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 63.0 in stage 14.0 (TID 1080)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 48.0 in stage 14.0 (TID 1065) in 377 ms on localhost (48/200)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000047_1064' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000047
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000047_1064: Committed
15/08/06 17:45:16 INFO Executor: Finished task 47.0 in stage 14.0 (TID 1064). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 64.0 in stage 14.0 (TID 1081, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 64.0 in stage 14.0 (TID 1081)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 47.0 in stage 14.0 (TID 1064) in 393 ms on localhost (49/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@769b7580
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000050_1067/part-00050
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000049_1066' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000049
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000049_1066: Committed
15/08/06 17:45:16 INFO Executor: Finished task 49.0 in stage 14.0 (TID 1066). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 65.0 in stage 14.0 (TID 1082, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 65.0 in stage 14.0 (TID 1082)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 49.0 in stage 14.0 (TID 1066) in 406 ms on localhost (50/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5abc870f
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000052_1069/part-00052
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d52913
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7272042c
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000051_1068/part-00051
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@60531360
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000054_1071/part-00054
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6e1d3507
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51979128
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000053_1070/part-00053
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5bdb846c
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000057_1074/part-00057
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4abebeeb
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000055_1072/part-00055
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6719ea6a
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4aec9787
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22791a82
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000058_1075/part-00058
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16d613dd
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@40b21de1
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000056_1073/part-00056
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@550ba621
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6fc7ec25
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000052_1069' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000052
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000052_1069: Committed
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000050_1067' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000050
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000050_1067: Committed
15/08/06 17:45:16 INFO Executor: Finished task 52.0 in stage 14.0 (TID 1069). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Finished task 50.0 in stage 14.0 (TID 1067). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 66.0 in stage 14.0 (TID 1083, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 66.0 in stage 14.0 (TID 1083)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 52.0 in stage 14.0 (TID 1069) in 385 ms on localhost (51/200)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4c4d75aa
15/08/06 17:45:16 INFO TaskSetManager: Starting task 67.0 in stage 14.0 (TID 1084, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000061_1078/part-00061
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ee48b21
15/08/06 17:45:16 INFO Executor: Running task 67.0 in stage 14.0 (TID 1084)
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO TaskSetManager: Finished task 50.0 in stage 14.0 (TID 1067) in 402 ms on localhost (52/200)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2280e041
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000059_1076/part-00059
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@43bf6494
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@59d3fcef
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000053_1070' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000053
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000053_1070: Committed
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000054_1071' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000054
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000054_1071: Committed
15/08/06 17:45:16 INFO Executor: Finished task 53.0 in stage 14.0 (TID 1070). 781 bytes result sent to driver
15/08/06 17:45:16 INFO Executor: Finished task 54.0 in stage 14.0 (TID 1071). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 68.0 in stage 14.0 (TID 1085, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 68.0 in stage 14.0 (TID 1085)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 53.0 in stage 14.0 (TID 1070) in 407 ms on localhost (53/200)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 69.0 in stage 14.0 (TID 1086, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 69.0 in stage 14.0 (TID 1086)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 54.0 in stage 14.0 (TID 1071) in 401 ms on localhost (54/200)
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2c410e67
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000057_1074' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000057
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000057_1074: Committed
15/08/06 17:45:16 INFO Executor: Finished task 57.0 in stage 14.0 (TID 1074). 781 bytes result sent to driver
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000058_1075' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000058
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000058_1075: Committed
15/08/06 17:45:16 INFO TaskSetManager: Starting task 70.0 in stage 14.0 (TID 1087, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 70.0 in stage 14.0 (TID 1087)
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000055_1072' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000055
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000055_1072: Committed
15/08/06 17:45:16 INFO Executor: Finished task 58.0 in stage 14.0 (TID 1075). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Finished task 57.0 in stage 14.0 (TID 1074) in 196 ms on localhost (55/200)
15/08/06 17:45:16 INFO Executor: Finished task 55.0 in stage 14.0 (TID 1072). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 71.0 in stage 14.0 (TID 1088, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 71.0 in stage 14.0 (TID 1088)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 58.0 in stage 14.0 (TID 1075) in 197 ms on localhost (56/200)
15/08/06 17:45:16 INFO TaskSetManager: Starting task 72.0 in stage 14.0 (TID 1089, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 72.0 in stage 14.0 (TID 1089)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a51ac68
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000060_1077/part-00060
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO TaskSetManager: Finished task 55.0 in stage 14.0 (TID 1072) in 222 ms on localhost (57/200)
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d24bc7c
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000062_1079/part-00062
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000061_1078' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000061
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000061_1078: Committed
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO Executor: Finished task 61.0 in stage 14.0 (TID 1078). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 73.0 in stage 14.0 (TID 1090, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 73.0 in stage 14.0 (TID 1090)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@27f8b9d7
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000063_1080/part-00063
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO TaskSetManager: Finished task 61.0 in stage 14.0 (TID 1078) in 174 ms on localhost (58/200)
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@76e36fb5
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000064_1081/part-00064
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34bdd2
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@298c0c2d
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000059_1076' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000059
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000059_1076: Committed
15/08/06 17:45:16 INFO Executor: Finished task 59.0 in stage 14.0 (TID 1076). 781 bytes result sent to driver
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d0e0d0a
15/08/06 17:45:16 INFO TaskSetManager: Starting task 74.0 in stage 14.0 (TID 1091, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO Executor: Running task 74.0 in stage 14.0 (TID 1091)
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO TaskSetManager: Finished task 59.0 in stage 14.0 (TID 1076) in 199 ms on localhost (59/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@cdcfb85
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4df5c
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000065_1082/part-00065
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000062_1079' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000062
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000062_1079: Committed
15/08/06 17:45:16 INFO Executor: Finished task 62.0 in stage 14.0 (TID 1079). 781 bytes result sent to driver
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO TaskSetManager: Starting task 75.0 in stage 14.0 (TID 1092, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO Executor: Running task 75.0 in stage 14.0 (TID 1092)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 62.0 in stage 14.0 (TID 1079) in 204 ms on localhost (60/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000063_1080' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000063
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000063_1080: Committed
15/08/06 17:45:16 INFO Executor: Finished task 63.0 in stage 14.0 (TID 1080). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 76.0 in stage 14.0 (TID 1093, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 76.0 in stage 14.0 (TID 1093)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 63.0 in stage 14.0 (TID 1080) in 204 ms on localhost (61/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77127f51
15/08/06 17:45:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000064_1081' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000064
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000064_1081: Committed
15/08/06 17:45:16 INFO Executor: Finished task 64.0 in stage 14.0 (TID 1081). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 77.0 in stage 14.0 (TID 1094, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 77.0 in stage 14.0 (TID 1094)
15/08/06 17:45:16 INFO TaskSetManager: Finished task 64.0 in stage 14.0 (TID 1081) in 205 ms on localhost (62/200)
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@45875ac2
15/08/06 17:45:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000067_1084/part-00067
15/08/06 17:45:16 INFO CodecConfig: Compression set to false
15/08/06 17:45:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:16 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:16 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000065_1082' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000065
15/08/06 17:45:16 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000065_1082: Committed
15/08/06 17:45:16 INFO Executor: Finished task 65.0 in stage 14.0 (TID 1082). 781 bytes result sent to driver
15/08/06 17:45:16 INFO TaskSetManager: Starting task 78.0 in stage 14.0 (TID 1095, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:16 INFO Executor: Running task 78.0 in stage 14.0 (TID 1095)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 65.0 in stage 14.0 (TID 1082) in 283 ms on localhost (63/200)
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d4be11b
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000066_1083/part-00066
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@120f80b8
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7993c1cd
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000068_1085/part-00068
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4f5dd73a
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@186d9d43
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000069_1086/part-00069
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7e0ad03d
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000072_1089/part-00072
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000067_1084' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000067
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000067_1084: Committed
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO Executor: Finished task 67.0 in stage 14.0 (TID 1084). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 79.0 in stage 14.0 (TID 1096, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO Executor: Running task 79.0 in stage 14.0 (TID 1096)
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57c78d00
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000070_1087/part-00070
15/08/06 17:45:17 INFO TaskSetManager: Finished task 67.0 in stage 14.0 (TID 1084) in 236 ms on localhost (64/200)
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6dfa2805
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000066_1083' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000066
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000066_1083: Committed
15/08/06 17:45:17 INFO Executor: Finished task 66.0 in stage 14.0 (TID 1083). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 80.0 in stage 14.0 (TID 1097, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 80.0 in stage 14.0 (TID 1097)
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b88b0ea
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO TaskSetManager: Finished task 66.0 in stage 14.0 (TID 1083) in 249 ms on localhost (65/200)
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12cae076
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5145143
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000071_1088/part-00071
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7a058da7
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@290da81f
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000073_1090/part-00073
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@200f885a
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000075_1092/part-00075
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5bc0dd27
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000076_1093/part-00076
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@52f75a80
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000074_1091/part-00074
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d56b40d
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000077_1094/part-00077
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5746f98f
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000078_1095/part-00078
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@74aa480c
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000079_1096/part-00079
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@576047aa
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000080_1097/part-00080
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3bdb7616
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000068_1085' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000068
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@58f587bb
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000068_1085: Committed
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@309ec3f2
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO Executor: Finished task 68.0 in stage 14.0 (TID 1085). 781 bytes result sent to driver
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000070_1087' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000070
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000070_1087: Committed
15/08/06 17:45:17 INFO Executor: Finished task 70.0 in stage 14.0 (TID 1087). 781 bytes result sent to driver
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@411cd51b
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000051_1068' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000051
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000051_1068: Committed
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4beb0102
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@318f513e
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000056_1073' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000056
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000056_1073: Committed
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000060_1077' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000060
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000060_1077: Committed
15/08/06 17:45:17 INFO Executor: Finished task 51.0 in stage 14.0 (TID 1068). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 81.0 in stage 14.0 (TID 1098, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 81.0 in stage 14.0 (TID 1098)
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO TaskSetManager: Starting task 82.0 in stage 14.0 (TID 1099, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Finished task 56.0 in stage 14.0 (TID 1073). 781 bytes result sent to driver
15/08/06 17:45:17 INFO Executor: Running task 82.0 in stage 14.0 (TID 1099)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 83.0 in stage 14.0 (TID 1100, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 83.0 in stage 14.0 (TID 1100)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 84.0 in stage 14.0 (TID 1101, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Finished task 60.0 in stage 14.0 (TID 1077). 781 bytes result sent to driver
15/08/06 17:45:17 INFO Executor: Running task 84.0 in stage 14.0 (TID 1101)
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO TaskSetManager: Starting task 85.0 in stage 14.0 (TID 1102, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO Executor: Running task 85.0 in stage 14.0 (TID 1102)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 68.0 in stage 14.0 (TID 1085) in 656 ms on localhost (66/200)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 56.0 in stage 14.0 (TID 1073) in 856 ms on localhost (67/200)
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4fb9d54e
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO TaskSetManager: Finished task 51.0 in stage 14.0 (TID 1068) in 1072 ms on localhost (68/200)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 70.0 in stage 14.0 (TID 1087) in 650 ms on localhost (69/200)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 60.0 in stage 14.0 (TID 1077) in 809 ms on localhost (70/200)
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@99c2ecf
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67f06391
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000072_1089' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000072
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000072_1089: Committed
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO Executor: Finished task 72.0 in stage 14.0 (TID 1089). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 86.0 in stage 14.0 (TID 1103, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 86.0 in stage 14.0 (TID 1103)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 72.0 in stage 14.0 (TID 1089) in 656 ms on localhost (71/200)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000073_1090' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000073
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000073_1090: Committed
15/08/06 17:45:17 INFO Executor: Finished task 73.0 in stage 14.0 (TID 1090). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 87.0 in stage 14.0 (TID 1104, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 87.0 in stage 14.0 (TID 1104)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000078_1095' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000078
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000078_1095: Committed
15/08/06 17:45:17 INFO TaskSetManager: Finished task 73.0 in stage 14.0 (TID 1090) in 652 ms on localhost (72/200)
15/08/06 17:45:17 INFO Executor: Finished task 78.0 in stage 14.0 (TID 1095). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 88.0 in stage 14.0 (TID 1105, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 88.0 in stage 14.0 (TID 1105)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000075_1092' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000075
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000075_1092: Committed
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000074_1091' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000074
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000074_1091: Committed
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000076_1093' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000076
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000076_1093: Committed
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000079_1096' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000079
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000079_1096: Committed
15/08/06 17:45:17 INFO Executor: Finished task 75.0 in stage 14.0 (TID 1092). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Finished task 78.0 in stage 14.0 (TID 1095) in 495 ms on localhost (73/200)
15/08/06 17:45:17 INFO Executor: Finished task 74.0 in stage 14.0 (TID 1091). 781 bytes result sent to driver
15/08/06 17:45:17 INFO Executor: Finished task 76.0 in stage 14.0 (TID 1093). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 89.0 in stage 14.0 (TID 1106, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Finished task 79.0 in stage 14.0 (TID 1096). 781 bytes result sent to driver
15/08/06 17:45:17 INFO Executor: Running task 89.0 in stage 14.0 (TID 1106)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 75.0 in stage 14.0 (TID 1092) in 624 ms on localhost (74/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 90.0 in stage 14.0 (TID 1107, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000077_1094' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000077
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000077_1094: Committed
15/08/06 17:45:17 INFO Executor: Running task 90.0 in stage 14.0 (TID 1107)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 74.0 in stage 14.0 (TID 1091) in 640 ms on localhost (75/200)
15/08/06 17:45:17 INFO Executor: Finished task 77.0 in stage 14.0 (TID 1094). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 91.0 in stage 14.0 (TID 1108, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 91.0 in stage 14.0 (TID 1108)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 92.0 in stage 14.0 (TID 1109, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000080_1097' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000080
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000080_1097: Committed
15/08/06 17:45:17 INFO TaskSetManager: Finished task 79.0 in stage 14.0 (TID 1096) in 470 ms on localhost (76/200)
15/08/06 17:45:17 INFO Executor: Running task 92.0 in stage 14.0 (TID 1109)
15/08/06 17:45:17 INFO Executor: Finished task 80.0 in stage 14.0 (TID 1097). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Finished task 76.0 in stage 14.0 (TID 1093) in 620 ms on localhost (77/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 93.0 in stage 14.0 (TID 1110, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 93.0 in stage 14.0 (TID 1110)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 77.0 in stage 14.0 (TID 1094) in 610 ms on localhost (78/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 94.0 in stage 14.0 (TID 1111, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 94.0 in stage 14.0 (TID 1111)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 80.0 in stage 14.0 (TID 1097) in 464 ms on localhost (79/200)
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7fb87bfb
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000085_1102/part-00085
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d626809
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000082_1099/part-00082
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b1f5a5e
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000084_1101/part-00084
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6766000e
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000087_1104/part-00087
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42b3d076
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2b3b1530
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000081_1098/part-00081
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@33ec187c
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d4b8e6d
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b73a521
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@155e403a
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5ecaaacb
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000093_1110/part-00093
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000092_1109/part-00092
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78427249
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000089_1106/part-00089
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22952fd
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000090_1107/part-00090
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7f5b6f70
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000083_1100/part-00083
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@38d524e6
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000091_1108/part-00091
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31699530
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000088_1105/part-00088
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@ac19ba8
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2cd2381c
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7fea7578
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79c65ae3
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d792b8a
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7285792b
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23cb1701
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000094_1111/part-00094
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000086_1103/part-00086
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000082_1099' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000082
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000082_1099: Committed
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@45d6a7d8
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO Executor: Finished task 82.0 in stage 14.0 (TID 1099). 781 bytes result sent to driver
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e89cd27
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@41777d99
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO TaskSetManager: Starting task 95.0 in stage 14.0 (TID 1112, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO Executor: Running task 95.0 in stage 14.0 (TID 1112)
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO TaskSetManager: Finished task 82.0 in stage 14.0 (TID 1099) in 292 ms on localhost (80/200)
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2839b72d
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000084_1101' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000084
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000084_1101: Committed
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000087_1104' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000087
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000087_1104: Committed
15/08/06 17:45:17 INFO Executor: Finished task 84.0 in stage 14.0 (TID 1101). 781 bytes result sent to driver
15/08/06 17:45:17 INFO Executor: Finished task 87.0 in stage 14.0 (TID 1104). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 96.0 in stage 14.0 (TID 1113, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 96.0 in stage 14.0 (TID 1113)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 97.0 in stage 14.0 (TID 1114, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 97.0 in stage 14.0 (TID 1114)
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@38201b4
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000081_1098' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000081
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO TaskSetManager: Finished task 84.0 in stage 14.0 (TID 1101) in 304 ms on localhost (81/200)
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000081_1098: Committed
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO Executor: Finished task 81.0 in stage 14.0 (TID 1098). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Finished task 87.0 in stage 14.0 (TID 1104) in 283 ms on localhost (82/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 98.0 in stage 14.0 (TID 1115, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 98.0 in stage 14.0 (TID 1115)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 81.0 in stage 14.0 (TID 1098) in 309 ms on localhost (83/200)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000089_1106' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000089
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000089_1106: Committed
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000083_1100' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000083
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000083_1100: Committed
15/08/06 17:45:17 INFO Executor: Finished task 83.0 in stage 14.0 (TID 1100). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 99.0 in stage 14.0 (TID 1116, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 99.0 in stage 14.0 (TID 1116)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000092_1109' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000092
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000092_1109: Committed
15/08/06 17:45:17 INFO Executor: Finished task 89.0 in stage 14.0 (TID 1106). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Finished task 83.0 in stage 14.0 (TID 1100) in 315 ms on localhost (84/200)
15/08/06 17:45:17 INFO Executor: Finished task 92.0 in stage 14.0 (TID 1109). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 100.0 in stage 14.0 (TID 1117, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 100.0 in stage 14.0 (TID 1117)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 89.0 in stage 14.0 (TID 1106) in 289 ms on localhost (85/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 101.0 in stage 14.0 (TID 1118, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 101.0 in stage 14.0 (TID 1118)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 92.0 in stage 14.0 (TID 1109) in 290 ms on localhost (86/200)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000088_1105' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000088
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000088_1105: Committed
15/08/06 17:45:17 INFO Executor: Finished task 88.0 in stage 14.0 (TID 1105). 781 bytes result sent to driver
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000090_1107' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000090
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000090_1107: Committed
15/08/06 17:45:17 INFO TaskSetManager: Starting task 102.0 in stage 14.0 (TID 1119, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 102.0 in stage 14.0 (TID 1119)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000091_1108' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000091
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000091_1108: Committed
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000086_1103' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000086
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000086_1103: Committed
15/08/06 17:45:17 INFO TaskSetManager: Finished task 88.0 in stage 14.0 (TID 1105) in 300 ms on localhost (87/200)
15/08/06 17:45:17 INFO Executor: Finished task 90.0 in stage 14.0 (TID 1107). 781 bytes result sent to driver
15/08/06 17:45:17 INFO Executor: Finished task 91.0 in stage 14.0 (TID 1108). 781 bytes result sent to driver
15/08/06 17:45:17 INFO Executor: Finished task 86.0 in stage 14.0 (TID 1103). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 103.0 in stage 14.0 (TID 1120, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 103.0 in stage 14.0 (TID 1120)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000094_1111' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000094
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000094_1111: Committed
15/08/06 17:45:17 INFO TaskSetManager: Finished task 90.0 in stage 14.0 (TID 1107) in 298 ms on localhost (88/200)
15/08/06 17:45:17 INFO Executor: Finished task 94.0 in stage 14.0 (TID 1111). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Finished task 91.0 in stage 14.0 (TID 1108) in 299 ms on localhost (89/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 104.0 in stage 14.0 (TID 1121, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 104.0 in stage 14.0 (TID 1121)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 86.0 in stage 14.0 (TID 1103) in 317 ms on localhost (90/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 105.0 in stage 14.0 (TID 1122, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 105.0 in stage 14.0 (TID 1122)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 106.0 in stage 14.0 (TID 1123, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 106.0 in stage 14.0 (TID 1123)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 94.0 in stage 14.0 (TID 1111) in 296 ms on localhost (91/200)
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000069_1086' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000069
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000069_1086: Committed
15/08/06 17:45:17 INFO Executor: Finished task 69.0 in stage 14.0 (TID 1086). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 107.0 in stage 14.0 (TID 1124, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 107.0 in stage 14.0 (TID 1124)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 69.0 in stage 14.0 (TID 1086) in 1060 ms on localhost (92/200)
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000071_1088' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000071
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000071_1088: Committed
15/08/06 17:45:17 INFO Executor: Finished task 71.0 in stage 14.0 (TID 1088). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 108.0 in stage 14.0 (TID 1125, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 71.0 in stage 14.0 (TID 1088) in 1070 ms on localhost (93/200)
15/08/06 17:45:17 INFO Executor: Running task 108.0 in stage 14.0 (TID 1125)
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@47d165a1
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000097_1114/part-00097
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ee3af64
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000096_1113/part-00096
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@46292f44
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@643ad77a
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000095_1112/part-00095
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51a9d2a5
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000098_1115/part-00098
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@64447885
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000099_1116/part-00099
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@20667726
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000103_1120/part-00103
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ee7dac8
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@538dded1
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000102_1119/part-00102
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@286775cb
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@754f3592
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000105_1122/part-00105
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78f6371a
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000106_1123/part-00106
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000097_1114' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000097
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000097_1114: Committed
15/08/06 17:45:17 INFO Executor: Finished task 97.0 in stage 14.0 (TID 1114). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 109.0 in stage 14.0 (TID 1126, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 109.0 in stage 14.0 (TID 1126)
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6787c30d
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@237bd045
15/08/06 17:45:17 INFO TaskSetManager: Finished task 97.0 in stage 14.0 (TID 1114) in 171 ms on localhost (94/200)
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@74e8154e
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000104_1121/part-00104
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d7abbe
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@534eb15d
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7eef6f88
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5af95bf4
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000095_1112' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000095
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000095_1112: Committed
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@412fd776
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000096_1113' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000096
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000096_1113: Committed
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:17 INFO Executor: Finished task 95.0 in stage 14.0 (TID 1112). 781 bytes result sent to driver
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000098_1115' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000098
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000098_1115: Committed
15/08/06 17:45:17 INFO Executor: Finished task 96.0 in stage 14.0 (TID 1113). 781 bytes result sent to driver
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000102_1119' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000102
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000102_1119: Committed
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO TaskSetManager: Starting task 110.0 in stage 14.0 (TID 1127, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Finished task 98.0 in stage 14.0 (TID 1115). 781 bytes result sent to driver
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO Executor: Running task 110.0 in stage 14.0 (TID 1127)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 111.0 in stage 14.0 (TID 1128, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 111.0 in stage 14.0 (TID 1128)
15/08/06 17:45:17 INFO Executor: Finished task 102.0 in stage 14.0 (TID 1119). 781 bytes result sent to driver
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8aec379
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000101_1118/part-00101
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO TaskSetManager: Finished task 95.0 in stage 14.0 (TID 1112) in 220 ms on localhost (95/200)
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO TaskSetManager: Finished task 98.0 in stage 14.0 (TID 1115) in 202 ms on localhost (96/200)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 96.0 in stage 14.0 (TID 1113) in 208 ms on localhost (97/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 112.0 in stage 14.0 (TID 1129, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 112.0 in stage 14.0 (TID 1129)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 113.0 in stage 14.0 (TID 1130, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 113.0 in stage 14.0 (TID 1130)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 102.0 in stage 14.0 (TID 1119) in 188 ms on localhost (98/200)
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000103_1120' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000103
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000099_1116' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000099
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000103_1120: Committed
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000099_1116: Committed
15/08/06 17:45:17 INFO Executor: Finished task 99.0 in stage 14.0 (TID 1116). 781 bytes result sent to driver
15/08/06 17:45:17 INFO Executor: Finished task 103.0 in stage 14.0 (TID 1120). 781 bytes result sent to driver
15/08/06 17:45:17 INFO TaskSetManager: Starting task 114.0 in stage 14.0 (TID 1131, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 114.0 in stage 14.0 (TID 1131)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 99.0 in stage 14.0 (TID 1116) in 206 ms on localhost (99/200)
15/08/06 17:45:17 INFO TaskSetManager: Starting task 115.0 in stage 14.0 (TID 1132, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:17 INFO Executor: Running task 115.0 in stage 14.0 (TID 1132)
15/08/06 17:45:17 INFO TaskSetManager: Finished task 103.0 in stage 14.0 (TID 1120) in 194 ms on localhost (100/200)
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2055cac3
15/08/06 17:45:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@37e74581
15/08/06 17:45:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000100_1117/part-00100
15/08/06 17:45:17 INFO CodecConfig: Compression set to false
15/08/06 17:45:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:17 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:17 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000104_1121' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000104
15/08/06 17:45:17 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000104_1121: Committed
15/08/06 17:45:17 INFO Executor: Finished task 104.0 in stage 14.0 (TID 1121). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 116.0 in stage 14.0 (TID 1133, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 116.0 in stage 14.0 (TID 1133)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 104.0 in stage 14.0 (TID 1121) in 203 ms on localhost (101/200)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6958084a
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000107_1124/part-00107
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d5d0af5
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000101_1118' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000101
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000101_1118: Committed
15/08/06 17:45:18 INFO Executor: Finished task 101.0 in stage 14.0 (TID 1118). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 117.0 in stage 14.0 (TID 1134, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 117.0 in stage 14.0 (TID 1134)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1e974f2a
15/08/06 17:45:18 INFO TaskSetManager: Finished task 101.0 in stage 14.0 (TID 1118) in 235 ms on localhost (102/200)
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000100_1117' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000100
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000100_1117: Committed
15/08/06 17:45:18 INFO Executor: Finished task 100.0 in stage 14.0 (TID 1117). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 118.0 in stage 14.0 (TID 1135, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 118.0 in stage 14.0 (TID 1135)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 100.0 in stage 14.0 (TID 1117) in 253 ms on localhost (103/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000107_1124' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000107
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000107_1124: Committed
15/08/06 17:45:18 INFO Executor: Finished task 107.0 in stage 14.0 (TID 1124). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 119.0 in stage 14.0 (TID 1136, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 107.0 in stage 14.0 (TID 1124) in 327 ms on localhost (104/200)
15/08/06 17:45:18 INFO Executor: Running task 119.0 in stage 14.0 (TID 1136)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000093_1110' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000093
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000093_1110: Committed
15/08/06 17:45:18 INFO Executor: Finished task 93.0 in stage 14.0 (TID 1110). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 120.0 in stage 14.0 (TID 1137, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 120.0 in stage 14.0 (TID 1137)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 93.0 in stage 14.0 (TID 1110) in 705 ms on localhost (105/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000085_1102' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000085
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000085_1102: Committed
15/08/06 17:45:18 INFO Executor: Finished task 85.0 in stage 14.0 (TID 1102). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 121.0 in stage 14.0 (TID 1138, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b02a169
15/08/06 17:45:18 INFO Executor: Running task 121.0 in stage 14.0 (TID 1138)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000108_1125/part-00108
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO TaskSetManager: Finished task 85.0 in stage 14.0 (TID 1102) in 742 ms on localhost (106/200)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6f68db88
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@395a34bc
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000109_1126/part-00109
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000108_1125' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000108
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000108_1125: Committed
15/08/06 17:45:18 INFO Executor: Finished task 108.0 in stage 14.0 (TID 1125). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 122.0 in stage 14.0 (TID 1139, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 122.0 in stage 14.0 (TID 1139)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 108.0 in stage 14.0 (TID 1125) in 358 ms on localhost (107/200)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fe2433b
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000111_1128/part-00111
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2247d9ff
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000114_1131/part-00114
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d6478f4
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@521ed0b5
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000110_1127/part-00110
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@372a8159
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000112_1129/part-00112
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31628056
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000113_1130/part-00113
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@39d7a76e
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d31446c
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a1923e
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000115_1132/part-00115
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f03affe
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000116_1133/part-00116
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1f29aebd
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f8fa481
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7db16d6c
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@736c7a07
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000109_1126' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000109
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000109_1126: Committed
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@29505ddb
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO Executor: Finished task 109.0 in stage 14.0 (TID 1126). 781 bytes result sent to driver
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000114_1131' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000114
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000114_1131: Committed
15/08/06 17:45:18 INFO TaskSetManager: Starting task 123.0 in stage 14.0 (TID 1140, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 123.0 in stage 14.0 (TID 1140)
15/08/06 17:45:18 INFO Executor: Finished task 114.0 in stage 14.0 (TID 1131). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Finished task 109.0 in stage 14.0 (TID 1126) in 364 ms on localhost (108/200)
15/08/06 17:45:18 INFO TaskSetManager: Starting task 124.0 in stage 14.0 (TID 1141, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 124.0 in stage 14.0 (TID 1141)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 114.0 in stage 14.0 (TID 1131) in 321 ms on localhost (109/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000111_1128' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000111
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000111_1128: Committed
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ef9ec7b
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000117_1134/part-00117
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO Executor: Finished task 111.0 in stage 14.0 (TID 1128). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 125.0 in stage 14.0 (TID 1142, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 125.0 in stage 14.0 (TID 1142)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 111.0 in stage 14.0 (TID 1128) in 338 ms on localhost (110/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000115_1132' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000115
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000115_1132: Committed
15/08/06 17:45:18 INFO Executor: Finished task 115.0 in stage 14.0 (TID 1132). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 126.0 in stage 14.0 (TID 1143, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 126.0 in stage 14.0 (TID 1143)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000110_1127' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000110
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000110_1127: Committed
15/08/06 17:45:18 INFO Executor: Finished task 110.0 in stage 14.0 (TID 1127). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Finished task 115.0 in stage 14.0 (TID 1132) in 332 ms on localhost (111/200)
15/08/06 17:45:18 INFO TaskSetManager: Starting task 127.0 in stage 14.0 (TID 1144, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 127.0 in stage 14.0 (TID 1144)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000116_1133' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000116
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000116_1133: Committed
15/08/06 17:45:18 INFO TaskSetManager: Finished task 110.0 in stage 14.0 (TID 1127) in 346 ms on localhost (112/200)
15/08/06 17:45:18 INFO Executor: Finished task 116.0 in stage 14.0 (TID 1133). 781 bytes result sent to driver
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000113_1130' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000113
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000113_1130: Committed
15/08/06 17:45:18 INFO TaskSetManager: Starting task 128.0 in stage 14.0 (TID 1145, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 116.0 in stage 14.0 (TID 1133) in 326 ms on localhost (113/200)
15/08/06 17:45:18 INFO Executor: Finished task 113.0 in stage 14.0 (TID 1130). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 129.0 in stage 14.0 (TID 1146, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 129.0 in stage 14.0 (TID 1146)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 113.0 in stage 14.0 (TID 1130) in 346 ms on localhost (114/200)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15bb0ef0
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO Executor: Running task 128.0 in stage 14.0 (TID 1145)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@66b04e13
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000118_1135/part-00118
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1f920c28
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000121_1138/part-00121
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b6a0475
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a8c159b
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000119_1136/part-00119
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000117_1134' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000117
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000117_1134: Committed
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO Executor: Finished task 117.0 in stage 14.0 (TID 1134). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 130.0 in stage 14.0 (TID 1147, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 130.0 in stage 14.0 (TID 1147)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 117.0 in stage 14.0 (TID 1134) in 335 ms on localhost (115/200)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14e85425
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ac34b87
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35681d54
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000120_1137/part-00120
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000118_1135' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000118
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000118_1135: Committed
15/08/06 17:45:18 INFO Executor: Finished task 118.0 in stage 14.0 (TID 1135). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 131.0 in stage 14.0 (TID 1148, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 131.0 in stage 14.0 (TID 1148)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 118.0 in stage 14.0 (TID 1135) in 336 ms on localhost (116/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000121_1138' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000121
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000121_1138: Committed
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO Executor: Finished task 121.0 in stage 14.0 (TID 1138). 781 bytes result sent to driver
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO TaskSetManager: Starting task 132.0 in stage 14.0 (TID 1149, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 132.0 in stage 14.0 (TID 1149)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 121.0 in stage 14.0 (TID 1138) in 173 ms on localhost (117/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000106_1123' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000106
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000106_1123: Committed
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000119_1136' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000119
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000119_1136: Committed
15/08/06 17:45:18 INFO Executor: Finished task 106.0 in stage 14.0 (TID 1123). 781 bytes result sent to driver
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@588ee472
15/08/06 17:45:18 INFO Executor: Finished task 119.0 in stage 14.0 (TID 1136). 781 bytes result sent to driver
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000105_1122' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000105
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000105_1122: Committed
15/08/06 17:45:18 INFO TaskSetManager: Starting task 133.0 in stage 14.0 (TID 1150, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 133.0 in stage 14.0 (TID 1150)
15/08/06 17:45:18 INFO Executor: Finished task 105.0 in stage 14.0 (TID 1122). 781 bytes result sent to driver
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO TaskSetManager: Finished task 106.0 in stage 14.0 (TID 1123) in 588 ms on localhost (118/200)
15/08/06 17:45:18 INFO TaskSetManager: Starting task 134.0 in stage 14.0 (TID 1151, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 134.0 in stage 14.0 (TID 1151)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 119.0 in stage 14.0 (TID 1136) in 191 ms on localhost (119/200)
15/08/06 17:45:18 INFO TaskSetManager: Starting task 135.0 in stage 14.0 (TID 1152, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 135.0 in stage 14.0 (TID 1152)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 105.0 in stage 14.0 (TID 1122) in 593 ms on localhost (120/200)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48ed10bc
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000122_1139/part-00122
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34b6fb81
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6be5a09b
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000125_1142/part-00125
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19529996
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000124_1141/part-00124
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7de153d6
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000127_1144/part-00127
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@32efefb0
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000126_1143/part-00126
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000122_1139' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000122
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000122_1139: Committed
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@623b51f3
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO Executor: Finished task 122.0 in stage 14.0 (TID 1139). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 136.0 in stage 14.0 (TID 1153, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 136.0 in stage 14.0 (TID 1153)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3fcb7af5
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO TaskSetManager: Finished task 122.0 in stage 14.0 (TID 1139) in 207 ms on localhost (121/200)
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@499c7795
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@59acf39f
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000123_1140/part-00123
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b8175ec
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64eaf528
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000127_1144' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000127
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000127_1144: Committed
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000125_1142' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000125
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000125_1142: Committed
15/08/06 17:45:18 INFO Executor: Finished task 127.0 in stage 14.0 (TID 1144). 781 bytes result sent to driver
15/08/06 17:45:18 INFO Executor: Finished task 125.0 in stage 14.0 (TID 1142). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 137.0 in stage 14.0 (TID 1154, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 137.0 in stage 14.0 (TID 1154)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 127.0 in stage 14.0 (TID 1144) in 163 ms on localhost (122/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000126_1143' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000126
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000126_1143: Committed
15/08/06 17:45:18 INFO TaskSetManager: Starting task 138.0 in stage 14.0 (TID 1155, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 138.0 in stage 14.0 (TID 1155)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@71d63e87
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000130_1147/part-00130
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO TaskSetManager: Finished task 125.0 in stage 14.0 (TID 1142) in 173 ms on localhost (123/200)
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO Executor: Finished task 126.0 in stage 14.0 (TID 1143). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 139.0 in stage 14.0 (TID 1156, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 139.0 in stage 14.0 (TID 1156)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 126.0 in stage 14.0 (TID 1143) in 168 ms on localhost (124/200)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d5364bd
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000133_1150/part-00133
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@661d3389
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000123_1140' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000123
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000123_1140: Committed
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b605baf
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000129_1146/part-00129
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO Executor: Finished task 123.0 in stage 14.0 (TID 1140). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 140.0 in stage 14.0 (TID 1157, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 140.0 in stage 14.0 (TID 1157)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 123.0 in stage 14.0 (TID 1140) in 288 ms on localhost (125/200)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7b7c4f66
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000128_1145/part-00128
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e323c9a
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000132_1149/part-00132
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@233d897
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000131_1148/part-00131
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62f0080a
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000134_1151/part-00134
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3a432d14
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@104499ee
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000135_1152/part-00135
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d5e09a
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6ae27547
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@58df5d2c
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@305e2149
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@649616ce
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d5e8457
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000134_1151' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000134
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000132_1149' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000132
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000134_1151: Committed
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000132_1149: Committed
15/08/06 17:45:18 INFO Executor: Finished task 132.0 in stage 14.0 (TID 1149). 781 bytes result sent to driver
15/08/06 17:45:18 INFO Executor: Finished task 134.0 in stage 14.0 (TID 1151). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 141.0 in stage 14.0 (TID 1158, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 141.0 in stage 14.0 (TID 1158)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO TaskSetManager: Finished task 134.0 in stage 14.0 (TID 1151) in 262 ms on localhost (126/200)
15/08/06 17:45:18 INFO TaskSetManager: Starting task 142.0 in stage 14.0 (TID 1159, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 142.0 in stage 14.0 (TID 1159)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 132.0 in stage 14.0 (TID 1149) in 270 ms on localhost (127/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000135_1152' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000135
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000135_1152: Committed
15/08/06 17:45:18 INFO Executor: Finished task 135.0 in stage 14.0 (TID 1152). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 143.0 in stage 14.0 (TID 1160, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 143.0 in stage 14.0 (TID 1160)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 135.0 in stage 14.0 (TID 1152) in 265 ms on localhost (128/200)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@578529c4
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000136_1153/part-00136
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@28fc551c
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b15d1ae
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000138_1155/part-00138
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000136_1153' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000136
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000136_1153: Committed
15/08/06 17:45:18 INFO Executor: Finished task 136.0 in stage 14.0 (TID 1153). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 144.0 in stage 14.0 (TID 1161, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 144.0 in stage 14.0 (TID 1161)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 136.0 in stage 14.0 (TID 1153) in 250 ms on localhost (129/200)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@44311b0a
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@73e6507f
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000137_1154/part-00137
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000112_1129' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000112
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000112_1129: Committed
15/08/06 17:45:18 INFO Executor: Finished task 112.0 in stage 14.0 (TID 1129). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 145.0 in stage 14.0 (TID 1162, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 145.0 in stage 14.0 (TID 1162)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 112.0 in stage 14.0 (TID 1129) in 738 ms on localhost (130/200)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@576f4e31
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4838a78a
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000139_1156/part-00139
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000138_1155' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000138
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000138_1155: Committed
15/08/06 17:45:18 INFO Executor: Finished task 138.0 in stage 14.0 (TID 1155). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 146.0 in stage 14.0 (TID 1163, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 146.0 in stage 14.0 (TID 1163)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 138.0 in stage 14.0 (TID 1155) in 246 ms on localhost (131/200)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4ad0a072
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000137_1154' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000137
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000137_1154: Committed
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO Executor: Finished task 137.0 in stage 14.0 (TID 1154). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 147.0 in stage 14.0 (TID 1164, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 147.0 in stage 14.0 (TID 1164)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 137.0 in stage 14.0 (TID 1154) in 265 ms on localhost (132/200)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@261b665a
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000140_1157/part-00140
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000139_1156' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000139
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000139_1156: Committed
15/08/06 17:45:18 INFO Executor: Finished task 139.0 in stage 14.0 (TID 1156). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 148.0 in stage 14.0 (TID 1165, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 148.0 in stage 14.0 (TID 1165)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 139.0 in stage 14.0 (TID 1156) in 288 ms on localhost (133/200)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6085b2ea
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7b11337
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000142_1159/part-00142
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a4821f3
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000141_1158/part-00141
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@504b688c
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000143_1160/part-00143
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c44f3e9
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32e97552
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000140_1157' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000140
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000140_1157: Committed
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO Executor: Finished task 140.0 in stage 14.0 (TID 1157). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 149.0 in stage 14.0 (TID 1166, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 149.0 in stage 14.0 (TID 1166)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 140.0 in stage 14.0 (TID 1157) in 218 ms on localhost (134/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000120_1137' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000120
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000120_1137: Committed
15/08/06 17:45:18 INFO Executor: Finished task 120.0 in stage 14.0 (TID 1137). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 150.0 in stage 14.0 (TID 1167, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 150.0 in stage 14.0 (TID 1167)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 120.0 in stage 14.0 (TID 1137) in 613 ms on localhost (135/200)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@57f86f93
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000142_1159' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000142
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000142_1159: Committed
15/08/06 17:45:18 INFO Executor: Finished task 142.0 in stage 14.0 (TID 1159). 781 bytes result sent to driver
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000141_1158' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000141
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000141_1158: Committed
15/08/06 17:45:18 INFO TaskSetManager: Starting task 151.0 in stage 14.0 (TID 1168, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 151.0 in stage 14.0 (TID 1168)
15/08/06 17:45:18 INFO Executor: Finished task 141.0 in stage 14.0 (TID 1158). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Finished task 142.0 in stage 14.0 (TID 1159) in 179 ms on localhost (136/200)
15/08/06 17:45:18 INFO TaskSetManager: Starting task 152.0 in stage 14.0 (TID 1169, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 152.0 in stage 14.0 (TID 1169)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 141.0 in stage 14.0 (TID 1158) in 184 ms on localhost (137/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000143_1160' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000143
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000143_1160: Committed
15/08/06 17:45:18 INFO Executor: Finished task 143.0 in stage 14.0 (TID 1160). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 153.0 in stage 14.0 (TID 1170, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 153.0 in stage 14.0 (TID 1170)
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO TaskSetManager: Finished task 143.0 in stage 14.0 (TID 1160) in 189 ms on localhost (138/200)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3851a014
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000144_1161/part-00144
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78f92619
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000145_1162/part-00145
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d34715b
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000124_1141' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000124
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000124_1141: Committed
15/08/06 17:45:18 INFO Executor: Finished task 124.0 in stage 14.0 (TID 1141). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 154.0 in stage 14.0 (TID 1171, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 154.0 in stage 14.0 (TID 1171)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 124.0 in stage 14.0 (TID 1141) in 578 ms on localhost (139/200)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a397e31
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@548c01a7
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000146_1163/part-00146
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000144_1161' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000144
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000144_1161: Committed
15/08/06 17:45:18 INFO Executor: Finished task 144.0 in stage 14.0 (TID 1161). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 155.0 in stage 14.0 (TID 1172, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 155.0 in stage 14.0 (TID 1172)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 144.0 in stage 14.0 (TID 1161) in 184 ms on localhost (140/200)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@331ecb88
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@233206f1
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000148_1165/part-00148
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@30a4c2fe
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000147_1164/part-00147
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000145_1162' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000145
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000145_1162: Committed
15/08/06 17:45:18 INFO Executor: Finished task 145.0 in stage 14.0 (TID 1162). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 156.0 in stage 14.0 (TID 1173, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 156.0 in stage 14.0 (TID 1173)
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@eb2dd58
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:18 INFO TaskSetManager: Finished task 145.0 in stage 14.0 (TID 1162) in 197 ms on localhost (141/200)
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78f22f2c
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000146_1163' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000146
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000146_1163: Committed
15/08/06 17:45:18 INFO Executor: Finished task 146.0 in stage 14.0 (TID 1163). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Starting task 157.0 in stage 14.0 (TID 1174, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 157.0 in stage 14.0 (TID 1174)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 146.0 in stage 14.0 (TID 1163) in 194 ms on localhost (142/200)
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000148_1165' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000148
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000148_1165: Committed
15/08/06 17:45:18 INFO Executor: Finished task 148.0 in stage 14.0 (TID 1165). 781 bytes result sent to driver
15/08/06 17:45:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000147_1164' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000147
15/08/06 17:45:18 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000147_1164: Committed
15/08/06 17:45:18 INFO TaskSetManager: Starting task 158.0 in stage 14.0 (TID 1175, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 158.0 in stage 14.0 (TID 1175)
15/08/06 17:45:18 INFO Executor: Finished task 147.0 in stage 14.0 (TID 1164). 781 bytes result sent to driver
15/08/06 17:45:18 INFO TaskSetManager: Finished task 148.0 in stage 14.0 (TID 1165) in 167 ms on localhost (143/200)
15/08/06 17:45:18 INFO TaskSetManager: Starting task 159.0 in stage 14.0 (TID 1176, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:18 INFO Executor: Running task 159.0 in stage 14.0 (TID 1176)
15/08/06 17:45:18 INFO TaskSetManager: Finished task 147.0 in stage 14.0 (TID 1164) in 195 ms on localhost (144/200)
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@437e3db0
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000149_1166/part-00149
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1aef1482
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000150_1167/part-00150
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1bc72f4a
15/08/06 17:45:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@72bdac60
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000151_1168/part-00151
15/08/06 17:45:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2800d7a
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000153_1170/part-00153
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression set to false
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:18 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:18 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b383660
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@779e696
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000152_1169/part-00152
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000128_1145' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000128
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000128_1145: Committed
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64164899
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO Executor: Finished task 128.0 in stage 14.0 (TID 1145). 781 bytes result sent to driver
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO TaskSetManager: Starting task 160.0 in stage 14.0 (TID 1177, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 160.0 in stage 14.0 (TID 1177)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 128.0 in stage 14.0 (TID 1145) in 793 ms on localhost (145/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000129_1146' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000129
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000129_1146: Committed
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@73c8cb12
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6e502258
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO Executor: Finished task 129.0 in stage 14.0 (TID 1146). 781 bytes result sent to driver
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000131_1148' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000131
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000133_1150' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000133
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000131_1148: Committed
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000133_1150: Committed
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO TaskSetManager: Starting task 161.0 in stage 14.0 (TID 1178, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO Executor: Running task 161.0 in stage 14.0 (TID 1178)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000130_1147' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000130
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000130_1147: Committed
15/08/06 17:45:19 INFO TaskSetManager: Finished task 129.0 in stage 14.0 (TID 1146) in 795 ms on localhost (146/200)
15/08/06 17:45:19 INFO Executor: Finished task 131.0 in stage 14.0 (TID 1148). 781 bytes result sent to driver
15/08/06 17:45:19 INFO Executor: Finished task 133.0 in stage 14.0 (TID 1150). 781 bytes result sent to driver
15/08/06 17:45:19 INFO Executor: Finished task 130.0 in stage 14.0 (TID 1147). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 162.0 in stage 14.0 (TID 1179, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 162.0 in stage 14.0 (TID 1179)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 163.0 in stage 14.0 (TID 1180, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000149_1166' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000149
15/08/06 17:45:19 INFO Executor: Running task 163.0 in stage 14.0 (TID 1180)
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000149_1166: Committed
15/08/06 17:45:19 INFO TaskSetManager: Finished task 133.0 in stage 14.0 (TID 1150) in 739 ms on localhost (147/200)
15/08/06 17:45:19 INFO Executor: Finished task 149.0 in stage 14.0 (TID 1166). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Finished task 131.0 in stage 14.0 (TID 1148) in 755 ms on localhost (148/200)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 164.0 in stage 14.0 (TID 1181, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 164.0 in stage 14.0 (TID 1181)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 165.0 in stage 14.0 (TID 1182, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 165.0 in stage 14.0 (TID 1182)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 130.0 in stage 14.0 (TID 1147) in 773 ms on localhost (149/200)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 149.0 in stage 14.0 (TID 1166) in 320 ms on localhost (150/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000150_1167' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000150
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000150_1167: Committed
15/08/06 17:45:19 INFO Executor: Finished task 150.0 in stage 14.0 (TID 1167). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 166.0 in stage 14.0 (TID 1183, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 166.0 in stage 14.0 (TID 1183)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 150.0 in stage 14.0 (TID 1167) in 330 ms on localhost (151/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000152_1169' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000152
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000152_1169: Committed
15/08/06 17:45:19 INFO Executor: Finished task 152.0 in stage 14.0 (TID 1169). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 167.0 in stage 14.0 (TID 1184, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 167.0 in stage 14.0 (TID 1184)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 152.0 in stage 14.0 (TID 1169) in 321 ms on localhost (152/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000151_1168' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000151
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000151_1168: Committed
15/08/06 17:45:19 INFO Executor: Finished task 151.0 in stage 14.0 (TID 1168). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 168.0 in stage 14.0 (TID 1185, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 168.0 in stage 14.0 (TID 1185)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 151.0 in stage 14.0 (TID 1168) in 329 ms on localhost (153/200)
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2f72cb3
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000154_1171/part-00154
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42b17892
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000155_1172/part-00155
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@222b72db
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1a8c6096
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@793f5245
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000156_1173/part-00156
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22e0ecf0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000157_1174/part-00157
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000154_1171' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000154
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000154_1171: Committed
15/08/06 17:45:19 INFO Executor: Finished task 154.0 in stage 14.0 (TID 1171). 781 bytes result sent to driver
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d36fbe3
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO TaskSetManager: Starting task 169.0 in stage 14.0 (TID 1186, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO Executor: Running task 169.0 in stage 14.0 (TID 1186)
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO TaskSetManager: Finished task 154.0 in stage 14.0 (TID 1171) in 324 ms on localhost (154/200)
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@71b933e7
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000155_1172' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000155
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000155_1172: Committed
15/08/06 17:45:19 INFO Executor: Finished task 155.0 in stage 14.0 (TID 1172). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 170.0 in stage 14.0 (TID 1187, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 170.0 in stage 14.0 (TID 1187)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 155.0 in stage 14.0 (TID 1172) in 327 ms on localhost (155/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000156_1173' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000156
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000156_1173: Committed
15/08/06 17:45:19 INFO Executor: Finished task 156.0 in stage 14.0 (TID 1173). 781 bytes result sent to driver
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000157_1174' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000157
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000157_1174: Committed
15/08/06 17:45:19 INFO TaskSetManager: Starting task 171.0 in stage 14.0 (TID 1188, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 171.0 in stage 14.0 (TID 1188)
15/08/06 17:45:19 INFO Executor: Finished task 157.0 in stage 14.0 (TID 1174). 781 bytes result sent to driver
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f83cff3
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000158_1175/part-00158
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO TaskSetManager: Finished task 156.0 in stage 14.0 (TID 1173) in 315 ms on localhost (156/200)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 172.0 in stage 14.0 (TID 1189, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 172.0 in stage 14.0 (TID 1189)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 157.0 in stage 14.0 (TID 1174) in 306 ms on localhost (157/200)
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4fd84783
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000159_1176/part-00159
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2bed942d
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d985b8e
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000158_1175' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000158
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000158_1175: Committed
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:19 INFO Executor: Finished task 158.0 in stage 14.0 (TID 1175). 781 bytes result sent to driver
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000159_1176' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000159
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000159_1176: Committed
15/08/06 17:45:19 INFO TaskSetManager: Starting task 173.0 in stage 14.0 (TID 1190, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO Executor: Finished task 159.0 in stage 14.0 (TID 1176). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Finished task 158.0 in stage 14.0 (TID 1175) in 325 ms on localhost (158/200)
15/08/06 17:45:19 INFO Executor: Running task 173.0 in stage 14.0 (TID 1190)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 174.0 in stage 14.0 (TID 1191, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 174.0 in stage 14.0 (TID 1191)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 159.0 in stage 14.0 (TID 1176) in 324 ms on localhost (159/200)
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4dce7e7f
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000164_1181/part-00164
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b49538b
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000162_1179/part-00162
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19774c14
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000160_1177/part-00160
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@32c6f0f5
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000165_1182/part-00165
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d37e860
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000161_1178/part-00161
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34a26e40
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000168_1185/part-00168
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@13f446bf
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d9678b6
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e1c7243
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000163_1180/part-00163
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b707436
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000166_1183/part-00166
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@442f4cca
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b9b2084
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1573f14d
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5149f23d
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ef2b81d
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000167_1184/part-00167
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c18c11c
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76e26d4a
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000162_1179' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000162
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000162_1179: Committed
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO Executor: Finished task 162.0 in stage 14.0 (TID 1179). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 175.0 in stage 14.0 (TID 1192, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 175.0 in stage 14.0 (TID 1192)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000161_1178' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000161
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000161_1178: Committed
15/08/06 17:45:19 INFO TaskSetManager: Finished task 162.0 in stage 14.0 (TID 1179) in 200 ms on localhost (160/200)
15/08/06 17:45:19 INFO Executor: Finished task 161.0 in stage 14.0 (TID 1178). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 176.0 in stage 14.0 (TID 1193, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 176.0 in stage 14.0 (TID 1193)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 161.0 in stage 14.0 (TID 1178) in 205 ms on localhost (161/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000160_1177' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000160
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000160_1177: Committed
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000164_1181' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000164
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000164_1181: Committed
15/08/06 17:45:19 INFO Executor: Finished task 160.0 in stage 14.0 (TID 1177). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 177.0 in stage 14.0 (TID 1194, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 177.0 in stage 14.0 (TID 1194)
15/08/06 17:45:19 INFO Executor: Finished task 164.0 in stage 14.0 (TID 1181). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Finished task 160.0 in stage 14.0 (TID 1177) in 215 ms on localhost (162/200)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 178.0 in stage 14.0 (TID 1195, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30411db
15/08/06 17:45:19 INFO Executor: Running task 178.0 in stage 14.0 (TID 1195)
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO TaskSetManager: Finished task 164.0 in stage 14.0 (TID 1181) in 204 ms on localhost (163/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000166_1183' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000166
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000166_1183: Committed
15/08/06 17:45:19 INFO Executor: Finished task 166.0 in stage 14.0 (TID 1183). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 179.0 in stage 14.0 (TID 1196, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 179.0 in stage 14.0 (TID 1196)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 166.0 in stage 14.0 (TID 1183) in 194 ms on localhost (164/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000163_1180' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000163
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000163_1180: Committed
15/08/06 17:45:19 INFO Executor: Finished task 163.0 in stage 14.0 (TID 1180). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 180.0 in stage 14.0 (TID 1197, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 180.0 in stage 14.0 (TID 1197)
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:19 INFO TaskSetManager: Finished task 163.0 in stage 14.0 (TID 1180) in 224 ms on localhost (165/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000167_1184' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000167
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000167_1184: Committed
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:19 INFO Executor: Finished task 167.0 in stage 14.0 (TID 1184). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 181.0 in stage 14.0 (TID 1198, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 181.0 in stage 14.0 (TID 1198)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 167.0 in stage 14.0 (TID 1184) in 206 ms on localhost (166/200)
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@40808f51
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000169_1186/part-00169
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f750ae3
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000171_1188/part-00171
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@71fd7ac0
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000172_1189/part-00172
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6204f391
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@789be909
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000170_1187/part-00170
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2008ba2c
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3223e2a9
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7aa3830d
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1b68aa63
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000173_1190/part-00173
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000169_1186' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000169
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000169_1186: Committed
15/08/06 17:45:19 INFO Executor: Finished task 169.0 in stage 14.0 (TID 1186). 781 bytes result sent to driver
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000153_1170' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000153
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000153_1170: Committed
15/08/06 17:45:19 INFO TaskSetManager: Starting task 182.0 in stage 14.0 (TID 1199, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@68227a68
15/08/06 17:45:19 INFO Executor: Running task 182.0 in stage 14.0 (TID 1199)
15/08/06 17:45:19 INFO Executor: Finished task 153.0 in stage 14.0 (TID 1170). 781 bytes result sent to driver
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000174_1191/part-00174
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO TaskSetManager: Finished task 169.0 in stage 14.0 (TID 1186) in 411 ms on localhost (167/200)
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO TaskSetManager: Starting task 183.0 in stage 14.0 (TID 1200, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 183.0 in stage 14.0 (TID 1200)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 153.0 in stage 14.0 (TID 1170) in 778 ms on localhost (168/200)
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34a0ab9d
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000172_1189' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000172
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000172_1189: Committed
15/08/06 17:45:19 INFO Executor: Finished task 172.0 in stage 14.0 (TID 1189). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 184.0 in stage 14.0 (TID 1201, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 184.0 in stage 14.0 (TID 1201)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000171_1188' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000171
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000171_1188: Committed
15/08/06 17:45:19 INFO TaskSetManager: Finished task 172.0 in stage 14.0 (TID 1189) in 401 ms on localhost (169/200)
15/08/06 17:45:19 INFO Executor: Finished task 171.0 in stage 14.0 (TID 1188). 781 bytes result sent to driver
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000170_1187' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000170
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000170_1187: Committed
15/08/06 17:45:19 INFO TaskSetManager: Starting task 185.0 in stage 14.0 (TID 1202, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Finished task 170.0 in stage 14.0 (TID 1187). 781 bytes result sent to driver
15/08/06 17:45:19 INFO Executor: Running task 185.0 in stage 14.0 (TID 1202)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 171.0 in stage 14.0 (TID 1188) in 406 ms on localhost (170/200)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 186.0 in stage 14.0 (TID 1203, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 186.0 in stage 14.0 (TID 1203)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 170.0 in stage 14.0 (TID 1187) in 418 ms on localhost (171/200)
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@9775f30
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000173_1190' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000173
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000173_1190: Committed
15/08/06 17:45:19 INFO Executor: Finished task 173.0 in stage 14.0 (TID 1190). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 187.0 in stage 14.0 (TID 1204, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 187.0 in stage 14.0 (TID 1204)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 173.0 in stage 14.0 (TID 1190) in 390 ms on localhost (172/200)
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000174_1191' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000174
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000174_1191: Committed
15/08/06 17:45:19 INFO Executor: Finished task 174.0 in stage 14.0 (TID 1191). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 188.0 in stage 14.0 (TID 1205, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 188.0 in stage 14.0 (TID 1205)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 174.0 in stage 14.0 (TID 1191) in 395 ms on localhost (173/200)
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@70781615
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000176_1193/part-00176
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b06fbc
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000177_1194/part-00177
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@50342de4
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000175_1192/part-00175
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@79ef9502
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@74c34fa3
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d86bcb4
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@18cf0dce
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000179_1196/part-00179
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7cfa06a8
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000178_1195/part-00178
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@cef5fa0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000180_1197/part-00180
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@304da60a
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000181_1198/part-00181
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000176_1193' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000176
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000176_1193: Committed
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000177_1194' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000177
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000177_1194: Committed
15/08/06 17:45:19 INFO Executor: Finished task 176.0 in stage 14.0 (TID 1193). 781 bytes result sent to driver
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@40481882
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO Executor: Finished task 177.0 in stage 14.0 (TID 1194). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 189.0 in stage 14.0 (TID 1206, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO Executor: Running task 189.0 in stage 14.0 (TID 1206)
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO TaskSetManager: Finished task 176.0 in stage 14.0 (TID 1193) in 383 ms on localhost (174/200)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 190.0 in stage 14.0 (TID 1207, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 190.0 in stage 14.0 (TID 1207)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 177.0 in stage 14.0 (TID 1194) in 378 ms on localhost (175/200)
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14525974
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000175_1192' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000175
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000175_1192: Committed
15/08/06 17:45:19 INFO Executor: Finished task 175.0 in stage 14.0 (TID 1192). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 191.0 in stage 14.0 (TID 1208, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 191.0 in stage 14.0 (TID 1208)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 175.0 in stage 14.0 (TID 1192) in 396 ms on localhost (176/200)
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3318d531
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17d4c0cb
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000165_1182' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000165
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000165_1182: Committed
15/08/06 17:45:19 INFO Executor: Finished task 165.0 in stage 14.0 (TID 1182). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 192.0 in stage 14.0 (TID 1209, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 192.0 in stage 14.0 (TID 1209)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 165.0 in stage 14.0 (TID 1182) in 604 ms on localhost (177/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000168_1185' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000168
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000168_1185: Committed
15/08/06 17:45:19 INFO Executor: Finished task 168.0 in stage 14.0 (TID 1185). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 193.0 in stage 14.0 (TID 1210, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 193.0 in stage 14.0 (TID 1210)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000180_1197' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000180
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000180_1197: Committed
15/08/06 17:45:19 INFO TaskSetManager: Finished task 168.0 in stage 14.0 (TID 1185) in 586 ms on localhost (178/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000181_1198' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000181
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000181_1198: Committed
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO Executor: Finished task 181.0 in stage 14.0 (TID 1198). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 194.0 in stage 14.0 (TID 1211, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 194.0 in stage 14.0 (TID 1211)
15/08/06 17:45:19 INFO Executor: Finished task 180.0 in stage 14.0 (TID 1197). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Finished task 181.0 in stage 14.0 (TID 1198) in 387 ms on localhost (179/200)
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO TaskSetManager: Starting task 195.0 in stage 14.0 (TID 1212, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 195.0 in stage 14.0 (TID 1212)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 180.0 in stage 14.0 (TID 1197) in 401 ms on localhost (180/200)
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@59eda1d7
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000182_1199/part-00182
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@40135757
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000183_1200/part-00183
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@15e093ce
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000184_1201/part-00184
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f74f876
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000186_1203/part-00186
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3214a320
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@68e1a38f
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@12cf1d9a
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000185_1202/part-00185
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b44e2c2
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5381918a
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@16f32bbe
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000182_1199' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000182
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000182_1199: Committed
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3779b18e
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000187_1204/part-00187
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO Executor: Finished task 182.0 in stage 14.0 (TID 1199). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 196.0 in stage 14.0 (TID 1213, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 196.0 in stage 14.0 (TID 1213)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 182.0 in stage 14.0 (TID 1199) in 192 ms on localhost (181/200)
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000184_1201' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000184
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000185_1202' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000185
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000186_1203' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000186
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000185_1202: Committed
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000186_1203: Committed
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000184_1201: Committed
15/08/06 17:45:19 INFO Executor: Finished task 186.0 in stage 14.0 (TID 1203). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Starting task 197.0 in stage 14.0 (TID 1214, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 197.0 in stage 14.0 (TID 1214)
15/08/06 17:45:19 INFO Executor: Finished task 184.0 in stage 14.0 (TID 1201). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Finished task 186.0 in stage 14.0 (TID 1203) in 180 ms on localhost (182/200)
15/08/06 17:45:19 INFO TaskSetManager: Starting task 198.0 in stage 14.0 (TID 1215, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 198.0 in stage 14.0 (TID 1215)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 184.0 in stage 14.0 (TID 1201) in 186 ms on localhost (183/200)
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c32f140
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000188_1205/part-00188
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@147cd00a
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO Executor: Finished task 185.0 in stage 14.0 (TID 1202). 781 bytes result sent to driver
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO TaskSetManager: Starting task 199.0 in stage 14.0 (TID 1216, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/06 17:45:19 INFO Executor: Running task 199.0 in stage 14.0 (TID 1216)
15/08/06 17:45:19 INFO TaskSetManager: Finished task 185.0 in stage 14.0 (TID 1202) in 192 ms on localhost (184/200)
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30729598
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000187_1204' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000187
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000187_1204: Committed
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51c238c1
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000189_1206/part-00189
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO Executor: Finished task 187.0 in stage 14.0 (TID 1204). 781 bytes result sent to driver
15/08/06 17:45:19 INFO TaskSetManager: Finished task 187.0 in stage 14.0 (TID 1204) in 218 ms on localhost (185/200)
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/06 17:45:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7afd1c8c
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000192_1209/part-00192
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@364379b3
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51dd6085
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000191_1208/part-00191
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1cebe5c0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000190_1207/part-00190
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000188_1205' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000188
15/08/06 17:45:19 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000188_1205: Committed
15/08/06 17:45:19 INFO Executor: Finished task 188.0 in stage 14.0 (TID 1205). 781 bytes result sent to driver
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO TaskSetManager: Finished task 188.0 in stage 14.0 (TID 1205) in 234 ms on localhost (186/200)
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4611d2f9
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000193_1210/part-00193
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7ddad0cd
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@345a07a0
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@151e4d34
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a0ab45e
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000195_1212/part-00195
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1aa19353
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000194_1211/part-00194
15/08/06 17:45:19 INFO CodecConfig: Compression set to false
15/08/06 17:45:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@65cf3bcd
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:19 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:19 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:19 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000189_1206' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000189
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000189_1206: Committed
15/08/06 17:45:20 INFO Executor: Finished task 189.0 in stage 14.0 (TID 1206). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 189.0 in stage 14.0 (TID 1206) in 326 ms on localhost (187/200)
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@505999c4
15/08/06 17:45:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77158711
15/08/06 17:45:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000190_1207' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000190
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000190_1207: Committed
15/08/06 17:45:20 INFO Executor: Finished task 190.0 in stage 14.0 (TID 1207). 781 bytes result sent to driver
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000192_1209' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000192
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000192_1209: Committed
15/08/06 17:45:20 INFO Executor: Finished task 192.0 in stage 14.0 (TID 1209). 781 bytes result sent to driver
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000191_1208' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000191
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000191_1208: Committed
15/08/06 17:45:20 INFO TaskSetManager: Finished task 190.0 in stage 14.0 (TID 1207) in 341 ms on localhost (188/200)
15/08/06 17:45:20 INFO Executor: Finished task 191.0 in stage 14.0 (TID 1208). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 192.0 in stage 14.0 (TID 1209) in 318 ms on localhost (189/200)
15/08/06 17:45:20 INFO TaskSetManager: Finished task 191.0 in stage 14.0 (TID 1208) in 336 ms on localhost (190/200)
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000195_1212' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000195
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000195_1212: Committed
15/08/06 17:45:20 INFO Executor: Finished task 195.0 in stage 14.0 (TID 1212). 781 bytes result sent to driver
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000194_1211' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000194
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000194_1211: Committed
15/08/06 17:45:20 INFO TaskSetManager: Finished task 195.0 in stage 14.0 (TID 1212) in 309 ms on localhost (191/200)
15/08/06 17:45:20 INFO Executor: Finished task 194.0 in stage 14.0 (TID 1211). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 194.0 in stage 14.0 (TID 1211) in 314 ms on localhost (192/200)
15/08/06 17:45:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7aeb3750
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000196_1213/part-00196
15/08/06 17:45:20 INFO CodecConfig: Compression set to false
15/08/06 17:45:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:20 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:20 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4f025314
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000199_1216/part-00199
15/08/06 17:45:20 INFO CodecConfig: Compression set to false
15/08/06 17:45:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:20 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:20 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ba75fb8
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000197_1214/part-00197
15/08/06 17:45:20 INFO CodecConfig: Compression set to false
15/08/06 17:45:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:20 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:20 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@297d6874
15/08/06 17:45:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@f26bb08
15/08/06 17:45:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@40dbcd6b
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/_temporary/attempt_201508061745_0014_m_000198_1215/part-00198
15/08/06 17:45:20 INFO CodecConfig: Compression set to false
15/08/06 17:45:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/06 17:45:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/06 17:45:20 INFO ParquetOutputFormat: Dictionary is on
15/08/06 17:45:20 INFO ParquetOutputFormat: Validation is off
15/08/06 17:45:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b5b27b0
15/08/06 17:45:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000199_1216' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000199
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000199_1216: Committed
15/08/06 17:45:20 INFO Executor: Finished task 199.0 in stage 14.0 (TID 1216). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 199.0 in stage 14.0 (TID 1216) in 282 ms on localhost (193/200)
15/08/06 17:45:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78941945
15/08/06 17:45:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000197_1214' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000197
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000197_1214: Committed
15/08/06 17:45:20 INFO Executor: Finished task 197.0 in stage 14.0 (TID 1214). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 197.0 in stage 14.0 (TID 1214) in 304 ms on localhost (194/200)
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000178_1195' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000178
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000178_1195: Committed
15/08/06 17:45:20 INFO Executor: Finished task 178.0 in stage 14.0 (TID 1195). 781 bytes result sent to driver
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000179_1196' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000179
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000179_1196: Committed
15/08/06 17:45:20 INFO TaskSetManager: Finished task 178.0 in stage 14.0 (TID 1195) in 801 ms on localhost (195/200)
15/08/06 17:45:20 INFO Executor: Finished task 179.0 in stage 14.0 (TID 1196). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 179.0 in stage 14.0 (TID 1196) in 797 ms on localhost (196/200)
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000198_1215' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000198
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000198_1215: Committed
15/08/06 17:45:20 INFO Executor: Finished task 198.0 in stage 14.0 (TID 1215). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 198.0 in stage 14.0 (TID 1215) in 326 ms on localhost (197/200)
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000183_1200' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000183
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000183_1200: Committed
15/08/06 17:45:20 INFO Executor: Finished task 183.0 in stage 14.0 (TID 1200). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 183.0 in stage 14.0 (TID 1200) in 582 ms on localhost (198/200)
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000193_1210' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000193
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000193_1210: Committed
15/08/06 17:45:20 INFO Executor: Finished task 193.0 in stage 14.0 (TID 1210). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 193.0 in stage 14.0 (TID 1210) in 730 ms on localhost (199/200)
15/08/06 17:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508061745_0014_m_000196_1213' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_temporary/0/task_201508061745_0014_m_000196
15/08/06 17:45:20 INFO SparkHiveWriterContainer: attempt_201508061745_0014_m_000196_1213: Committed
15/08/06 17:45:20 INFO Executor: Finished task 196.0 in stage 14.0 (TID 1213). 781 bytes result sent to driver
15/08/06 17:45:20 INFO TaskSetManager: Finished task 196.0 in stage 14.0 (TID 1213) in 694 ms on localhost (200/200)
15/08/06 17:45:20 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/08/06 17:45:20 INFO DAGScheduler: Stage 14 (runJob at InsertIntoHiveTable.scala:93) finished in 5.077 s
15/08/06 17:45:20 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@49f4b117
15/08/06 17:45:20 INFO DAGScheduler: Job 8 finished: runJob at InsertIntoHiveTable.scala:93, took 12.136605 s
15/08/06 17:45:20 INFO StatsReportListener: task runtime:(count: 200, mean: 375.730000, stdev: 207.545072, max: 1072.000000, min: 161.000000)
15/08/06 17:45:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:20 INFO StatsReportListener: 	161.0 ms	176.0 ms	189.0 ms	206.0 ms	317.0 ms	407.0 ms	705.0 ms	797.0 ms	1.1 s
15/08/06 17:45:20 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.285000, stdev: 0.902095, max: 10.000000, min: 0.000000)
15/08/06 17:45:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:20 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	10.0 ms
15/08/06 17:45:20 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/06 17:45:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:20 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/06 17:45:20 INFO StatsReportListener: task result size:(count: 200, mean: 781.000000, stdev: 0.000000, max: 781.000000, min: 781.000000)
15/08/06 17:45:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:20 INFO StatsReportListener: 	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B
15/08/06 17:45:20 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 98.259181, stdev: 3.436934, max: 99.865229, min: 51.987768)
15/08/06 17:45:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:20 INFO StatsReportListener: 	52 %	97 %	97 %	98 %	99 %	99 %	99 %	100 %	100 %
15/08/06 17:45:20 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.104623, stdev: 0.421178, max: 5.319149, min: 0.000000)
15/08/06 17:45:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:20 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 5 %
15/08/06 17:45:20 INFO StatsReportListener: other time pct: (count: 200, mean: 1.636197, stdev: 3.405262, max: 48.012232, min: 0.134771)
15/08/06 17:45:20 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:20 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 1 %	 2 %	 3 %	 3 %	48 %
15/08/06 17:45:21 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/_SUCCESS;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/_SUCCESS;Status:true
15/08/06 17:45:21 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00000;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00000;Status:true
15/08/06 17:45:21 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00001;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00001;Status:true
15/08/06 17:45:21 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00002;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00002;Status:true
15/08/06 17:45:21 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00003;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00003;Status:true
15/08/06 17:45:21 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00004;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00004;Status:true
15/08/06 17:45:21 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00005;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00005;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00006;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00006;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00007;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00007;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00008;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00008;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00009;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00009;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00010;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00010;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00011;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00011;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00012;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00012;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00013;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00013;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00014;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00014;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00015;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00015;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00016;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00016;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00017;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00017;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00018;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00018;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00019;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00019;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00020;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00020;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00021;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00021;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00022;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00022;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00023;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00023;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00024;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00024;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00025;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00025;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00026;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00026;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00027;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00027;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00028;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00028;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00029;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00029;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00030;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00030;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00031;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00031;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00032;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00032;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00033;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00033;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00034;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00034;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00035;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00035;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00036;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00036;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00037;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00037;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00038;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00038;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00039;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00039;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00040;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00040;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00041;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00041;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00042;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00042;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00043;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00043;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00044;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00044;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00045;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00045;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00046;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00046;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00047;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00047;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00048;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00048;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00049;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00049;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00050;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00050;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00051;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00051;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00052;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00052;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00053;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00053;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00054;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00054;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00055;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00055;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00056;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00056;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00057;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00057;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00058;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00058;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00059;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00059;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00060;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00060;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00061;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00061;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00062;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00062;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00063;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00063;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00064;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00064;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00065;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00065;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00066;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00066;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00067;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00067;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00068;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00068;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00069;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00069;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00070;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00070;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00071;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00071;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00072;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00072;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00073;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00073;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00074;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00074;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00075;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00075;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00076;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00076;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00077;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00077;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00078;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00078;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00079;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00079;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00080;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00080;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00081;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00081;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00082;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00082;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00083;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00083;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00084;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00084;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00085;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00085;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00086;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00086;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00087;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00087;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00088;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00088;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00089;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00089;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00090;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00090;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00091;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00091;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00092;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00092;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00093;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00093;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00094;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00094;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00095;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00095;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00096;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00096;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00097;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00097;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00098;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00098;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00099;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00099;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00100;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00100;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00101;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00101;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00102;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00102;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00103;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00103;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00104;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00104;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00105;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00105;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00106;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00106;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00107;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00107;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00108;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00108;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00109;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00109;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00110;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00110;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00111;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00111;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00112;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00112;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00113;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00113;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00114;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00114;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00115;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00115;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00116;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00116;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00117;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00117;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00118;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00118;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00119;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00119;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00120;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00120;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00121;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00121;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00122;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00122;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00123;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00123;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00124;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00124;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00125;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00125;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00126;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00126;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00127;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00127;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00128;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00128;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00129;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00129;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00130;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00130;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00131;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00131;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00132;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00132;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00133;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00133;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00134;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00134;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00135;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00135;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00136;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00136;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00137;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00137;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00138;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00138;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00139;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00139;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00140;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00140;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00141;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00141;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00142;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00142;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00143;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00143;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00144;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00144;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00145;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00145;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00146;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00146;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00147;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00147;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00148;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00148;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00149;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00149;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00150;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00150;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00151;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00151;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00152;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00152;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00153;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00153;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00154;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00154;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00155;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00155;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00156;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00156;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00157;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00157;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00158;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00158;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00159;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00159;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00160;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00160;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00161;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00161;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00162;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00162;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00163;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00163;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00164;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00164;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00165;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00165;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00166;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00166;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00167;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00167;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00168;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00168;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00169;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00169;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00170;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00170;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00171;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00171;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00172;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00172;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00173;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00173;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00174;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00174;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00175;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00175;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00176;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00176;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00177;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00177;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00178;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00178;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00179;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00179;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00180;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00180;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00181;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00181;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00182;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00182;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00183;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00183;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00184;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00184;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00185;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00185;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00186;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00186;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00187;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00187;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00188;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00188;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00189;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00189;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00190;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00190;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00191;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00191;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00192;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00192;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00193;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00193;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00194;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00194;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00195;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00195;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00196;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00196;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00197;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00197;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00198;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00198;Status:true
15/08/06 17:45:22 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-06_17-45-04_057_6156746524566066917-1/-ext-10000/part-00199;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00199;Status:true
15/08/06 17:45:22 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/06 17:45:22 INFO DAGScheduler: Got job 9 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/06 17:45:22 INFO DAGScheduler: Final stage: Stage 15(collect at SparkPlan.scala:84)
15/08/06 17:45:22 INFO DAGScheduler: Parents of final stage: List()
15/08/06 17:45:22 INFO DAGScheduler: Missing parents: List()
15/08/06 17:45:22 INFO DAGScheduler: Submitting Stage 15 (MappedRDD[82] at map at SparkPlan.scala:84), which has no missing parents
15/08/06 17:45:22 INFO MemoryStore: ensureFreeSpace(3256) called with curMem=1850799, maxMem=3333968363
15/08/06 17:45:22 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 3.2 KB, free 3.1 GB)
15/08/06 17:45:22 INFO MemoryStore: ensureFreeSpace(1958) called with curMem=1854055, maxMem=3333968363
15/08/06 17:45:22 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 1958.0 B, free 3.1 GB)
15/08/06 17:45:22 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:42907 (size: 1958.0 B, free: 3.1 GB)
15/08/06 17:45:22 INFO BlockManagerMaster: Updated info of block broadcast_20_piece0
15/08/06 17:45:22 INFO DefaultExecutionContext: Created broadcast 20 from broadcast at DAGScheduler.scala:838
15/08/06 17:45:22 INFO DAGScheduler: Submitting 1 missing tasks from Stage 15 (MappedRDD[82] at map at SparkPlan.scala:84)
15/08/06 17:45:22 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
15/08/06 17:45:22 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 1217, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/06 17:45:22 INFO Executor: Running task 0.0 in stage 15.0 (TID 1217)
15/08/06 17:45:22 INFO Executor: Finished task 0.0 in stage 15.0 (TID 1217). 618 bytes result sent to driver
15/08/06 17:45:22 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 1217) in 7 ms on localhost (1/1)
15/08/06 17:45:22 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/08/06 17:45:22 INFO DAGScheduler: Stage 15 (collect at SparkPlan.scala:84) finished in 0.007 s
15/08/06 17:45:22 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@16387618
15/08/06 17:45:22 INFO DAGScheduler: Job 9 finished: collect at SparkPlan.scala:84, took 0.022008 s
Time taken: 18.814 seconds
15/08/06 17:45:22 INFO CliDriver: Time taken: 18.814 seconds
15/08/06 17:45:22 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:45:22 INFO PerfLogger: </PERFLOG method=releaseLocks start=1438883122537 end=1438883122537 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/06 17:45:22 INFO StatsReportListener: task runtime:(count: 1, mean: 7.000000, stdev: 0.000000, max: 7.000000, min: 7.000000)
15/08/06 17:45:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:22 INFO StatsReportListener: 	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms	7.0 ms
15/08/06 17:45:22 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/06 17:45:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:22 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/06 17:45:22 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 42.857143, stdev: 0.000000, max: 42.857143, min: 42.857143)
15/08/06 17:45:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:22 INFO StatsReportListener: 	43 %	43 %	43 %	43 %	43 %	43 %	43 %	43 %	43 %
15/08/06 17:45:22 INFO StatsReportListener: other time pct: (count: 1, mean: 57.142857, stdev: 0.000000, max: 57.142857, min: 57.142857)
15/08/06 17:45:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/06 17:45:22 INFO StatsReportListener: 	57 %	57 %	57 %	57 %	57 %	57 %	57 %	57 %	57 %
15/08/06 17:45:22 INFO SparkUI: Stopped Spark web UI at http://sandbox.hortonworks.com:4040
15/08/06 17:45:22 INFO DAGScheduler: Stopping DAGScheduler
15/08/06 17:45:23 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
15/08/06 17:45:23 INFO MemoryStore: MemoryStore cleared
15/08/06 17:45:23 INFO BlockManager: BlockManager stopped
15/08/06 17:45:23 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/06 17:45:23 INFO SparkContext: Successfully stopped SparkContext
