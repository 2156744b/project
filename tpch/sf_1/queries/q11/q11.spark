-- number of partitions when shuffling data for aggregates and joins
set spark.sql.shuffle.partitions=5;

DROP TABLE q11_important_stock_spark;
DROP TABLE q11_part_tmp_spark;

-- create the target table
create table q11_important_stock_spark(ps_partkey INT, value DOUBLE) STORED AS orc;
create table q11_part_tmp_spark(ps_partkey int, part_value double) STORED AS orc;

-- the query
insert into table q11_part_tmp_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation n
        join supplier s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_spark
select ps_partkey, part_value as value
from q11_part_tmp_spark
        join (select sum(part_value) as total_value from q11_part_tmp_spark) sum_tmp
where part_value > total_value * 0.0001
order by value desc;

